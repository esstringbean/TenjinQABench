{"case_id": "98debf018d6ed82c9f7297d0e59681b0", "answer_text": "\n\nPublicly available datasets play a crucial role in evaluating type inference systems for Python, as they provide the necessary ground truth type annotations required for model training and performance benchmarking [12]. Several notable datasets have been developed and utilized in this domain.\n\n### Core Datasets [0][1][3][2][13] \n\nManyTypes4Py [0]\nOne of the primary datasets is **ManyTypes4Py**. This dataset is specifically tailored for machine-learning (ML)-based type inference in Python and contains a total of 5,382 Python projects with more than 869K type annotations [0]. The dataset is rigorously curated to remove duplicate source code files and is split into training, validation, and test sets, facilitating comprehensive and reliable model evaluation [3]. The ManyTypes4Py dataset also comes with a lightweight static analyzer pipeline, LibSA4Py, to extract type information from abstract syntax trees (ASTs), which aids in generating the required features for ML model training [1]. This dataset has been extensively used in various studies for training and evaluating type inference models given its comprehensiveness and public availability [2][3].\n\nWe have presented PYInfer, a learning-based approach to generate type annotations for Python automatically. Our main contribution is the end-to-end PYInfer framework to infer variable types for Python statically. Our research combines symbolic and probabilistic methods to generate type annotations for variables, which turns out to be highly effective and efficient [4]. One of our key insights is to consider contextual information for variables, which enhances the model by encoding additional code semantics. PYInfer is capable of handling user-defined types by formulating the type inference as a classification problem. It achieves 5.2X more on code coverage and 187X faster than a state-of-the-art technique for Python type inference, and covers 5X more variables compared to PySonar2 [4]. It outperforms TypeWriter on inferring types for variables and function arguments [4]. Finally, we propose a method of data collection and contribute a large dataset consisting of 77,089,946 type annotations from 4,577 popular Python projects [6]. We make our tool and datasets publicly available to facilitate further research [4]. \nAnother significant dataset is **PYInfer**. This dataset consists of 77,089,946 type annotations from 4,577 popular Python projects [4]. The dataset is collected using PySonar2, an advanced semantic indexer for Python, and refined through data cleaning techniques to ensure high-quality annotations [11]. PYInfer is specifically designed to support end-to-end learning-based type inference systems and has been shown to provide substantial improvements in both coverage and speed over traditional methods [8].\n\nCrossDomainTypes4Py [7]\nAdditionally, there is the **CrossDomainTypes4Py** dataset, which is used to evaluate the cross-domain generalizability of type inference systems. This dataset comprises 7,912 repositories from the scientific calculation and web domains, with a total of 1,023,383 type annotations [7]. The dataset is split into training, validation, and test sets, and is preprocessed to remove duplicate repositories and files. The CrossDomainTypes4Py dataset is particularly valuable for understanding how well type inference models generalize across different application domains and addressing the challenges associated with dataset shift [12].\n\n### Additional Datasets [0][1][3][12] \n\nType4Py [12][14]. \nThe **Type4Py** dataset, which is essentially an updated and enhanced version of ManyTypes4Py, focuses on practical deep similarity learning-based type inference for Python [13]. This dataset includes projects that depend on the mypy type checker, which increases the likelihood of having type annotations. The dataset is meticulously cleaned to eliminate noisy ground truth through type-checking with mypy, ensuring the reliability of the training and evaluation data [14].\n\n\nTypeWriter [10]\n\nOther noteworthy datasets include **TypeWriter**, which focuses on neural type prediction with search-based validation. This dataset is constructed from a large internal code repository and a public OSS corpus from GitHub, filtered for projects tagged with python3 and having mypy as a dependency [10]. This dataset supports advanced evaluation by excluding trivial types to avoid skewing results in favor of the model.\n\n### Related Datasets [0][1][2][3]\n\nManyTypes4TypeScript [5].\nLastly, the **ManyTypes4TypeScript** dataset, although primarily for TypeScript, is another comprehensive dataset for sequence-based type inference models. It includes over 9 million type annotations across 13,953 projects, providing a substantial benchmark for evaluating type inference in a language similar to Python [5].\n\n### Evaluation Metrics and Real-World Applications[0][1][3][4]\n\n\nTo accurately evaluate these models, certain key metrics and evaluation criteria are often used. Common evaluation metrics include precision, recall, and F1-score which measure the accuracy, coverage, and balance between precision and recall of the type predictions [8]. Particularly, F1-score is considered a critical metric as it helps in balancing between false positives and false negatives, offering a comprehensive perspective on the model's performance [8]. Moreover, cross-domain generalizability is examined by training on one domain and testing on another, which is essential for models that aim to perform well across varied types of codebases [9].\n\n### Applications\n\nMachine learning-based type inference can assist in type annotations for dynamically typed programming languages such as Python by leveraging large datasets such as ManyTypes4Py [0].\n\nIn recent years, dynamic programming languages (DPLs) like Python have become immensely popular due to their rapid prototyping capabilities, but they lack static typing, which can cause issues such as unexpected run-time exceptions and sub-optimal support for integrated development environments (IDEs) [3].\n\nMoreover, ML-based techniques require a sufficiently large dataset to facilitate effective training. The ManyTypes4Py dataset, featuring 5,382 Python projects with more than 869K type annotations, is designed to bolster such training by providing extensive type annotation data [0].\n\nAnother example is the PYInfer framework, which combines symbolic and probabilistic methods to generate type annotations for variables, achieving significantly higher code coverage and speed compared to state-of-the-art techniques [4]. This framework also emphasizes the importance of contextual code semantics for accurate type inference [8]. \nIn real-world scenarios, these datasets and their associated tools have far-reaching applications. For instance, they enable developers to enhance Integrated Development Environments (IDEs) with more accurate type suggestions and error detection, which can significantly improve coding efficiency and software reliability [3]. Companies can integrate these advanced type inference models into their continuous integration pipelines to automatically suggest and validate type annotations, thereby maintaining a higher standard of code quality without requiring extensive manual effort from developers. Additionally, these datasets support academic research in the field of programming languages and software engineering, fostering the development of new methodologies and tools for type inference and related tasks [0]. \n\nIn recent years, dynamic programming languages (DPLs) have become immensely popular as they give developers fast prototyping. However, DPLs lack static typing, which causes several issues such as unexpected run-time exceptions, sub-optimal support for integrated development environments (IDEs), and less precise program analysis. To address these issues, optional static typing is introduced for DPLs like Python, JavaScript, and PHP. Yet, developers are required to manually add type annotations to their existing codebases, which is a laborious task. To ease the type annotation burden, researchers have recently employed machine learning (ML) techniques to infer types for DPLs. ML techniques need a sufficiently large dataset to achieve an acceptable level of generalization for the task at hand. Concerning the ML-based type inference for DPLs, it is difficult to create a benchmark dataset that contains software projects with a sufficient number of type annotations. Because of the optional static typing, many software projects written in DPLs lack type annotations. Nevertheless, to train an ML-based type inference model for Python, researchers created their own dataset by either gathering a small set of projects with type annotations or employ static type inference tools to add type annotations to existing projects. We believe that there is a need for a large benchmark dataset that facilitates training ML-based type inference models, especially for Python. Unlike TypeScript's compiler, the Python interpreter cannot infer the type of variables or function signatures at compile time. Motivated by this, we present the ManyTypes4Py dataset, a large dataset to train ML models for predicting type annotations in Python. Currently, we are working on the Type4Py model, which is trained on the earlier version of the ManyTypes4Py dataset [2].\nIn summary, the datasets commonly used for evaluating type inference systems in Python, such as ManyTypes4Py [0], PYInfer [4], CrossDomainTypes4Py [7], Type4Py [13], and TypeWriter [10], have been designed to provide comprehensive and high-quality type annotations. These datasets facilitate the development of robust and accurate type inference models by addressing various challenges such as dataset shift [7], cross-domain generalizability [7], and the necessity for precise ground truth data [14]. Exploring datasets from similar languages like ManyTypes4TypeScript [5] can offer valuable insights and benchmarks. Combined with precise evaluation metrics, these datasets enable researchers to develop and fine-tune type inference systems that are not only accurate but also robust across different domains and coding scenarios, underscoring their importance in driving forward the field of automated type inference.\n\n\nReferences:\n[0] ManyTypes4Py: A Benchmark Python Dataset for Machine Learning-based Type Inference In this paper, we present ManyTypes4Py, a large Python dataset for machine learning (ML)-based type inference. The dataset contains a total of 5,382 Python projects with more than 869K type annotations. Duplicate source code files were removed to eliminate the negative effect of the duplication bias. To facilitate training and evaluation of ML models, the dataset was split into training, validation and test sets by files. To extract type information from abstract syntax trees (ASTs), a light-weight static analyzer pipeline is developed and accompanied with the dataset. Using this pipeline, the collected Python projects were analyzed and the results of the AST analysis were stored in JSON-formatted files. The ManyTypes4Py dataset is shared on zenodo and its tools are publicly available on GitHub.\n\n[1] ManyTypes4Py: A Benchmark Python Dataset for Machine Learning-based Type Inference In summary, the paper has the following contributions:•ManyTypes4Py dataset, which features 5,382 Python projects with more than 869K type annotations. The latest version of the dataset can be downloaded on zenodo111https://zenodo.org/record/4479714.•LibSA4Py tool, a light-weight static analyzer pipeline to process Python projects and extract type hints/features for training ML-based type inference models. The tool is publicly available on a GitHub repository222https://github.com/saltudelft/libsa4py.\n\n[2] ManyTypes4Py: A Benchmark Python Dataset for Machine Learning-based Type Inference hand. Concerning the ML-based type inference for DPLs, it is difficult to create a benchmark dataset that contains software projects with a sufficient number of type annotations. Because of the optional static typing, many software projects written in DPLs lack type annotations. Nevertheless, to train an MLbased type inference model for Python, researchers created their own dataset by either gathering a small set of projects with type annotations or employ static type inference tools to add type annotations to existing projects. We believe that there is a need for a large benchmark dataset that facilitates training ML-based type inference models, especially for Python. Unlike TypeScript's compiler, the Python interpreter cannot infer the type of variables or function signatures at compile time. Motivated by this, we present the ManyTypes4Py dataset, a large dataset to train ML models for predicting type annotations in Python. Currently, we are working on the Type4Py model, which is trained on the earlier version of the ManyTypes4Py dataset. The experimental results show that the model trained on our dataset is overall more accurate when compared to the same model trained on a smaller dataset. In summary, the paper has the following contributions: • ManyTypes4Py dataset, which features 5,382 Python projects with more than 869K type annotations. The latest version of the dataset can be downloaded on zenodo 1 . • LibSA4Py tool, a light-weight static analyzer pipeline to process Python projects and extract type hints/features for training ML-based type inference models. The tool is publicly available on\n\n[3] ManyTypes4Py: A Benchmark Python Dataset for Machine Learning-based Type Inference ManyTypes4Py: A Benchmark Python Dataset for Machine Learning-based Type Inference In this paper, we present ManyTypes4Py, a large Python dataset for machine learning (ML)-based type inference. The dataset contains a total of 5,382 Python projects with more than 869K type annotations. Duplicate source code files were removed to eliminate the negative effect of the duplication bias. To facilitate training and evaluation of ML models, the dataset was split into training, validation and test sets by files. To extract type information from abstract syntax trees (ASTs), a lightweight static analyzer pipeline is developed and accompanied with the dataset. Using this pipeline, the collected Python projects were analyzed and the results of the AST analysis were stored in JSON-formatted files. The ManyTypes4Py dataset is shared on zenodo and its tools are publicly available on GitHub. I. INTRODUCTION In recent years, dynamic programming languages (DPLs) have become immensely popular as they give developers fast prototyping. However, DPLs lack static typing, which causes several issues such as unexpected run-time exceptions, sub-optimal support for integrated development environments (IDEs), and less precise program analysis. To address these issues, optional static typing is introduced for DPLs like Python, JavaScript, and PHP. Yet, developers are required to manually add type annotations to their existing codebases, which is a laborious task. To ease the type annotation burden, researchers have recently employed machine learning (ML) techniques to infer types for DPLs-. ML techniques need a sufficiently large dataset to achieve an acceptable level of generalization for the task at\n\n[4] PYInfer: Deep Learning Semantic Type Inference for Python Variables We have presented PYInfer, a learning-based approach to generate typeannotations for Python automatically.Our main contribution is the end-to-end PYInfer framework to infer variable types for Python statically.Our research combines symbolic and probabilistic methods to generatetype annotations for variables, which turns out to be highly effective and efficient.One of our key insights is to considercontextual information for variables, which enhances the modelby encoding additional code semantics. PYInfer is capable ofhandling user-defined types by formulating the type inference as aclassification problem. It achieves 5.2X more on code coverage and187X faster than a state-of-the-art technique for Python type inference, andcovers 5X more variables compared to PySonar2.It outperforms TypeWriter on inferring types for variables and function arguments.Finally, we propose a method of data collection and contribute a large datasetconsisting of 77,089,946 type annotations from 4,577 popular Python projects. Wemake our tool and datasets publicly available to facilitate further research.\n\n[5] ManyTypes4TypeScript: A Comprehensive TypeScript Dataset for Sequence-Based Type Inference ManyTypes4TypeScript: A Comprehensive TypeScript Dataset for Sequence-Based Type Inference In this paper, we present ManyTypes4TypeScript, a very large corpus for training and evaluating machine-learning models for sequence-based type inference in TypeScript. The dataset includes over 9 million type annotations, across 13,953 projects and 539,571 files. The dataset is approximately 10x larger than analogous type inference datasets for Python, and is the largest available for Type-Script. We also provide API access to the dataset, which can be integrated into any tokenizer and used with any state-of-the-art sequence-based model. Finally, we provide analysis and performance results for state-of-the-art code-specific models, for baselining. ManyTypes4TypeScript is available on Huggingface, Zenodo, and CodeXGLUE.\n\n[6] PYInfer: Deep Learning Semantic Type Inference for Python Variables information, which is different from graph-based code structure with AST and graph neural networks. VII. CONCLUSIONS We have presented PYInfer, a learning-based approach to generate type annotations for Python automatically. Our main contribution is the end-to-end PYInfer framework to infer variable types for Python statically. Our research combines symbolic and probabilistic methods to generate type annotations for variables, which turns out to be highly effective and efficient. One of our key insights is to consider contextual information for variables, which enhances the model by encoding additional code semantics. PYInfer is capable of handling user-defined types by formulating the type inference as a classification problem. It achieves 5.2X more on code coverage and 187X faster than a state-of-the-art technique for Python type inference, and covers 5X more variables compared to PySonar2. It outperforms TypeWriter on inferring types for variables and function arguments. Finally, we propose a method of data collection and contribute a large dataset consisting of 77,089,946 type annotations from 4,577 popular Python projects. We make our tool and datasets publicly available to facilitate further research.\n\n[7] Cross-Domain Evaluation of a Deep Learning-Based Type Inference System methods. In addition, we study the out-of-vocabulary-words problem, which is a general issue when using source code as input for machine learning systems. In summary, we contribute the following: CrossDomainTypes4Py Dataset Our dataset is publicly available 1 and contains 7,912 repositories from the scientific calculation and the web domain with 682,354 and 341,029 type annotations, respectively. For the preprocessed version of the data, we removed duplicate repositories and files. We split the remaining data into training, validation & test, extract relevant information and prepare them as input for the Type4Py system (see Section IV-C). Cross-domain Experiments with Type4Py We perform extensive cross-domain experiments with the state-of-the-art type inference system Type4Py and provide a detailed evaluation (see Section V). We investigate how well the system can generalize across domains, which problems occur, what has to be considered, and possible ways to mitigate these issues. In order to ensure the reproducibility of our experiments, we make our mining and preprocessing scripts to create the dataset avaiable 2 , as well as our experimental pipeline. Furthermore, we provide a repository list of our dataset and a ready-to-use version of the preprocessed data. The paper is structured as described in the following paragraph. In Section II, a literature review on deep learningbased type inference systems and existing datasets is given. This is followed by Section III, which covers the basics of dataset shift and the Type4Py method. Section IV describes the creation of the dataset including the preprocessing steps. We use Section V to present our research questions, evaluate\n\n[8] PYInfer: Deep Learning Semantic Type Inference for Python Variables Python type inference is challenging in practice. Due to its dynamic properties and extensive dependencies on third-party libraries without type annotations, the performance of traditional static analysis techniques is limited. Although semantics in source code can help manifest intended usage for variables (thus help infer types), they are usually ignored by existing tools. In this paper, we propose PYInfer, an end-to-end learning-based type inference tool that automatically generates type annotations for Python variables. The key insight is that contextual code semantics is critical in inferring the type for a variable. For each use of a variable, we collect a few tokens within its contextual scope, and design a neural network to predict its type. One challenge is that it is difficult to collect a high-quality human-labeled training dataset for this purpose. To address this issue, we apply an existing static analyzer to generate the ground truth for variables in source code. Our main contribution is a novel approach to statically infer variable types effectively and efficiently. Formulating the type inference as a classification problem, we can handle user-defined types and predict type probabilities for each variable. Our model achieves 91.2% accuracy on classifying 11 basic types in Python and 81.2% accuracy on classifying 500 most common types. Our results substantially outperform the state-of-the-art type annotators. Moreover, PYInfer achieves 5.2X more code coverage and is 187X faster than a state-of-the-art learning-based tool. With similar time consumption, our model annotates 5X more variables than a state-of-the-art static analysis tool. Our model also outperforms a learning-based function-level annotator on annotating types for variables and function arguments. All our tools and datasets are publicly available to facilitate future research in this direction.\n\n[9] Cross-Domain Evaluation of a Deep Learning-Based Type Inference System across the sets, which increased the F1-score in our experiment by 7 percentage points. While mining our CrossDomainTypes4Py dataset, we want to increase the number of repositories, which contain type annotations by searching for projects that depend on the type checker Mypy. This biases the sampling of the repositories, but is an approved method used by ManyTypes4Py and TypeWriter. In our preprocessing pipeline, we want to maintain comparability to the ManyTypes4Py approach, that is why we use the LibSA4Py library for information extraction. It is restricted by its parsing module, which can only handle Python 3, but not the older version Python 2. VII. CONCLUSION We perform the first study of cross-domain generalizability in the field of type inference. We enable this by our publicly available CrossDomainTypes4Py dataset, which consists of two subsets from the two domains web and scientific calculation with in total over 1,000,000 type annotations. We gain new insights by conducting extensive cross-domain experiments in various setups. For instance, we observe that the system performs significantly worse when evaluating on another than the training domain. This is due to the differences between the domains and datasets. We discover that they do not share the same data types and also that the features differ between the domains, which lowers the accuracy of the system. In our investigations, we also showed that a large number of out-of-vocabulary words and the domain-specific visible type hints have no significant impact on the results of the system. Moreover, due to the long-tailed distribution of the dataset,\n\n[10] TypeWriter: neural type prediction with search-based validation replicable, we use two datasets: Internal code base We collect Python from a large internal code repository. OSS corpus We search GitHub for all projects tagged as python3. We also search Libraries.io for all Python projects that include mypy as a dependency. We then remove all projects that have less than 50 stars on GitHub, to ensure that the included projects are of substantial public interest. To ease future work to compare with TypeWriter, all results for the OSS corpus are available for download. 3 The resulting dataset statistics can be found in Table 1. The internal dataset is much larger in size, but both datasets are comparable in terms of the percentage of annotated code. By restricting the type vocabulary to a fixed size, we exclude around 10% of all type occurrences for both datasets. This percentage is similar for both datasets, despite their different sizes, because types follow a longtail distribution, i.e., relatively few types account for the majority of all type occurrences. We ignore some types because they are trivial to predict, such as the return type of __str__, which always is str, or the type of the self argument of a method, which always is the surrounding class. TypeWriter could easily predict many of these trivial types, but a simple syntactic analysis would also be sufficient. We ignore trivial types for the evaluation to avoid skewing the results in favor of TypeWriter. Figure 4 shows examples of successful and unsuccessful type predictions in the OSS dataset. Example 1 presents a case where\n\n[11] PYInfer: Deep Learning Semantic Type Inference for Python Variables We collect the typePY dataset by annotating Python source code in top-star GitHub repositories.For each variable, we store the link to the repository,file name, variable name, start and end token location, type annotation, and corresponding source code.To obtain type annotations, we adopt the PySonar2, an advanced semantic indexerfor Python , to infer types for each variable.We obtain 77,089,946 annotations from 320,402 Python source code files.As PySonar2 has only 49.47% coverage on annotations ,we also refine our dataset with a series of data cleaning techniques.We eliminate all meaningless types, such as “question marks” and “None” types,and perform deduplication.Eventually, we obtain 42,560,876 valid annotations.To the best of our knowledge,this is the largest Python annotation dataset. We release this dataset to facilitate research on this topic.\n\n[12] Cross-Domain Evaluation of a Deep Learning-Based Type Inference System knowledge, no previous study conducts a cross-domain evaluation of deep learning-based type inference systems and investigates the corresponding problems. We choose the Type4Py approach for our investigations because its source code is available and according to the evaluation by Mir et al., it outperforms other state-ofthe-art deep learning-based type inference systems. B. Datasets There are already some extensive Python corpora-. However, these were not created specifically for type inference and thus no focus was placed on whether the projects had type annotations. These are needed as ground truth data for supervised learning and evaluating the systems. Many projects do not have type annotations and are therefore unsuitable for this task. The authors of machine learning-based type inference methods for Python usually present their own datasets. These datasets have the following downsides: only partly publicly available, not very comprehensive, and partly designed for special preprocessing steps,,. An example that does not come with these downsides is the large and publicly available ManyTypes4Py dataset. However, for a cross-domain evaluation, several datasets containing various domains are required. Therefore, we present Cross-DomainTypes4Py with two subsets from different domains. III. THEORETICAL BACKGROUND In the first section, we explain the dataset shift and its important sub-types. In the following part, we describe the structure and operation of the Type4Py system. A. Dataset Shift Dataset shift is an important topic in machine learning since many real-world applications are affected by shifts and this harms the performance of the systems,. For a\n\n[13] Type4Py: Practical Deep Similarity Learning-Based Type Inference for Python For this work, we have created a new version of our ManyTypes4Py dataset (Miret al., 2021), i.e., v0.7. The rest of this section describes the creation of the dataset. To find Python projects with type annotations, on Libraries.io, we searched for projects that depend on the mypy package ([n. d., [n.d.e), i.e., the official and most popular type checker for Python. Intuitively, these projects are more likely to have type annotations. The search resulted in 5.2K Python projects that are available on GitHub. Initially, the dataset has 685K source files and 869K type annotations.\n\n[14] Type4Py: Practical Deep Similarity Learning-Based Type Inference for Python Recent studies show that developer-provided types rarely type-check and Python projects may contain type-related defects (Oreet al., 2018; Rak-amnouykit et al., 2020; Khanet al., 2021). Therefore, we believe that it is essential to type-check the dataset to eliminate noisy ground truth (i.e. incorrect type annotations). Not only noisy ground truth can be considered a threat to the validity of results but also it may make the discrimination of types in type clusters more difficult (Garciaet al., 2015). To clean the dataset from noisy ground truth, we perform basic analysis as follows:•First, we use mypy to type-check 288,760 source files in the dataset. Of which, 184,752 source files are successfully type-checked.•Considering the remaining 104,008 source files, for further analysis, we ignore source files that cannot be type-checked further by mypy due to the syntax error or other fatal exceptions. This amounts to 63,735 source files in the dataset.•Given 40,273 source files with type errors, we remove one type annotation at a time from a file and run mypy. If it type-checks, we include the file. Otherwise, we continue this step up to 10 times. This basic analysis fixes 16,861 source files with type errors, i.e, 42% of the given set of files.\n\n"}
{"case_id": "7b42d39d9f8fdb3a8ec1dd783d66d6c2", "answer_text": "\n\nLeading methods for generating hard examples of the Boolean Satisfiability (SAT) problem vary significantly in their approaches, strengths, and weaknesses. These methods are critical for advancing the development of robust SAT solvers, as generating challenging instances facilitates better training and evaluation of these algorithms [4][6].\n\nOne prominent method is the Monte Carlo method, which relies on probabilistic experiments to estimate expected values and generate difficult instances [2]. This method's strength lies in its ability to provide statistical guarantees about the hardness of the generated instances, given a sufficiently large number of observations. However, its main weakness is the potential computational expense, as it requires numerous iterations to produce statistically valid results, which can be time-consuming and resource-intensive. Additionally, the randomness inherent in this method can sometimes lead to generating instances that are not uniformly difficult, thereby making it inconsistent for certain applications.\n\nAnother effective approach is based on the phase transition phenomenon observed in random SAT instances [4][6][8]. The Balanced SAT algorithm, for example, uses the cluster coefficient graph statistic to generate k-SAT instances with a high difficulty level [6]. This algorithm excels in creating hard instances that are computationally challenging for solvers, without making the instances prohibitively large. However, its dependency on specific parameters (such as the clustering coefficient) may limit its applicability across different types of SAT problems. For instance, small changes in the clustering coefficient can significantly alter the difficulty of the generated instances. Fine-tuning these parameters requires domain-specific knowledge and understanding of SAT problem characteristics, making it a complex and sometimes cumbersome process.\n\nThe No-Triangle SAT method is another significant technique, which generates hard SAT instances by preventing the formation of triangles in the corresponding graph representation. This approach has been shown to create instances that are particularly difficult for solvers due to the increased complexity and reduced redundancy in the problem structure [6]. Its primary strength is its ability to produce consistently hard instances for various parameter values. However, it shares the limitation of requiring careful tuning of parameters to maintain the desired difficulty level. For instance, the absence of triangles must be precisely controlled to ensure maximum complexity; otherwise, the instances may vary in hardness. Additionally, the generation process can be computationally intensive, which may not be suitable for scenarios requiring rapid instance creation.\n\nMachine learning-based methods have also made significant strides in generating hard SAT instances. Techniques like QuerySAT develop neural SAT solvers that are trained to generate difficult instances in an unsupervised fashion [9]. These methods leverage the ability of neural networks to learn intricate patterns and structures in the data, producing instances that can challenge even the most advanced solvers. The advantage of these methods is their adaptability and potential for continuous improvement as more data is fed into the learning process. However, they often require substantial training data and computational power, which can be a barrier to their widespread adoption. There are also risks of overfitting to specific types of instances, which can limit the generalizability of the generated instances. For example, the effectiveness of QuerySAT can heavily depend on the diversity and quality of the training data, which if biased, might result in less effective generalization across different SAT problems.\n\nOne notable example of machine learning-based methods includes G2SAT, which merges nodes in bipartite graphs from a forest to generate pseudo-industrial instances [11]. These instances follow certain structural characteristics and mimic real-world problems, which can be more challenging for SAT solvers than random instances typically used in competitions. Nonetheless, the reliance on accurately mimicking industrial characteristics means that errors in the structural assumptions can lead to suboptimal instance generation, demonstrating the sensitivity to initial parameter settings and underlying assumptions.\n\nHybrid approaches that combine quantum computing techniques with classical methods are also emerging as promising avenues for generating hard SAT instances. For instance, the Quantum Approximate Optimization Algorithm (QAOA) has been applied to SAT problems, showing potential for outperforming classical algorithms in some cases [10]. These methods offer the possibility of leveraging quantum speedups to tackle the inherent complexity of SAT problems. Currently, QAOA can match and sometimes surpass the performance of leading classical solvers like WalkSATlm, especially with an increasing number of ansatz layers [10]. However, the practical implementation of these techniques is still in the early stages, and their effectiveness is often limited by the current state of quantum hardware. There are significant challenges in scaling these methods for larger instances due to quantum decoherence and other hardware-related issues. Moreover, the parameter settings are critical for adjusting to current hardware limitations, necessitating careful calibration to optimize performance.\n\nIt is important to mention that new methods continually emerge, such as the one using the Chinese Remainder Theorem for transforming instances of the integer factorization problem into SAT instances [12]. This method has been shown to produce instances that are as hard as factorizing 100-bit integers, providing an alternative source of difficult SAT problems. However, this approach is highly specialized and might not be applicable across a wide range of SAT problems.\n\nIn summary, methods for generating hard SAT instances include probabilistic approaches like the Monte Carlo method, phase transition-based techniques such as the Balanced SAT and No-Triangle SAT algorithms, machine learning-based solvers like QuerySAT and G2SAT, and hybrid quantum-classical methods like QAOA. Each of these methods has its strengths and weaknesses, from statistical robustness and complexity management to adaptability and computational requirements [2][6][9][10]. The choice of method often depends on the specific requirements of the problem at hand and the available computational resources, as well as the sensitivity of the parameters influencing the hardness of generated instances.\n\n\nReferences:\n[0] Using Monte Carlo Method for Searching Partitionings of Hard Variants of Boolean Satisfiability Problem To estimate the expected value E​[ξC,A​(X~)Edelimited-[subscript𝜉𝐶𝐴~𝑋\\mathrm{E}\\left[\\xi_{C,A}\\left(\\tilde{X}\\right)\\right we will use the Monte Carlo method. According to this method, a probabilistic experiment that consists of N𝑁N independent observations of values of an arbitrary random variable ξ𝜉\\xi is used to approximately calculate E​[ξEdelimited-[𝜉\\mathrm{E}\\left[\\xi\\right. Let ζ1,…,ζNsuperscript𝜁1…superscript𝜁𝑁\\zeta^{1},\\ldots,\\zeta^{N} be results of the corresponding observations. They can be considered as a single observation of N𝑁N independent random variables with the same distribution as ξ𝜉\\xi.If E​[ξEdelimited-[𝜉\\mathrm{E}\\left[\\xi\\right and Var​(ξ)Var𝜉\\mathrm{Var}\\left(\\xi\\right) are both finite then from the Central Limit Theorem we have the main formula of the Monte Carlo methodPr​{|1N⋅∑j=1Nζj−E​[ξ|<δγ⋅σN}=γ.Pr⋅1𝑁superscriptsubscript𝑗1𝑁superscript𝜁𝑗Edelimited-[𝜉⋅subscript𝛿𝛾𝜎𝑁𝛾\\mathrm{Pr}\\left\\{\\left|\\frac{1}{N}\\cdot\\sum\\limits_{j=1}^{N}\\zeta^{j}-\\mathrm{E}\\left[\\xi\\right\\right|<\\frac{\\delta_{\\gamma}\\cdot\\sigma}{\\sqrt{N}}\\right\\}=\\gamma.(3)Here σ=V​a​r​(ξ)𝜎𝑉𝑎𝑟𝜉\\sigma=\\sqrt{Var\\left(\\xi\\right)} stands for a standard deviation, γ𝛾\\gamma – for a confidence level, γ=Φ​(δγ)𝛾Φsubscript𝛿𝛾\\gamma=\\Phi\\left(\\delta_{\\gamma}\\right), where Φ​(⋅)Φ⋅\\Phi\\left(\\cdot\\right) is the normal cumulative distribution function. It means that under the considered assumptions the value1N⋅∑j=1Nζj⋅1𝑁superscriptsubscript𝑗1𝑁superscript𝜁𝑗\\frac{1}{N}\\cdot\\sum\\limits_{j=1}^{N}\\zeta^{j}is a good approximation of E​[ξEdelimited-[𝜉\\mathrm{E}\\left[\\xi\\right, when the number of observations N𝑁N is large enough.\n\n[1] New Models for Generating Hard Random Boolean Formulas and Disjunctive Logic Programs New Models for Generating Hard Random Boolean Formulas and Disjunctive Logic Programs We propose two models of random quantified boolean formulas and their natural random disjunctive logic program counterparts. The models extend the standard models of random k-CNF formulas and the Chen-Interian model of random 2QBFs. The first model controls the generation of programs and QSAT formulas by imposing a specific structure on rules and clauses, respectively. The second model is based on a family of QSAT formulas in a non-clausal form. We provide theoretical bounds for the phase transition region in our models, and show experimentally the presence of the easy-hard-easy pattern and its alignment with the location of the phase transition. We show that boolean formulas and logic programs from our models are significantly harder than those obtained from the standard k-CNF and Chen-Interian models, and that their combination yields formulas and programs that are\"super-hard\"to evaluate. We also provide evidence suggesting that formulas from one of our models are well suited for assessing solvers tuned to real-world instances. Finally, it is noteworthy that, to the best of our knowledge, our models and results on random disjunctive logic programs are the first of their kind. Introduction Models for generating random instances of search problems have received much attention from the artificial intelligence community in the last twenty years. The results obtained for boolean satisfiability (SAT),39 and constraint satisfaction (CP) have had a major impact on the development of fast and robust solvers, significantly expanding their range of effectiveness as general purpose tools for\n\n[2] Generating Difficult SAT Instances by Preventing Triangles Generating Difficult SAT Instances by Preventing Triangles When creating benchmarks for SAT solvers, we need SAT instances that are easy to build but hard to solve. A recent development in the search for such methods has led to the Balanced SAT algorithm, which can create k-SAT instances with m clauses of high difficulty, for arbitrary k and m. In this paper we introduce the No-Triangle SAT algorithm, a SAT instance generator based on the cluster coefficient graph statistic. We empirically compare the two algorithms by fixing the arity and the number of variables, but varying the number of clauses. The hardest instances that we find are produced by No-Triangle SAT. Furthermore, difficult instances from No-Triangle SAT have a different number of clauses than difficult instances from Balanced SAT, potentially allowing a combination of the two methods to find hard SAT instances for a larger array of parameters. Introduction The Boolean Satisfiability Problem, commonly called SAT, is the problem of assigning values to a set of variables while satisfying a set of disjunctive clauses, each consisting of literals representing either a variable or its negation. Stateof-the-art SAT solvers are commonly tested on benchmarks of various sizes and complexities. Creating these benchmarks requires the construction of many instances that present a challenge to the solvers. Ideally this challenge should not come exclusively from a prohibitively high total number of literals. Indeed, at least some instances of moderate and even small sizes should be considered hard to solve. For this reason, and more generally to help advance the understanding\n\n[3] Machine Learning Methods in Solving the Boolean Satisfiability Problem Stochastic local search (SLS) algorithms are effective for solving random and hard combinatorial instances, a typical example of which is WalkSAT Selman et al. (1996), as shown in Algorithm 2. As an incomplete solver, it starts from an initial variable assignment and flips the value of a selected variable at each iteration, until a legal assignment is found or the time limit is exceeded. To avoid getting trapped in the local minima, stochastic restarts are performed during the search if the restart criterion is met. The key heuristics involved in an SLS solver is the restart policy, initialization scheme, and variable selection for flipping. For example, GSAT Selman et al. (1992) does not restart and chooses the variable that minimizes the number of unsatisfied clauses after flipping. Sec. 3.3 discusses how to create new heuristics for SLS solvers with machine learning techniques.\n\n[4] Machine Learning Methods in Solving the Boolean Satisfiability Problem QuerySAT Ozolins et al. (2021) develops a recurrent neural SAT solver that is trained in an unsupervised fashion. By relaxing the variables to continuous values,101,1, the unsupervised loss ℒϕ​(x)subscriptℒitalic-ϕ𝑥\\mathcal{L}_{\\phi}(x) for a formula ϕitalic-ϕ\\phi is defined asVc​(x)=1−∏i∈c+(1−xi)​∏i∈c−xi,ℒϕ​(x)=∏c∈ϕVc​(x),formulae-sequencesubscript𝑉𝑐𝑥1subscriptproduct𝑖superscript𝑐1subscript𝑥𝑖subscriptproduct𝑖superscript𝑐subscript𝑥𝑖subscriptℒitalic-ϕ𝑥subscriptproduct𝑐italic-ϕsubscript𝑉𝑐𝑥V_{c}(x)=1-\\prod_{i\\in c^{+}}\\left(1-x_{i}\\right)\\prod_{i\\in c^{-}}x_{i},\\ \\mathcal{L}_{\\phi}(x)=\\prod_{c\\in\\phi}V_{c}(x),(1)where xisubscript𝑥𝑖x_{i} is the value of the i𝑖i-th variable and c+superscript𝑐c^{+} gives the set of variables that occur in the clause c𝑐c in the positive form and c−superscript𝑐c^{-} in the negated form. The authors proved that this loss function is sufficient to uniquely identify the SAT formula ϕitalic-ϕ\\phi.Different from Selsam et al. (2019), this loss function is not only used at the final layer but also calculated for each query: at every time step, QuerySAT produces a query and evaluates a loss along with its gradient w.r.t. the query, which are then used for updating state vectors.The model is optimized towards minimizing the sum of all losses and produces a variable assignment. For empirical validation, QuerySAT used multiple benchmarks including k𝑘k-SAT, 3-SAT, and some combinatorial problems and achieved accuracy over 90%.\n\n[5] Solving boolean satisfiability problems with the quantum approximate optimization algorithm The quantum approximate optimization algorithm (QAOA) is one of the most prominent proposed applications for near-term quantum computing. Here we study the ability of QAOA to solve hard constraint satisfaction problems, as opposed to optimization problems. We focus on the fundamental boolean satisfiability problem, in the form of random $k$-SAT. We develop analytic bounds on the average success probability of QAOA over random boolean formulae at the satisfiability threshold, as the number of variables $n$ goes to infinity. The bounds hold for fixed parameters and when $k$ is a power of 2. We complement these theoretical results with numerical results on the performance of QAOA for small $n$, showing that these match the limiting theoretical bounds closely. We then use these results to compare QAOA with leading classical solvers. In the case of random 8-SAT, we find that for around 14 ansatz layers, QAOA matches the scaling performance of the highest-performance classical solver we tested, WalkSATlm. For larger numbers of layers, QAOA outperforms WalkSATlm, with an ultimate level of advantage that is still to be determined. Our methods provide a framework for analysing the performance of QAOA for hard constraint satisfaction problems and finding further speedups over classical algorithms.\n\n[6] Machine Learning Methods in Solving the Boolean Satisfiability Problem Currently, there are primarily three patterns for this combination to boost SAT solving: 1) standalone SAT solvers with pure ML methods; 2) replacing some components of existing CDCL solvers with learning-directed heuristics, and 3) modifying the local search solvers with learning-aided modules.Moreover, ML techniques help remove the limitation that SAT instances are mainly from SAT competition. SAT instance generation aims to provide sufficient representative training/test samples in industrial scenarios. Different from the generation of random and combinatorial instances, generating pseudo-industrial instances requires following certain structural characteristics, as well as the problem scale. Typical examples of pseudo-industrial SAT instance generation include SATGEN Wu andRamanujan (2019) and G2SAT You et al. (2019), where SATGEN uses an unsupervised generative model that implicitly portrays the intrinsic features, and G2SAT proposes a node-merging (-splitting) algorithm to generate bipartite graphs from (to) a forest.\n\n[7] Hard instance generation for SAT Hard instance generation for SAT We propose an algorithm of generating hard instances for the Satisfying Assignment Search Problem (in short, SAT). The algorithm transforms instances of the integer factorization problem into SAT instances efficiently by using the Chinese Remainder Theorem. For example, it is possible to construct SAT instances with about 5,600 variables that is as hard as factorizing 100 bit integers. Introduction The satisfiability problem (SAT) is a central problem in various fields of computer science. Precisely speaking, we consider the following \"search problem\": For a given propositional Boolean formula, find an assignment of values to the propositional variables so that the formula evaluates to true. This paper investigates the way of generating hard SAT instances. (In this paper, we consider only \"positive\" instances, namely, satisfiable Boolean formulas. Also we consider only conjunctive formulas; a formula may be a k-conjunctive normal form formula, i.e., a conjunction of disjunctions of k (or less) literals, or it may be an k-extended conjunctive form formula, i.e., a conjunction of finite functions on k (or less) variables. ) While it has been known that SAT is NP-hard, we do not know 1 so much about its 1 There have been quite a lot investigations for solving SAT, and we have made important observations on the hardness of SAT (see, e.g., [Joh96) Nevertheless, our knowledge is far from satisfiable one. concrete hardness. This contrasts to the factorization problem (FACT), i.e., the problem of computing the prime factorization of a given number. While we do not know whether FACT is\n\n"}
{"case_id": "a006865cdd1db1cfc34bfbec067d47e4", "answer_text": "\n\nUnique k-SAT is a specialized version of the k-SAT problem, where the given Boolean formula has either one unique satisfying assignment or none at all [0]. The significance of Unique k-SAT lies in its theoretical implications for understanding the broader complexity of the satisfiability problem (SAT), a cornerstone problem in computational complexity theory [1]. \n\nTheoretical Insights and Complexity: The relationship between the time complexities of Unique k-SAT and k-SAT is well-documented [2]. Specifically, it has been shown that if Unique k-SAT can be solved in 2^O(n) time, then k-SAT can also be solved in 2^O(n) time [2]. This implies that Unique k-SAT is as hard to solve as k-SAT, reinforcing the notion that improvements in the algorithms for one problem can directly benefit the other [2]. Additionally, the exponential complexity of Unique k-SAT and k-SAT has been demonstrated to be essentially equivalent [4]. This insight is crucial as it helps to understand that reductions and algorithmic improvements in one domain can have profound implications in the other [4].\n\n## Equivalence in Complexity to k-SAT [2] \nMultiple studies have demonstrated that Unique k-SAT is, in fact, as hard as the general k-SAT problem. Calabro et al. [0][1] established that solving Unique k-SAT is as computationally challenging as solving general k-SAT. They provided evidence by showing that the runtime complexities of algorithms designed for Unique k-SAT parallel those for k-SAT. This suggests that advances in solving Unique k-SAT would directly translate to improvements in general k-SAT solutions [2][8].\n\nRandomized Reductions and Isolation Lemmas [0].\nA key technique in understanding Unique k-SAT has been the isolation lemma, introduced by Valiant and Vazirani. This lemma probabilistically reduces a general k-CNF formula to a uniquely satisfiable one with a non-trivial but exponentially small success probability [1]. These reductions have been fundamental in proving that Unique k-SAT is at least as hard as k-SAT under randomized reductions, helping to map the complexity landscape of these problems [2][10].\n\nSuper Strong Exponential Time Hypothesis (Super SETH) [3]. \nSuper SETH posits that there are no algorithms that can solve k-SAT in sub-exponential time in the worst case. Studies such as those by Williams and others have extended this hypothesis to Unique k-SAT. They have shown that if Unique k-SAT can be solved in 2^(1-f(k)/k)n time for an unbounded function f, then k-SAT can be solved in 2^(1-f(k)(1-ε)/k)n time for any ε > 0, thus indicating their runtime exponents are essentially equivalent [4][5]. This implies that refuting Super SETH for Unique k-SAT would refute it for general k-SAT as well, underscoring the close ties in their complexity.\n\nAlgorithm Analysis and Improvements\nThe best-known algorithms for solving k-SAT often apply similarly to Unique k-SAT. For example, the PPSZ algorithm, designed by Paturi, Pudlak, and Zane, runs efficiently on Unique k-SAT instances with simplified analysis compared to general k-SAT instances [10]. Hertli [5] resolved an open question by showing that the PPSZ algorithm's efficiency for Unique k-SAT could indeed match its performance on general k-SAT instances. Moreover, recent advancements such as deterministic algorithms that derandomize the PPSZ algorithm [10] have further refined the understanding and capabilities in solving Unique k-SAT. These algorithms use hashing techniques to simplify the analysis and potentially offer more deterministic performance guarantees.\n\n#### Unique k-SAT in Randomized and Regular Structures [2][7]. \nThe study of Unique k-SAT extends to specific constrained forms, such as d-regular (k,s)-SAT, where each variable appears in a fixed number of clauses with balanced positive and negative occurrences. Research by Matthews and others has shown that even in such constrained scenarios, the complexity remains comparable to the general SAT problem [7][11]. Additionally, exploring random k-SAT demonstrates threshold behaviors where satisfiability shifts dramatically based on clause-to-variable ratios, a phenomenon also observed in Unique k-SAT under certain random distributions [13][14].\n\n\n\nStatement: Broader Implications and Applications:\n\n\n\n#### Theoretical Foundations [1]\n\nThe equivalence in complexity between Unique k-SAT and k-SAT has broader implications in theoretical computer science. This equivalence suggests that any super-polynomial improvement in algorithms for Unique k-SAT could translate universally to k-SAT, reinforcing the interconnectedness of NP-complete problems and compelling further exploration into isolation lemmas and randomized reductions [2].\n\n#### Applications Beyond SAT\n\nReferences:\n[2] Title: Unique kSAT is as Hard as kSAT Text: Unique kSAT is as Hard as kSAT In this work we show that Unique k-SAT is as Hard as k-SAT for every k ∈ N. This settles a conjecture by Calabro, Impagliazzo, Kabanets and Paturi [CIKP03. To provide an affirmative answer to this conjecture, we develop a randomness optimal construction of Isolation Lemma(see Valiant and Vazirani [VV85) for k-SAT, using Ω (log s) number of random bits, to isolate an element of a family of size O (2s). It shown by Chari, Rohatgi and Srinivasan in [CRS93, that Ω (log s) number of random bits is the lower-bound of randomness complexity for Isolation Lemma.\n\n[9] Title: Theoretical analysis of optimization problems - Some properties of random k-SAT and k-XORSAT Text: theoretical relevance; many problems, from theorem proving procedures in propositional logic (the original motivation for k-sat), to learning models in artificial intelligence, to inference and data analysis, can all be expressed as CNF formulae. Second, because it is directly involved in a large number of practical problems, from VLSI circuits design to cryptography, from scheduling to communication protocols, all of which actually require solving or optimizing real instances of k-sat formulae. Third, and probably most notably, because of its central role in complexity theory, which I shall discuss in the next Section. The questions of interest in the study of k-sat can be divided in two broad families: on one hand those regarding the general properties of CNF formulae and of their solutions (when they exist); on the other hand, those concerning the algorithms capable of answering the different questions one may ask (decision, optimization, . . . ); and of course, the intersection of the two (for example, proving that a certain algorithm succeeds in finding a solution under some assumptions also proves that a formula verifying those same assumptions must be sat). Also the answers that one can seek can be divided in two (or rather, their qualitative types): on one hand the results that are true in general and for any instance of k-sat (under certain conditions), and on the other hand results that are true in a probabilistic way. Let me clarify this last case with an example. Suppose one considers the ensemble of all possible k-sat formulae with given N and\n\n\nBeyond SAT-specific problems, the principles gleaned from studying Unique k-SAT are applicable to other domains requiring unique solutions [1]. For instance, in optimization problems where a unique optimal solution is sought or in cryptographic protocols where unique key derivation is critical, the insights and algorithmic strategies from Unique k-SAT research could offer significant advantages [9]. \n\nAlgorithm Efficiency and Real-World Problems[9]\nThe insights from analyzing Unique k-SAT have practical consequences in improving the efficiency of algorithms used in various applications, including hardware and software verification [4], learning models in artificial intelligence [9], cryptography [9], and VLSI design [9]. Since these fields often deal with large and complex problem spaces, understanding the efficient reduction of problems to their unique counterparts can help in creating optimized algorithms that perform better in real-world scenarios. \n\nIn conclusion, Unique k-SAT functions as both a benchmark and a lens to understand the inherent difficulty of general k-SAT. The equivalence in their complexity challenges the boundaries of current algorithmic approaches and supports the hypothesis that optimizing one problem translates to the other [0]. This theoretical synergy between Unique k-SAT and general k-SAT continues to drive significant research in computational complexity [1], creating ripple effects that extend into practical applications across various fields [9].\n\n\nReferences:\n[0] The complexity of unique k-SAT: an isolation lemma for k-CNFs The complexity of unique k-SAT: an isolation lemma for k-CNFs We provide some evidence that unique k-SAT is as hard to solve as general k-SAT, where k-SAT denotes the satisfiability problem for k-CNFs and unique k-SAT is the promise version where the given formula has 0 or 1 solutions. Namely, defining for each k/spl ges/1, s/sub k/=inf{/spl delta//spl ges/0|/spl exist/aO(2/sup /spl delta/n/)-time randomized algorithm for k-SAT} and, similarly, /spl sigma//sub k/=inf{/spl delta//spl ges/0|/spl exist/aO(2/sup /spl delta/n/)-time randomized algorithm for Unique k-SAT}, we show that lim/sub k/spl rarr//spl infin//s/sub k/=lim/sub k/spl rarr//spl infin///spl sigma//sub k/. As a corollary, we prove that, if Unique 3-SAT can be solved in time 2/sup /spl epsi/n/ for every /spl epsi/>0, then so can k-SAT for k/spl ges/3. Our main technical result is an isolation lemma for k-CNFs, which shows that a given satisfiable k-CNF can be efficiently probabilistically reduced to a uniquely satisfiable k-CNF, with nontrivial, albeit exponentially small, success probability.\n\n[1] The complexity of unique k-SAT: an isolation lemma for k-CNFs We provide some evidence that unique k-SAT is as hard to solve as general k-SAT, where k-SAT denotes the satisfiability problem for k-CNFs and unique k-SAT is the promise version where the given formula has 0 or 1 solutions. Namely, defining for each k/spl ges/1, s/sub k/=inf{/spl delta//spl ges/0|/spl exist/aO(2/sup /spl delta/n/)-time randomized algorithm for k-SAT} and, similarly, /spl sigma//sub k/=inf{/spl delta//spl ges/0|/spl exist/aO(2/sup /spl delta/n/)-time randomized algorithm for Unique k-SAT}, we show that lim/sub k/spl rarr//spl infin//s/sub k/=lim/sub k/spl rarr//spl infin///spl sigma//sub k/. As a corollary, we prove that, if Unique 3-SAT can be solved in time 2/sup /spl epsi/n/ for every /spl epsi/>0, then so can k-SAT for k/spl ges/3. Our main technical result is an isolation lemma for k-CNFs, which shows that a given satisfiable k-CNF can be efficiently probabilistically reduced to a uniquely satisfiable k-CNF, with nontrivial, albeit exponentially small, success probability.\n\n[2] Unique kSAT is as Hard as kSAT Unique kSAT is as Hard as kSAT In this work we show that Unique k-SAT is as Hard as k-SAT for every k ∈ N. This settles a conjecture by Calabro, Impagliazzo, Kabanets and Paturi [CIKP03. To provide an affirmative answer to this conjecture, we develop a randomness optimal construction of Isolation Lemma(see Valiant and Vazirani [VV85) for k-SAT, using Ω (log s) number of random bits, to isolate an element of a family of size O (2s). It shown by Chari, Rohatgi and Srinivasan in [CRS93, that Ω (log s) number of random bits is the lower-bound of randomness complexity for Isolation Lemma.\n\n[3] Results on a Super Strong Exponential Time Hypothesis k -SAT than of k -SAT. Improving we show the time complexities of Unique k -SAT and k -SAT are very tightly related: if Unique k -SAT is in n (1 − f ( k ) /k ) time for an unbounded f , then k -SAT is in 2 n (1 − f ( k )(1 − ε ) /k ) time for every ε > 0 . Thus, refuting Super Strong ETH in the unique solution case would refute Super Strong ETH in general. 1. It has also been hypothesized that k-SAT is hard to solve for randomly chosen instances near the \"critical threshold\", where the clause-to-variable ratio is 2 k ln 2 − Θ(1). We give a randomized algorithm which refutes the Super-Strong ETH for the case of random k-SAT and planted k-SAT for any clause-to-variable ratio. In particular, given any random k-SAT instance F with n variables and m clauses, our algorithm decides satisfiability for F in 2 n(1−Ω(log k)/k) time, with high probability (over the choice of the formula and the randomness of the algorithm). It turns out that a well-known algorithm from the literature on SAT algorithms does the job: the PPZ algorithm of Paturi, Pudlak, and Zane (1998). 2. The Unique k-SAT problem is the special case where there is at most one satisfying assignment. It is natural to hypothesize that the worst-case (exponential-time) complexity of Unique k-SAT is substantially less than that of k-SAT. Improving prior reductions, we show the time complexities of Unique k-SAT and k-SAT are\n\n[4] Results on a Super Strong Exponential Time Hypothesis very tightly related: if Unique k-SAT is in 2 n(1−f (k)/k) time for an unbounded f , then k-SAT is in 2 n(1−f (k)(1−ε)/k) time for every ε > 0. Thus, refuting Super Strong ETH in the unique solution case would refute Super Strong ETH in general. Introduction k-SAT is the canonical NP-complete problem for k ≥ 3: Given a Boolean formula in conjunctive normal form with clauses of width at most k, is it satisfiable? In practice, k-SAT is often cited as a \"solved problem\" (Gomes et al. 2008), due to the incredible performance of modern SAT solvers on instances arising from practice (mostly hardware and software verification). However, it is very possible that in the future, the demands and designs from practice will change significantly, leading to significantly different SAT instances. In general, the worst-case complexity of k-SAT is far from understood, in spite of tremendous effort devoted to finding faster worstcase algorithms. Because it is widely believed that P = NP, the search has been confined to super-polynomial-time algorithms. Although is trivial to obtain an algorithm running in 2 n · m O(1) time on k-SAT instances with m clauses and n variables, we cannot seem to improve the base of the exponent below 2: there are are no known algorithms for k-SAT which run in (2 − ) n · m O(1) time for a universal constant > 0, independent of k. This apparent barrier to algorithms led researchers to the following two popular hypotheses which strengthen P = NP: • Exponential\n\n[5] Results on a Super Strong Exponential Time Hypothesis to Unique k-SAT, showing that the runtime exponents of the two problems are essentially equivalent: Theorem 4. An algorithm running in 2 (1−f (k)/k)n time for Unique k-SAT (where f (k) is unbounded) implies a 2 (1−f (k)/k+O((log f (k))/k))n time algorithm for k-SAT. As mentioned earlier, the current best algorithm for k-SAT PPSZ (Paturi et al. 2005) has a much easier analysis for Unique k-SAT, and in fact it was an open question to show that its running time on general instances of k-SAT matches the running time for Unique k-SAT; this was eventually resolved by (Hertli 2014). Theorem 4 implies that, in order to obtain faster algorithms for k-SAT which break Super-Strong ETH, it is sufficient to restrict ourselves to Unique k-SAT, which might simplify the analysis as in the case of PPSZ. Conclusion The proofs of our results can be found in the SAT'19 conference version (Vyas and Williams 2019). Since that paper appeared, another algorithm for random k-SAT has been announced (based on local search) that achieves a running time of 2 n(1−O(log 2 k)/k) (Lincoln and Yedidia 2019).\n\n[6] Uniquely Satisfiable d-Regular (k,s)-SAT Instances regular (k, s)-CNF formula requires that each clause contains exactly k variables and each variable occurs in exactly s clauses. The d-regular (k, s)-CNF formula also requires that the absolute value of the difference between positive and negative occurrences of each variable is no more than a nonnegative integer d. In this paper, we investigate the existence condition of uniquely satisfiable d-regular (k, s)-SAT Instances, and present a method to construct a uniquely satisfiable d-regular (k, s)-formula. We also give a parsimonious reduction from k-CNF to d-regular (k, s)-CNF, and further explain the constrained density is not enough to describe the structural features of a CNF formula. Related Works Unique SAT is the promised version of the SAT, where a given CNF formula has 0 or 1 solution. Valiant and Vazirani in gave a randomized polynomial time reduction from SAT to Unique SAT, and showed that deciding whether a CNF formula has zero or one solution is essentially as difficult as SAT in general. Calabro et al. in proved that Unique k-SAT is no easier than k-SAT, not just for polynomial time algorithms but also super-polynomial time algorithms. They in pointed out it does not matter whether there has a promise that a formula has a solution. Matthews in studied the complexity of UNIQUE-(k, s)-SAT and proved that f (k) ≤ u(k) ≤ f (k) + 2 for k ≥ 3, where u(k) is the minimal value of s so that uniquely satisfiable (k, s)-CNF formulas exist and f (k) represents the\n\n[7] LP-branching algorithms based on biased graphs the choice b ′ = b is found. Via known lower bounds for Unique k-SAT, we can also show a similar lower bound for explicitly represented instances assuming SETH (the Strong Exponential-Time Hypothesis), 19,5. Recall that SETH is the hypothesis that for every ε > 0 there is a k ∈ N such that k-SAT cannot be solved in time O * (2 (1−ε)n ). Corollary 2. Unless SETH is false, there is no algorithm that solves Biased Graph Cleaning in time O * ((2 − ε) k ) for any ε > 0, even if the class of balanced cycles of the input graph is provided through an explicit circuit. Proof. Unique k-SAT is the promise problem where the input is a k-CNF formula F with the promise that F has at most one satisfying assignment, and the question is whether F is satisfiable. Calabro et al. show that under SETH, k-SAT and Unique k-SAT have the same asymptotic complexity. In other words, for every ε > 0 there is a k ∈ N such that if Unique k-SAT can be solved in time O * (2 (1−ε)n ), then SETH is false. Assume that Biased Graph Cleaning can be solved in time O * ((2 − ε) k ) for some ε > 0, and let q ∈ N be such that Unique q-SAT cannot be solved in O * (2 (1−ε)n ) time under SETH. Let F be a an instance of Unique q-SAT, and let k be the number of\n\n[8] Theoretical analysis of optimization problems - Some properties of random k-SAT and k-XORSAT theoretical relevance; many problems, from theorem proving procedures in propositional logic (the original motivation for k-sat), to learning models in artificial intelligence, to inference and data analysis, can all be expressed as CNF formulae. Second, because it is directly involved in a large number of practical problems, from VLSI circuits design to cryptography, from scheduling to communication protocols, all of which actually require solving or optimizing real instances of k-sat formulae. Third, and probably most notably, because of its central role in complexity theory, which I shall discuss in the next Section. The questions of interest in the study of k-sat can be divided in two broad families: on one hand those regarding the general properties of CNF formulae and of their solutions (when they exist); on the other hand, those concerning the algorithms capable of answering the different questions one may ask (decision, optimization, . . . ); and of course, the intersection of the two (for example, proving that a certain algorithm succeeds in finding a solution under some assumptions also proves that a formula verifying those same assumptions must be sat). Also the answers that one can seek can be divided in two (or rather, their qualitative types): on one hand the results that are true in general and for any instance of k-sat (under certain conditions), and on the other hand results that are true in a probabilistic way. Let me clarify this last case with an example. Suppose one considers the ensemble of all possible k-sat formulae with given N and\n\n[9] Simpler Partial Derandomization of PPSZ for $k$-SAT upper bound for General k-SAT, comparing with that for Unique k-SAT (the formula guarantees to have at most one solution). Nevertheless, Unique k-SAT is believed to be at least as hard as General k-SAT [CIKP08. In [Her14, it is shown that the bound for Unique k-SAT holds in general, making PPSZ the current fastest randomized k-SAT algorithm: 3-SAT can be solved in time 1.308 n with one-sided error. The solely known result towards derandomizing PPSZ is from [Rol05, which only works for Unique k-SAT. Their method of small sample space (cf. §16.2 in [AS16) approximates the uniform distribution using a discrete subset with polynomial size, which complicates the analysis by introducing the precision of real numbers and convergence rate. As mentioned above, the analysis of PPSZ for the General case can be much more challenging, and it is an important open question of whether this case can be derandomized even with a moderate sacrifice in the running time. In this paper, we provide a very simple deterministic algorithm that matches the upper bound of the randomized PPSZ algorithm when the formula has sub-exponential number of solutions. (The algorithm needs not to know the number of solutions in advance.) Our analysis is simpler than the original randomized version [PPSZ05. Comparing with the complicated construction of small sample space in the previous derandomization [Rol05, our proof only uses hashing. Techniques and Main Result To get a sense of how our approach works, we now sketch the PPSZ algorithm and give the high-level ideas in our derandomization, then formally\n\n[10] Uniquely Satisfiable d-Regular (k,s)-SAT Instances MiniSat, Glucose, Maple. The conflict-driven clause learning technique is an important algorithm to improve the efficiency of these SAT solver. Yet, how these solvers can be so successful has remained elusive. In order to analyze and improve SAT solvers, some random SAT models were propose. A natural measure of the solution space is the number of solutions. Unique k-SAT denotes the promise search problem of k-SAT where the number of solutions is either 0 or 1. The harder instances should have fewer solutions. But Calabro and Paturi in proved that the exponential complexity of deciding whether a k-CNF formula has a solution is the same as that of deciding whether it has exactly one solution, both when it is promised and when it is not promised that the input formula has a solution. Thus, the research of uniquely satisfiable SAT instances is a very significant work. The (k, s)-SAT denotes the family of satisfiability problems restricted to CNF formulas with exactly k distinct variables per clause and at most s occurrences of each variable. Regular (k, s)-SAT is a class of special (k, s)-SAT which each variable occurs in exactly s clauses. By some polynomial time reductions, it is discovered that some SAT problems with regular structures are NP-complete, such as (3,4)-SAT problem in and regular (3,4)-SAT problem in. Experimental results and theoretical analysis on a random k-SAT problem showed that the constrained density α of a CNF formula is an important parameter affecting the formula satisfiability and the solving\n\n[11] Super Strong ETH is False for Random k-SAT such that no 3-SAT algorithm runs in 2 αn time. • Strong Exponential Time hypothesis (SETH) [CIP09 There does not exist a constant ǫ > 0 such that for all k, k-SAT can be solved in (2 − ǫ) n time. The current best known algorithms for k-SAT all have runtime 2 n(1−O( 1 k )) . This bound is achieved by multiple paradigms, such as randomized backtracking [PPZ99,PPSZ05, local search [Sch99, and the polynomial method [CW16. Even for simpler variants such as unique-k-SAT, no faster algorithms are known. Hence it is possible that this runtime of 2 n(1−O( 1 k )) is actually optimal. This has been termed the Super-Strong ETH [Wil15. Super-SETH: Super Strong exponential time hypothesis. Super-SETH states that there are no 2 n(1−ω( 1 k )) algorithms for k-SAT. There are related questions of a) finding solutions of a random k-SAT instance where each clause is drawn uniformly and independently from the set of all possible clauses and of b) finding solutions of a planted k-SAT instance where first a random (and hidden) solution σ is sampled and then each clause is drawn uniformly and independently from the set of all possible clauses satisfying σ. Random k-SAT displays a threshold behaviour in which, for α sat = 2 k ln 2 − θ(1) and for all constants ǫ > 0, the k-SAT instances are satisfiable with high probability when m < (α sat − ǫ)n and unsatisfiable with high probability when m > (α sat + ǫ)n. Note that as far as\n\n[12] The replica symmetric phase of random constraint satisfaction problems and/or q,13,19,23,32,39. We observed that as a byproduct Theorem 2.7 yields lower bounds on the satisfiability thresholds of several problems, particularly hypergraph colouring and random k-SAT for small k, which are at least as good (and likely better) than the ones obtained in prior work,13,39. While in Section 3 we will see many examples of random CSPs that satisfy the assumptions SYM, BAL, etc., there are a few interesting ones that don't. For instance, the random k-SAT problem fails to satisfy SYM. At the same time, it is easy to prove that in random k-SAT the number of solutions is not as tightly concentrated as Theorem 2.7 shows it is in the case of problems that satisfy our assumptions. In fact, the random k-SAT partition function has multiplicative fluctuations of order exp(Ω( n)). Thus, random k-SAT is materially different. Theorems 2.6 and 2.11 can be seen as generalisations of results obtained in,25 for the stochastic block model, a planted version of the Potts model that has become a prominent benchmark for Bayesian inference,65. In the stochastic block model the Kesten-Stigum bound marks the point from where an efficient algorithm is known to solve the detection problem. But generally the Kesten-Stigum bound is strictly greater than the condensation threshold, and it has been conjectured that in the intermediate regime the detection problem can be solved in exponential but not in polynomial time. In light of Theorems 2.6 and 2.11 it would be interesting to see if the detection problem can be\n\n"}
{"case_id": "b069a1248503c4caa98dab0014d1a55a", "answer_text": "\n\nIn recommendation systems, the evaluation of methods that optimize diversity typically involves a combination of several metrics to measure both the quality and variety of recommendations [13]. These evaluations primarily focus on two key aspects: individual diversity and aggregate diversity [6]. \n\n**1. Individual Diversity:** Recommendation diversity is mainly measured in individual diversity and aggregate diversity. The individual diversity focuses on the local diversity in each recommended item list, which aims to balance user-item similarities and item-item dissimilarities (Bradley and Smyth, 2001) [5]. \n\nIndividual diversity pertains to the variety within the recommendation list provided to a single user. It is often quantified using metrics such as Intra-list Distance (ILD), which measures the average pairwise distance between items in a user's recommendation list [3]. This metric captures user-novelty, ensuring that the recommendations include diverse content that could surprise and engage the user. Evaluation methodologies commonly compute ILD using the cosine similarity between items’ category membership vectors, making sure that the items in the list are not too similar to one another [3]. \n\n **2. Aggregate Diversity:** [5] \n\nAggregate diversity, on the other hand, looks at the diversity of recommendations across all users. It is typically measured through the coverage of items recommended, which indicates the fraction of items in the entire catalog that have been recommended to at least one user [4]. A higher coverage implies a more diverse recommendation system from a global perspective. Additionally, metrics like the Gini index can be used to understand the equity in the distribution of recommendations, evaluating how evenly the recommendations are spread across different items [4].\n\n**3. Trade-off with Accuracy:** [2]\n\nOne of the primary challenges in optimizing for diversity is maintaining an acceptable level of recommendation accuracy. Many studies have highlighted the trade-off between diversity and accuracy [2][1]. For example, the work by [1] introduces a new recommendation ranking method known as \"Total Diversity Effect Ranking,\" which maintains a balance between diversity and accuracy, showing improvements in diversity quality in top-N recommendations while preserving accuracy. The Determinantal Point Process (DPP) model is another example, which uses a repulsion phenomenon to diversify recommendations while retaining accuracy. This model includes a tunable parameter that allows control over the diversity level [9]. Additionally, the submodular function-based algorithms proposed by [0] achieve a balance between diversity and accuracy through their inherent properties. Techniques like these offer practical solutions for the accuracy-diversity trade-off.\n\n**4. Evaluation Frameworks and Metrics:**[13]\n\nResearchers have developed various frameworks and metrics to provide a more holistic evaluation of diversity in recommendation systems. For instance, RADio introduces rank-aware divergence metrics to measure normative diversity, considering full distributional shifts and the user’s propensity to observe items further down the list [7][8]. This ensures that recommendations not only meet accuracy and diversity standards but also align with broader goals such as fairness and user satisfaction. Various other metric frameworks have also been proposed. For example, Binomial Diversity Framework (BinomDiv) captures genre-based diversity by simultaneously considering coverage, redundancy, and size-awareness [12]. These unified frameworks combine topical diversity and accuracy, satisfying several desired properties and theoretical axioms that ensure a balanced evaluation [13].\n\n**5. Experimental Validation:**[3]\n\nExperimental validation using diverse, real-world datasets is a critical step in evaluating new diversity-optimizing methods. Studies often employ techniques like 5-fold cross-validation to ensure the robustness and reliability of results [1]. Comparative evaluations against baseline algorithms help in demonstrating the improvements brought by the new methods. The effectiveness of new algorithms has been validated on various datasets including MovieLens-1M, Netflix, and others such as the Microsoft News Dataset, demonstrating significant improvements in diversity while maintaining high recommendation quality [0][7]. Specifically, the experiments demonstrated that algorithms designed for the new objective also perform well on sales diversity metrics, and even some intent-aware diversity metrics, thereby justifying the validity of the new composite diversity metrics [0].\n\n**6. Temporal Aspects:**[10]\n\nTemporal diversity is another nuanced aspect of evaluation, focusing on how the diversity of recommendations evolves over time. This involves examining whether users continue to receive fresh and varied recommendations, rather than repetitive suggestions. For instance, [10] showcased how Collaborative Filtering (CF) algorithms could be evaluated based on their temporal diversity by analyzing changes in user rating patterns over time and conducting user surveys to understand their preferences. Techniques have been proposed to maximize temporal diversity without excessively compromising accuracy, ensuring that user interest is maintained over prolonged interactions with the system. This involves creating recommendation systems that adapt over time to provide new and varied items, thereby preventing users from facing recommendation fatigue [10].\n\nIn summary, the evaluation of methods optimizing diversity in recommendation systems is multi-faceted, involving metrics for both individual and aggregate diversity [5], trade-offs with accuracy [1], robust experimental validation [0], and temporal considerations [10]. These comprehensive evaluations ensure that the developed methods provide meaningful, engaging, and diverse recommendations to users [2]. \n\n\nReferences:\n[0] A new system-wide diversity measure for recommendations with efficient algorithms A new system-wide diversity measure for recommendations with efficient algorithms Recommender systems often operate on item catalogs clustered by genres, and user bases that have natural clusterings into user types by demographic or psychographic attributes. Prior work on system-wide diversity has mainly focused on defining intent-aware metrics among such categories and maximizing relevance of the resulting recommendations, but has not combined the notions of diversity from the two point of views of items and users. In this work, (1) we introduce two new system-wide diversity metrics to simultaneously address the problems of diversifying the categories of items that each user sees, diversifying the types of users that each item is shown, and maintaining high recommendation quality. We model this as a subgraph selection problem on the bipartite graph of candidate recommendations between users and items. (2) In the case of disjoint item categories and user types, we show that the resulting problems can be solved exactly in polynomial time, by a reduction to a minimum cost flow problem. (3) In the case of non-disjoint categories and user types, we prove NP-completeness of the objective and present efficient approximation algorithms using the submodularity of the objective. (4) Finally, we validate the effectiveness of our algorithms on the MovieLens-1m and Netflix datasets, and show that algorithms designed for our objective also perform well on sales diversity metrics, and even some intent-aware diversity metrics. Our experimental results justify the validity of our new composite diversity metrics. Often, such recommendation systems operate on item catalogs and user bases that have\n\n[1] Enhancing Diversity-Accuracy Technique on User-Based Top-N Recommendation Algorithms Enhancing Diversity-Accuracy Technique on User-Based Top-N Recommendation Algorithms In this paper we demonstrate how each item in top-N recommendation list has an impact on total diversity of the list in recommender systems. We proposed a new recommendation ranking method, namely \"Total Diversity Effect Ranking\", based on the total diversity effect of each item. Typically, diversity and accuracy in recommender systems are conversely. The finding shows that the proposed ranking method can guarantee the improvement of diversity quality in top-N recommendations. We proposed two new hybrid ranking approaches to make them more balance and designed experiments to evaluate their performance in comparison with the standard ranking approaches. To make results of our experiments more reliable, we apply the 5-fold cross validation technique to the MovieLens dataset provided by the GroupLens Research Project. In an evaluation process, we suggest using harmonic mean to measure the quality of recommendation approaches. The findings show that the proposed approaches give better performance than the standard approaches.\n\n[2] Improving Aggregate Recommendation Diversity Using Ranking-Based Techniques Improving Aggregate Recommendation Diversity Using Ranking-Based Techniques Recommender systems are becoming increasingly important to individual users and businesses for providing personalized recommendations. However, while the majority of algorithms proposed in recommender systems literature have focused on improving recommendation accuracy (as exemplified by the recent Netflix Prize competition), other important aspects of recommendation quality, such as the diversity of recommendations, have often been overlooked. In this paper, we introduce and explore a number of item ranking techniques that can generate substantially more diverse recommendations across all users while maintaining comparable levels of recommendation accuracy. Comprehensive empirical evaluation consistently shows the diversity gains of the proposed techniques using several real-world rating data sets and different rating prediction algorithms.\n\n[3] A new system-wide diversity measure for recommendations with efficient algorithms Intra-list Distance (ILD): We define a recommendation set’s intra-list distance as the average pairwise distance among items. This is used to measure the diversity of an individual user’s recommendations and quantifies user-novelty. The distance d​i​s​t​(vk,vj)𝑑𝑖𝑠𝑡subscript𝑣𝑘subscript𝑣𝑗dist(v_{k},v_{j}) between items we consider is measured using the cosine similarity between the items’ category membership vectors. Given a list L𝐿L of recommendations, defined by item lists of length cusubscript𝑐𝑢c_{u} for user u𝑢u,the intra-list distance is defined as follows (we use L𝐿L to denote the left-side of the bipartite graph representation representing the users, and N​(ui)𝑁subscript𝑢𝑖N(u_{i}) to represent the neighbors of user uisubscript𝑢𝑖u_{i}, which are items in the right-hand side recommended to her).\n\n[4] A new system-wide diversity measure for recommendations with efficient algorithms Aggregate diversity is simply the fraction of items in the catalog which have been recommended to at least one user, and it measures coverage. The Gini index measures how inequitable the recommendation distribution is. More concretely, if the degree distribution of the items is given as a sorted list {di}i=1rsuperscriptsubscriptsubscript𝑑𝑖𝑖1𝑟\\{d_{i}\\}_{i=1}^{r}, then the Gini index is defined as follows.G=1−1r​(r+1−2​∑i=1r(r+1−i)​di∑i=1rdi)𝐺11𝑟𝑟12superscriptsubscript𝑖1𝑟𝑟1𝑖subscript𝑑𝑖superscriptsubscript𝑖1𝑟subscript𝑑𝑖G=1-\\frac{1}{r}\\left(r+1-2\\frac{\\sum_{i=1}^{r}(r+1-i)d_{i}}{\\sum_{i=1}^{r}d_{i}}\\right)\n\n[5] Improving Accuracy and Diversity in Matching of Recommendation With Diversified Preference Network Conventional recommendation models usually regard recommendation accuracy metrics such like Click-through-rate (CTR) as their central objectives, in which popular items clicked by users are more preferred.However, such objectives will lead to homogenization issues that reduce personalization and harm user experiences.To solve this issue, recommendation diversity is considered to evaluate the overall recommendation performances from another aspect (Bradley and Smyth, 2001).It is measured in two classical ways: the individual diversity and the aggregate diversity (Kunaver andPožrl, 2017). The individual diversity focuses on the local diversity in each recommended item list, which aims to balance user-item similarities and item-item dissimilarities (Chenet al., 2018). In contrast, the aggregate diversity focuses on the global diversity in the overall recommendation, which is usually measured by the coverage of items that could be recommended by models in the entire corpus (Karakaya andAytekin, 2018).The significance of diversity has been widely verified to provide highly idiosyncratic items in recommendation (Zhanget al., 2019b), which should be considered in real-world scenarios.\n\n[6] Improving Accuracy and Diversity in Matching of Recommendation With Diversified Preference Network Merely using CTR-oriented objectives will make hot items hotter, which inevitably brings in serious homogenization issues that may degrade user experiences (Zhang and Hurley, 2008).The significance of diversity has been verified by lots of efforts, since it could provide highly idiosyncratic items with less homogeneity for users in personalized recommendation (Bradley and Smyth, 2001; Zhanget al., 2019b).Recommendation diversity is mainly measured in individual diversity and aggregate diversity (Kunaver andPožrl, 2017).The individual diversity focuses on the local diversity in recommended list. (Bradley and Smyth, 2001) and (Ziegleret al., 2005) focus on intra-list item dissimilarities. (Zhang and Hurley, 2008) proposes a novel item novelty, which measures the additional information from new items. Some works measure diversity with the varieties of taxonomy in item lists (Ziegleret al., 2005).In contrast, the aggregate diversity measures the global diversity in overall systems. (Karakaya andAytekin, 2018) measures this diversity with the coverage of recommended items.The higher item coverage indicates that the model could recommend more long-tail items, which implies a more diversified system from the global aspect.\n\n[7] RADio – Rank-Aware Divergence Metrics to Measure Normative Diversity in News Recommendations In traditional recommender system literature, diversity is often seen as the opposite of similarity, and typically defined as the distance between identified topics, categories or word models. However, this is not expressive of the social science’s interpretation of diversity, which accounts for a news organization’s norms and values and which we here refer to as normative diversity. We introduce RADio, a versatile metrics framework to evaluate recommendations according to these normative goals. RADio introduces a rank-aware Jensen Shannon (JS) divergence. This combination accounts for (i) a user’s decreasing propensity to observe items further down a list and (ii) full distributional shifts as opposed to point estimates. We evaluate RADio’s ability to reflect five normative concepts in news recommendations on the Microsoft News Dataset and six (neural) recommendation algorithms, with the help of our metadata enrichment pipeline. We find that RADio provides insightful estimates that can potentially be used to inform news recommender system design.\n\n[8] RADio – Rank-Aware Divergence Metrics to Measure Normative Diversity in News Recommendations In this paper we have made a first attempt at constructing and implementing new evaluation criteria for news recommender systems, with a foundation in normative theory. Based on the DART metrics, first theoretically conceptualized in earlier work, we propose to look at diversity as a divergence score, observing differences between the issued recommendations and a metric-specific target distribution. We proposed RADio, a unified rank-aware f-Divergence metric framework that is mathematically grounded and that fits several possible use cases within the original DART metrics and we hope beyond in future work. We showed that JS divergence was preferred over other divergence metrics. At first mathematically, as JS is a proper distance metric, and empirically, via a sensitivity analysis to different cutoff, rank-awareness and divergence metric regimes.When our approach is adopted in practice, it enables the evaluation of news recommender systems on normative principles beyond user relevance. Finally, we wish to emphasize that the metrics proposedare meant to supplement standard recommender system evaluation metrics, in the same way that current beyond-accuracy metrics do. Most importantly, they are meant to bridge the gap between different disciplines involved in the process of news recommendation and to support more informed discussion between them. We hope for future research to foster interdisciplinary teams, leveraging each fields’ unique skills and specialties.\n\n[9] Improving the Diversity of Top-N Recommendation via Determinantal Point Process Recommender systems take the key responsibility to help users discover items that they might be interested in. Many recommendation algorithms are built upon similarity measures, which usually result in low intra-list diversity. The deficiency in capturing the whole range of user interest often leads to poor satisfaction. To solve this problem, increasing attention has been paid on improving the diversity of recommendation results in recent years. \nIn this paper, we propose a novel method to improve the diversity of top-$N$ recommendation results based on the determinantal point process (DPP), which is an elegant model for characterizing the repulsion phenomenon. We propose an acceleration algorithm to greatly speed up the process of the result inference, making our algorithm practical for large-scale scenarios. We also incorporate a tunable parameter into the DPP model which allows the users to smoothly control the level of diversity. More diversity metrics are introduced to better evaluate diversification algorithms. We have evaluated our algorithm on several public datasets, and compared it thoroughly with other reference algorithms. Results show that our proposed algorithm provides a much better accuracy-diversity trade-off with comparable efficiency.\n\n[10] Temporal diversity in recommender systems Collaborative Filtering (CF) algorithms, used to build web-based recommender systems, are often evaluated in terms of how accurately they predict user ratings. However, current evaluation techniques disregard the fact that users continue to rate items over time: the temporal characteristics of the system's top-N recommendations are not investigated. In particular, there is no means of measuring the extent that the same items are being recommended to users over and over again. In this work, we show that temporal diversity is an important facet of recommender systems, by showing how CF data changes over time and performing a user survey. We then evaluate three CF algorithms from the point of view of the diversity in the sequence of recommendation lists they produce over time. We examine how a number of characteristics of user rating patterns (including profile size and time between rating) affect diversity. We then propose and evaluate set methods that maximise temporal recommendation diversity without extensively penalising accuracy.\n\n[11] Fairness and Diversity in Recommender Systems: A Survey Category-based diversity:  Distance-based diversity is criticized in (Vargas et al., 2014) due to the failure of ensuring the consistency between the diversity value and users’ experience.They (Vargas et al., 2014) leverage category/genre information to capture item diversity, which better corresponds to users’ perceptions. They propose a novel binomial framework to capture genre-based diversity  (Vargas et al., 2014) which considers three perspectives simultaneously: coverage, redundancy, and size-awareness. When genre information is available, they compute the diversity score as BinomDiv​(ℛu)=Coverage​(ℛu)∗NonRed​(ℛu)BinomDivsubscriptℛ𝑢Coveragesubscriptℛ𝑢NonRedsubscriptℛ𝑢\\text{BinomDiv}(\\mathcal{R}_{u})=\\text{Coverage}(\\mathcal{R}_{u})*\\text{NonRed}(\\mathcal{R}_{u}) where coverage mainly captures how many different genres are presented in the recommendation and non-redundancy (i.e., NonRed) encourages the genre uniqueness in the recommendation. When explicit category information is unavailable, Chen et. al (Chen and Pu, 2007) propose to group items into categories based on their attributes. Thereafter, diversity is defined as the category dissimilarity.\n\n[12] Towards Unified Metrics for Accuracy and Diversity for Recommender Systems Recommender systems evaluation has evolved rapidly in recent years. However, for offline evaluation, accuracy is the de facto standard for assessing the superiority of one method over another, with most research comparisons focused on tasks ranging from rating prediction to ranking metrics for top-n recommendation. Simultaneously, recommendation diversity and novelty have become recognized as critical to users’ perceived utility, with several new metrics recently proposed for evaluating these aspects of recommendation lists. Consequently, the accuracy-diversity dilemma frequently shows up as a choice to make when creating new recommendation algorithms. We propose a novel adaptation of a unified metric, derived from one commonly used for search system evaluation, to Recommender Systems. The proposed metric combines topical diversity and accuracy, and we show it to satisfy a set of desired properties that we formulate axiomatically. These axioms are defined as fundamental constraints that a good unified metric should always satisfy. Moreover, beyond the axiomatic analysis, we present an experimental evaluation of the metric with collaborative filtering data. Our analysis shows that the metric respects the desired theoretical constraints and behaves as expected when performing offline evaluation.\n\n"}
{"case_id": "e2e6287d2687772bb02cfc5107047cb6", "answer_text": "\n\nIn robotics, several methods have been developed to automatically learn terrain traversability costs from robot experience. These methods can be broadly categorized into supervised learning, self-supervised learning, and inverse reinforcement learning (IRL). Each of these approaches leverages different types of data and learning paradigms to estimate the difficulty or cost associated with traversing various terrains [4][7][3].\n\n\n\n**Supervised Learning Approaches**[6]\n\n\n\nSupervised learning methods rely on labeled data to train models for predicting terrain traversability. One common strategy is to use simulations or real-world experiments to generate datasets where different terrain types are associated with traversability costs. For instance, terrain traversability can be learned by creating traversability cost maps using point cloud data and supervised learning to map inertial measurement unit (IMU) responses to terrain patches [12]. This method allows the prediction of terrain costs without the need for the robot to physically traverse the terrain. Another example involves using a heightmap classification task, where classifiers trained on simulation data can predict the traversability of real-world terrains [9]. \n\nHowever, supervised learning approaches often require extensive labeled datasets and may not generalize well to unknown terrains. To mitigate this problem, transfer learning and data augmentation techniques can be employed. Transfer learning allows models pre-trained on related tasks to be fine-tuned for specific traversability tasks, reducing the amount of new labeled data needed [10]. Data augmentation techniques create synthetic variations of the existing dataset to effectively increase the data volume and diversity, improving model robustness [11].\n\n**Real-World Application Example:** In real-world applications, supervised learning has been successfully implemented in autonomous agricultural robots, where the robots navigate different soil types and crop layouts by leveraging techniques like transfer learning and data augmentation to generalize better across varying conditions [13]. \n\n\n**Self-Supervised Learning Approaches** [4]\n\n\nSelf-supervised learning techniques eliminate the need for manually labeled data by using the robot's own sensor readings and behaviors as supervision signals. These methods have gained traction due to their ability to adapt to diverse and unstructured environments. For example, terrain traversability costs can be learned by combining exteroceptive environmental information with proprioceptive feedback such as force-torque measurements, IMU readings, and odometry errors [7]. This approach allows for the dynamic adaptation of cost maps based on real-time interactions between the robot and the terrain, leading to smoother navigation and better understanding of terrain interactions. \n\nFor example, in a study by Wellhausen et al., a self-supervised learning framework was proposed to predict terrain properties by using recorded force-torque measurements and associating them with corresponding image data via foothold projection [14]. Similarly, Frey et al. proposed an online traversability estimation approach that uses self-supervision to infer costs dynamically without predefined tasks, making it adaptable to multiple terrains and tasks [4]. These self-supervised methods offer flexibility and adaptability. Although they may initially show less accuracy compared to supervised methods, they improve over time as they gather more data.\n\n**Real-World Application Example:** In the real world, self-supervised learning has been applied in autonomous all-terrain vehicles (ATVs) for off-road exploration. These vehicles use a combination of LIDAR, IMU, and force-torque data to adapt their navigation strategies on challenging terrains such as forests and mountainous regions, demonstrating improved adaptability and performance over time [7]. \n\n**Inverse Reinforcement Learning (IRL) Approaches** [3]\n\nInverse Reinforcement Learning (IRL) is another prominent method for learning terrain traversability costs. IRL approaches aim to infer the cost function based on observed behaviors of the robot, thereby bypassing the need for predefined metrics. This method has shown robustness and scalability compared to predetermined cost functions [3][5]. The Maximum Margin Planning (MMP) framework, for example, has been applied to learn traversal costs using overhead terrain data [5]. Additionally, the MEDIRL framework leverages deep neural networks to model nonlinear reward functions for traversability cost learning, offering more complex and adaptable models compared to traditional approaches [5]. These IRL approaches are advantageous because they can adapt to new terrains and robot behaviors without requiring extensive manual tuning. \n\nFor instance, in a study utilizing deep IRL, the model learned terrain characteristics directly from the robot’s traversing behaviors, achieving a more robust and scalable cost representation [3]. IRL methods generally outperform both supervised and self-supervised methods in terms of flexibility and scalability, especially in dynamic and unstructured environments.\n\n**Real-World Application Example:** IRL has been successfully implemented in search and rescue missions, where robots need to navigate through debris and unpredictable terrain. By learning from past interactions, these robots adapt their navigation strategies to optimize for safety and efficiency, significantly improving mission success rates in complex environments like collapsed buildings and disaster-stricken areas [3]. \n\n**Hybrid and Advanced Techniques**[0]\n\nHybrid and advanced techniques integrate multiple learning paradigms and leverage additional data modalities to enhance terrain traversability estimation. For instance, TerraPN combines environmental textures, bumpiness, and deformability with dynamic window-based planning to predict navigability cost maps [13]. This hybrid approach dynamically adjusts the resolution of cost maps for improved computational efficiency and incorporates direct feedback from robot-terrain interactions.\n\nAnother advanced technique involves using neural networks to learn the distribution of traversability costs with a focus on tail-risk. This approach, which aims to produce more robust cost maps, tackles stochasticity and uncertainty in unstructured terrains by learning the distribution of costs and focusing on extreme values that significantly impact navigation safety [10]. By addressing the tail risks, these models ensure the robot can handle unexpected and challenging terrain conditions more effectively.\n\nThese methods often combine the strengths of supervised, self-supervised, and IRL approaches. For example, in planetary rovers, combining simulation-based supervised learning with real-time self-supervised learning allows the rover to adaptively navigate the unpredictable terrains of planetary surfaces [13]. This results in improved mission success rates and more efficient scientific exploration. \n\n**Real-World Application Example:** Hybrid techniques have seen real-world application in planetary rovers, where the challenging and unknown terrain of planetary surfaces necessitates robust and adaptive navigation strategies [13]. By combining different learning approaches, these rovers can effectively navigate and conduct scientific exploration missions on planets like Mars, overcoming the limitations of single-method approaches [9]. \n\nIn summary, the leading methods for learning terrain traversability costs from robot experience span supervised, self-supervised, and inverse reinforcement learning paradigms. Supervised learning methods offer straightforward but data-intensive solutions that benefit from extensive labeled datasets and techniques like transfer learning. Self-supervised learning approaches provide adaptability to diverse and unstructured environments with reduced labeling requirements, improving accuracy over time with ongoing data collection [7]. Inverse Reinforcement Learning stands out for its robustness and scalability without the need for predefined cost functions, making it particularly suitable for dynamic environments [3]. Hybrid and advanced techniques, which combine these paradigms, exhibit enhanced performance and are well-suited for complex applications such as planetary exploration and search and rescue missions. The choice of method depends on the specific application requirements, available data, and computational resources, with each method offering distinct advantages for different scenarios. \n\n\nReferences:\n[0] Online Incremental Learning of the Terrain Traversal Cost in Autonomous Exploration Online Incremental Learning of the Terrain Traversal Cost in Autonomous Exploration In this paper, we address motion efficiency in autonomous robot exploration with multi-legged walking robots that can traverse rough terrains at the cost of lower efficiency and greater body vibration. We propose a robotic system for online and incremental learning of the terrain traversal cost that is immediately utilized to reason about next navigational goals in building spatial model of the robot surrounding. The traversal cost experienced by the robot is characterized by incrementally constructed Gaussian Processes using Bayesian Committee Machine. During the exploration, the robot builds the spatial terrain model, marks untraversable areas, and leverages the Gaussian Process predictive variance to decide whether to improve the spatial model or decrease the uncertainty of the terrain traversal cost. The feasibility of the proposed approach has been experimentally verified in a fully autonomous deployment with the hexapod walking robot.\n\n[1] Energy-Based Legged Robots Terrain Traversability Modeling via Deep Inverse Reinforcement Learning Recently, Inverse Reinforcement Learning (IRL) for terrain traversability modeling has received increased interests, particularly for autonomous vehicles , 15, 16, 17. Instead of using a predetermined metric to define traversability cost and associating it with terrain features, IRL-based methods aim to learn this cost directly from a robot’s (traversing) behaviors. Doing so bypasses the effort and incompleteness of manually designing the cost function and parts of the learning pipeline. The resulting cost representations of IRL-based methods have also shown better robustness and scalability than the predetermined cost and the cost learned in supervised methods , 18.\n\n[2] Self-Supervised Terrain Representation Learning from Unconstrained Robot Experience Cost Learning:Several methods have applied self-supervision to assign traversability costs for the downstream off-road navigation task, 38, 26, 39, 40, 41, 42. Specifically, these methods rely on inertial spectral features, future predictive models, inertial-odometry errors, or force-torque values from foothold positions, 43 as self-supervision signals to learn a traversability cost map, used to evaluate candidate actions. More recently, Frey et al. have proposed an online traversability estimation approach inspired by the above self-supervision schemes. Instead of inferring costs or rewards using self-supervision for a fixed task, in this work, we focus on learning relevant visual features from unconstrained robot experiences that could be used in downstream tasks. This framework allows a designer to reuse features across tasks without retraining entirely from scratch.\n\n[3] Energy-Based Legged Robots Terrain Traversability Modeling via Deep Inverse Reinforcement Learning Early works include applying the Maximum Margin Planning (MMP) framework to traversal cost modeling using overhead terrain data . Later, the MEDIRL framework , 14 is proposed to exploit the power of deep neural networks in expressing nonlinear reward functions and becomes a common paradigm for traversability cost learning.\n\n[4] Self-Supervised Terrain Representation Learning from Unconstrained Robot Experience Baselines:To perform quantitative evaluations for Q1subscript𝑄1Q_{1}, we compare sterling with se-r, rca, ganav, geometric-only planning, and a fully-supervised baseline. se-r and rca perform self-supervised learning from unconstrained robot experience to learn terrain representations and traversability costs respectively, making them relevant baselines for this problem. Since there is no open-source implementation of rca, we replicate it to the best of our abilities. The geometric-only approach ignores terrain costs (ℒt​e​r​r​a​i​nsubscriptℒ𝑡𝑒𝑟𝑟𝑎𝑖𝑛\\mathcal{L}_{terrain}) and plans with geometric cost (ℒg​e​o​msubscriptℒ𝑔𝑒𝑜𝑚\\mathcal{L}_{geom}) only, making it a relevant ablation on the cost formulation for preference-aware planning. ganav222https://github.com/rayguan97/GANav-offroad is a segmentation-based approach trained on the RUGD dataset. We additionally train the fully-supervised baseline in which the terrain cost function is learned end-to-end using supervised learning from linear extrapolation of operator preferences. ganav and the fully-supervised baseline require supervision via terrain labels to learn and hence serve as references for comparison.We normalize the terrain cost predicted by all methods to be between 0 and 1 for a fair comparison.\n\n[5] How Does It Feel? Self-Supervised Costmap Learning for Off-Road Vehicle Traversability How Does It Feel? Self-Supervised Costmap Learning for Off-Road Vehicle Traversability Estimating terrain traversability in off-road environments requires reasoning about complex interaction dynamics between the robot and these terrains. However, it is challenging to build an accurate physics model, or create informative labels to learn a model in a supervised manner, for these interactions. We propose a method that learns to predict traversability costmaps by combining exteroceptive environmental information with proprioceptive terrain interaction feedback in a self-supervised manner. Additionally, we propose a novel way of incorporating robot velocity in the costmap prediction pipeline. We validate our method in multiple short and large-scale navigation tasks on a large, autonomous all-terrain vehicle (ATV) on challenging off-road terrains, and demonstrate ease of integration on a separate large ground robot. Our short-scale navigation results show that using our learned costmaps leads to overall smoother navigation, and provides the robot with a more fine-grained understanding of the interactions between the robot and different terrain types, such as grass and gravel. Our large-scale navigation trials show that we can reduce the number of interventions by up to 57% compared to an occupancy-based navigation baseline in challenging off-road courses ranging from 400 m to 3150 m. I. INTRODUCTION Outdoor, unstructured environments are challenging for robots to navigate. Rough interactions with terrain can result in a number of undesirable effects, such as rider discomfort, error in state estimation, or even failure of robot components. Unfortunately, it can be challenging to predict these interactions a priori from exteroceptive information alone. Certain characteristics of the terrain,\n\n[6] Learning Ground Traversability From Simulations We presented a complete framework for traversability estimation that casts the problem as a heightmap classification task, and applies to any modality of robot locomotion. Classifiers trained on simulation data using procedurally-generated terrains capture relevant terrain characteristics and can efficiently and accurately estimate oriented traversability maps on large unseen real-world terrains. Such maps can be used to plan paths exploring the trade-off between length (to be minimized) and probability of being traversable (to be maximized).\n\n[7] Learning Risk-Aware Costmaps for Traversability in Challenging Environments Learning Risk-aware Costmaps for Traversability in Challenging Environments One of the main challenges in autonomous robotic exploration and navigation in unknown and unstructured environments is determining where the robot can or cannot safely move. A significant source of difficulty in this determination arises from stochasticity and uncertainty, coming from localization error, sensor sparsity and noise, difficult-to-model robot-ground interactions, and disturbances to the motion of the vehicle. Classical approaches to this problem rely on geometric analysis of the surrounding terrain, which can be prone to modeling errors and can be computationally expensive. Moreover, modeling the distribution of uncertain traversability costs is a difficult task, compounded by the various error sources mentioned above. In this work, we take a principled learning approach to this problem. We introduce a neural network architecture for robustly learning the distribution of traversability costs. Because we are motivated by preserving the life of the robot, we tackle this learning problem from the perspective of learning tail-risks, i.e. the Conditional Value-at-Risk (CVaR). We show that this approach reliably learns the expected tail risk given a desired probability risk threshold between 0 and 1, producing a traversability costmap which is more robust to outliers, more accurately captures tail risks, and is more computationally efficient, when compared against baselines. We validate our method on data collected a legged robot navigating challenging, unstructured environments including an abandoned subway, limestone caves, and lava tube caves. model and even more difficult to accurately compute in real time -especially in unstructured environments such as those filled with slopes, rough terrain,\n\n[8] Locomotion Policy Guided Traversability Learning using Volumetric Representations of Complex Environments robot configuration and motion command is motivated by the fact that traversing up or down a slope imposes varying risk and e.g. within tight corridors only a subset of actions can be executed collision-free. The traversability cost tensor allows to capture this information adequately and measures traversability in a range of 0 to 1, where 0 is untraversable and 1 risk-free traversable. Our objective is to find the function f trav which predicts each element within the traversability cost tensor T, given the occupancy map representation O ∈ O and state of the robot s ∈ S. B. Simulating Traversability: The traversability cost is evaluated by simulating locomotion over randomly generated terrain in NVIDIA's Isaac-Gym. To achieve accurate sim-to-real transfer the robot is controlled by the same locomotion policy later deployed on the real robot. In this work, the traversability cost is defined as the success rate of reaching the desired goal pose, which is of main interest for autonomous navigation. The goal pose is determined by the motion command with respect to the robots starting pose. Evaluation of multiple trials per motion command under domain randomization results in a robust geometric traversability risk assessment. We introduce the terrain generation procedure in (Sec. III-B.1), followed by the traversability data collection in (Sec. III-B.2). 1) Terrain Generation: We aim to generate a large and diverse terrain dataset covering the domain of terrains that the robot may encounter during deployment. Acquisition of a large amount of experience has shown to increase generalization performance for various tasks, including\n\n[9] How Rough Is the Path? Terrain Traversability Estimation for Local and Global Path Planning How Rough Is the Path? Terrain Traversability Estimation for Local and Global Path Planning Perception and interpretation of the terrain is essential for robot navigation, particularly in off-road areas, where terrain characteristics can be highly variable. When planning a path, features such as the terrain gradient and roughness should be considered, and they can jointly represent the traversability cost of the terrain. Despite this range of contributing factors, most cost maps are currently binary in nature, solely indicating traversible versus non-traversible areas. This work presents a joint local and global planning methodology for building continuous cost maps using LIDAR, based on a novel traversability representation of the environment. We investigate two approaches. The first, a statistical approach, computes terrain cost directly from the point cloud. The second, a learning-based approach, predicts an IMU response solely from geometric point cloud data using a 2D-Convolutional-LSTM neural network. This allows us to estimate the cost of a patch without directly driving over it, based on a data set that maps IMU signals to point cloud patches. Based on the terrain analysis, two continuous cost maps are generated to jointly select the optimal path considering distance and traversability cost for local navigation. We present a real-time terrain analysis strategy applicable for local planning, and furthermore demonstrate the straightforward application of the same approach in batch mode for global planning. Off-road autonomous driving experiments in a large and hybrid site illustrate the applicability of the method. We have made the code available online for users to test the method.\n\n[10] Overleaf Example TerraPN: Unstructured terrain navigation through Online Self-Supervised Learning We present TerraPN, a novel method to learn the surface characteristics (texture, bumpiness, deformability, etc.) of complex outdoor terrains for autonomous robot navigation. Our method predicts navigability cost maps for different surfaces using patches of RGB images, odometry, and IMU data. Our method dynamically varies the resolution of the output cost map based on the scene to improve its computational efficiency. We present a novel extension to the Dynamic-Window Approach (DWA-O) to account for a surface's navigability cost while computing robot trajectories. DWA-O also dynamically modulates the robot's acceleration limits based on the variation in the robot-terrain interactions. In terms of perception, our method learns to predict navigability costs in $\\sim 20$ minutes for five different surfaces, compared to 3-4 hours for previous scene segmentation methods and leads to a decrease in inference time. In terms of navigation, our method outperforms previous works in terms of vibration costs and generates robot velocities suitable for different surfaces. I. INTRODUCTION Autonomous robots are currently being used for a variety of outdoor applications such as food/grocery delivery, agriculture, surveillance, planetary exploration, etc. Robot navigation methods must account for a terrain's geometric properties such as slope or elevation changes and its surface characteristics such as texture, bumpiness (level of undulations), softness/deformability, etc to compute smooth and efficient robot trajectories. In addition to a terrain's geometry, its surface properties determine its navigability for a robot. For instance, a surface's texture determines the traction experienced by the robot, its bumpiness determines the vibrations experienced,\n\n[11] Energy-Based Legged Robots Terrain Traversability Modeling via Deep Inverse Reinforcement Learning Wellhausen et al. propose a self-supervised learning framework to predict terrain properties for a quadruped robot. They compute a ground reaction score from recorded force-torque measurements and associate the score with corresponding image data via foothold projection. The automatically annotated images are used to train a score regression network. Similarly, Faigl and Prágr also use robot proprioceptive features to characterize three traversal costs, i.e., mean instantaneous power consumption, mean forward velocity and attitude stability, and attach those costs to the corresponding exteroceptive features for regression.\n\n"}
{"case_id": "bb7198e650267504d67b14b6e4c67c7c", "answer_text": "\n\nCombining neural networks (NNs) with partial differential equation (PDE) solvers for fluid simulation has shown great potential in improving both efficiency and accuracy [0]. However, several open challenges remain that need to be addressed to fully harness these benefits. The key open challenges can be classified into generalization, integration, computational cost, and physical fidelity.\n\n\n### 1. Generalization\n\nGeneralization in computational fluid dynamics (CFD) simulations and other applications involving the approximation of partial differential equations (PDEs) has seen significant improvements with the integration of deep learning techniques. Traditional numerical approaches for solving large, complex PDEs, such as those in CFD, are computationally expensive. This has driven interest in deep learning methods to approximate PDE solutions. However, these approaches often do not generalize well to entirely novel scenarios [0],[1]. By combining traditional graph convolutional networks with an embedded differentiable fluid dynamics simulator, recent work has shown substantial advances in generalizing to new situations while also achieving faster predictions and better performance compared to traditional coarse CFD simulations alone [1],[3].\n\n\nOne of the most significant challenges in using neural networks for fluid simulations is their generalizability to novel or unseen scenarios. Purely learning-based models often struggle to generalize beyond the specific conditions they were trained on. This issue is exacerbated in fluid dynamics, where the range of possible flow conditions is vast and complex. Studies have shown that while neural networks can enhance coarse CFD simulations, their ability to generalize remains limited [1][2][3]. Hybrid models that integrate differentiable fluid dynamics simulators with neural networks hold promise, but ensuring these models can generalize to new, unseen scenarios remains a key area of ongoing research [2][3].\n\n## 2. Integration of NN and PDE Solvers[^0].\n\n[^0]: [0]Title: Combining Differentiable PDE Solvers and Graph Neural Networks for Fluid Flow Prediction Text: Combining Differentiable PDE Solvers and Graph Neural Networks for Fluid Flow Prediction Solving large complex partial differential equations (PDEs), such as those that arise in computational fluid dynamics (CFD), is a computationally expensive process. This has motivated the use of deep learning approaches to approximate the PDE solutions, yet the simulation results predicted from these approaches typically do not generalize well to truly novel scenarios. In this work, we develop a hybrid (graph) neural network that combines a traditional graph convolutional network with an embedded differentiable fluid dynamics simulator inside the network itself. \n\n\n\nIntegrating neural networks with traditional PDE solvers presents significant methodological challenges. Conventional CFD solvers operate on non-uniform meshes, and embedding these solvers into NN architectures, such as graph convolutional networks (GCNs), necessitates sophisticated techniques to handle the non-linearity and complexity of fluid dynamics [2]. Additionally, the differentiation of CFD simulators (to allow for gradients to be computed) is complex and must be carefully managed to ensure stability and accuracy in the coupled system [6][13]. Achieving seamless and efficient integration of these components remains an ongoing challenge.\n\n### 3. Computational Cost [0]\n\nWhile neural networks have the potential to accelerate fluid simulations, the upfront computational cost of training these models can be prohibitive. Training requires large datasets, often obtained from expensive high-fidelity simulations, which raises questions about the amortized cost of using neural surrogates in practical applications [10]. Furthermore, the augmented computational requirements due to the increased context length in hybrid models can add to the inefficiency [6]. Methods to reduce the training cost while maintaining model accuracy and efficiency are needed.\n\n\n\n4. Physical Fidelity and Stability [1]\n\n\n\nEnsuring that neural network predictions adhere to physical laws and constraints is another critical challenge. Physics-informed neural networks (PINNs) and differentiable physics-based networks attempt to embed physical laws directly into the learning process by minimizing the residuals of the PDEs [4][8][12]. However, ensuring stability and physical fidelity over long simulation times and in complex multi-physics scenarios remains challenging [13]. There are also difficulties in balancing the trade-offs between computational efficiency and physical accuracy [7]. The development of robust techniques that maintain physical consistency across varied conditions and temporal scales is crucial.\n\n\n### 5. Unknown Phenomena and Source Terms\n\nNeural networks (NN) are typically used to model or replace a part of the conventional PDE solver to improve aspects of the solving process [6].\n\n\nFluid simulations often involve reacting flows or other complex phenomena that are hard to model due to unknown source terms. Neural networks can be used to model these unknown components, but training such networks requires accurate data from both non-reactive and reactive simulations [6]. Developing methods to capture and model these unknown phenomena accurately is still an area of active research.\n\n## 6. Scalability and Real-World Applicability [10]. \n\nFinally, achieving scalability for real-world applications is a significant challenge. While hybrid models have shown promising results in controlled environments, their scalability to more complex, real-world fluid dynamics problems is less explored [14]. Evaluating the trade-offs between speedup and accuracy, particularly in large-scale simulations, remains an open question [10].\n\nIn summary, while combining neural networks with PDE solvers for fluid simulation holds great promise, significant challenges remain. Addressing issues related to generalization[^1], integration[^3], computational cost[^0], physical fidelity[^2], modeling unknown phenomena[^6], and scalability[^10] will be crucial for the future development and real-world application of these hybrid techniques. \n\n\n\nReferences:\n[0] Combining Differentiable PDE Solvers and Graph Neural Networks for Fluid Flow Prediction Combining Differentiable PDE Solvers and Graph Neural Networks for Fluid Flow Prediction Solving large complex partial differential equations (PDEs), such as those that arise in computational fluid dynamics (CFD), is a computationally expensive process. This has motivated the use of deep learning approaches to approximate the PDE solutions, yet the simulation results predicted from these approaches typically do not generalize well to truly novel scenarios. In this work, we develop a hybrid (graph) neural network that combines a traditional graph convolutional network with an embedded differentiable fluid dynamics simulator inside the network itself. By combining an actual CFD simulator (run on a much coarser resolution representation of the problem) with the graph network, we show that we can both generalize well to new situations and benefit from the substantial speedup of neural network CFD predictions, while also substantially outperforming the coarse CFD simulation alone. Introduction In recent years, the many empirical successes of deep learning have motivated scientists to explore its application in other areas, such as predicting the evolution of physical systems. In this context, several recent papers have explored the application of deep models to approximate the solutions to partial differential equations (PDEs), particularly in the context of simulating fluid dynamics (Afshar et al., 2019;Guo et al., 2016;Wiewel et al., 2018;Um et al., 2017). The behavior of fluids is a well-studied problem in the physical sciences, and predicting their dynamics involves solving the nonlinear Navier-Stokes PDEs. In order to perform compu- † Work partly done while at Bosch LLC. 1 School of Computer Science,\n\n[1] Combining Differentiable PDE Solvers and Graph Neural Networks for Fluid Flow Prediction Solving large complex partial differential equations (PDEs), such as those that arise in computational fluid dynamics (CFD), is a computationally expensive process. This has motivated the use of deep learning approaches to approximate the PDE solutions, yet the simulation results predicted from these approaches typically do not generalize well to truly novel scenarios. In this work, we develop a hybrid (graph) neural network that combines a traditional graph convolutional network with an embedded differentiable fluid dynamics simulator inside the network itself. By combining an actual CFD simulator (run on a much coarser resolution representation of the problem) with the graph network, we show that we can both generalize well to new situations and benefit from the substantial speedup of neural network CFD predictions, while also substantially outperforming the coarse CFD simulation alone.\n\n[2] Combining Differentiable PDE Solvers and Graph Neural Networks for Fluid Flow Prediction In this paper, we explore a hybrid approach that combines the benefits of (graph) neural networks for fast predictions, with the physical realism of an industry-grade CFD simulator. Our system has two main components. First, we construct a graph convolution network (Kipf & Welling, 2016) (GCN), which operates directly upon the non-uniform mesh used in typical CFD simulation tasks. This use of GCNs is crucial because all realistic CFD solvers operate on these unstructured meshes rather than directly on the regular grid used by most prior work, which has typically used convolutional networks to approximate CFD simulations. Second, and more fundamentally, we embed a (differentiable) CFD solver, operating on a much coarser resolution, directly into the GCN itself. Although typically treated as black-boxes, modern CFD simulators are themselves perfectly well-suited to act as (costly) “layers” in a deep network. Using well-studied adjoint methods, modern solvers can compute gradients of the output quantities of a simulation with respect to the input mesh. This allows us to integrate a fast CFD simulation (made fast because it is operating on a much smaller mesh) into the network itself, and allows us to jointly train the GCN and the mesh input into the simulation engine, all in an end-to-end fashion.\n\n[3] Combining Differentiable PDE Solvers and Graph Neural Networks for Fluid Flow Prediction We demonstrate that this combined approach performs substantially better than the coarse CFD simulation alone (i.e., the network is able to provide higher fidelity results than simply running a faster simulation to begin with), and generalizes to novel situations much better than a pure graph-network-based approach. Moreover, the approach is still substantially faster than running the CFD simulation on the original size mesh itself. We believe that in total this represents a substantial advance towards integrating deep learning and existing state-of-the-art simulation software.\n\n[4] Physics-informed neural networks (PINNs) for fluid mechanics: a review We construct two fully-connected neural networks, Net U and Net W, where the outputs of Net U represents a surrogate model for the PDE solutions u, v, p, and ϕitalic-ϕ\\phi and the outputs of Net W are PDE solutions ω𝜔\\omega, ψ1subscript𝜓1\\psi_{1}, and ψ2subscript𝜓2\\psi_{2}. Each network has 9 hidden layers with 20 neurons per layer. The total loss L is a combination of different losses as:L=ω1​LP​D​E+ω2​LI​C+ω3​LB​C+ω4​Ld​a​t​a,𝐿subscript𝜔1subscript𝐿𝑃𝐷𝐸subscript𝜔2subscript𝐿𝐼𝐶subscript𝜔3subscript𝐿𝐵𝐶subscript𝜔4subscript𝐿𝑑𝑎𝑡𝑎L=\\omega_{1}L_{PDE}+\\omega_{2}L_{IC}+\\omega_{3}L_{BC}+\\omega_{4}L_{data},(19)where LP​D​Esubscript𝐿𝑃𝐷𝐸L_{PDE} is the PDEs residual loss, LI​Csubscript𝐿𝐼𝐶L_{IC} is the initial condition loss, LB​Csubscript𝐿𝐵𝐶L_{BC} is the boundary condition loss, and Ld​a​t​asubscript𝐿𝑑𝑎𝑡𝑎L_{data} is the data loss. In particular:LP​D​E​(θ,𝝀;XP​D​E)subscript𝐿𝑃𝐷𝐸𝜃𝝀subscript𝑋𝑃𝐷𝐸\\displaystyle L_{PDE}(\\theta,\\bm{\\lambda};X_{PDE})=1|XP​D​E|​∑x∈XP​D​E∥f​(x,∂tu^,∂xu^,…;𝝀)∥22,absent1subscript𝑋𝑃𝐷𝐸subscriptxsubscript𝑋𝑃𝐷𝐸superscriptsubscriptdelimited-∥∥𝑓xsubscript𝑡^usubscript𝑥^u…𝝀22\\displaystyle=\\frac{1}{\\left|X_{PDE}\\right|}\\sum\\limits_{\\textbf{x}\\in X_{PDE}}\\left\\lVert f(\\textbf{x},\\partial_{t}{\\hat{\\textbf{u}}},\\partial_{x}\\hat{\\textbf{u}},...;\\bm{\\lambda})\\right\\rVert_{2}^{2},(20)LB​C​(θ,𝝀;XB​C)subscript𝐿𝐵𝐶𝜃𝝀subscript𝑋𝐵𝐶\\displaystyle L_{BC}(\\theta,\\bm{\\lambda};X_{BC})=1|XB​C|​∑x∈XB​C∥𝔅​(u^,x)∥22,absent1subscript𝑋𝐵𝐶subscriptxsubscript𝑋𝐵𝐶superscriptsubscriptdelimited-∥∥𝔅^ux22\\displaystyle=\\frac{1}{\\left|X_{BC}\\right|}\\sum\\limits_{\\textbf{x}\\in X_{BC}}\\left\\lVert\\mathfrak{B(\\hat{\\textbf{u}},\\textbf{x})}\\right\\rVert_{2}^{2},(21)LI​C​(θ,𝝀;XI​C)subscript𝐿𝐼𝐶𝜃𝝀subscript𝑋𝐼𝐶\\displaystyle L_{IC}(\\theta,\\bm{\\lambda};X_{IC})=1|XI​C|​∑x∈XI​C∥u^−ut0∥22,absent1subscript𝑋𝐼𝐶subscriptxsubscript𝑋𝐼𝐶superscriptsubscriptdelimited-∥∥^usubscriptusubscript𝑡022\\displaystyle=\\frac{1}{\\left|X_{IC}\\right|}\\sum\\limits_{\\textbf{x}\\in X_{IC}}\\left\\lVert\\hat{\\textbf{u}}-\\textbf{u}_{t_{0}}\\right\\rVert_{2}^{2},(22)Ld​a​t​a​(θ,𝝀;Xd​a​t​a)subscript𝐿𝑑𝑎𝑡𝑎𝜃𝝀subscript𝑋𝑑𝑎𝑡𝑎\\displaystyle L_{data}(\\theta,\\bm{\\lambda};X_{data})=1|Xd​a​t​a|​∑x∈Xd​a​t​a∥u^−ud​a​t​a∥22,absent1subscript𝑋𝑑𝑎𝑡𝑎subscriptxsubscript𝑋𝑑𝑎𝑡𝑎superscriptsubscriptdelimited-∥∥^usubscriptu𝑑𝑎𝑡𝑎22\\displaystyle=\\frac{1}{|X_{data}|}\\sum\\limits_{\\textbf{x}\\in X_{data}}\\left\\lVert\\hat{\\textbf{u}}-\\textbf{u}_{data}\\right\\rVert_{2}^{2},(23)where ω1subscript𝜔1\\omega_{1}, ω2subscript𝜔2\\omega_{2}, ω3subscript𝜔3\\omega_{3}, and ω4subscript𝜔4\\omega_{4} are the weights of each term. The training sets XP​D​Esubscript𝑋𝑃𝐷𝐸X_{PDE}, XB​Csubscript𝑋𝐵𝐶X_{BC}, and XI​Csubscript𝑋𝐼𝐶X_{IC} are sampled from the inner spatio-temporal domain, boundaries, and initial snapshot, respectively. Xd​a​t​asubscript𝑋𝑑𝑎𝑡𝑎X_{data} is the set that contains sensor coordinates and point measurements;⋅|\\left|\\cdot\\right| denotes the number of training data in the training set. In particular, 𝔅𝔅\\mathfrak{B} represents a combination of the Dirichlet and Neumann residuals at boundaries. Finally, we optimize the model parameters 𝜽𝜽\\bm{\\theta} and the PDE parameters 𝝀=[κ𝝀delimited-[𝜅\\bm{\\lambda}=[\\kappa by minimizing the total loss L​(𝜽,𝝀)𝐿𝜽𝝀L(\\bm{\\theta},\\bm{\\lambda}) iteratively until the loss satisfies the stopping criteria. Minimizing the total loss is an optimization process for 𝝀𝝀\\bm{\\lambda} such that the outputs of the PINN satisfy the PDE system, initial/boundary conditions, and point measurements.\n\n[5] Incomplete to complete multiphysics forecasting: a hybrid approach for learning unknown phenomena In this context, neural networks (NN) are typically used to model or replace a part of the conventional PDE solver to improve aspects of the solving process. For example, Tompson et al.,Özbay et al. and Ajuria et al. proposed a convolutional neural network based approach to solve the Poisson equation in CFD simulation. Belbute-Peres et al. and Kochkov et al. applied the integration of differentiable CFD simulator to obtain faster fine mesh solutions from coarse mesh inputs. Um et al. showed the advantages of training neural networks with differentiable physics to correct the numerical errors that arise in the discretization of PDEs. These approaches demonstrated the capabilities of neural networks to correct errors in fast, under-resolved simulations. In this paper, we explore the applicability of these hybrid NN-PDE approaches to correct a partial PDE description of a system by modeling the effects of the unknown source terms. Specifically, we demonstrate this approach on reacting flows: the neural network that serves as a corrector agent of the incomplete PDEs is trained making use of a differentiable non-reactive PDE solver together with suitable data (coming from a reactive PDE solver). Given that one of the main bottlenecks in simulating reacting flows is the computation of the source terms in the energy and species transport equations, a neural network will be used to model the effects of such source terms. Beyond the recovery of the full, coupled physical model, we will demonstrate that this approach has the added benefit of relaxing the time-step\n\n[6] Differentiable hybrid neural modeling for fluid-structure interaction In this work, we present a differentiable hybrid neural model for predicting coupled fluid-structure dynamics, which integrate governing PDEs into deep learning architectures to accelerate FSI simulations while maintaining a balance between predictive accuracy and efficiency. The proposed hybrid neural model is constructed within the differentiable programming framework using JAX, a Python library designed for high-performance ML research and differentiable modeling. Specifically, a hybrid recurrent network unit of fluid-structure coupling is built based on a differentiable incompressible Navier-Stokes solver on very coarse grids combined with trainable convResNet blocks to ensure accurate fluid predictions. Meanwhile, the solid dynamics is directly solved on a high-resolution grid via superresolution operations, and coupled with fluid using IBM direct forcing to enforce two-way interference . These hybrid FSI network units are combined to form a sequential recurrent network based on Long Short-Term Memory (LSTM), allowing sequence-to-sequence (Seq2Seq) training and prediction. The outstanding performance of the proposed hybrid neural FSI model is demonstrated through extensive experiments on two benchmark cases, where we compare its accuracy, robustness, generalizability, and efficiency with other SOTA baseline methods. To the best of the authors’ knowledge, this study represents the first attempt to integrate deep learning with numerical solvers for predictive FSI modeling within the differentiable programming paradigm. The paper is organized as follows: Section 2 outlines the overall methodology, including problem formulations and model designs. Section 3 presents the results of our neural FSI model for two different benchmark cases, along with comparisons to basline methods. We discuss the robustness, generalizability, and speedup of the proposed hybrid neural model in Section 4, and conclude the paper in Section 5.\n\n[7] Physics-informed neural networks (PINNs) for fluid mechanics: a review In the context of the vanilla PINNs raissi2019physics , a fully-connected feed-forward neural network, which is composed of multiple hidden layers, is used to approximate the solution of the PDE u^^𝑢\\hat{u} by taking the space and time coordinates (𝐱,t)𝐱𝑡(\\mathbf{x},t) as inputs, as shown in the blue panel in Fig. 1. Let the hidden variable of the kt​hsuperscript𝑘𝑡ℎk^{th} hidden layer be denoted by 𝐳ksuperscript𝐳𝑘\\mathbf{z}^{k}, then the neural network can be expressed as𝐳0=(𝐱,t),superscript𝐳0𝐱𝑡\\displaystyle\\mathbf{z}^{0}=(\\mathbf{x},t),(2)𝐳k=σ​(𝐖k​𝐳k−1+𝐛k),1≤k≤L−1formulae-sequencesuperscript𝐳𝑘𝜎superscript𝐖𝑘superscript𝐳𝑘1superscript𝐛𝑘1𝑘𝐿1\\displaystyle\\mathbf{z}^{k}=\\sigma(\\mathbf{W}^{k}\\mathbf{z}^{k-1}+\\mathbf{b}^{k}),\\quad 1\\leq k\\leq L-1𝐳k=𝐖k​𝐳k−1+𝐛k,k=L,formulae-sequencesuperscript𝐳𝑘superscript𝐖𝑘superscript𝐳𝑘1superscript𝐛𝑘𝑘𝐿\\displaystyle\\mathbf{z}^{k}=\\mathbf{W}^{k}\\mathbf{z}^{k-1}+\\mathbf{b}^{k},\\quad k=L,where the output of the last layer is used to approximate the true solution, namely u^≈𝐳L^𝑢superscript𝐳𝐿\\hat{u}\\approx\\mathbf{z}^{L}. 𝐖ksuperscript𝐖𝑘\\mathbf{W}^{k} and 𝐛ksuperscript𝐛𝑘\\mathbf{b}^{k} denote the weight matrix and bias vector of the kt​hsuperscript𝑘𝑡ℎk^{th} layer; σ​(⋅)𝜎⋅\\sigma(\\cdot) is a nonlinear activation function. All the trainable model parameters, i.e., weights and biases, are denoted by θ𝜃\\theta in this paper.\n\n[8] Accelerating Simulation of Two-Phase Flows with Neural PDE Surrogates A future research direction would be to investigate more two-phase flow datasets (Linga et al., 2019, 2020; Almasi et al., 2021) and to evaluate more neural PDE solver architectures, for example Lu et al. (2021); Cao (2021); Brandstetter et al. (2022); Tran et al. (2023). Additionally, a probabilistic approach could help in taking the uncertainties in bifurcating dynamics into account (Cachay et al., 2023; Yang & Sommer, 2023; Minartz et al., 2023; Lippe et al., 2023; Bergamin et al., 2024). Finally, one could investigate the speed-up compared to reference numerical solvers and their scalability in more detail. In particular, while inference of the resulting networks is significantly faster, a dataset of expensive simulations must be generated up front. An interesting question is to consider the amortized cost of using neural surrogates in a downstream application. For example, one could do a large number of inverse problem optimizations using either only the high-fidelity numerical method, or by first generating a dataset, training a neural surrogate, and then consequently using a combination of both for the optimization. Such an evaluation could better evaluate the real-world applicability of the investigated methods.\n\n[9] Accelerating Simulation of Two-Phase Flows with Neural PDE Surrogates One relatively popular used approach for solving two-phase flow problems with neural networks is Physics-Informed Neural Networks (PINNs) (Raissi et al., 2019).In this context, example applications of PINNs include the simulation of two-phase flows in porous media (Zhang et al., 2023; Hanna et al., 2022), simulation of the interface of two fluids (Zhu et al., 2023), simulating two-phase material microstructures (Ren & Lyu, 2024), and the simulation of the dynamics of bubbles (Lin et al., 2021b; Zhai et al., 2022; Buhendwa et al., 2021). Although PINNs inherently exploit domain knowledge by minimizing the residual of the PDE directly, they generally need to be optimized from scratch for every PDE instance and parameterization. As one of the main goals of our work is accelerating two-phase flow simulations, this makes them impractical for our setting.\n\n[10] Differentiable hybrid neural modeling for fluid-structure interaction Recently, a new direction in PIML is to incorporate known physics into neural architecture . This is inspired by the relationship between DNN architecture and differential equations , 41. Liu et al.  proposed a PDE-preserved neural network (PPNN) that integrates discretized PDE operators from partially known governing physics into the DNN architecture through convolutional residual connection network (ConvResNet) blocks in a multi-resolution setting. Compared to purely black-kbox ConResNet and other state-of-the-art (SOTA) neural operators, PPNN has been demonstrated to have much better performance in terms of accuracy, speed, and generalizability for predicting various spatiotemporal physics. From the numerical modeling perspective, the PDE-preserving portion in PPNN can be interpreted as numerical differentiation and time stepping implemented as convolutional residual network blocks, which can be seamlessly integrated with any black-box neural network structures to form a hybrid neural solver. Thanks to differentiable programming , the gradient can be back-propagated through the entire neural solver, allowing efficient end-to-end, sequence-to-sequence training of the entire model as a whole. Lately, leveraging physics in their discretized forms based on classical numerical techniques, along the line of differentiable programming to construct a differentiable hybrid solver has been attracting increasing attention , 43, 44, 45, 46. The differentiable programming framework has been recently applied in learning turbulence parametrization, 44, 48, where subgrid-scale closures or other correlations are learned based on a posteriori criteria. End-to-end differentiable learning has enabled significant improvement of a posteriori predictions compared to loosely-coupled hybrid DNN-closure that struggle with stability issues as mentioned above . The integration of DNNs to construct hybrid neural solvers poses significant challenges for most traditional legacy solvers due to the lack of AD capability and GPU compatibility. To overcome this issue, there are growing efforts to develop differentiable CFD solvers such as PhiFlow , Jax-CFD , and Jax-Fluids . Although the hybrid differentiable neural modeling paradigm has shown a great promise, this field is still in its early stages and requires further development, especially for modeling complex multi-physics problems such as two-way coupled FSI dynamics.\n\n[11] Accelerating Simulation of Two-Phase Flows with Neural PDE Surrogates Constructing approximate solutions of PDEs with neural networks has seen a surge in interest over the past few years, with example applications in fluid dynamics (Li et al., 2021; Brandstetter et al., 2022; Gupta & Brandstetter, 2023), climate and weather modeling (Nguyen et al., 2023; Bonev et al., 2023; Gao et al., 2023), and thermonuclear fusion (Poels et al., 2023; Gopakumar et al., 2024), among others. However, compared to these settings, the simulation of two-phase flow problems with neural networks that we consider is relatively underexplored.\n\n"}
{"case_id": "11e71107ddfdc824b8b87d4f5a2ef843", "answer_text": "\n\nData preprocessing is a crucial step for point cloud datasets before performing surface reconstruction, as it helps improve the quality and accuracy of the reconstructed surfaces. The main preprocessing steps include outlier removal, denoising, downsampling or resampling, surface normal estimation, alignment, and segmentation [3]. Each of these steps addresses specific challenges associated with point cloud data, which can significantly impact the effectiveness of subsequent surface reconstruction algorithms [1]. The importance of each step and the typical success metrics used to evaluate them are as follows:\n\nOutlier Removal [4]\n\nOutliers are points that deviate significantly from the rest of the data and can adversely affect the surface reconstruction process. Outlier removal filters identify and eliminate these points to prevent them from introducing errors during reconstruction. Techniques for outlier removal may involve using local density computations or radius-based filtering, where points with low local neighbor density are considered outliers and removed. Effective outlier removal helps in maintaining the integrity and accuracy of the point cloud, which is crucial for producing high-quality reconstructed surfaces. Success metrics include the reduction in surface reconstruction errors and the percentage of outliers effectively removed [1][5][7].\n\nDenoising [1][4][8][9].\n\nPoint cloud data often contains noise due to various factors such as sensor inaccuracies and environmental conditions. Denoising smoothens the data by removing these noise points, ensuring that only relevant geometric information is retained. Common denoising techniques include statistical filters and moving least squares (MLS) methods, which help in reducing the noise while preserving important features of the point cloud. Reducing noise is particularly important as it prevents the introduction of artifacts and ensures that the reconstructed surface closely matches the original object. Success metrics for denoising involve measuring the smoothness and fidelity of surfaces post-process, as well as the retention of critical features [3][8][9].\n\nDownsampling or Resampling [1].\n\nHigh-resolution point clouds can be computationally expensive to process. Downsampling or resampling reduces the number of points in the cloud while maintaining the structural integrity of the data. This step helps in managing the data size and computational load while retaining the essential details needed for accurate surface reconstruction. Techniques like voxel grid downsampling and farthest point sampling (FPS) are commonly used for this purpose. Effective downsampling maintains the balance between computational efficiency and the preservation of critical geometric details necessary for accurate reconstruction. Success is typically evaluated based on the balance between reduced data size and error metrics in the reconstructed surfaces [1][5][8].\n\nSurface Normal Estimation\n\nSurface normals are critical for many reconstruction algorithms as they provide information about the orientation of the surface at each point. Accurate normal estimation is essential for high-quality surface reconstruction. Methods for normal estimation typically involve fitting a plane or higher-order surface to the local neighborhood of each point and computing the normal vector from this fit. Accurate normal estimation ensures that subsequent surface fitting algorithms can more precisely reconstruct the surface geometry, leading to better fidelity in the final model. The success of normal estimation is measured by the accuracy of the normals compared to the ground truth normals when available [5][11].\n\nData Alignment and Registration\n\nFor point clouds captured from multiple views or scans, aligning and registering these different datasets into a common coordinate system is crucial. This ensures that the entire surface can be reconstructed seamlessly. Techniques such as the Iterative Closest Point (ICP) algorithm are often employed for this purpose, which iteratively minimizes the distance between corresponding points in different scans. Good alignment and registration are essential for creating a coherent and continuous surface, particularly when integrating multiple scans. Success metrics include the reduction in registration error and the continuity in the reconstructed surface [7].\n\n\nSegmentation aimed to logically divide the point cloud into subsets, a set for each natural surface so that each subset including just those points group from a specific surface. However, subdividing objects in 3D point clouds is an essential task [3]. The sampling density of points is ordinarily unequal due to the divergent in scanners specifications. Likewise, the surface shape can be subjective with sharp features and the absence of a statistical distribution model for the data. These issues represent a challenge in developing a segmentation rule [3]. There are many methodologies that have been suggested for the segmentation of 3D point clouds. Generally, these methods can be categorized into five groups [3].\n\n\nSegmenting the point cloud into distinct subsets corresponding to different surfaces or objects can greatly facilitate the reconstruction process. Segmentation helps in isolating individual surfaces and treating them independently, thereby improving the accuracy and efficiency of the reconstruction [3]. Methods for segmentation include:\n\n - **Edge-Based Methods**: These methods identify boundaries within the point cloud using edge detection techniques, which are helpful in delineating sharp features and distinct surfaces [3]. \n- **Region Growing**: This approach starts from seed points and expands to include neighboring points that share similar properties, such as surface normals or curvature, effectively grouping points into meaningful regions [3].\n**Clustering**: Techniques like K-means clustering or DBSCAN are used to group points based on their spatial proximity and other features, which is particularly useful for segmenting large and complex point clouds [3]. \n\nEffective segmentation ensures that complex scenes are broken down into manageable parts, allowing for more precise and targeted reconstruction efforts. The effectiveness of segmentation is typically assessed based on the accuracy and completeness of the segmented regions compared to ground truth data [3]. Advantages of these techniques include improved handling of complex and varying surface geometries, as well as enhanced robustness to noise and outliers in the point cloud data.\n\n### Real-World Applications[0][8][12] \n\n\n\nPreprocessing steps like outlier removal, denoising, and alignment play crucial roles in various real-world applications [1]. For example, in architecture and construction, accurate point cloud data is essential for creating precise building information models (BIM) [3]. Outlier removal and denoising ensure that the models are free from anomalies that could mislead construction plans. In autonomous vehicle navigation, preprocessing steps help in creating accurate 3D maps from LiDAR scans, which are critical for vehicle localization and obstacle detection [8]. In medical imaging, preprocessing of point cloud data obtained from scans can aid in constructing detailed models of anatomical structures for surgical planning and simulation. \n\n### Application-Specific Considerations\n\nThe preprocessing steps may vary depending on the type of point cloud data and the intended application. For instance, point clouds obtained from LiDAR scans for autonomous vehicles may prioritize rapid denoising and alignment to ensure real-time navigation [8], whereas medical imaging data may require more precise segmentation and outlier removal to accurately model anatomical structures [3]. Architectural applications might emphasize the downsampling and alignment processes to manage large datasets and ensure the integrity of the reconstructed models [1].\n\nPreprocessing not only improves the accuracy of surface reconstruction but also simplifies the complexity of subsequent processing steps. By addressing the common imperfections and noise in the raw point cloud data, preprocessing ensures that the reconstructed surfaces are more accurate and reliable, providing a solid foundation for further applications such as reverse engineering, simulation, and analysis [1][4][6].\n\n\nReferences:\n[0] Surface Reconstruction from Point Clouds by Learning Predictive Context Priors To resolve this issue, we propose to learn SDFs as a Predictive Context Prior for highly accurate surface reconstruction from point clouds, as shown in Fig. 1. Specifically, we first train a neural network to represent local SDFs of local regions across a large dataset of point clouds. This aims to capture a local context prior in a local coordinate system, similar as in previous work. Our main contribution is that during surface reconstruction at inference time, we specialize the pre-trained local context prior into a Predictive Context Prior for a specific point cloud by learning Predictive Queries. More concretely, Predictive Queries learn to predict query locations for the pre-trained local SDF from queries given in the global coordinate system of the specific point cloud. This is achieved by sampling a set of queries in the global coordinate system, and learning to predict queries for the local SDF to minimize surface reconstruction error.\n\n[1] Surface Reconstruction from Point Clouds: A Survey and a Benchmark While existing surface reconstruction methods can be compared using the data prepared in Section 5.1, for most of them, their performance could be greatly improved via some standard pipeline of point cloud pre-processing.In this work, we compare existing methods both without and with such a pre-processing pipeline. For synthetic data, the pipeline is in the order of outlier removal, de-noising, and point re-sampling; details are given as follows. For real-scanned data, we use the inbuilt pre-processing of different scanners, and use a final step of FPS to re-sample 200,000200000200,000 points for each point cloud.\n\n[2] Surface segmentation and reconstruction in reverse engineering using MATLAB will be introduced to execute the methodology. Point cloud data preprocessing The precept of processing the data is to reduce massive data and smooth it in the premise of not downing the precision of the generated surface. Figure 1 shows the regular-used pre-processing procedure. The commonly used pre-processing procedure passes through the following major steps but using different approaches.  Delete noise data.  Data smoothing.  Data reduction. The characteristics of data are determining the detailed and sequence of the processes. Favourable preprocessing will secure the required accuracy in data for the following stages; not just uptrend the accuracy of the following stages but also eliminate the complexity of the following stages. Surface segmentation Segmentation aimed to logically divide the point cloud into subsets, a set for each natural surface so that each subset including just those points group from a specific surface. However, subdividing objects in 3D point clouds is an essential task. The sampling density of points is ordinarily unequal due to the divergent in scanners specifications. Likewise, the surface shape can be subjective with sharp features and the absence of a statistical distribution model for the data. These issues represent a challenge in developing a segmentation rule. There are many methodologies that have been suggested for the segmentation of 3D point clouds. Generally, these methods can be categorized into five groups; as shown in figure 2. Edge Based Method (EBM) The EBM principle is attempting to find borders in the point data representing ledges between surfaces. If\n\n[3] Surface Reconstruction from Point Clouds: A Survey and a Benchmark releasing the whole benchmark to facilitate the community research. More specifically, we randomly sample 22 synthetic surfaces of object instances, whose distribution in the three groups of low-, middle-, and high-complexity is 12 : 6 : 4; as described in Section 4.1, for each instance we conduct six ways of synthetic scanning, which produce either a clean point cloud or point clouds with various imperfections, giving rise to a total of 308 input-output pairs for benchmarking algorithms. We also use all the 10 synthetic scene surfaces and all the 20 real-scanned surfaces for the studies. For some of existing methods using learning-based priors, we prepare an auxiliary set of training data consisting of surfaces from ShapeNet and ABC, and also the remaining synthetic data of object instances from our benchmark. Pre-processing While existing surface reconstruction methods can be compared using the data prepared in Section 5.1, for most of them, their performance could be greatly improved via some standard pipeline of point cloud pre-processing. In this work, we compare existing methods both without and with such a pre-processing pipeline. For synthetic data, the pipeline is in the order of outlier removal, denoising, and point re-sampling; details are given as follows. For real-scanned data, we use the inbuilt pre-processing of different scanners, and use a final step of FPS to re-sample 200, 000 points for each point cloud. Outlier removal -Performance of surface reconstruction degrades severely when extreme outliers exist in a point cloud; fortunately, these outliers are easy to be removed. We use a\n\n[4] Computational Fluid Dynamics on 3D Point Set Surfaces in Figure 4, the dataset has also a lot of hidden parts. These datasets are chosen to cover a wide range of 3D scanning techniques. This decision is also motivated by specific CFD applications, trying to cover a broad range of industrial uses. Each simulation test case is described in Section 6. Point cloud preprocessing In Section 4, we present the EIMLS implicit scalar function computed from point clouds. To do so, the point cloud must be preprocessed to remove the outliers, be subsampled, and compute surface normals. We present these preprocessing steps on the three datasets. More details on why these specific preprocessing steps are needed are given in Section 4. Outliers removal Real-world point clouds always contain outlier points due to laser echo misinterpreted by the scanner software. An easy approach for removing these outliers is to compute the local density on each point and suppress points with too low density. The density can be computed from various ways. For the MMS dataset, we chose to compute the distance to the third closest point and to use a 30 cm threshold. The TLS scan features more challenging structured noise due to the laser hitting sharp corners and producing a ghost trailing set of points in the laser shooting axis. To get rid of these points, we compute unit normal of the i-th point (lying on the unit sphere) S P underlying smooth surface sampled by P α : R d → R implicit function that represents S P Table 1: Notations and definitions for\n\n[5] Study on Wiped Area Measurement Method Based on 3D Scanning  The abnormal data points usually include singular points, noise points and other data, which should be removed in the pretreatment. Common point clouds data preprocessing methods include: removing abnormal data, filling in the missing points, data smoothing, filtering and de-noising, merging of multi-views clouds or redundant data and feature extraction. Satisfactory point clouds can be obtained after data simplification, as shown in Figure 5. Surface reconstruction and error adjustment Surface reconstruction is to use the pre-processed point clouds data to construct an approximate model close to the windshield surface through differencing or fitting methods. With the functions of rapid construction of freedom surface modeling and surface editing tools, specialized reverse engineering software on the market allow users to dynamically adjust the surface until requirements are met. The real-time surface detection tool can detect the error and continuity between surface and point clouds, so that users can conveniently construct a high quality surface. After surface being modeled and reconstructed, it is necessary to check surface precision, curvature and continuity. The difference between the reconstructed windshield surface and the point cloud can be viewed intuitively by using the error checking tool of professional software. Throughout the error of the model analysis, as shown in Figure 6, the maximum error is 0.35 millimeters, the standard deviation is 0.099 millimeters. The error does not meet the accuracy requirement, so it is need to improve the precision and smoothness of reconstruction in the further adjustment. After optimization, the reconstruction is relatively satisfied, as shown in Figure 7, the maximum error\n\n[6] Underwater 3D Reconstruction Based on Geometric Transformation of Sonar and Depth Information the corresponding translation matrix and rotation matrix, respectively. Then all the second scan points   can be transformed to the same coordinate system as the first scan. Similarly, we can calculate each scan's translation displacement and rotation angle relative to the last scan, and transform all the other scans to the same coordinate system as the first scan, just as equation (4). 1 1 With all the measurements scanned at different depths transformed to the coordinate system of the first scan, we can process the surface reconstruction of underwater environment with the point cloud data from sonar and depth information. Surface reconstruction of point clouds data For the point cloud data we collect is discrete and irregular, we should preprocess the point cloud at first before surface reconstruction. Outlier removal Radius outlier removal filter is implemented to remove the outliers. The principle is as shown in figure.4. At first, we set each point as the centre of a circle with radius R, respectively. Then, we set the minimum neighbours represented using N within R. If the numbers of the neighbours within R is Surface fitting based on MLS As we all know, the cloud data can't be connected together simply to process surface fitting, especially when it is discrete. If we know the style of the surface, we can use least square method to get the fitting surface by solving linear equation sets. However, the least square method won't work if the discrete point cloud data is larger and the style of the surface is\n\n[7] Comparison of Depth Camera and Terrestrial Laser Scanner in Monitoring Structural Deflections the deflection. Pre-processing of the point cloud data is involves preparing the data before proceeding to the data processing step. LiDAR point cloud data inputs to tools found in the surrounding must be pre-processed to remove outliers and/or noise, errors, and non-target data points through manual editing. To decrease the errors resulting from manual cropping of the targeted area from the raw data, we processed the data multiple times in each case and took the average. Denoising When acquiring point cloud data, owing to the impacts of equipment accuracy, operator experience, environmental conditions, and other factors, some noise points (that is, points we cannot use) are inevitable in point cloud data. We need to remove these points before processing the data. Outlier and noise removal(denoising) in pre-processing is interpreted differently by many researchers. However, to the best of our knowledge, outlier removal uses a noise removal filter that permits deletion of the lonely points outside a detected surface, whereas the noise remover filter removes points that do not match the local geometrical behavior of the point cloud. Specifically, data from both sensors were subjected to different denoising techniques for better output. The following two subsections describe the techniques we used on raw data before processing it. Interquartile Range Outliers are part of the data that are distant from other observations. Although outliers are unknown in the acquisition phase, they may result from result from errors during data collection and indicate variance in our data. In a vast database, differentiating the outliers from the actual\n\n[8] Pre-Processing and Surface Reconstruction of Points Cloud Based on Chord Angle Algorithm Technique points cloud is involving some unnecessary and noisy data points, due to the resolution of some scanners and the reflectivity property of the scanned surfaces, also the contrast problems. Point cloud acquiring using a 3D laser scanner may be contained more information for a ten of thousands of massive data, so that, the huge data points in cloud which used to surface model reconstruction, subsequent processing, and storage, represent the main difficulties in 3D laser scanning process. In other word, not all of the data points can be used in follow-up process. Therefore, there is need to extract only the data point clouds which reflect the surface of scanned shape, and removing the redundant data points. Therefore, it's essential to determine the noisy data and delete it for the points cloud set, as manner to gives the pretty representations of the 3D scanned surfaces, and remining the required geometrical features for surface. So, that, the pre-processing of 3D points cloud and reconstruction of surface from these points cloud, has more attention of researchers in this domain. Many researchers showed in their literatures different algorithms for simplification of data points obtained from 3D laser scanners. J. Liu et al proposed an effective and simple reconstruction algorithm based on interpolation of B-spline surfaces of blade surface by section curves with less acquired points, to improve the aerodynamic performance for the blade surfaces. The less acquired points of the section curves were gritted using the Coordinate Measuring Machine. B. Cyganek et al proposed an approach for\n\n[9] Point Cloud Upsampling and Normal Estimation using Deep Learning for Robust Surface Reconstruction Since there are no datasets directly available for point cloud upsampling with normal inference, we create them artificially by downsampling point clouds from existing point cloud datasets.We use the PU-Net [Yu et al., 2018b dataset providing point cloud patches segmented from large meshes.The dataset consists of 4000 patches containing 4096 points in each point cloud with corresponding normals.We use non-uniform downsampling to extract 1024 points from each point cloud to feed them as input to the network.To avoid overfitting, we apply on-the-fly data augmentation using random shifting, rotation, scaling, and adding random Gaussian noise.\n\n[10] Surface Reconstruction from Point Clouds: A Survey and a Benchmark Datasets and BenchmarksExisting datasets that support surface reconstruction studies are based on synthetic or real-scanned data; they may include object- and/or scene-level surfaces. For synthetic datasets, surface meshes are usually provided from which point clouds can be sampled. For example, the ShapeNet and ModelNet are two commonly used synthetic datasets consisting of simple, object-level shapes. More complex synthetic object surfaces are provided in the datasets of 3DNet, ABC, Thingi10k, and Three D Scans. The datasets of SceneNet and 3D-FRONT provides synthetic, scene-level surfaces. In the meanwhile, there exist datasets of real-scanned, object-level surfaces, 26, 27 and those of real-scanned, scene-level surfaces, 29; however, due to the lack of high-precision scanning, their reconstruction ground truths are usually obtained by appropriate surface reconstruction algorithms, which jeopardizes their roles for benchmarking different methods.Most of the above datasets do not consider sensing imperfections that may appear in practically scanned point clouds, except for that uses virtual scanning to simulate point cloud imperfections; however, the dataset is relatively small, with only eight instances of object surfaces.In contrast, our contributed benchmarking dataset is more comprehensive, including both synthetic and real-scanned data, and covering both object- and scene-level surfaces; we intentionally inject various sensing imperfections into point cloud data of the dataset, including point-wise noise, non-uniform distribution of surface points, point outliers, missing of surface points, and misalignment among point sets from multi-view scanning. We expect our benchmark would facilitate more thorough studies in future research.\n\n"}
{"case_id": "ce433b751f38f7d0173095c2faa2f75a", "answer_text": "\n\nSelecting the right number of topics is crucial for effective topic modeling, and various methods exist to aid this decision [9]. Each method has its own advantages and disadvantages [10]. Below, I have organized the answer with subheadings for better readability.\n\nPerplexity and Coherence Metrics[10].\n\nPerplexity and coherence are two of the most commonly used metrics for evaluating topic models. Perplexity measures how well a statistical model predicts a sample, with lower perplexity indicating better generalization performance [0]. The coherence measure, on the other hand, evaluates the degree of semantic similarity between the top words in a topic, with higher coherence indicating more interpretable topics [10]. \n\n**Advantages:**\n[0] This method provides four advantages to selecting the optimal number of topics: (1) good predictive ability, (2) high isolation between topics, (3) no duplicate topics, and (4) repeatability [0]. First, we use three general datasets to compare our proposed method with existing methods, and the results show that the optimal topic number selection method has better selection results [0]. Then, we collected the patent policies of various provinces and cities in China (excluding Hong Kong, Macao, and Taiwan) as datasets. By using the optimal topic number selection method proposed in this study, we can classify patent policies well [0]. \n1. **Perplexity:** It is a well-established metric that aligns well with the likelihood framework used in probabilistic models like LDA [0][3].\n2. **Coherence:** It focuses on the semantic similarity of words, making it easier to judge the meaningfulness of topics [1][4].\n\n\n**Disadvantages:** \n\n- One-time computation [1].\n- Multiple sampling increases computational time consumption [1]. \n- It is unqualified to work with a small amount of data  [2].\n- Labor-intensive inspection is required due to the excessive number of topics generated [14].\n\n1. **Perplexity:** While useful, low perplexity does not always guarantee interpretable or meaningful topics and can often lead to overfitting [3][7].\n2. **Coherence:** This metric requires additional word embeddings or external resources and may not always align with the practical use of the model [9].\n\n**Real-World Applications:** Topic modeling is widely used in various domains due to its ability to handle large volumes of unstructured text and uncover hidden topics within documents. The Latent Dirichlet Allocation (LDA) method, compared to other techniques, is advantageous in drawing a number of different topics from a vast scope of informal data, which leads to ready interpretation of results and solves the problem of overfitting [7]. Additionally, topic modeling techniques, such as BERTopic, offer features like multilingual analysis and support for variations such as dynamic topic modeling, making them versatile tools in both social sciences and hard sciences [5]. Selecting the optimal number of topics is crucial for effective topic modeling, and various methods, including perplexity and coherence scores, have been proposed to aid in this process [0]. \n- **Perplexity:** Often used in academic research for tuning language models in natural language processing (NLP) tasks [3].\n\n- **Coherence:** Employed in fields like social sciences to ensure topics generated from survey data or social media posts are meaningful and interpretable by human analysts [5]. \n\n\n### Rate of Perplexity Change (RPC) [3]. \n\nThe rate of perplexity change (RPC) is a heuristic method that evaluates the variation in perplexity as the number of topics increases [3]. \n\n**Advantages:** [0] \n1. **Efficiency:** It focuses on finding the point where increasing the number of topics offers diminishing returns, making it computationally less intensive than exhaustive search methods [3].\n2. **Stability:** This method has been shown to provide stable results across different data types and corpora [4].\n\n**Disadvantages:** The main disadvantage of LDA is its inability to work effectively with a small amount of data [2]. Additionally, the process of developing an optimal topic model can be arduous and requires systematic sensitivity studies to find the best set of model parameters [3]. Another significant disadvantage is the lack of consensus on how to determine the appropriate number of topics, which often requires subjective evaluations [4]. Moreover, LDA can produce an excessive number of topics, making the model too complex for practical use [13]. There is also the issue that traditional topic models like LDA cannot generate topic distributions within a single document, assigning each document to a single topic instead [2]. \n1. **Heuristic Nature:** Being heuristic, it may not always find the global optimal number of topics and might miss subtle nuances present in the data [3][4].\n\nReal-World Applications: Topic modeling has been applied effectively across various domains, demonstrating its versatility and stability [5]. For instance, Latent Dirichlet Allocation (LDA) has been used to analyze social media discourse and to classify patent policies, illustrating its utility in processing large datasets of unstructured text [0]. Additionally, topic modeling techniques such as BERTopic have proven useful in multilingual analysis and in reducing the number of topics to more manageable sets for qualitative analysis [2].\n\n\n- **RPC:** Applied in iterative development processes where computational efficiency is critical, such as in real-time analytics for content recommendation systems [3].\n\n### Bayesian Non-Parametric Methods\n\nBayesian non-parametric methods generally do not assume a fixed number of parameters for the model, which allows for more flexibility in terms of the model's complexity. A widely-used Bayesian non-parametric method is the Dirichlet Process (DP), which can be understood as a distribution over distributions, producing an infinite mixture model. The DP is often used in clustering problems where the number of clusters is not known a priori [14]. Latent Dirichlet Allocation (LDA) is another example of Bayesian non-parametric methods, frequently used for topic modeling [3]. This method assigns a distribution over a potentially infinite set of topics to each document within a corpus, enabling the model to adapt to the complexity of the data [0].\n\n\n\nHierarchical Dirichlet Process (HDP) is a popular Bayesian non-parametric approach that extends LDA by allowing the number of topics to be inferred from the data [1]. \n\n**Advantages:**[0][1][3][4][5][7][8][9][12]\n\n\n1. **Flexibility:** HDP automatically determines the number of topics, which can be particularly useful for datasets where the true number of topics is unknown [0][1].\n2. **Adaptability:** It adapts to the complexity of the data, adding more topics as necessary to best represent the underlying structure [3].\n\n**\n\nDisadvantages:\n\nThere is no consensus in the literature on the optimal number of topics, making it difficult to determine a suitable number [9]. Selecting an appropriate number of topics is crucial for further analysis and remains a central task for researchers [13]. The qualitative consideration can be complemented and assisted by statistical measures [13]. However, model development can be arduous and tedious, and often requires time-consuming subjective evaluations to compare models [3]. \n\n**\n1. **Complexity:** The computational cost and complexity increase significantly, making it less feasible for very large datasets [1].\n2. **Overfitting:** It can sometimes generate too many topics, making the model less interpretable and harder to use in practice [7].\n\nReal-World Applications such as topic modeling in various domains, including patent policy analysis, social media data, and multilingual text collections, have demonstrated the versatility and effectiveness of methods like LDA and BERTopic in uncovering hidden themes and patterns within large datasets [3]. Another compelling use case is the selection of the optimal number of topics, which significantly influences the quality and interpretability of the generated topics [9]. Research underscores the importance of using reliable metrics, like perplexity and coherence, to determine the most appropriate number of topics to enhance model performance and stability [10].\n- **HDP:** Ideal for exploratory data analysis in large-scale datasets such as those found in digital marketing or bioinformatics, where the number of underlying topics is not predefined [8]. \n\n### Practical Approaches\n\nPractical approaches for selecting the optimal number of topics in topic models include using comprehensive judgment indices, heuristic approaches, and various algorithmic adjustments. According to recent studies, methods that incorporate perplexity, isolation, stability, and coincidence have shown superior results in topic selection across multiple datasets [0]. Another commonly adopted heuristic method examines the rate of perplexity change (RPC) to estimate the optimal number of topics [3]. This method has demonstrated stability and effectiveness across different types of data, simplifying the otherwise burdensome process of parameter sensitivity analysis [4]. Moreover, some frameworks offer a more empirical basis for evaluation by creating synthetic corpora to measure how well algorithms assign topics at the token level, providing a clearer measure of topic modeling accuracy [12]. \n\nAdditionally, BERTopic has emerged as a robust option due to its ability to work well across various aspects of topic modeling, from dynamic to class-based implementations. It is especially useful in contexts involving multiple languages or complex data structures [2]. Meanwhile, traditional methods like LDA remain advantageous for extracting numerous topics from extensive datasets, permitting easy interpretation and mitigating the problem of overfitting [7]. Ultimately, the choice of method might also depend on qualitative assessments and the specific needs of the research, such as the required level of detail and the nature of the data [10]. \n\nPractical methods often combine multiple metrics and qualitative evaluation. For instance, generating several models with different numbers of topics and manually comparing their interpretability can provide a more balanced approach [9].\n\n\n\nAdvantages: [0]\n\n\n1. **Comprehensive:** Combining multiple metrics and qualitative assessments often yields a more balanced and well-rounded model [10][13].\n2. **Human Insight:** Incorporating human judgment helps to ensure the topics are not just statistically sound but also meaningful and usable [13].\n\n\nDisadvantages: [1]\n\n1. **Time-Consuming:** This approach is labor-intensive and may require significant time and effort, especially when working with large datasets [9][10].\n2. **Subjectivity:** The qualitative aspect introduces subjectivity, which might lead to differing results depending on who is evaluating the topics [8][9].\n\nReal-World Applications: Topic modeling has been widely applied across various domains. For instance, Latent Dirichlet Allocation (LDA) is the most frequently used topic modeling method and has demonstrated its usefulness in drawing different topics from a vast scope of informal data, especially useful for analyzing large numbers of documents [7]. Another example is the use of BERTopic, which employs pre-trained transformer-based language models to generate document embeddings and cluster these embeddings to form coherent topic representations [14]. There are also heuristic approaches to determine an appropriate number of topics in topic modeling, such as the rate of perplexity change (RPC) method, which is demonstrated to choose the best number of topics for different types of grounded-truth datasets [3]. These various approaches illustrate the versatility and practicality of topic modeling methodologies in handling large and complex datasets. \n\n- **Combination Methods:** Frequently used in market research to categorize customer feedback or reviews, ensuring that the identified topics align with human understanding and business needs [9].\n\n\n**Implementation Guide for Combined Methods:**[3] \nWhen combining methods, it's essential to start with computational metrics like perplexity and coherence to narrow down the range of topic numbers [0]. Next, apply heuristic methods such as RPC for fine-tuning [3]. Finally, conduct qualitative evaluations to ensure the topics are meaningful and actionable [13]. This layered approach ensures a balance between computational efficiency and interpretability.\n\nAdvanced Methods using Embeddings [5]. \n\nModern topic models such as BERTopic leverage embeddings generated from pre-trained transformer-based models, followed by clustering algorithms [14]. \n\n **Advantages:**\n\nThere are several advantages to using topic models for our task. First, they naturally model the class-based nature of selectional preferences, but don't take a pre-defined set of classes as input. Instead, they compute the classes automatically [8]. This leads to better lexical coverage since the issue of matching a new argument to a known class is side-stepped [8]. Second, the models naturally handle ambiguous arguments, as they are able to assign different topics to the same phrase in different contexts [8]. Inference in these models is also scalable -linear in both the size of the corpus as well as the number of topics [8].\n\nMoreover, there are several scalability enhancements such as SparseLDA (Yao et al., 2009), and an approximation of the Gibbs Sampling procedure can be efficiently parallelized (Newman et al., 2009) [8]. Finally we note that, once a topic distribution has been learned over a set of training relations, one can efficiently apply inference to unseen relations (Yao et al., 2009) [8]. \n1. **High Coherence:** These methods often produce highly coherent topics due to the semantic richness captured by transformer-based language models [2][5].\n2. **Flexibility:** They can handle complex and multi-lingual datasets effectively, making them versatile for various applications [2][5].\n\n**Disadvantages:** The main disadvantage of Top2Vec, however, is that it is unqualified to work with a small amount of data (Abuzayed and Al-Khalifa, 2021; e.g., <1,000 documents) [2]. \n1. **Computational Cost:** The use of transformer models can be computationally expensive, making it less feasible for resource-constrained environments [5].\n2. **Over-Segmentation:** Embedding-based models might generate too many topics, leading to an excess number of very niche topics that require additional steps for merging and fine-tuning [5][14].\n\n**Real-World Applications:** [14].\n- **BERTopic:** Widely used in text mining applications for social media analysis, news aggregation, and customer sentiment analysis due to its ability to process and interpret short and unstructured text efficiently [5]. \n\n### Conclusion\n\nChoosing the optimal number of topics in an LDA topic model is crucial for ensuring good predictive ability, high isolation between topics, no duplicate topics, and repeatability [0]. While there is no consensus on a single best method for determining the appropriate number of topics, various approaches such as perplexity, coherence, and other statistical measures are commonly used [13]. Additionally, methods like BERTopic and Top2Vec offer advantages in scalability and handling vast quantities of data, but may generate more topics than expected, requiring labor-intensive inspection [2]. Researchers often need to balance between quantitative performance metrics and qualitative interpretability to select the most suitable number of topics for their specific needs [10]. Overall, the selection process can be complex and may benefit from a combination of heuristic and statistical methods to achieve the best results [3]. \n\nIn conclusion, the optimal method for selecting the number of topics in topic modeling highly depends on the specific requirements of the task, computational resources, and the nature of the dataset [3]. Employing a combination of metrics and qualitative evaluation often provides a balanced and comprehensive approach to identifying the optimal number of topics [13].\n\n\nReferences:\n[0] Selection of the Optimal Number of Topics for LDA Topic Model—Taking Patent Policy Analysis as an Example This study constructs a comprehensive index to effectively judge the optimal number of topics in the LDA topic model. Based on the requirements for selecting the number of topics, a comprehensive judgment index of perplexity, isolation, stability, and coincidence is constructed to select the number of topics. This method provides four advantages to selecting the optimal number of topics: (1) good predictive ability, (2) high isolation between topics, (3) no duplicate topics, and (4) repeatability. First, we use three general datasets to compare our proposed method with existing methods, and the results show that the optimal topic number selection method has better selection results. Then, we collected the patent policies of various provinces and cities in China (excluding Hong Kong, Macao, and Taiwan) as datasets. By using the optimal topic number selection method proposed in this study, we can classify patent policies well.\n\n[1] Selection of the Optimal Number of Topics for LDA Topic Model—Taking Patent Policy Analysis as an Example one-time computation. In addition, it should be noted that if we want to obtain a more reliable optimal number of topic selection, we also need to perform multiple sampling, which will increase the computational time consumption of the optimal topic selection method. In summary, our proposed optimal topic number selection method has several advantages. First, in terms of the number of topics selected, the optimal topic number selection method can better select the appropriate number of topics for the LDA model. Compared with several other topic number selection methods, the optimal number of topics derived from our method is closest to the real number of topics. In our example, the number of topics selected by the perplexity method and the coherence method is much smaller than the real number of topics. The optimal number of topics selected by the JS divergence method is at the maximum boundary of the topic selection interval, which makes us skeptical about whether this selected number of topics is optimal. The optimal number of topics selected by the HDP method is much larger than the true number of topics. Second, from the score curves, the score curve of the number of topics derived from our proposed optimal topic number selection method is closest to a U-shaped curve, which makes it possible to find the optimal extreme value point in the score curve. On the one hand, the score curve derived from the perplexity method has an optimal point at the position where the number of topics is small. However, when the\n\n[2] A Topic Modeling Comparison Between LDA, NMF, Top2Vec, and BERTopic to Demystify Twitter Posts modeling variations, such as guided topic modeling, dynamic topic modeling, or class-based topic modeling. Its main strength lies in the fact that the algorithm performs well on most aspects of the topic modeling domain, whereas others typically excel in one single aspect. Additionally, after having trained a BERTopic model, it is also possible to reduce the number of topics (Sánchez-Franco and Rey-Moreno, 2022), subsequently allowing researchers to settle on a number of (realistic) topics based on how many were actually produced. Slightly different from BERTopic and the implementation of c-TF-IDF, Top2Vec creates jointly embedded word, document, and topic vectors to find topic descriptions (Angelov, 2020). The intuition behind this algorithm is that every input is considered a vector, and pivoting between them is trivial. Hence, Top2Vec can scale a large number of topics and vast quantities of data. Such strength is especially required when multiple languages emerge within a corpus (Hendry et al., 2021). The main disadvantage of Top2Vec, however, is that it is unqualified to work with a small amount of data (Abuzayed and Al-Khalifa, 2021; e.g., <1,000 documents). In fact, BERTopic and Top2Vec have a number of issues in common. For example, although outlier generation might be beneficial in some cases, the solutions might actually generate more outliers than expected. Meanwhile, another flaw involves topic distributions: they cannot be generated within a single document because each document is assigned to a single topic. Although probabilities can indeed be extracted, they are not equivalent to an actual topic distribution. With regards to NMF and LDA,\n\n[3] A heuristic approach to determine an appropriate number of topics in topic modeling. Topic modelling is an active research field in machine learning. While mainly used to build models from unstructured textual data, it offers an effective means of data mining where samples represent documents, and different biological endpoints or omics data represent words. Latent Dirichlet Allocation (LDA) is the most commonly used topic modelling method across a wide number of technical fields. However, model development can be arduous and tedious, and requires burdensome and systematic sensitivity studies in order to find the best set of model parameters. Often, time-consuming subjective evaluations are needed to compare models. Currently, research has yielded no easy way to choose the proper number of topics in a model beyond a major iterative approach. Based on analysis of variation of statistical perplexity during topic modelling, a heuristic approach is proposed in this study to estimate the most appropriate number of topics. Specifically, the rate of perplexity change (RPC) as a function of numbers of topics is proposed as a suitable selector. We test the stability and effectiveness of the proposed method for three markedly different types of grounded-truth datasets: Salmonella next generation sequencing, pharmacological side effects, and textual abstracts on computational biology and bioinformatics (TCBB) from PubMed. The proposed RPC-based method is demonstrated to choose the best number of topics in three numerical experiments of widely different data types, and for databases of very different sizes. The work required was markedly less arduous than if full systematic sensitivity studies had been carried out with number of topics as a parameter. We understand that additional investigation is needed to substantiate the method's theoretical basis, and to establish its generalizability in terms of dataset characteristics.\n\n[4] A heuristic approach to determine an appropriate number of topics in topic modeling and textual documents, demonstrating some generalizability across data types. Choosing the best number of topics is an omnipresent concern in topic modelling, as well as other latent variable methodologies. The comparatively simple RPC-based heuristic we propose could simplify topic model development, generally, for many applications, and offer an easier means to use development time for better fine tuning of models. evaluation, and assisted with writing the manuscript. All authors read and approved the final manuscript.\n\n[5] A Topic Modeling Comparison Between LDA, NMF, Top2Vec, and BERTopic to Demystify Twitter Posts sides of applying embedded vs. standard topic models, but it also offers social science researchers insights into methodological challenges that may hinder knowledge generation. Foreseeing that social scientists may indeed hesitate to choose an appropriate algorithm when analyzing social media data, this study presents possible methodological issues and promotes the efficacy of two different types of topic models. To be more precise, applying BERTopic to generate insights from short and unstructured text offers the most potential when it comes to embedding-based topic models. Thus, this study acknowledges the capability of BERTopic to encode contextual information (Chong and Chen, 2021), an aspect that may remain concealed by other models. Regarding traditional topic model algorithms, social science research is encouraged to consider NMF as an alternative approach to the commonly-adopted LDA (Gallagher et al., 2017). Certainly, however, it is essential to note that each model has its own strengths and shortcomings, and the findings require intensive qualitative interpretation. Finally, this study also strives to make another important contribution by outlining • High versatility and stability across domains • Allows for multilingual analysis • Supports topic modeling variations (guided topic modeling, dynamic topic modeling, or class-based topic modeling) • It uses embeddings, so no preprocessing of the original data is needed • Automatically finds the number of topics • Supports hierarchical topic reduction • Contains built-in search functions (easy to go from topic to documents, search topics, etc.) • Broader support of embedding models than Top2Vec • The embedding approach might result in too many topics, requiring labor-intensive inspection\n\n[6] Determination of Motivating Factors of Urban Forest Visitors through Latent Dirichlet Allocation Topic Modeling possible trends within the community. Notably, the LDA method, compared to the other topic modeling techniques, is advantageous in drawing a number of different topics (frames) from a vast scope of informal data, as it leads to ready interpretation of results and solves the problem of overfitting. Such an algorithm is especially useful for identifying hidden topics and themes based on the word co-occurrence for each document in the corpus. It can be applied when the task requires the analysis of topics across a large number of documents since it is relatively simple for a person to analyze the topics in one or two documents, but the analysis of topics across hundreds or thousands of different documents is not easy. In addition, topic modeling is a good way to \"let the text talk\" because the identified topics do not depend on the evaluators' individual perspectives or experiences. Moreover, there are numerous possible extensions of LDA, hence LDA topic modeling has been recently applied across fields,33,34. In Figure 1, the hyperparameter is the topic number set directly by the user. It is the number that determines the specific number of words, that is, the number of theme words and keywords contributing to the topic, while the optimum number of documents depends on the number of topics determining the number of clusters that point toward the theme words. The formation of a single topic is thus proceeded by converging the words with a high probability contribution, and if the number of words under a topic is\n\n[7] A Latent Dirichlet Allocation Method for Selectional Preferences initial code for our samplers using the Hierarchical Bayes Compiler (Daume III, 2007). Advantages of Topic Models There are several advantages to using topic models for our task. First, they naturally model the class-based nature of selectional preferences, but don't take a pre-defined set of classes as input. Instead, they compute the classes automatically. This leads to better lexical coverage since the issue of matching a new argument to a known class is side-stepped. Second, the models naturally handle ambiguous arguments, as they are able to assign different topics to the same phrase in different contexts. Inference in these models is also scalable -linear in both the size of the corpus as well as the number of topics. In addition, there are several scalability enhancements such as SparseLDA (Yao et al., 2009), and an approximation of the Gibbs Sampling procedure can be efficiently parallelized (Newman et al., 2009). Finally we note that, once a topic distribution has been learned over a set of training relations, one can efficiently apply inference to unseen relations (Yao et al., 2009). Experiments We perform three main experiments to assess the quality of the preferences obtained using topic models. The first is a task-independent evaluation using a pseudo-disambiguation experiment (Section 4.2), which is a standard way to evaluate the quality of selectional preferences (Rooth et al., 1999;Erk, 2007;Bergsma et al., 2008). We use this experiment to compare the various topic models as well as the best model with the known state of the art approaches to selectional preferences. Secondly, we show\n\n[8] Combining Topic Modeling with Grounded Theory: Case Studies of Project Collaboration suggested in the literature (Griffiths and Steyvers, 2004). In particular, we set alpha to 0.5, beta to 0.02, and the number of words in a topic to 10. With regards to the number of topics, there is no consensus in the literature on a robust setting (Dimaggio et al., 2013). Unfortunately, the literature has not agreed on a solution to determine the right number of topics. While some scholars use error measuring-based algorithms to decide the optimum number of topics, most social science scholars either decide on a number (Croidieu and Kim, 2018) or generate several alternative numbers of topics (Baumer et al., 2017) and use the most relevant one for the analysis. For social sciences, the second approach for setting the number of topics is the most popular one (Dimaggio, 2015;Maier et al., 2018;Nelson, 2020). Since the proposed methodology uses the big data approach, this choice of the number of topics or words has not significantly changed the result. For example, when we changed the number of words from 10 to 12, the TM added 2 new relevant words to the existing ones. In this example, we choose three alternatives for the number of topics (40, 50, and 60 topics). Then we compared topic groups and sought the similarity between 40-50 topics, 40-60 topics and 50-60 topics. We saw that the similarity of 40-50 and 40-60 are higher than the similarity of 50-60. While 85% and 87.5% of 40 topics were covered in 50 topics and 60 topics, respectively, 60 topics compromised 78% of 50\n\n[9] Topic Modeling: How and Why to Use in Management Research a small number of topics (less than 20). This polarization of choice of the number of topics to be generated in topic modeling is caused by a split in the influences that the studies based their analyses. If a researcher comes from a traditional social sciences background, he or she will usually choose a small number of topics in order to focus on a comprehensive and meticulous description and discussion of each topic. On the other hand, if the background of the researcher is a more quantitative \"hard science\", then the number of topics will be higher because of the focus might be getting the best model fit to the data (this is generally achieved by an immense number of topics in a model). There is no consensus in the prescription of the number of topics that a researcher must anchor their decision. Some argue that it depends on the level of 'resolution' a social scientist desires to obtain (Nikolenko, Koltcov & Koltsova, 2017). While others argue that it depends on the performance metrics of the model: such as perplexity (Blei et al., 2003) or coherence (Mimno et al., 2011). Perhaps a more sensible approach would be to let the performance metrics guide you but leave the final decision pending a thorough inspection by the researchers (DiMaggio, 2015). Some studies depicted in table 1 also employ further quantitative analysis of the data generated by topic models. As already covered in previous sections, researchers can apply topic modeling to textual data to generate non-biased categories and labels.\n\n[10] A new evaluation framework for topic modeling algorithms based on synthetic corpora In this work, we present a new framework for topic model evaluation relying on generating a synthetic corpus containing an unambiguous ground truth.First, we propose a novel way to generate synthetic corpora that generalizes upon previous approaches.Our approach allows us to isolate the impact of various corpus characteristics, such as size, number of topics, the signal-to-noise ratio, burstiness, or fraction of stopwords, which in real-world corpora are either unknown or impossible to tone.Second, we propose a new evaluation metric based on the normalized mutual information that compares the agreement between planted and inferred topics on the level of individual word tokens.Our approach yields an absolute measure of topic modeling accuracy, eliminating the need for post-inference heuristics such as “topic matching”.While synthetic ground truth has been used for topic model evaluation in the past, ours is the first framework for evaluating how well topic modeling algorithms perform the key task of inferring per-token topic assignments.Altogether, the formalization of synthetic corpora allows us to probe more accurately the ability of different topic modeling algorithms to resolve a wide range of topic structures, beyond simplistic assumptions of LDA.We present experiments showing how different popular topic modeling algorithms fare as these characteristics change, for one type of synthetic corpus.We show how our measurement framework leads to new insights, including evidence of an “undetectable region” for sufficiently weak topic structures, or how the choice of hyperparameters can bias the inference result.Finally, we show that our approach is predictive of the performance of topic modeling algorithms in classification tasks in real-world corpora.\n\n[11] Mapping the field of psychology: Trends in research topics 1995–2015 1: Details on Structural Topic Models Although STMs solve other technical issues like finding the optimal starting parameters and providing consistent results by a \"spectral initialization\" (Arora et al. 2013), selecting an appropriate number of topics is crucial for any further analysis. It remains a central task for researchers to interpret the latent semantic space qualitatively and decide whether the presented topics are meaningful or one is only \"reading tea leaves\" (Chang et al. 2009). Besides the problem of \"garbage in, garbage out\" ousting for all models (McFarland et al. 2016), determining the number of topics (k) is a wide-ranging decision made by the researcher. Insufficient numbers render models coarse, an excessive number could result in a model too complex for further investigation. Comparable to efforts in cluster analysis to determine the optimal number of clusters, there is no \"right\" answer to the question on how many topics are appropriate for a given corpus (Grimmer and Stewart 2013;Munoz-Najar Galvez et al. 2020). Due to the fact that there is not a single, correct number of topics found in a corpus, careful examination and pondering of different topic solutions is key to choose a topic model that allows for qualitative judgment of the researchers (Rule et al. 2015). Fortunately, the qualitative consideration can be complemented and assisted by statistical measures. Following this line of reasoning, we propose a twofold approach to choose the optimal number k of topics before we interpret the results of our STM qualitatively. First, we check internal validity of different choices of k\n\n[12] BERTopic: Neural topic modeling with a class-based TF-IDF procedure Topic models can be useful tools to discover latent topics in collections of documents. Recent studies have shown the feasibility of approach topic modeling as a clustering task. We present BERTopic, a topic model that extends this process by extracting coherent topic representation through the development of a class-based variation of TF-IDF. More specifically, BERTopic generates document embedding with pre-trained transformer-based language models, clusters these embeddings, and finally, generates topic representations with the class-based TF-IDF procedure. BERTopic generates coherent topics and remains competitive across a variety of benchmarks involving classical models and those that follow the more recent clustering approach of topic modeling.\n\n"}
{"case_id": "08a151282687d2f0cd4f3d2c9c13d67d", "answer_text": "\n\nThe pervasiveness of gaming citation metrics, such as citation count and the h-index, has been well-documented across multiple studies and reviews, indicating a broad recognition of these practices within the academic community [13].\n\nOne of the primary mechanisms for gaming these metrics is through self-citation. Self-citation is when authors cite their own previous work in new publications, which can artificially inflate their citation count and, subsequently, their h-index. Evidence suggests that self-citation can significantly inflate these metrics. For instance, the proportion of self-cites can be as high as 36% of all citations in some cases [7]. A study also demonstrated that by strategically self-citing, authors could increase their h-index with minimal effort, which is particularly problematic for accurately assessing an individual's scientific impact [5]. Additionally, a high correlation (r = 0.997) between standard h-index and h-index with self-citations removed suggests that while there is a strong general relationship, outliers with high self-citation rates can still skew the metric [5].\n\nThe impact of self-citation can vary across disciplines. In some fields, certain researchers have been found to heavily rely on self-citation, leading to significant inflations in their metrics. This practice is particularly notable in disciplines where there is a lower overall number of active researchers, making self-citations a more pronounced tool for inflating metrics. Individual case studies, such as the mock scientist Ike Antkare, have highlighted the extreme lengths to which gaming can go. In this exaggerated example, self-citation was used to boost metrics to absurdly high levels, demonstrating the potential for manipulation when unchecked [7].\n\nAnother form of gaming involves writing review articles, which are known to garner more citations than original research articles. Concrete examples from the literature highlight that authors who focus on review articles often see a marked increase in their citation counts. This increase is significant enough to distort the portrayal of their scholarly impact, since review articles synthesize existing knowledge rather than presenting new research findings [12]. This form of gaming is recognized in initiatives such as the San Francisco Declaration on Research Assessment (DORA), which advises against using metrics like citation counts and the h-index as the sole indicators of scientific performance [12]. DORA advocates for a more holistic approach to research assessment, emphasizing the importance of evaluating the content and quality of individual research outputs rather than relying solely on quantitative measures [12].\n\nManipulative practices also extend to journals and publishers. Some journals engage in \"citation cartels,\" where they mutually agree to cite each other excessively to boost their impact factors. This behavior not only inflates journal-level metrics but also indirectly affects author-level metrics by increasing the citations received by their papers published in these journals [13]. For instance, a notable example was the case where questionable editorial practices led to the University of Alexandria being catapulted into the Times Higher Education top 200 rankings despite not being the best university even within its own city [13]. These extreme cases highlight how systemic manipulation of citation practices can have broad-reaching impacts on perceived institutional and individual performance.\n\nThe literature also discusses the limitations of citation metrics, such as the h-index, in providing a valid measure of scientific impact. The h-index does not account for the context or quality of citations, and its simplicity makes it susceptible to manipulation through self-citation and other gaming strategies [10][14]. This metric is also criticized for not properly reflecting the true impact of researchers who may have fewer but highly significant publications [0]. Moreover, it does not account for disciplinary differences or the role of co-authors, which can lead to a misrepresentation of an individual’s contributions [14].\n\nDespite its widespread use, the h-index's and citation counts' vulnerability to gaming has led to calls for alternative metrics that are less susceptible to manipulation. For instance, Altmetrics, which measure the impact of research through social media mentions and other online platforms, have been proposed as faster and more inclusive alternatives. However, they too face challenges related to manipulation and the need for robust theoretical foundations. Altmetrics can be manipulated through the artificial inflation of social media metrics, akin to the gaming of citation counts [3][13]. \n\nAnother proposed alternative is the usage of the S-index, which aims to account for the quality of citations by distinguishing between different types of citations, such as those in high-impact vs. low-impact journals. However, this index is also under ongoing development and requires more validation before it can be widely adopted [11]. The growing interest in such metrics highlights the scientific community's recognition of the limitations and potential for gaming in traditional measures and the ongoing efforts to develop more robust and reliable indicators of research impact.\n\nIn summary, the gaming of citation metrics like citation count and h-index is pervasive and well-documented. Practices such as self-citation [5], writing review articles [12], and forming citation cartels [13] contribute to the inflation of these metrics, challenging their validity as measures of scientific impact. This recognition has spurred interest in developing alternative metrics that are less prone to manipulation, although these new metrics also bring their own sets of challenges.\n\n\nReferences:\n[0] Reflections around ‘the cautionary use’ of the h-index: response to Teixeira da Silva and Dobránszki Thus, anyone using the h-index must be aware of this predilection of the indicator for more distributed profiles of citations versus more concentrated ones. Moreover, even if A had published 5 papers of 20 citations each (h-index = 5), it would still have a lower h-index than B, illustrating how the h-index punishes selectivity Bordons 2007, 2008). This shows how the h-index has a preference towards scholars who produce many moderately cited publications over those who prefer to produce a few high impact papers. These examples illustrate how the h-index, like essentially any other indicator, incorporates specific choices and preferences. This directly challenges the idea of the h-index as a general (objective) indicator of individual scientific performance, which seems to be a quite common widespread idea in research evaluation practices. A profusion of platforms providing individual-level indicators TSD's letter raises and important issue: there is a proliferation of sources providing h-indexes and collecting bibliographic and citation data at the individual level. Typically, these new sources offer the promise of faster and easier performance evaluations of individual scholars. TSD mention Google Scholar, ResearchGate, Academia.edu and Loop, but the same goes for Microsoft Academic, AMiner, Scholar Universe or SemanticScholar.org. Many of these platforms usually offer the more traditional bibliometric indicators (P, C, h-index), as well as indicators of downloads/views, social media metrics and even more complex indicators such as the RG-score, citation velocity, highly influential citations, diversity or rising star, etc. As pointed out by TSD, the proliferation of these sources confronts users with different (if not contradicting)\n\n[1] (E-) Valuative Metrics as a Contested Field: A Comparative Analysis of the Altmetrics- and the Leiden Manifesto be assessed by thousands of conversations and bookmarks in a week\" (ibid.). Not only are Altmetrics faster than traditional filters, their speed derives from open participation by many different assessors. The tension arising between peer review and Altmetrics consists in both the timeliness and openness of filters. Similarly, citation counting is criticized for being both slow and narrow: \"Metrics like the h-index are even slower than peer-review: a work's first citation can take years. Citation measures are narrow; influential work may remain uncited. These metrics are narrow; they neglect impact outside the academy, and also ignore the context and reasons for citation.\" (Priem et al., 2010). Regarding their slowness, citation measures are de-valuated as even worse a means to filter important literature than peer review. The assertion that citations can take years to occur is linked to a preprint by Brody and Harnad,9 which scrutinizes article downloads as a predictor of later citation counts, provided that articles are published OA. Thereby, the critique of citation counts' slowness implicitly refers the reader to their closedness too. The preprint concludes that correlation between download variance and citation counts indicates later citation impact, while the uncorrelated share of download variance indicated a different form of impact (Brody et al., 2006). This is mirrored by the AM's critique of narrowness: Citation metrics are denounced for ignoring the academic impact of non-cited work but also for overlooking impact outside the scientific community. Furthermore, their narrowness is related to the long-standing critique that it is unclear what citation counts indicate. On the\n\n[2] Using Google Scholar to Predict Self-Citation: A Case Study in Health Economics citation metrics, and perhaps more likely to selfcite. If this is the case, then the proportion of self-cites can be considered as an upper estimate. Additionally, it is difficult to argue that the selection bias would be of significantly different magnitude across different types of Health Economists, meaning that the relative self-citation rates are not easily explained by this factor. Are differential patterns of self-citation a major concern? In our data, the correlation coefficient between the standard h-index and the h-index with self-cites removed is 0.997, suggesting that inferences about the impact of researchers (as defined in this narrow way) are generally not impacted by self-citation. However, as Figure 3 demonstrates, there are sub-groups of researchers with very high self-citation, and judging their research output using an unadjusted h-index is problematic. A contribution of this analysis is to provide criteria against which individual self-citation can be benchmarked, something which is of value as citation data becomes an increasingly important part of academic careers.\n\n[3] Detecting h-index manipulation through self-citation analysis 21 citations. With only investing two self-citations, this author could inflate his h-index by one. A more subtle strategy would be to only cite papers that currently have fewer citations than the author's h-paper since citing already highly cited papers is unlikely to increase the h-index quickly. Given that up to 36% of all citations are self-citations, the potential inflation of bibliometric indicators could be enormous. We therefore focus on the following research questions: (1) How much can authors inflate their h-index through strategic self-citations? (2) How can we detect strategic self-citation? (3) What influence has the authors' productivity, quality, career length, and proportion of self-citations on the authors' h-index? Method To be able to investigate how far the h-index can be inflated we need to consider extreme authors that focus all their self-citations on increasing their h-indexes. We are currently not aware of a sufficient number of such extreme authors to be able to appropriately answer all our research questions with data from real authors. The only exception might be Ike Antkare, an mock scientist who is only citing himself We therefore did no base our analyses on existing authors, but focused on simulated authors. Wolfgang Glänzel and his co-authors,7 proposed a stochastic model for the publishing and citation process. However, here we make use of a more recent stochastic model proposed by Burrell, which is better suited for our simulation. The main result of this model as described in Eq. 1 defines is the expected number of papers that receive at\n\n[4] On the meaning of the h-index on the number of publications of a scientist or the total number of citations received. However, in any creative endeavor, such as physics research, the total amount of output is not necessarily the right metric for productivity. In fact, L. D. Landau himself kept a list of physicists that were ranked on a logarithmic scale of achievement. Recently, Hirsch introduced the h-index that attempts to capture the overall impact of an individual's publication record researcher by a single number. The total number of publications can be misleading because an individual could simply publish a large number of worthless articles. Conversely, the total number of citations could also be misleading because an individual might publish a single highly-cited article in a hot but transient subfield but then nothing else of scientific value. Such a citation record may not be valuable as that of someone who steadily authors good publications that are reasonably cited. The idea underlying the h-index is that an equitable integral measure of citation impact is provided by the value h, such that an individual has published at least h papers with at least h citations. It is obvious that the h-index of a prolific author of trivial publications and that of a researcher with a single great publication will be much less than someone who publishes good papers at a steady rate. Because of its obvious appeal, the h-index has become a universally-used metric of overall citation impact. As one example of the prominence of the hindex, it is immediately quoted in Web\n\n[5] S-index: Towards Better Metrics for Quantifying Research Impact use various regression techniques including support-vector regression (SVR) and linear regression (LR) on numerous features to predict field-specific paper citation count several years ahead. uses multiple classification models to determine whether a given paper will increase the author's h-index or not. identifies some important mechanisms that play a role in longterm paper citation count including aging and the Matthew effect. Analysis In this section, we qualitatively evaluate the performance of current state-of-the-art impact metrics which are commonly used today. Given the breadth of the previously described prior work, we select 4 representative approaches which sufficiently span the multitude of approaches. They are (a) citation count, (b) h-index, (c) JIF and (d) PageRank. Table 1 gives a high-level summary of the strengths and weaknesses of each approach with respect to the desiderata identified in Section 2. For computability results, we assume a heterogeneous citation graph G, defined as follows: DEFINITION 1 (CITATION GRAPH G). G hasP paper nodes,A| author nodes, andV venue nodes,Epp| paper-paper edges,Epa| paper-author edges andEpv| paper-venue edges where edge p1 → p2 denotes that paper p1 is cited by paper p2, p → a denotes that paper p is authored by author a and p → v denotes that paper p is published by venue v. Citation count Citation counting involves tallying the number of citations received by a paper, author or venue. We will denote the number of citations of a paper p by C(p). It is a purely volume sensitive metric, and\n\n[6] Peer review versus the h-index for evaluation of individual researchers in the biological sciences form of gaming by authors is to focus on writing review articles, rather than to conduct original laboratory or field-based research, simply because review articles are well known to garner more citations. 21 The need to more fairly reward authors for original research has recently been recognised in the 'San Francisco Declaration on Research Assessment' (https://sfdora.org/) which includes an advisory for authors to cite original research rather than reviews wherever it is possible to do so. The h-index ratchets upwards throughout a person's career and even continues to increase after they have become research-inactive. This is potentially a serious problem given that the purpose of performance evaluation is usually to allocate resources to individuals who are currently active. Reviewers for the NRF rating system in South Africa, for example, are expected to focus on the quality and impact of the research performed in the 8 years preceding the evaluation, and the overall h-index can be misleading in this regard. Only by focusing on the year-by-year trends in citations and publication quantity and quality over that period can a panel gain insight into the recent impact of a person in their field. Of course, the situation is completely different if the purpose of using the h-index is to award a prize for career impact to someone who has retired or who is approaching retirement, as opposed to using the information for allocation of resources. It also seems obvious that the h-index has limited utility for evaluating early-career researchers, although there have been some studies that have shown\n\n[7] Altmetrics (Chapter from Beyond Bibliometrics: Harnessing Multidimensional Indicators of Scholarly Impact) mentorships (Marchionini et al., 2006), and scholarly hyperlinks (Ingwersen, 1998) has typically preceded robust theoretical underpinnings; indeed, it is difficult to imagine otherwise. A theory of altmetrics should be a priority for the new field, but not a prerequisite. A second concern is the ease with which altmetrics counts can be manipulated. Again, this is a legitimate concern-but we should not imagine that extant metrics are free from it, either. Any metric will spawn attempts to exploit it (Espeland & Sauder, 2007). The Journal Impact Factor is a noteworthy example of a heavily gamed metric. The increasing importance of this measure has spawned a whole range of tips and tricks for artificially boosting citation scores (Falagas & Alexiou, 2008), including, recently, the formation of \"citation cartels\" (Davis, 2012;Franck, 1999) in which journals collude to cite one another. In an extreme example, a single scholar's questionable editorial practices were enough to catapult the University of Alexandria-\"not even the best university in Alexandria\" into the Times Higher Education top 200 rankings (Guttenplan, 2010, para. 6). For every extreme case like this, there are likely many more going undetected. This said, it is certainly true that the relative ease of creating and using social media profiles seems to make generating false data easier for malfeasants. Indeed, artificial inflation of social media metrics is already a well-established practice outside academia. However, successful businesses and tools have evolved immune systems to combat this kind of gaming, in the form of anti-spam and anti-gaming measures. Perhaps the highest-profile of these is Google;\n\n[8] No relationship between researcher impact and replication effect: an analysis of five studies with 100 replications replicate ego depletion'? We believe the h-index provides an objective metric that combines number of publications and impact of those publications. As such, it avoids such circularity (see Barnes, 2016;Barnes, 2017 for further information on the h-index debate). Better metrics could be used in the future with further large-scale replication attempts to tease apart the nature of any relationship, although none was found here. The results here could also call into to question the relevance of the h-index as a valid marker of researcher 'expertise and diligence'. Although the motivation for this example of testing a metascientific claim involves the explicit prediction that researcher h-index (Strack, 2017) or its analogue number of citations (Cunningham & Baumeister, 2016); which h-index correlates with at .96; (Ruscio et al., 2012) validly marks 'expertise and diligence', such scientometrics may not index the 'right' kind of skills. The h-index will vary, for example, depending on the area of research of an author, does not control for self-citation, and is blind to the role the co-author had in the actual publications-it is the same for being skilled or merely for providing useful structures and materials. In short, the link between h-index and a researcher's ability to conduct good studies is hypothetical, and it could be at best an imperfect one 2 . It is this explicit hypothesis, however that we sought to test. It would be desirable to eventually develop alternative metrics of 'ability to conduct good studies' to put similar unenumerated hypotheses to the test. Until then, such unenumerated alternate metrics\n\n"}
{"case_id": "2a84ae2f6196e574f7b9a9121187a37b", "answer_text": "\n\nOpen-source publishing frameworks have significantly impacted the global dissemination of academic research by enhancing accessibility, transparency, and collaboration among researchers and institutions worldwide [3]. These systems have democratized access to scholarly content, particularly benefiting researchers in low- and middle-income countries [9], and have facilitated the widespread distribution of knowledge across various disciplines [1]. \n\n**Global Distribution of Academic Content** [0] [1] [7].\n\n\nOne of the most notable effects of these platforms is their ability to provide global distribution of digital books and academic materials. Initiatives like OAPEN in the Netherlands and OpenEdition in France have developed structures that enable the worldwide distribution of digital books and metadata, which integrate seamlessly with other information management systems [0]. OAPEN has successfully distributed thousands of academic titles globally, significantly increasing the reach and impact of scholarly works published through their system. Similarly, OpenEdition has offered a robust platform for the dissemination of research, particularly in the humanities and social sciences, making thousands of books and journals freely available to a global audience.\n\nEnhancing Accessibility and Usability [3]. \n\nThese frameworks adopt various methodologies to boost accessibility and usability. For example, they often utilize metadata structures compatible with other information management systems to ensure seamless integration and discoverability. This approach facilitates easy searching and access of academic content through other scholarly databases and search engines, thereby increasing their visibility and reach [0].\n\nThe rise of open access (OA) journals has also been closely linked to the adoption of these platforms. OA models, including 'gold' OA where articles are freely accessible upon publication, and 'green' OA where authors self-archive their manuscripts, have significantly increased the volume of freely available research [6]. The Directory of Open Access Journals (DOAJ), for instance, now lists over 12,000 OA journals, demonstrating the widespread adoption and success of this model. Similarly, PubMed Central provides free access to millions of articles in the biomedical sciences, making critical research more accessible to both researchers and the public [3].\n\nInnovative Scholarly Journal Distribution Systems[1].\n\nThese systems have contributed to the development of innovative scholarly journal distribution methods. Platforms like PubMed Central and ScienceCentral employ open-source tools to enhance the accessibility and usability of academic journals [1]. They often incorporate advanced features such as graphical abstracts, interactive PDFs, and semantic enhancements, which improve user experience and facilitate the dissemination of complex scientific information. For instance, ScienceCentral has improved the accessibility of Korean medical journals, increasing their readership both locally and internationally.\n\n**Addressing Publication Costs**[3]\n\n\n\nIn addition to addressing accessibility, these platforms have also tackled the issue of publication costs. The emergence of 'platinum' OA journals, which do not charge authors or readers, has provided an equitable solution for researchers with limited financial resources [8]. One notable example is the journal \"Elementa: Science of the Anthropocene,\" which operates on a platinum OA model and has achieved a significant impact factor, demonstrating that high-quality, peer-reviewed research can be published without financial barriers for authors or readers.\n\nInnovations in Peer Review and Academic Evaluation [1].\n\nOpen-source frameworks have driven significant changes in the peer review and academic evaluation systems. Platforms like the Open Library of Humanities' Janeway and the collaborative publishing services of the Ubiquity Partner Network illustrate how these infrastructures support transparent and robust peer review processes [0]. These systems facilitate open peer review, where reviewer names and comments are made public, thereby increasing transparency and accountability in the review process. They also offer functionalities for post-publication peer review, enabling ongoing assessment and discussion of published research, which enhances the quality and reliability of academic publications.\n\nThe success of models like Janeway, which has published numerous open access articles without charging APCs, underscores the transformative potential of these systems to not only democratize access to academic content but also to refine and innovate peer review mechanisms [0]. These open peer review systems help maintain high-quality standards in academic publishing while promoting openness and collaboration [2].\n\nChallenges and Tailored Solutions\n\nHowever, the journey toward global open access is not without challenges. While these platforms have made significant strides, there are still gaps in our understanding of how these models can be optimized for developing regions [14]. For example, the assumption that digital formats are universally accessible may not hold true in areas with limited internet connectivity or where mobile devices are the primary means of accessing information. Additionally, language barriers and the dominance of English in global scholarly publishing pose significant obstacles for non-English speaking researchers. Platforms need to provide multilingual support and consider regional digital infrastructure when disseminating academic content.\n\nExamples of tailored solutions addressing these challenges include initiatives like INASP's (International Network for the Availability of Scientific Publications) work to improve access to research in developing countries [13]. They focus on capacity building, providing training, and supporting local publishing infrastructure to ensure that researchers in these regions can effectively participate in the global academic community. Furthermore, platforms like African Journals Online (AJOL) and SciELO in Latin America have been pivotal in increasing the visibility and accessibility of locally published research, thereby contributing to a more inclusive global scholarly communication system [13]. \n\nConclusion is an area where there are lots of opportunities to build sustainable and professional workflows by collaboration (Taylor & Jensen, 2018) [0]. Others such as OAPEN in the Netherlands and OpenEdition in France have also contributed to facilitating book workflows by providing platforms for global distribution of digital books and metadata structures that work with other information management systems (OAPEN. (n.d.), OpenEdition. (n.d.)) [0]. Furthermore, there are presses who outsource certain parts of the publishing process elements through a variety of publishing services [0]. Many major publishers do exactly the same when they outsource their technical workflows to specialised services companies, so the collaborative method is well tested [0]. \n\nIn summary, open-source publishing frameworks have revolutionized the global distribution of academic research by enhancing accessibility [13], reducing costs [11], and promoting transparency and collaboration [3]. While challenges remain, particularly in ensuring that these platforms meet the diverse needs of global researchers [14], their continued development and adoption hold promise for a more equitable and inclusive academic landscape [0]. \n\n\nReferences:\n[0] Rebels with a Cause?:Supporting Library and Academic-led Open Access Publishing for licensing of open access books as well as the possibility to disseminate and sell hard copies on a global scale. Workflow & infrastructure To publish a book is not simply an assembly line from the point of entry of academic content to the exit point when it is published online and/or in print. Book publishing is much more complex than that, which could explain why many authors only trust their content to be managed by a traditional publisher that they believe can accommodate this need. However, a large and complex publisher organisation is likely to also be rather expensive, a critique often voiced by funding organisations and other academic stakeholders. For academic-led or library publishers this is an area where there are lots of opportunities to build sustainable and professional workflows by collaboration (Taylor & Jensen, 2018). Others such as OAPEN in the Netherlands and OpenEdition in France have also contributed to facilitating book workflows by providing platforms for global distribution of digital books and metadata structures that work with other information management systems (OAPEN. (n.d.), OpenEdition. (n.d.)). Furthermore, there are presses who outsource certain parts of the publishing process elements through a variety of publishing services such as the Ubiquity Partner Network (2019), Open Library of Humanities' Janeway platform (Eve, 2018), or Vega (2018)a collaboration between American university presses. Many major publishers do exactly the same when they outsource their technical workflows to specialised services companies, so the collaborative method is well tested. In the UK, Jisc has developed a dynamic purchasing service (DPS)\n\n[1] Latest trends in innovative global scholarly journal publication and distribution platforms Latest trends in innovative global scholarly journal publication and distribution platforms This review article presents the latest trends in innovative global scholarly journal publication and distribution platforms, with implications for local journals. Changes have taken place in distribution policies, as pre-publication distribution has become a viable option, and for post-publication distribution, public access or mandatory open access policies have been introduced for articles supported by public or governmental funds. New formats of articles include graphical abstracts, interactive PDFs, the application of semantic enhancements, and the utilization of research data, social networking sites, such as Mendeley and ResearchGate, have become common sites for information exchange. Altmetrics have been adopted to complement traditional journal metrics. PubMed Central, F1000Research, KoreaMed Synapse, and ScienceCentral have been introduced as innovative full-text scholarly journal distribution systems. To publish web-based scholarly journals, it is necessary to adopt an open platform and to explore options such as an author profile database, an online collaborative editing module, and Crossref text and data mining services. To maximize the influence of local journals, it is necessary to integrate various external tools, such as researcher ID, research data, social media, and altmetrics services.\n\n[2] Predatory publishing in management research: A call for open peer review academic publishing system (Suber, 2016). However, the APC-based Open Access system also brings new problems for academic knowledge production. One of them is the rise of predatory Open Access journals (Harzing and Adler, 2016). Predatory Open Access journals accept submitted manuscripts very quickly (sometimes within a few days) and only supposedly or very superficially conduct peer review, in order to skim off as many APCs as possible (Xia, 2015). The emergence of predatory Open Access journals is fueled by growing institutional Open Access funds and growing pressure on academics worldwide to publish in international and peer reviewed journals (Beall, 2013(Beall, , 2018Djuric, 2015;Omobowale et al., 2014). Predatory publishing surfaces problems that arise from path dependencies and market concentration in the academic publishing business. It also makes visible problems with the global institutionalization of peer review, which over the last decades has developed into \"a unifying principle for a remarkably fragmented [academic field\" (Biagioli, 2002: 34). The more the ideal of international peer review gets exported into academic fields that have traditionally assessed academic quality through other means (e.g. in the Global South), the greater the demand for publishing outlets that are able to link local cultures of knowledge production with the ideal of international, blinded peer review. Since the early 2000s, the number of Open Access journals has increased rapidly. In 2002, the Directory of Open Access Journals (DOAJ) listed 33 journals. In 2019, it lists about 12,000 reputable journals. With a temporal lack, this growth has been matched by the market for predatory journals as\n\n[3] Comprehensive Approach to Open Access Publishing: Platforms and Tools. The Open Access Initiative is gaining momentum due to the worldwide availability of advanced digital tools, online publishing platforms, and systems for tracking academic contributions. Several declarations and initiatives, including Plan S, have already laid a foundation for moving away from subscription to full and immediate open-access publishing. The global initiatives imply targeting journals satisfying the upgraded quality and visibility criteria. To meet these criteria, a comprehensive approach to Open Access is recommended. This article overviews the essential components of the comprehensive approach, increasing transparency, adherence to ethical standards, and diversification of evaluation metrics. With the increasing volume of quality open-access journals, their indexing with free databases and search engines is becoming increasingly important. The Directory of Open Access Journals and PubMed Central currently free searches of open-access sources. These services, however, cannot fully satisfy the increasing demands of the users, and attempts are underway to upgrade the indexing and archiving of open-access sources in China, Japan, Korea, Russia, and elsewhere. The wide use of identifiers is essential for transparency of scholarly communications. Peer reviewers are now offered credits from Publons. These credits are transferrable to their Open Researcher and Contributor iDs. Various social media channels are increasingly used by scholars to comment on articles. All these comments are tracked by related metric systems, such as Altmetrics. Combined with traditional citation evaluations, the alternative metrics can help timely identify and promote publications influencing education, research, and practice.\n\n[4] Developing an open science ‘mindset’ income institutions, nations, and individuals (Grahe, Cuccolo, Leighton, & Cramblet Alvarez, 2020). These access issues have recently come into sharp focus through lobbies for research supported by taxpayer-funded grants to be made publicly available. This pressure has resulted in the formation of groups such as cOAlition 'S', an international consortium of research funders who, through plan 'S' ('S' for 'shock'), have instigated sweeping changes to research dissemination by working with publishers and researchers to make all publicly-funded research open access (cOAlition, 2021). More broadly, such initiatives have coincided with the rapid rise of open access journals in which all articles are made openly available, usually under a Creative Commons Attribution license (CC BY) license, or hybrid journals which offer the option of open access, both of which shift the cost of publication to the author or their funder. This is known as a 'gold' open access publishing model, in which submitted research is made openly available on acceptance to the journal after a peer review process. The drawback is that fees are often high, and out of reach of many individual researchers, or place an undue burden on researchers and funders from lowor middle-income countries, with publishers' pledge of open access fee waivers often confined to a small number of countries. 'Green' open access is an alternative in which the accepted manuscript version of a research article published in a subscription journal is self-archived by the author, often after an embargo period. There has also been a rise of journals that offer Researchers with an open\n\n[5] Toward Open Research: A Narrative Review of the Challenges and Opportunities for Open Humanities developed by the large commercial publishing companies, that today own around 70% of journals globally and are now increasingly buying the software and infrastructure for the new scholarly ecosystem (Larivière, Haustein, & Mongeon, 2015). While some renowned university presses and learned societies have developed new strategies and have professional marketing teams, approximately half of the smaller enterprises have entered into co-publishing arrangements with the major commercial companies to provide them with the necessary sales, copyright, editing, and new technology services (Fyfe et al., 2017). But this has resulted in very limited knowledge among academic societies of how these processes are being driven by the interests of their for-profit making publishing partners, which maintain copyright and limit open access to research outputs (Inger & Gardner, 2013). Added to this disruption has been the unsustainable rise in subscription costs and \"big deal\" packages aimed at providing seamless access to online literature that has outstripped library budgets of even the most prestigious universities (Barbour & Nicholls, 2019). To address growing frictions with the commercial publishing industry, individual countries, and universities-primarily those in developed countries-are now entering into one-on-one \"transitional agreements\" and membership programs with major publishers to reduce open access and post-print embargoes, and offer discounts on subscription fees and article and book process charges (Borrego, Anglada, & Abadal, 2021). While some have defined these transformative agreements as temporary and transitional (aimed at constraining costs of scholarly communication, fostering equity in scholarly publishing, retaining author copyright, and ensuring that their needs and those of their institutions are addressed), others\n\n[6] The Rise of Platinum Open Access Journals with Both Impact Factors and Zero Article Processing Charges It appears that open access (OA) academic publishing is better for science because it provides frictionless access to make significant advancements in knowledge. OA also benefits individual researchers by providing the widest possible audience and concomitant increased citation rates. OA publishing rates are growing fast as increasing numbers of funders demand it and is currently dominated by gold OA (authors pay article processing charges (APCs)). Academics with limited financial resources perceive they must choose between publishing behind pay walls or using research funds for OA publishing. Worse, many new OA journals with low APCs did not have impact factors, which reduces OA selection for tenure track professors. Such unpleasant choices may be dissolving. This article provides analysis with a free and open source python script to collate all journals with impact factors with the now more than 12,000 OA journals that are truly platinum OA (neither the author nor the readers pay for the peer-reviewed work). The results found platinum OA is growing faster than both academic publishing and OA publishing. There are now over 350 platinum OA journals with impact factors over a wide variety of academic disciplines, giving most academics options for OA with no APCs.\n\n[7] Open access in Angola: a survey among higher education institutions countries in the Global South, there have been some common key tensions in creating an equitable environment for access and participation in global research. One key barrier is language, where English has become established as the default language for global scholarly publishing, adding a threshold for academics in non-English speaking countries to participate. Another key barrier relates to business models of international journal publishing that create financial thresholds for participation, both through subscription-fees or article processing charges (APCs) for publishing in open access (OA) journals. In the Global North scholarly publishing has grown and been shaped over centuries together with the gradual establishment and expansion of university education and research in the surrounding societies. How this relationship has developed around the world differs, as many countries and regions have been struggling with poverty, political unrest, and even war that has slowed down or hindered the capability to develop a strong local HEI sector. There is a lack of knowledge on how countries that have more recently been building or rebuilding higher education and research sectors are interfacing with scholarly journal publishing. OA has become a central concept to scholarly publishing and offers many new ways that individuals and institutions can interact with a global audience, but at the same time there is concern that some of the ways that OA is funded creates new barriers to equal participation (e.g. Gray, 2020;Raju & Claassen, 2022;Siler et al., 2018). Raju et al. (2020, p. 55) state that \" [… open access is viewed as a means for social\n\n[8] Open Digital Repositories - The Movement of Open Access in Opposition to the Oligopoly of Scientific Publishers Open Digital Repositories - The Movement of Open Access in Opposition to the Oligopoly of Scientific Publishers This paper shows how the market structure of scientific publications works and how the free-software movement and the open source code have expanded and generated new developments in a period of approximately twenty years, in opposition to an oligopolistic structure. The free-software movement did not happen by chance, and neither did its subsequent developments. Researchers, tired of contributing towards the production of scientific articles for private publishers, and also working as reviewers or taking part in editorial boards, launched many alternatives within the editorial market in clear opposition to the publishing industry, which has been making handsome profits on packaged periodicals sold to academic libraries. Some of these alternatives are: the Copyleft and the Creative Commons in opposition to the Copyright; the Open Access and the Open Digital Repositories of educational and research institutions, freely available on the internet, opposing to the closed repositories of commercial publishers that offer their database at high prices; and the creation of h-index, g-index, Google Scholar Citations (GSC) and other impact measurements that come up against the impact factor controlled by private publishers. In the editorial process, while educational and research institutions, through their researchers, provide all workforce necessary for the production, arbitration and editorial board, publishers are in charge of organizing services, providing reliable browsing on their closed database, and keeping high levels of impact for their publications. Nowadays, search providers like Google also offer reliable search engines to browse open digital\n\n[9] International Development and Research Capacities: Increasing Access to African Scholarly Publishing This paper examines the evolving relationship between Canada and the African academic research community through the promotion of a concept known as Information and Communication Technology for Development (ICT4D) and with an eye to its implications for increasing the circulation of research through such means as open access (OA) publishing models. We analyze the programmatic discourse of Canada’s International Development Research Centre’s (IDRC) African research initiatives, and report on an IDRC research and development project assessing the means of increasing access to African scholarly journals through the use of open source software platforms and open access publishing and archiving models. Consistent with IDRC’s multi-year effort to contribute directly to university-based research capacities by investing in ICT infrastructure in Africa, our survey of African editors, librarians, and faculty from ﬁ ve African nations reveals a similar interest in developing those capacities, despite numerous challenges, through the use of online publishing systems and OA publishing models, which hold some promise of increasing access to research published in Africa. Cet article analyse l’evolution de la relation entre le Canada et la communaute de recherche africaine en avancant le concept de la Technologie de l’Information et de la Communication pour le Developpement (TICpD), ainsi que de son potentiel pour augmenter la circulation de la recherche grâce a des modeles de publication comme l’acces libre. Nous analysons le discours programmatique des initiatives africaines emanant du Centre de Recherche pour le Developpement International canadien (CRDI), et nous faisons le bilan d’un projet de recherche et de developpement portant sur l’evaluation de moyens visant a faisant croitre l’acces aux revues savantes africaines, en particulier grâce a des plates-formes de logiciels libres et grâce a des modeles de publication et d’archivage en acces libre. De concert avec l’effort pluriannuel du CRDI de contribuer directement aux capacites de recherches universitaires par son investissement dans les infrastructures TIC africaines, notre sondage de redacteurs en chef, de bibliothecaires et de chercheurs de cinq pays d’Afrique, revele ces derniers s’interessent a developper le genre d’outils, en depit de nombreux obstacles, offrant la possibilite d’ameliorer l’acces aux recherches publiees en Afrique, et en particulier les outils de publication en ligne ainsi que les modeles de publication en acces libre.\n\n[10] On the Path to Global Open Access: A Few More Miles to Go both because of traditional publishers who have not yet implemented OA, even in the developed world, and because substantial gaps remain in our knowledge about how OA will work for the developing world. Hence, there is some way to go before this model of publishing can become the norm worldwide. Despite the best intentions of open-access publishers, we have failed to reach out adequately to debate with researchers and readers in the less-developed world about the potential benefits of open access. Instead, as is often the case when the developed world prescribes for the less-developed world, we have assumed that what works well in Paris, London, or San Francisco will work just as well in Addis Ababa, Beirut, or Lima. Some examples of these active concerns about OA: first, are OA journals being delivered in the best format for readers in the developing world? If print really is better in some places, are we doing our best to ensure that the online journals are optimized for rapid downloading and printing of articles? If access to online journals will be primarily via mobile devices rather than computers, are we delivering the content in appropriate formats? Second, do we understand the reputation metrics outside of Europe or the US that will ensure that the new OA journals are trusted and meet the requirements authors face for academic promotions or other professional needs? Even more importantly, are there OA journals available that cater to the needs of readers and authors across the developing world? Should publishers be helping\n\n"}
{"case_id": "ab3651c422a54f40aa4bac3fa630c13e", "answer_text": "\n\nLarge language models (LLMs) like ChatGPT significantly impact the diversity of published scientific ideas by enhancing both the lexical and syntactical diversity of the generated content [5]. The increased diversity ranges from the structural composition of paraphrases [0] to the breadth of vocabulary used in scientific communications [2].\n\nSyntactical and Lexical Diversity [0][2][5] \nFirstly, LLMs like ChatGPT demonstrate higher syntactical diversity when generating paraphrases compared to human crowdsourced data. This is measured using tree edit distance (TED), a metric that assesses the structural variations in paraphrases. For example, the study by Chen et al. (2019) found that ChatGPT-generated data had a higher mean TED value compared to human-generated data, indicating greater syntactical diversity [0]. The increased syntactical diversity can contribute to richer and more varied expressions of scientific ideas.\n\nChatGPT also exhibits higher lexical diversity. This is evident from an increase in the number of unique words used in datasets generated by ChatGPT compared to those produced by human workers. Studies have shown a 28.75% and 11.37% increase in unique words for datasets collected using prompt mode and taboo mode, respectively, when generated by ChatGPT [2]. This lexical variability can lead to a richer exploration of terminology and concepts in scientific literature, potentially fostering more innovation and varied perspectives in research papers.\n\n### Applications in Scientific Discovery\n\nLarge language models like GPT-4 have shown substantial potential in scientific research, demonstrating capabilities such as generating diverse molecular structures by adjusting input parameters and conditions, thus showcasing their flexibility in scientific tasks [1]. The potential of these models extends to their ability to perform zero-shot molecular sampling, although challenges remain in accurately predicting certain atom types and bond types [3]. Additionally, ChatGPT has been used to generate professional textual explanations for molecular properties, which can greatly aid in downstream tasks such as molecular classification and property prediction [13]. Furthermore, the utility of LLMs in supporting systematic reviews has been explored, although current findings suggest that the technology is still in its infancy and requires significant development for reliable integration into scientific workflows [4].\nIn the realm of scientific discovery, LLMs have shown adaptability in generating diverse molecular structures and configurations by adjusting input parameters and conditions. For instance, GPT-4, when prompted with different thermal conditions and an emphasis on diversity, was capable of generating varied molecular structures [1]. This ability to produce a wide range of configurations based on different input criteria can lead to more extensive exploration and innovation within scientific research. However, the model does face some challenges, evidenced by the incorrect prediction of certain atom types and bond types, indicating potential areas for refinement and improvement in GPT-4’s overall effectiveness [3][7].\n\nChallenges and Limitations [4].\nDespite these advancements, the application of LLMs in scientific research comes with significant challenges. The reliability of such technologies in ensuring the accuracy and validity of scientific content remains a concern. Studies show that systematic reviews utilizing AI-assisted technologies like ChatGPT benefit from increased efficiency and reduced time and financial costs, but the accuracy and coherence of the generated content are vital. Flawed or misleading information can propagate errors in scientific discourse, and experts advise caution, indicating that while AI can aid in systematic reviews, the technology is still in its infancy and requires significant development for rigorous scientific applications [4].\n\n\n\n### Ethical Implications\n\nThe emergence of ChatGPT and other large language models (LLMs) has sparked considerable debate among academics and researchers regarding their potential misuse, particularly concerning issues such as plagiarism and unethical academic practices [8]. The widespread concern is that these models could lead to increased dependency among researchers and students, making them less self-reliant and prone to academic dishonesty [8]. Additionally, the ability of these models to generate large volumes of text quickly raises concerns about the potential for misinformation and the spread of an \"AI-driven infodemic\" in public health, necessitating the development of policies to manage this risk [12].\n\n\nThe ethical implications of using LLMs in scientific research also influence the diversity of published ideas. Concerns about plagiarism, loss of originality, and researchers becoming overly reliant on AI tools are valid. These ethical challenges necessitate the development of robust policies to ensure the responsible use of LLMs in academia [8][12]. The integration of AI with academic platforms to monitor and curb plagiarism can help maintain the integrity of scientific research while still leveraging the benefits of LLMs in enhancing content diversity [8].\n\nConclusion\nIn conclusion, large language models like ChatGPT enhance the diversity of published scientific ideas by introducing greater syntactical and lexical variation in generated content [0][2]. They offer potential for innovation and diverse perspectives in scientific research [1] while also presenting challenges in terms of accuracy, reliability, and ethical use [12]. As the technology evolves, careful consideration and implementation of guidelines will be essential to harness the advantages of LLMs while mitigating their drawbacks [8]. \n\n\nReferences:\n[0] ChatGPT to Replace Crowdsourcing of Paraphrases for Intent Classification: Higher Diversity and Comparable Model Robustness We also compare the collected datasets on syntactical diversity to better assess the structural variations in paraphrases. We do this by calculating the tree edit distance value (TED) Chen et al. (2019) between all pairs of paraphrases sharing the same intent. TED is first calculated pair-wise for each phrase in the intent and data split (separate for human, GPT, original). This is then averaged to get the mean – this should represent how syntactically diverse the phrases are - the higher the number of mean TED is, the more diversity is present. When comparing prompt datasets, ChatGPT created more diverse sentence structures with a mean TED value of 19.001 compared to a 13.686 mean TED value for crowdsourced data. The same holds for the taboo datasets: the crowdsourced taboo dataset has a mean TED value of 15.483 while the ChatGPT collected dataset has 18.442. It should be noted that while the data collected using human workers have higher TED value for the taboo method than for the prompt method, the same cannot be said about the data generated from ChatGPT - the introduction of taboo words to ChatGPT does not increase the syntactical diversity. We have confirmed our findings by running a Mann-Whitney-U test between datasets with p = 0.001 for the measured TED values. We conclude that data collected via ChatGPT has higher syntactical diversity than that collected from human workers for the same data collection method.\n\n[1] The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4 Furthermore, GPT-4 can directly sample molecular structures by taking into account the thermal fluctuation in the prompt. In Fig. 4.42, GPT-4 generates three structures; however, these structures exhibit significant similarities. To sample more diverse configurations, we increase the temperature from 300K to 1000K and emphasize diversity in the prompt (Fig. 4.43). This adjustment leads to the generation of more varied molecular structures, demonstrating GPT-4’s potential ability to adapt and produce a wider range of configurations based on the input parameters and conditions.\n\n[2] ChatGPT to Replace Crowdsourcing of Paraphrases for Intent Classification: Higher Diversity and Comparable Model Robustness We evaluated the lexical diversity in the same way as in the original study: by vocabulary size of different datasets. From this point of view, the smallest vocabulary (number of unique words) can be observed in the crowdsourced prompt mode data with 946 unique words. Compared to this, the ChatGPT prompt mode data features 1218 unique words, which is a 28.75% increase in vocabulary size. The crowdsourced dataset collected through the taboo mode had even higher vocabulary size with 1487 unique words. However, the ChatGPT taboo mode beats it even more with 1656 unique words (an increase of 11.37%). We conclude that data collected via ChatGPT has higher lexical diversity compared to crowdsourced data when the same data collection mode is used. This can also be seen on the visualization of the number of unique words in Figure 3.\n\n[3] The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4 As depicted in Fig. 4.38, GPT-4 refuses to generate the 3D configuration for the given molecule. Furthermore, attempts to generate the SDF file, as shown in Fig. 4.39, result in a structure that is physically implausible. We further assess the zero-shot molecular sampling by requesting GPT-4 to provide an estimate, with the additional hint in the prompt that lengths and angles of bonds should be considered. Fig. 4.40 displays an interesting aspect of GPT-4’s performance, where it effectively estimates the 3D position of each atom in the molecule and utilizes the bond information to determine these positions. Despite this impressive capability, the model does face some challenges, evidenced by the incorrect prediction of certain atom types and bond types, which indicates potential areas for refinement and improvement in GPT-4’s overall effectiveness. When additional information regarding the quantity of each atom type is provided in the prompt (Fig. 4.41), GPT-4 successfully predicts the correct atom types but still faces difficulties in determining the accurate bond types. Two of the hydrogen atoms should be bonded to the same nitrogen atom; however, all hydrogen atoms are bonded to carbon atoms in the resulting configuration.\n\n[4] Are ChatGPT and large language models \"the answer\" to bringing us closer to systematic review automation? In this commentary, we discuss ChatGPT and our perspectives on its utility to systematic reviews (SRs) through the appropriateness and applicability of its responses to SR related prompts. The advancement of artificial intelligence (AI)-assisted technologies leave many wondering about the current capabilities, limitations, and opportunities for integration AI into scientific endeavors. Large language models (LLM)-such as ChatGPT, designed by OpenAI-have recently gained widespread attention with their ability to respond to various prompts in a natural-sounding way. Systematic reviews (SRs) utilize secondary data and often require many months and substantial financial resources to complete, making them attractive grounds for developing AI-assistive technologies. On February 6, 2023, PICO Portal developers hosted a webinar to explore ChatGPT's responses to tasks related to SR methodology. Our experience from exploring the responses of ChatGPT suggest that while ChatGPT and LLMs show some promise for aiding in SR-related tasks, the technology is in its infancy and needs much development for such applications. Furthermore, we advise that great caution should be taken by non-content experts in using these tools due to much of the output appearing, at a high level, to be valid, while much is erroneous and in need of active vetting.\n\n[5] ChatGPT to Replace Crowdsourcing of Paraphrases for Intent Classification: Higher Diversity and Comparable Model Robustness To answer these questions, we have conducted a quasi-replication of an existing study Larson et al. (2020), where paraphrases were crowdsourced to augment intent classification data (using also taboo words technique to induce more example diversity). In our own study, we followed the same protocol (seeds, taboo words, scale), but replaced the crowd workers with ChatGPT (for approximately 1:600 lesser price) as the most widely used LLM and Falcon-40B Almazrouei et al. (2023) as one of the best performing open source LLM at the time of writing this paper. This allowed us to directly compare properties of crowd- and LLM-generated data, with following findings:1) ChatGPT is highly reliable in generating valid paraphrases,2) Falcon-40B struggled in generating valid and unique paraphrases,3) ChatGPT outputs lexically and syntactically more diverse data than human workers, and4) models trained on ChatGPT data have comparable robustness to those trained on crowd-generated data.\n\n[6] The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4 In the case of most capabilities, we primarily adopt a qualitative approach, carefully designing tasks and questions that not only showcase GPT-4’s capabilities in terms of its scientific expertise but also address the fundamental inquiry: the extent of GPT-4’s proficiency in scientific research. Our objective is to elucidate the depth and flexibility of its understanding of diverse concepts, skills, and fields, thereby demonstrating its versatility and potential as a powerful tool in scientific research. Moreover, we scrutinize GPT-4’s responses and actions, evaluating their consistency, coherence, and accuracy, while simultaneously identifying potential limitations and biases. This examination allows us to gain a deeper understanding of the system’s potential weaknesses, paving the way for future improvements and refinements. Throughout our study, we present numerous intriguing cases spanning each scientific domain, illustrating the diverse capabilities of GPT-4 in areas such as concept capture, knowledge comprehension, and task assistance.\n\n[7] ChatGPT in scientific and academic research: future fears and reassurances \nPurpose\nThis purpose of this study is to examine future fears and reassurances about the nature of the recent artificial intelligence (AI) language model-based application, ChatGPT, use in the fields of scientific research and academic works and assignments. This study aims at exploring the positive and negative aspects of the use of ChatGPT by researchers and students. This paper recommends some practical academic steps and suggestions that help the researchers and publishers curtail the percentage of spread of unethical works such as plagiarism.\n\n\nDesign/methodology/approach\nThe emergence of OpenAI’s Generative Pre-Trained Transformer 3 (GPT-3) has recently sparked controversy and heated debate among academics worldwide about its use and application. The concern of experts and researchers about the GPT-3 platform entails how it would be of much support to the researchers and academic staff and how it might be used and misused to negatively affect academic and scholarly works. This research explored future fears and reassurances about the nature of Chat GPT-3 use at academic and scientific levels. The data for this research was obtained through the qualitative interviews of seven experts in AI, scientific research and academic works. The findings of the study showed that ChatGPT-3 has significant potential and is helpful if used wisely and ethically at scientific and academic levels. On the other hand, the results reported the experts' fears of the frequent use of ChatGPT including the misuse of ChatGPT as a tool to plagiarize and make the researchers dependent, not self-reliant and lazy. The widespread concern of many scholars is that ChatGPT would lead to an increase in the possibility of plagiarism and provide less control over research and writing ethics. This study proposed some stages and suggested that AI language model programs, including ChatGPT, should be integrated with widespread publishers and academic platforms to curtail the percentage of plagiarism and organize the process of publishing and writing scientific research and academic works to save the rights of researchers and writers.\n\n\nFindings\nThe findings of the research presented that ChatGPT can act as a potential and useful tool if used wisely and ethically at scientific and academic fields. On contrast, the results also reported the negative aspects of the extensive ChatGPT's that leads to the spread of plagiarism and making the researchers and the students machine-dependent, not self-reliant and lazy. This study proposed some stages and suggested that AI language model programs, including ChatGPT, should be integrated with widespread publishers and academic platforms to curtail the percentage of plagiarism and organize the process of publishing and writing scientific research and academic works to save rights of researchers and writers.\n\n\nOriginality/value\nTo the best of the authors’ knowledge, this paper is the first of its kind to highlight the relationship between using ChatGPT and the spread of both positive and negative aspects of its extensive use in scientific research and academic work. The importance of this study lies in the fact that it presents the concerns and future fears of people in academia as they cope with and deal with the inevitable reality of AI language models such as ChatGPT.\n\n\n[8] ChatGPT and the rise of large language models: the new AI-driven infodemic threat in public health Large Language Models (LLMs) have recently gathered attention with the release of ChatGPT, a user-centered chatbot released by OpenAI. In this perspective article, we retrace the evolution of LLMs to understand the revolution brought by ChatGPT in the artificial intelligence (AI) field. The opportunities offered by LLMs in supporting scientific research are multiple and various models have already been tested in Natural Language Processing (NLP) tasks in this domain. The impact of ChatGPT has been huge for the general public and the research community, with many authors using the chatbot to write part of their articles and some papers even listing ChatGPT as an author. Alarming ethical and practical challenges emerge from the use of LLMs, particularly in the medical field for the potential impact on public health. Infodemic is a trending topic in public health and the ability of LLMs to rapidly produce vast amounts of text could leverage misinformation spread at an unprecedented scale, this could create an “AI-driven infodemic,” a novel public health threat. Policies to contrast this phenomenon need to be rapidly elaborated, the inability to accurately detect artificial-intelligence-produced text is an unresolved issue.\n\n[9] Can Large Language Models Empower Molecular Property Prediction? Captions as New Representations. With vivid world knowledge and amazing reasoning ability, LLMs have been widely applied in various AI domains He et al. (2023); Liu et al. (2023). Also, we reckon that LLMs can empower LLMs can greatly contribute to the understanding of molecular properties. Taking a commonly used dataset in the field of molecular prediction for a toy example, PTC Helma et al. (2001) is a collection of chemical molecules that reports their carcinogenicity in rodents.We conduct a keyword search using terms such as ‘toxicity’ ‘cancer’, and ‘harmful’ to retrieve all explanations generated by ChatGPT for the originally SMILES-format PTC dataset.Interestingly, we observed that the majority of these keywords predominantly appeared in entries labeled as -​1-1\\text{-}1.This demonstrates that ChatGPT is capable of providing meaningful and distinctive professional explanations for the raw SMILES strings, thereby benefiting downstream tasks.\n\n"}
{"case_id": "6f526f72804ce3eb59feb7046f319ccc", "answer_text": "\n\nDetecting AI-generated texts in situations where false positives are extremely expensive requires a combination of sophisticated detection techniques and strategic operational procedures. Here are some good practices based on recent research and findings [6].\n\nAlgorithmic Techniques\n\nUse of robust detection algorithms has become crucial in the face of the increasing use of AI for generating deceptive content. Recent studies have developed various approaches, such as watermarking, outlier detection, and retrieval-based methods, to effectively identify AI-generated text [0]. These techniques have shown significant promise in maintaining detection accuracy even when faced with paraphrased text [8]. For instance, retrieval-based methods store every output generated by a language model in a database and then compare candidate texts to these stored outputs to check for similarities, making it harder for paraphrased AI-generated text to evade detection [8].  \n\n**Watermarking and Retrieval Methods**: Traditional text detectors can be bypassed through paraphrasing attacks [0][1]. To enhance robustness, combining watermarking with retrieval methods has shown promise. Watermarking can synthetically introduce a noticeable distribution shift, making AI-generated texts identifiable even after paraphrasing [7]. Retrieval-based methods are particularly effective as they focus on the semantic content rather than syntactic structures. For instance, Krishna et al. (2023) demonstrated that retrieval-based detection remains effective even when paraphrased texts are used, as these approaches compare new texts against a database of previously generated outputs [8][9].\n\n**Positive-Unlabeled (PU) Learning**: Employing a Positive-Unlabeled (PU) framework, especially for short texts, enhances detection accuracy without sacrificing performance on longer texts [2][14]. The PU learning method treats human texts as \"Positive\" and short machine-generated texts as \"Unlabeled\", optimizing the detection criteria for various text lengths [12]. This method addresses the challenge of short AI-generated text detection, which is notoriously difficult with standard classifiers [12].\n\nSemantic and contextual analysis plays a crucial role in detecting AI-generated text. This approach involves examining the meanings and the surrounding context of the text to determine its authenticity [0]. Researchers have shown that paraphrasing can significantly reduce the detection accuracy of current AI-generated text detectors [1]. To combat this, retrieval-based methods have been proposed, which compare the semantic representation of a candidate text with previously generated AI text stored in a database, making them robust against paraphrasing attacks [8]. These methods leverage both traditional information retrieval techniques and advanced semantic vector representations to increase detection reliability [9]. Experiments have demonstrated that retrieval-based methods can detect a high percentage of paraphrased AI-generated texts with a low false positive rate [8].\n\n**Semantic Consistency Checks**: Ensuring that detection models focus on semantic content rather than syntactic features helps in distinguishing human-generated text from AI-generated ones, even after paraphrasing attacks [8]. Techniques like semantic vector representations and nearest neighbor searches help in maintaining robustness against paraphrasing [8][9].\n\n**Context-Aware Detection**: Incorporating context-aware detection mechanisms can significantly improve detection accuracy. Algorithms that account for the broader context in which a text is generated (such as surrounding text or specific usage scenarios) can provide additional layers of verification [0]. These methods can evaluate the consistency and coherence of the text within a larger context, making it harder for AI-generated texts to evade detection.\n\n#### 3. Optimization of Detection Thresholds [0] \n\n**False Positive Rate (FPR) Management**: In high-stakes environments where false positives are very costly, maintaining a low FPR is crucial. Research suggests setting very stringent thresholds for AI-text classification to ensure a low FPR [6]. While reducing the FPR, it is essential to evaluate the trade-offs with the true positive rate (TPR). Optimization techniques such as adjusting the detection threshold dynamically based on the context and nature of the text can help maintain a balance [11].\n\n**Use of Multiple Classifiers**: Employing multiple detection models and aggregating their results can improve robustness. Using both neural network-based detectors and simpler statistical models can help mitigate the weaknesses of a single detection approach and reduce the probability of false positives [5].\n\nProcedural Strategies\n\nContinuous Monitoring and Updating of Detection Models[8]. \n\n**Adaptive Learning**: Detectors need to be updated regularly to adapt to evolving AI-generated content. As new paraphrasing techniques and text generation models emerge, detection systems must continuously learn from new data [5][6]. Adaptive learning frameworks that integrate new examples of AI-generated texts and re-train models periodically can maintain high detection accuracies over time [7].\n\n2. Multi-Stage Detection Process[12]. \n\n**Human Expert Verification**: Implementing a multi-stage detection process where initial detections are followed by human expert verification can significantly reduce the likelihood of false positives [10][13]. This two-tier approach ensures that only highly suspect texts are flagged, adding an additional layer of scrutiny and verification.\n\nIn summary, the combination of advanced detection algorithms like watermarking and retrieval methods [8], optimization of detection thresholds, contextual and semantic analysis, continuous updating of models, and effective operational practices offers a robust framework for detecting AI-generated texts with minimal false positives. Such practices are essential in environments where the cost of false positives is extremely high [6]. By leveraging multiple detection strategies and adaptive learning, one can ensure the detection system remains effective and reliable. \n\n\nReferences:\n[0] Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense The rise in malicious usage of large language models, such as fake content creation and academic plagiarism, has motivated the development of approaches that identify AI-generated text, including those based on watermarking or outlier detection. However, the robustness of these detection algorithms to paraphrases of AI-generated text remains unclear. To stress test these detectors, we build a 11B parameter paraphrase generation model (DIPPER) that can paraphrase paragraphs, condition on surrounding context, and control lexical diversity and content reordering. Using DIPPER to paraphrase text generated by three large language models (including GPT3.5-davinci-003) successfully evades several detectors, including watermarking, GPTZero, DetectGPT, and OpenAI's text classifier. For example, DIPPER drops detection accuracy of DetectGPT from 70.3% to 4.6% (at a constant false positive rate of 1%), without appreciably modifying the input semantics. To increase the robustness of AI-generated text detection to paraphrase attacks, we introduce a simple defense that relies on retrieving semantically-similar generations and must be maintained by a language model API provider. Given a candidate text, our algorithm searches a database of sequences previously generated by the API, looking for sequences that match the candidate text within a certain threshold. We empirically verify our defense using a database of 15M generations from a fine-tuned T5-XXL model and find that it can detect 80% to 97% of paraphrased generations across different settings while only classifying 1% of human-written sequences as AI-generated. We open-source our models, code and data.\n\n[1] Can AI-Generated Text be Reliably Detected? With this intuition in mind, in §2, we use light-weight neural network-based paraphrasers (2.3×2.3\\times and 5.5×5.5\\times smaller than the source LLM in terms of the number of parameters) to rephrase the source LLM’s output text. Our experiments show that this automated paraphrasing attack can drastically reduce the accuracy of various detectors, including those using soft watermarking. For example, a PEGASUS-based paraphraser can drop the soft watermarking detector’s accuracy from 97%percent9797\\% to 80%percent8080\\% with just a degradation of 3.5 in the perplexity score. The area under the receiver operating characteristic (AUROC) curves of zero-shot detectors drop from 96.5%percent96.596.5\\% to 25.2%percent25.225.2\\% using a T5-based paraphraser. We also observe that the performance of neural network-based trained detectors deteriorate significantly after our paraphrasing attack. For instance, the true positive rate of the RoBERTa-Large-Detector from OpenAI drops from 100%percent100100\\% to 60%percent6060\\% at a realistic low false positive rate of 1%percent11\\%. In addition, we show that retrieval-based detector by Krishna et al. designed to evade paraphrase attacks are vulnerable to recursive paraphrasing. In fact, the accuracy of their detector falls from 100%percent100100\\% to 25%percent2525\\% with our recursive paraphrase attack. We also show that recursive paraphrasing can further deteriorate the performance of watermarking-based detectors (from 99%percent9999\\% to 15%percent1515\\% true positive rate at 1%percent11\\% false positive rate), zero-shot, and neural network-based detectors.\n\n[2] Multiscale Positive-Unlabeled Detection of AI-Generated Texts Recent releases of Large Language Models (LLMs), e.g. ChatGPT, are astonishing at generating human-like texts, but they may impact the authenticity of texts. Previous works proposed methods to detect these AI-generated texts, including simple ML classifiers, pretrained-model-based zero-shot methods, and finetuned language classification models. However, mainstream detectors always fail on short texts, like SMSes, Tweets, and reviews. In this paper, a Multiscale Positive-Unlabeled (MPU) training framework is proposed to address the difficulty of short-text detection without sacrificing long-texts. Firstly, we acknowledge the human-resemblance property of short machine texts, and rephrase AI text detection as a partial Positive-Unlabeled (PU) problem by regarding these short machine texts as partially ``unlabeled\". Then in this PU context, we propose the length-sensitive Multiscale PU Loss, where a recurrent model in abstraction is used to estimate positive priors of scale-variant corpora. Additionally, we introduce a Text Multiscaling module to enrich training corpora. Experiments show that our MPU method augments detection performance on long AI-generated texts, and significantly improves short-text detection of language model detectors. Language Models trained with MPU could outcompete existing detectors on various short-text and long-text detection benchmarks. The codes are available at https://github.com/mindspore-lab/mindone/tree/master/examples/detect_chatgpt and https://github.com/YuchuanTian/AIGC_text_detector.\n\n[3] Can AI-Generated Text be Reliably Detected? Paraphrasers: We show that a paraphrasing attack, where a lightweight neural network-based paraphraser is applied to the output text of the AI-generative model, can evade various types of detectors. Before highlighting the results, let us provide an intuition why this attack is successful. For a given sentence s𝑠s, suppose P​(s)𝑃𝑠P(s) is the set of all paraphrased sentences that have similar meanings to the sentence s𝑠s. Moreover, let L​(s)𝐿𝑠L(s) be the set of sentences the source LLM can output with meanings similar to s𝑠s. Suppose a user has generated s𝑠s using an LLM and wants to evade detection. IfL​(s)|≪|P​(s)|much-less-than𝐿𝑠𝑃𝑠|L(s)|\\ll|P(s)|, the user can randomly sample from P​(s)𝑃𝑠P(s) and avoid detection (if the detection model has a reasonably low false positive rate). Moreover, ifL​(s)|𝐿𝑠|L(s)| is comparable toP​(s)|𝑃𝑠|P(s)|, the detector cannot have low false positive and negative rates simultaneously.\n\n[4] Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense Detection accuracy: Our first metric measures how often the input text is correctly detected as AI-generated. Since detection rates are heavily dependent on the chosen detection threshold, the AUC-ROC metric is commonly used to measure detector performance (Mitchell et al., 2023), which considers the range of all possible thresholds. However, in this application, it is critical that the false positive rate (FPR) is low; in other words, human-written text must almost never be classified as machine-generated (OpenAI, 2023a; Kirchenbauer et al., 2023a). Hence, we fix the FPR to 1% for all detection algorithms (although even 1% is likely too high in practice), and adjust the detection threshold accordingly while reporting detection accuracies. Additionally, we also plot ROC curves focusing on the 0-1% FPR region. Overall, we expect detection rates to plummet on paraphrased text.\n\n[5] Towards Possibilities & Impossibilities of AI-generated Text Detection: A Survey Watermarking solves this problem, as it can synthetically introduce a distribution shift between the two distributions. However, detection can still be made difficult by paraphrasing the text (Sadasivan et al., 2023; Krishna et al., 2023). Let ℳ∗superscriptℳ\\mathcal{M^{*}} represent the distribution of the paraphrased samples. Then, for any watermarking scheme W𝑊W, Sadasivan et al. (2023) provide the following bound:ℙ𝐬∼ℳ∗​[𝐬​watermarked using W≤ℙ𝐬∼ℋ​[𝐬​watermarked using W+T​V​(ℳ∗,ℋ)subscriptℙsimilar-to𝐬superscriptℳdelimited-[𝐬watermarked using Wsubscriptℙsimilar-to𝐬ℋdelimited-[𝐬watermarked using W𝑇𝑉superscriptℳℋ\\mathbb{P}_{\\mathbf{s}\\sim\\mathcal{M^{*}}}[\\mathbf{s}~{}\\text{watermarked using W}\\leq\\mathbb{P}_{\\mathbf{s}\\sim\\mathcal{H}}[\\mathbf{s}~{}\\text{watermarked using W}+{TV}(\\mathcal{M^{*}},\\mathcal{H})(2)If the paraphrased distribution ℳ∗superscriptℳ\\mathcal{M^{*}} closely resembles the human text distribution ℋℋ\\mathcal{H}, then T​V​(ℳ∗,ℋ)𝑇𝑉superscriptℳℋ{TV}(\\mathcal{M^{*}},\\mathcal{H}) will be minimal. From the bound in Equation 2, this implies: (1) ℙ𝐬∼ℋ​[𝐬​watermarked using Wsubscriptℙsimilar-to𝐬ℋdelimited-[𝐬watermarked using W\\mathbb{P}_{\\mathbf{s}\\sim\\mathcal{H}}[\\mathbf{s}~{}\\text{watermarked using W} is high indicating an increase in false positives, i.e., misclassifying human-written text as AI-generated, or (2) ℙ𝐬∼ℳ∗​[𝐬​watermarked using Wsubscriptℙsimilar-to𝐬superscriptℳdelimited-[𝐬watermarked using W\\mathbb{P}_{\\mathbf{s}\\sim\\mathcal{M^{*}}}[\\mathbf{s}~{}\\text{watermarked using W} is low indicating increase in misclassification of watermarked text as human-authored.\n\n[6] Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense Given the vulnerability of AI-generated text detectors to paraphrasing, how can we defend against such attacks? In the second part of our paper (Section 5), we propose to use retrieval methods to detect AI-generated text instead of relying on statistical properties of the text or watermarking. First, an LLM API provider stores every output generated by their model in a database. The API provider then offers a service in which a semantic representation of a candidate text is compared to representations of every generation stored in the database. The search focuses on the semantics of the input and can leverage both standard IR methods such as BM-25 (Robertson et al., 1995) as well as semantic vector representations such as P-SP from Wieting et al. (2022). Since paraphrasing does not modify the semantics of the input, this algorithm is robust to paraphrasing attacks. Specifically, we find that 97.3% of PG19 paraphrases and 80.4% of Wikipedia paraphrases are successfully detected in a large database of over 15M generations, at a 1.0% false positive rate. We extensively discuss the limitations and scalability of retrieval-based detection in Section 5.4.\n\n[7] Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense Querying the database: Let y′superscript𝑦′y^{\\prime} be a candidate text and 𝐲′=fret​(y′)superscript𝐲′subscript𝑓retsuperscript𝑦′\\mathbf{y}^{\\prime}=f_{\\text{ret}}(y^{\\prime}) its encoded vector. Suppose a client wishes to know whether y′superscript𝑦′y^{\\prime} was generated by the API fLMsubscript𝑓LMf_{\\text{LM}}. The API provider can check this by seeing whether the maximum similarity score of y′superscript𝑦′y^{\\prime} to an entry in the database exceeds some detection threshold T𝑇T chosen by the API provider:output=score>T, where score=maxi∈{1,..N}⁡𝐲′⋅𝐲i|𝐲′|​|𝐲i|\\displaystyle=\\text{score}>T,\\text{ where }\\text{score}=\\max_{i\\in\\{1,..N\\}}\\frac{\\mathbf{y}^{\\prime}\\cdot\\mathbf{y}_{i}}{|\\mathbf{y}^{\\prime}|~{}|\\mathbf{y}_{i}|}We expect unperturbed machine-generated text to always get a score of 1.0, while paraphrasing the text may lower the detection score. Hence, lowering T𝑇T will increase the detection rate of heavily-paraphrased text but also increase the false positive rate (i.e., human-written text that resembles sequences previously generated by the LLM API can be falsely flagged). Since N𝑁N can be very large, the score can also be approximated using efficient nearest neighbor libraries like FAISS (Johnson et al., 2019). However, in this work we only compute exact inner products.\n\n[8] Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts Non-native speaker bias. Finally, we show how our model helps to mitigate the bias present in ML-based artificial text detectors. We follow Liang et al. (2023) who demonstrate that current artificial text detectors are often too hard on texts written by non-native speakers. We use OpenAI and GPTZero as the baselines (see Appendix for more results) and PHD and MLE classifiers,choosing the thresholds was chosen on data unrelated to this task, as the equal error classifier on introductions of Wikipedia articles (real vs GPT-3.5-turbo) where it achieved EER of 26.8% for PHD and 22.5% for MLE.On the left, Fig. 6 shows false positive rates (FPR) for three sets of student essays: TOEFL essays by non-native speakers (red), same texts processed by GPT-4 asked to improve the text (grey), and native speakers (blue).First, blue bars are almost invisible for all detectors because the FPR for a native speaker is very small (<1%absentpercent1<1\\%) while non-native speakers can be wrongly accused by OpenAI and GPTZero in 58%percent5858\\% and 52%percent5252\\% of the cases respectively. The PHD classifier reduces this discriminating rate by 2x, showing FPR 26%percent2626\\% for non-native speakers. After GPT-4 polishing, this rate further decreases to 7.7%percent7.77.7\\% compared to 19%percent1919\\% for GPTZero. Interestingly, OpenAI also deals with GPT-4 polished texts suprisingly well, its FPR drops by 15x. The MLE detector also demonstrates less biased behaviour compared to baselines, but worse than PHD.\n\n[9] On the Possibilities of AI-Generated Text Detection To study the AUROC for any detector D𝐷D, we start by invoking LeCam’s lemma (Le Cam, 2012; Wasserman, 2013) which states that for any distributions m𝑚m and hℎh, given an observation s𝑠s, the minimum sum of Type-I and Type-II error probabilities in testing whether s∼msimilar-to𝑠𝑚s\\sim m versus s∼hsimilar-to𝑠ℎs\\sim h is equal to 1−TV​(m,h)1TV𝑚ℎ1-\\texttt{TV}(m,h). Hence, mathematically, we can writeℙs∼h​(⋅)​[D​(s)≥γ⏟Type-I error (false positive)+ℙs∼m​(⋅)​[D​(s)<γ⏟Type-II error (false negative)≥1−TV​(m,h),subscript⏟subscriptℙsimilar-to𝑠ℎ⋅delimited-[𝐷𝑠𝛾Type-I error (false positive)subscript⏟subscriptℙsimilar-to𝑠𝑚⋅delimited-[𝐷𝑠𝛾Type-II error (false negative)1TV𝑚ℎ\\displaystyle\\underbrace{\\mathbb{P}_{s\\sim h(\\cdot)}[D(s)\\geq\\gamma}_{\\text{Type-I error (false positive)}}+\\underbrace{\\mathbb{P}_{s\\sim m(\\cdot)}[D(s)<\\gamma}_{\\text{Type-II error (false negative)}}\\geq 1-\\texttt{TV}(m,h),(5)for any detector D𝐷D and any threshold γ𝛾\\gamma.We note that the above bound is tight and can always be achieved with equality by likelihood-ratio-based detectors for any distribution m𝑚m and hℎh, by the Neyman-Pearson Lemma (Cover, 1999, Chapter 11).We restate the lemma for completeness and discuss its tightness in Appendix B.1.From the definitions of TPR and FPR in (3)-(4), it holds thatFPRγ+1−TPRγ≥1−TV​(m,h),subscriptFPR𝛾1subscriptTPR𝛾1TV𝑚ℎ\\displaystyle\\text{FPR}_{\\gamma}+1-\\text{TPR}_{\\gamma}\\geq 1-\\texttt{TV}(m,h),(6)which implies thatTPRγ≤min⁡{FPRγ+TV​(m,h),1},subscriptTPR𝛾subscriptFPR𝛾TV𝑚ℎ1\\displaystyle\\text{TPR}_{\\gamma}\\leq\\min\\{\\text{FPR}_{\\gamma}+\\texttt{TV}(m,h),1\\},(7)where min\\min is used because TPRγ∈,1subscriptTPR𝛾01\\text{TPR}_{\\gamma}\\in,1. The upper bound in (7) is called the ROC upper bound and is the bound leveraged in one of the recent works (Sadasivan et al., 2023) to derive AUROC upper bound AUC≤12+TV​(m,h)−TV​(m,h)22AUC12TV𝑚ℎTVsuperscript𝑚ℎ22\\texttt{AUC}\\leq\\frac{1}{2}+\\texttt{TV}(m,h)-\\frac{\\texttt{TV}(m,h)^{2}}{2} which holds for any D𝐷D. This upper bound led to the claim of the impossibility of detecting the AI-generated text whenever TV​(m,h)TV𝑚ℎ\\texttt{TV}(m,h) is small. \n\n[10] Multiscale Positive-Unlabeled Detection of AI-Generated Texts In this sense, we model the task of AI-generated text detection as a partial Positive-Unlabeled (PU) problem and formulate the Multiscale Positive-Unlabeled (MPU) training framework to address the challenging task of short text detection without sacrificing long texts. PU problems typically address binary classification tasks where positive data and unlabeled data are offered for training. Considering the partially \"Unlabeled\" property of short machine texts, we rephrase detector training as a partial PU problem and boost detectors’ performance on multiscale texts. In order to improve conventional PU optimization targets for texts of various lengths, a length-aware Multiscale PU (MPU) loss is proposed and applied during the training process. We are aware that the PU prior probability of a text being positive is length-variant. To this end, an abstract recurrent model is designed to adjust the PU prior probability automatically based on corpus length. Further, a Text Multiscaling module is also proposed to exert the effect of Multiscale PU loss by diversifying training corpora in terms of length. Experiments demonstrate that the MPU framework is significantly effective in improving short-text detection performance; meanwhile, detection on long texts is also augmented.\n\n[11] Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts Following Krishna et al. (2023), we report the detection accuracy with false positive rate (FPR) fixed at 1%. Table 2 shows that our PHD-based classifier outperforms all baselines with a large margin: +10%percent10+10\\% for GPT-3.53.53.5, +14%percent14+14\\% for OPT. Note that DetectGPT uses GPT-2 as the base model, which explains its results for GPT-2.PHD is also invulnerable to the DIPPER paraphrasing attack (Krishna et al., 2023). When generated texts are transformed by DIPPER, they lose some characteristic features of the generator, which causes a dramatic drop in quality for most detectors; but for the PHD classifier the accuracy of artificial text detection even increases slightly after this perturbation.Interestingly, the MLE dimension estimator also works quite well for this task, and even achieves 6%percent66\\% better detection for GPT-3.53.53.5 generations; but its adversarial robustness is significantly worse.\n\n[12] Multiscale Positive-Unlabeled Detection of AI-Generated Texts To overcome this issue, we model the task of multiscale text detection as a partial Positive Unlabeled problem (PU). In this problem, corpora from human are regarded as \"Positive\", but short texts from machines are given an additional \"Unlabeled\" mark for PU loss calculations (detailed in Sec. 2.3). Then our detector model is optimized within this partial PU context.\n\n"}
{"case_id": "803bc7891917f823a52948ebee89cf9d", "answer_text": "\n\nIn-context learning (ICL) for large language models (LLMs) differs fundamentally from traditional machine learning (ML) model training methods in several key aspects, including the learning paradigm [0], computational requirements, adaptability, and recent advancements in few-shot learning methodologies [3]. Both methodologies have unique advantages and limitations, which make them suitable for different scenarios and applications.\n\n In-context learning (ICL) has emerged as a new paradigm for instructing large language models (LLMs) to perform complex tasks [12]. In ICL, the task description and demonstration examples are represented in natural language to form a context, and the prediction of LMs conditioned on the context is transformed into answers according to pre-defined rules [12]. In this way, large LMs adapt to various downstream tasks without any modifications to parameters, making it a natural fit for knowledge editing on large LMs [12]. Furthermore, this paradigm has been shown to be effective in both few-shot and zero-shot learning scenarios [0]. \n\nTraditional ML models are typically trained using a supervised learning approach, where the model learns a mapping function \\(f(x)\\) from input data \\(x\\) to the corresponding output \\(y\\) via gradient-based optimization on a large dataset \\(S\\) [3]. This process involves multiple epochs of training to minimize a loss function, and once the training is complete, the model parameters are fixed and not updated during inference.\n\nIn contrast, ICL does not involve modifying model parameters during inference. Instead, it leverages a pre-trained model and adapts to new tasks by conditioning on a prompt that includes input-output examples [0]. This means that the model uses the context provided in the prompt to infer the task requirements and generate the output, acting more like an implicit Bayesian inference process [0]. This method has been likened to \"creating task vectors\" from the context, which then modulate the transformer to produce the desired output [5].\n\n\n\nDetailed Methods Used in ICL\n\nIn-context learning (ICL) in Large Language Models (LLMs) has emerged as a powerful new learning paradigm. However, its underlying mechanism is still not well understood [5]. In-context learning requires LLMs to recognize the task by scanning over the entire input to understand the label space [2]. Large language models (LMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. The LM learns from these examples without being explicitly pretrained to learn [0]. \n\n\n\nICL typically involves the following steps: [0]. \n\n1. **Prompt Engineering**: Carefully crafting the input prompt that includes task instructions and a few example pairs of inputs and outputs [0]. The way the prompt is structured can significantly impact the model's performance, as it provides the context necessary for the model to understand the task at hand [3]. \n  \n2. **Task Vector Generation**: The model processes the prompt to generate a task vector, which is a compressed representation of the task's requirements. This vector helps in modulating the transformer model to produce the appropriate outputs [5].\n\n3. **Contextual Adaptation**: Using the task vector, the model adapts its behavior dynamically during inference without any parameter updates. This allows the model to handle a diverse set of tasks by simply changing the provided context examples [0].\n\nComputational Requirements\n\nPretrained large language models (LLMs) are strong in-context learners that are able to perform few-shot learning without changing model parameters [4]. This ability allows them to adapt to new tasks by using few-shot examples as the input prefix [3]. Despite this progress, LLMs are still unable to solve algorithmic reasoning problems [11]. One area with significant room for improvement is the ability of LLMs to perform complex reasoning tasks [11]. Moreover, fine-tuning an LLM on any specific task generally destroys its in-context ability [4]. However, various methods have been introduced to preserve in-context learning abilities, such as the Prompt Tuning with MOdel Tuning (ProMoT) framework, which shows remarkable generalization ability on tasks that have different formats [4]. \n\n\nThe computational requirements for traditional ML training are substantial, often requiring extensive GPU resources for gradient-based optimization over large datasets. This process can be time-consuming and resource-intensive, especially for large models [3]. \n\nICL, however, reduces computational overhead during inference since it does not involve updating model parameters [12]. This makes ICL a natural fit for scenarios where computational resources are limited or where rapid adaptation to new tasks is required without the iterative process of traditional training.\n\n### Adaptability and Flexibility\n\nLarge language models (LMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. The LM learns from these examples without being explicitly pretrained to learn [0].\n\nThis learning paradigm, known as in-context learning (ICL), enables models to adapt to new tasks by using few-shot training examples as the input prefix, which has been shown to outperform methods like multi-task fine-tuning [3].\n\nMoreover, in-context learning allows LMs to preserve their in-context abilities even after fine-tuning on specific tasks, by offloading task-specific formats into a soft prompt that can be removed when performing other tasks [4].\n\nIn particular, in-context learning has emerged as a powerful approach to address a wide array of tasks without the need for extensive fine-tuning, thus providing a natural fit for knowledge editing on large LMs, as it reduces computation overhead and avoids side effects introduced by parameter updates [12].\n\nFurthermore, by supporting models to make use of dictionary definitions provided in the prompt, in-context learning assists in novel word acquisition and improves performance on tasks requiring such abilities [6].\n\n\nTraditional model training methods can suffer from overfitting to specific tasks, a phenomenon referred to as \"format specialization,\" where the model becomes specialized for the task it was fine-tuned on and loses its generalization ability [4]. Moreover, adapting a traditionally trained model to new tasks usually requires additional fine-tuning, which can lead to degradation of the model's performance on previously learned tasks.\n\nICL, on the other hand, offers a high degree of adaptability and flexibility. Since no parameter updates are involved, the same pre-trained model can be used for a wide array of tasks by simply changing the in-context examples. This makes it highly suitable for few-shot learning scenarios and tasks that demand quick adaptation [1][3]. Despite this flexibility, ICL's performance can be sensitive to the order and quality of the examples provided in the context, which can impact its effectiveness [0][3].\n\nSpecific Results and Applications\n\nICL is particularly effective in tasks requiring rapid adaptation to new or evolving data, such as language translation, text summarization, and question-answering. For example, in text summarization, ICL can leverage a few example summaries to adapt and generate coherent and relevant summaries for new documents without additional parameter training. Similarly, in translation tasks, ICL can quickly adapt to new linguistic patterns or dialects by conditioning on a few example translations [4].\n\nFurthermore, ICL has shown promise in specialized applications such as teaching algorithmic reasoning to LLMs. By formulating algorithmic tasks as sequences of input-output pairs, LLMs can learn and perform complex reasoning tasks more effectively [11]. However, it struggles with tasks requiring the processing of long contexts due to the finite context window of transformer models, often limiting its efficacy in long-document understanding or tasks with extensive input sequences [2, 8, 14].\n\nExpanded Limitations [2]. \n\nDespite its advantages, ICL comes with its own set of challenges and limitations [5]. \n\n1. **Context Window Limits**: Transformer-based LLMs have a fixed context window size, which restricts the length of input sequences they can process effectively. This limitation poses challenges for tasks requiring long-context understanding, such as processing lengthy documents or handling datasets with extensive input [2, 8, 14].\n\n2. **Ordering Sensitivity**: The performance of ICL can significantly depend on the order of examples provided in the context. Improper ordering can lead to suboptimal task adaptation and influence the output quality [0][3].\n\n3. **Task Complexity Handling**: While ICL is effective for certain tasks, it may fall short in complex domains requiring deep understanding and intricate reasoning, such as mathematical problem-solving or algorithmic tasks [11]. Simple prompts may not be sufficient for such tasks, necessitating more advanced prompt engineering or additional methods.\n\n4. **Pre-training Preference**: ICL's reliance on pre-trained models means that it can be influenced by the biases and preferences encoded during the model's pre-training phase. Overcoming these inherent biases can be challenging, especially when in-context label relationships conflict with the model's pre-trained preferences [1, 10].\n\n5. **Generalization vs. Specialization**: While ICL allows for task flexibility, it can also lead to suboptimal performance in scenarios requiring fine-tuned specialization. There is often a trade-off between maintaining generalization capabilities and achieving high performance on specialized tasks [4].\n\n### Comparison with Other Few-Shot Learning Methods [3] \n\nTo gain a broader perspective on few-shot learning, it is helpful to compare ICL with other popular methods in this domain, such as [3]. \n\n1. **Meta-Learning**: Approaches like Model-Agnostic Meta-Learning (MAML) adapt models to new tasks through gradient-based updates during meta-training. Unlike ICL, which does not require parameter updates during inference, MAML involves fine-tuning model parameters on a few-shot basis [13].\n\n2. **Instruction Tuning**: Instruction tuning involves training models on a diverse set of tasks with explicit instructions, enabling them to generalize better to new tasks. This method is closely related to ICL but differs in that it involves parameter updates during training to incorporate task instructions [3].\n\n3. **ProMoT (Prompt Tuning with Model Tuning)**: Targeted to mitigate the drawbacks of format specialization, ProMoT involves a two-stage fine-tuning process where a soft prompt is first trained before fine-tuning the model itself with this prompt attached. This approach preserves the model's in-context learning abilities while fine-tuning it for specific tasks [4].\n\nLarge language models (LLMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples [0]. The LM learns from these examples without being explicitly pretrained to learn. Thus, it is unclear what enables in-context learning [0]. In this paper, we study how in-context learning can emerge when pretraining documents have long-range coherence [0]. Here, the LM must infer a latent document-level concept to generate coherent next tokens during pretraining [0]. At test time, in-context learning occurs when the LM also infers a shared latent concept between examples in a prompt [0]. We prove when this occurs despite a distribution mismatch between prompts and pretraining data in a setting where the pretraining distribution is a mixture of HMMs [0]. In contrast to messy large-scale datasets used to train LMs capable of in-context learning, we generate a small-scale synthetic dataset (GINC) where Transformers and LSTMs both exhibit in-context learning [0]. Beyond the theory, experiments on GINC exhibit large-scale real-world phenomena including improved in-context performance with model scaling (despite the same pretraining loss), sensitivity to example order, and instances where zero-shot is better than few-shot in-context learning [0].\n\nIn summary, in-context learning for large language models offers a novel and flexible learning paradigm that contrasts sharply with traditional machine learning model training. While traditional ML relies on extensive gradient-based optimization and is prone to overfitting, ICL leverages pre-trained models to adapt to new tasks using prompt-based conditioning, thereby reducing computational overhead and enhancing adaptability. The success of ICL in specific applications like real-time text summarization and algorithmic reasoning demonstrates its potential, yet it also faces challenges with long-context understanding, sensitivity to example ordering, and pre-training preference alignment. Both approaches have unique sets of challenges and limitations that must be carefully considered depending on the application and requirements. Additionally, comparing ICL with other few-shot learning methods highlights the diversity and richness of approaches in this fast-evolving field, each offering different strengths that can be leveraged for various applications.\n\n\nReferences:\n[0] An Explanation of In-context Learning as Implicit Bayesian Inference An Explanation of In-context Learning as Implicit Bayesian Inference Large language models (LMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. The LM learns from these examples without being explicitly pretrained to learn. Thus, it is unclear what enables in-context learning. In this paper, we study how in-context learning can emerge when pretraining documents have long-range coherence. Here, the LM must infer a latent document-level concept to generate coherent next tokens during pretraining. At test time, in-context learning occurs when the LM also infers a shared latent concept between examples in a prompt. We prove when this occurs despite a distribution mismatch between prompts and pretraining data in a setting where the pretraining distribution is a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs capable of in-context learning, we generate a small-scale synthetic dataset (GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond the theory, experiments on GINC exhibit large-scale real-world phenomena including improved in-context performance with model scaling (despite the same pretraining loss), sensitivity to example order, and instances where zero-shot is better than few-shot in-context learning. Introduction Large language models (LMs) such as GPT-3 (Brown et al., 2020, Lieber et al., 2021, Radford et al., 2019, Wang and Komatsuzaki, 2021 are pretrained on massive text corpora to predict the next word given previous words. They demonstrate the surprising ability to do in-context learning, where an LM\n\n[1] In-Context Learning Learns Label Relationships but Is Not Conventional Learning With NH2, we explore how in-context label information trades off against the LLM’s pre-training preference, i.e. its zero-shot predictions based on label relationships inferred from pre-training and stored in the model parameters.Often, pre-training preference and in-context label relationships agree: e.g. in Fig. 3, performance is high zero-shot and then improves with ICL.To test NH2 if ICL can overcome pre-training preference, we create scenarios where pre-training preference and in-context observations are not aligned.We then study if ICL behavior is compatible with fully overcoming pre-training preference as we would expect from a conventional learner.\n\n[2] Long-context LLMs Struggle with Long In-context Learning In this paper, we propose to adopt in-context learning (ICL) on extreme-label classification tasks (Anil et al., 2022; Milios et al., 2023) to evaluate long-context LLMs. Unlike the prior tasks, in-context learning requires LLMs to recognize the task by scanning over the entire input to understand the label space. This task necessitates LLMs’ ability to comprehend the entire input to make predictions. Due to the massive label space, the task demonstration could easily become a long sequence. For example, Discovery (Sileo et al., 2019) encompasses 174 classes with each example taking an average of 61 tokens. Therefore, the minimum demonstration for 1 shot/class already exceeds 10K tokens. Normally, LLMs demand more than 1 shot/class to understand the nuances of different fine-grained labels. Thus, this task becomes a natural testbed for long-context understanding.\n\n[3] Meta-learning via Language Model In-context Tuning to perform few-shot in-context learning on a wide variety of training tasks. Similar to in-context learning, LMs trained with in-context tuning adapt to a new task by using few-shot training examples as the input prefix. Formally, during meta-training, we build the model input by concatenating the task instruction I T , task input-output pairs S T ⊆ D T , and the task input x target T 1 to be classified. We then fine-tune a pre-trained LM to predict y target T and hope that the model learns to use the in-context examples S T . Here is the few-shot in-context tuning objective L: (1) To adapt to a new taskT at test time, we directly concatenate the few-shot examples ST with the instruction IT and the target task input x target T to be classified to form the model input, and ask the model to predict its corresponding output. No gradient update is performed during adaptation. Gradient-based Task Adaptation We compare in-context tuning with two classical few-shot learning methods: multi-task fine-tuning (instruction tuning + fine-tuning) and MAML. Both methods adapt the model parameters to new tasks by gradient descent on few-shot examples. Instruction Tuning + Fine-tuning (InsT + FT) We extend the recent work on zero-shot instruction tuning (Wei et al., 2021) to the FSL setting as a multi-task fine-tuning baseline. During metatraining, the model is optimized to predict the task output given the task instruction and the task input on a wide range of tasks (Zhong et al., 2021). Formally, we train the model\n\n[4] Two-stage LLM Fine-tuning with Less Specialization and More Generalization Preserving In-Context Learning ability in Large Language Model Fine-tuning Pretrained large language models (LLMs) are strong in-context learners that are able to perform few-shot learning without changing model parameters. However, as we show, fine-tuning an LLM on any specific task generally destroys its in-context ability. We discover an important cause of this loss, format specialization, where the model overfits to the format of the fine-tuned task and is unable to output anything beyond this format. We further show that format specialization happens at the beginning of fine-tuning. To solve this problem, we propose Prompt Tuning with MOdel Tuning (ProMoT), a simple yet effective two-stage fine-tuning framework that preserves in-context abilities of the pretrained model. ProMoT first trains a soft prompt for the fine-tuning target task, and then fine-tunes the model itself with this soft prompt attached. ProMoT offloads task-specific formats into the soft prompt that can be removed when doing other in-context tasks. We fine-tune mT5 XXL with ProMoT on natural language inference (NLI) and English-French translation and evaluate the in-context abilities of the resulting models on 8 different NLP tasks. ProMoT achieves similar performance on the fine-tuned tasks compared with vanilla fine-tuning, but with much less reduction of in-context learning performances across the board. More importantly, ProMoT shows remarkable generalization ability on tasks that have different formats, e.g. fine-tuning on a NLI binary classification task improves the model's in-context ability to do summarization (+0.53 Rouge-2 score compared to the pretrained model), making ProMoT a promising method to build general purpose capabilities such as grounding and\n\n[5] In-Context Learning Creates Task Vectors In-context learning (ICL) in Large Language Models (LLMs) has emerged as a powerful new learning paradigm. However, its underlying mechanism is still not well understood. In particular, it is challenging to map it to the\"standard\"machine learning framework, where one uses a training set $S$ to find a best-fitting function $f(x)$ in some hypothesis class. Here we make progress on this problem by showing that the functions learned by ICL often have a very simple structure: they correspond to the transformer LLM whose only inputs are the query $x$ and a single\"task vector\"calculated from the training set. Thus, ICL can be seen as compressing $S$ into a single task vector $\\boldsymbol{\\theta}(S)$ and then using this task vector to modulate the transformer to produce the output. We support the above claim via comprehensive experiments across a range of models and tasks.\n\n[6] WinoDict: Probing language models for in-context word acquisition WinoDict: Probing language models for in-context word acquisition We introduce a new in-context learning paradigm to measure Large Language Models' (LLMs) ability to learn novel words during inference. In particular, we rewrite Winograd-style co-reference resolution problems by replacing the key concept word with a synthetic but plausible word that the model must understand to complete the task. Solving this task requires the model to make use of the dictionary definition of the new word given in the prompt. This benchmark addresses word acquisition, one important aspect of the diachronic degradation known to afflict LLMs. As LLMs are frozen in time at the moment they are trained, they are normally unable to reflect the way language changes over time. We show that the accuracy of LLMs compared to the original Winograd tasks decreases radically in our benchmark, thus identifying a limitation of current models and providing a benchmark to measure future improvements in LLMs ability to do in-context learning. Introduction Large Language Models (LLMs) such as GPT-3 (Brown et al., 2020) and PALM (Chowdhery et al., 2022) can only learn from information that is in their training corpus. However, this is naturally limiting because the training corpus itself is bounded in time to the point of its collection. As a result, recent work has studied how to adapt such models to new data without an expensive retraining phase. Methods range from using a semiparametric methods with access to external memory (e.g., Guu et al. 2020;Lewis et al. 2020), to continual learning (e.g., Dhingra et al. 2022;Lazaridou et\n\n[7] Long-context LLMs Struggle with Long In-context Learning Long In-context Learning on LLMs As pre-trained language models continue to grow in size, in-context learning (ICL) has emerged as a favored approach for addressing a wide array of tasks without the need for extensive fine-tuning  (Dong et al., 2023). A body of research has established that increasing the number of example demonstrations can enhance ICL performance  (Liu et al., 2022; Wu et al., 2023). Nonetheless, there are studies indicating that longer input prompts can actually diminish performance  (Liu et al., 2023), with the effectiveness of prior large language models (LLMs) being constrained by the maximum sequence length encountered during their training. It is also claimed in previous works that LLM+ICL falls short on specification-heavy tasks due to inadequate long-text understanding ability  (Peng et al., 2023c). To counter this issue, various works have introduced memory augmentation and extrapolation techniques to support ICL with an extensive set of demonstrations  (Li et al., 2023c; Wang et al., 2023).\n\n[8] In-Context Learning Learns Label Relationships but Is Not Conventional Learning In our experiments, we first examine if ICL predictions depend on the labels of in-context examples by studying how probabilistic metrics react to randomized in-context labels (§5, Fig. 1).Further, we study ICL on a truly novel task the LLM cannot know from pre-training (§6).Both experiments show that ICL typically considers in-context label relations.We then investigate if ICL is powerful enough to overcome prediction preferences learned from pre-training data (§7).In our experiments, we find this is typically not the case as ICL performance plateaus if label relations oppose pre-training preference.Further, while additional prompting can improve ICL here, we ultimately do not find prompts that lead to the desired behavior.Finally, we study if ICL treats all information provided in-context equally (§8).This is important when the context contains diverging information about a label relationship.By modifying label relations during ICL, we find it does not treat all in-context information equally, and, instead, ICL preferentially makes use of information closer to the query.\n\n[9] Teaching Algorithmic Reasoning via In-context Learning Teaching Algorithmic Reasoning via In-context Learning Large language models (LLMs) have shown increasing in-context learning capabilities through scaling up model and data size. Despite this progress, LLMs are still unable to solve algorithmic reasoning problems. While providing a rationale with the final answer has led to further improvements in multi-step reasoning problems, Anil et al. 2022 showed that even simple algorithmic reasoning tasks such as parity are far from solved. In this work, we identify and study four key stages for successfully teaching algorithmic reasoning to LLMs: (1) formulating algorithms as skills, (2) teaching multiple skills simultaneously (skill accumulation), (3) teaching how to combine skills (skill composition) and (4) teaching how to use skills as tools. We show that it is possible to teach algorithmic reasoning to LLMs via in-context learning, which we refer to as algorithmic prompting. We evaluate our approach on a variety of arithmetic and quantitative reasoning tasks, and demonstrate significant boosts in performance over existing prompting techniques. In particular, for long parity, addition, multiplication and subtraction, we achieve an error reduction of approximately 10x, 9x, 5x and 2x respectively compared to the best available baselines. Introduction Large language models (LLMs) have shown impressive progress in recent years, driven by the scaling up of models and training data sizes Wei et al., 2022a;Hoffmann et al., 2022) that has led to improved performance and sample efficiency Chen et al., 2021;Chowdhery et al., 2022). One area with significant room for improvement is the ability of LLMs to perform complex reasoning tasks. In this realm, mathematical\n\n[10] Can We Edit Factual Knowledge by In-Context Learning? Recently, in-context learning (ICL) (Brown et al., 2020) has emerged as a new paradigm for instructing LLMs to perform complex tasks.In ICL, the task description and demonstration examples are represented in natural language to form a context, and the prediction of LMs conditioned on the context is transformed into answers according to pre-defined rules (Brown et al., 2020).In this way, large LMs adapt to various downstream tasks without any modifications to parameters,making it a natural fit for knowledge editing on large LMs.First, it reduces the computation overhead by avoiding modifications to parameters, as well as eliminates the risk of side effects introduced by parameter updates.Most importantly, ICL provides an interpretable way for humans to calibrate LM behaviors.Despite these advantages, whether ICL is applicable to knowledge editing still remains unclear.\n\n[11] Meta-learning via Language Model In-context Tuning robust to distribution shift (Lester et al., 2021)? Can we combine in-context learning and gradient learning to get the benefit of both worlds (Wortsman et al., 2021)? Conclusion In this work, we propose meta-learning via incontext tuning, which recasts the few-shot learning process of task adaptation and task-specific prediction as a simple sequence prediction problem, where few-shot labeled examples are concatenated with the target example to form the model input. In-context tuning out-performs a wide variety of baselines in terms of accuracy, including raw LM prompting, MAML and instruction tuning. Meanwhile, sensitivity study shows that our FSL approach of in-context tuning is significantly less sensitive to few-shot examples and instruction wording compared to raw LM prompting. Given the empirical effectiveness of in-context tuning, we conjecture that the few-shot learning potential of large LMs (e.g., GPT-3) might be broadly underestimated, and that in-context tuning can eliminate well-known artifacts of few-shot LM prompting such as over-sensitivity to example ordering, example selection and instruction wording.\n\n[12] Long-context LLMs Struggle with Long In-context Learning Long Context EvaluationDue to the imperious demands for the support of long-range LLMs, there is a series of benchmarks focusing on long context evaluation. Long-Range Arena  (Tay et al., 2021) includes tasks consisting of sequences ranging from 1K to 16K tokens to evaluate variations of fast Transformers.LongBench  (Bai et al., 2023b) comprises 21 bilingual datasets within 6 types of tasks with an average length of around 6k words, which have been processed in a unified format to enable effortless evaluation. L-Eval Benchmark  (An et al., 2023) supports 20 sub-tasks with input lengths of 3K to 200K tokens. LooGLE  (Li et al., 2023b) focuses on summarization and four types of long dependency QA tasks with test instances exceeding 100k words. Most recently, ∞\\inftyBench  (Zhang et al., 2024) encompasses 12 tasks, collecting from realistic, auto-generated, and human-annotated datasets with an average length of 200K tokens. Another recent work explores the impact of extending input lengths on the capabilities of Large Language Models, especially on reasoning tasks  (Levy et al., 2024). Versatile as these benchmarks, none of them focus on exploring the capability of LLMs confronted with long in-context learning with extreme label space, which is quite different from the tasks of long-document understanding or synthetic needle in a haystack. Thus, our LongICLBench is proposed to fill the niche and make a more comprehensive long-context evaluation for LLMs.\n\n"}
{"case_id": "928646729f5d824dd9a52e9ddff70e58", "answer_text": "\n\nOverfitting in the context of in-context learning, particularly in vision-language models and unsupervised contrastive learning, has been a topic of growing interest. In general, overfitting occurs when a machine learning model captures noise or irrelevant details in the training data to the detriment of its performance on unseen data [2]. The phenomenon of overfitting can manifest differently depending on the specific task and learning paradigm.\n\n\n### Unsupervised Contrastive Learning [0]\n\nIn unsupervised contrastive learning, researchers have observed that models can indeed overfit given enough training epochs. Overfitting in this context often leads to the model failing to bring positive pair representations closer, which is a fundamental goal of contrastive learning. This phenomenon results in the positive similarity, or the similarity between representations of similar items, driving the overfitting [0]. An evident manifestation of this behavior is a decline in the ability of the model to discriminate between similar and dissimilar pairs effectively [1]. This outcome implies that while the model appears to perform well on the training data, it does not generalize well to new, unseen data due to its overreliance on specific training examples [5].\n\nVision-Language Models [13].\nPretrained vision-language models like CLIP, which are fine-tuned using continuous prompts, demonstrate a unique form of overfitting. Empirical evidence suggests that as these models are trained to adjust prompts for task-specific data, their performance initially improves but eventually deteriorates on both base and novel classes. This decline indicates that the model becomes overly specialized to the training prompts, adversely affecting its ability to generalize [13]. Analyzing gradient flow within these models has uncovered that the model initially favors generalizable features but later shifts towards spurious features, exacerbating the overfitting issue [13]. To counter this, techniques like Subspace Prompt Tuning and Novel Feature Learners have been proposed to maintain generalization throughout the training process, significantly mitigating the overfitting problem [13].\n\n### Mechanisms and Mitigation\n\nOverfitting in neural networks is commonly observed. When selecting how long to train a network, a simple choice is to train until overfitting occurs. One can detect overfitting in the classical supervised setting by observing that the training error decreases, while validation error begins to increase and diverge from the training error [1]. This signals that the network has “overfit” to the training data.\n\n\nResearch has delved into various mechanisms and mitigation strategies for overfitting. One proposed approach is regularization, such as dropout, which helps to balance the training and validation loss relationship, thus mitigating overfitting [2]. In reinforcement learning with partial observability, it has been observed that smaller state representations can decrease the risk of overfitting while increasing asymptotic bias [3]. This trade-off suggests that under certain conditions, models can be designed to minimize overfitting at the expense of a controlled amount of bias.\n\nGeneralization Despite Overfitting [12].\nInterestingly, the phenomenon of benign overfitting has been noted, where certain models, particularly highly complex ones, continue to generalize well despite apparent overfitting. This has been extensively studied in classical and quantum machine learning models, where the complexity of the model enables it to interpolate noisy data effectively while retaining generalization capabilities [12][14]. This suggests that in some scenarios, particularly with very complex models, overfitting does not necessarily preclude good generalization performance.\n\nImplications in Contextual Language Models[7].\nIn contextual language models, overfitting can lead to a diminished ability to incorporate contextual information. Models often overfit low-frequency or uniquely tokenized words, which leads to these words being processed in a manner that reflects their embeddings in only a few contexts, reducing their generalization across broader contexts [7][9]. This type of overfitting can be particularly problematic in tasks requiring high contextual sensitivity.\n\nOverall, while overfitting remains a critical challenge in in-context learning across different paradigms [2], ongoing research continues to uncover its mechanisms and develop sophisticated techniques to mitigate its impact, preserving the generalization capabilities of models [13]. This underscores the importance of balancing model complexity and training strategies to ensure robustness and adaptability in practical applications. \n\n\nReferences:\n[0] Overfitting In Contrastive Learning? Unsupervised contrastive learning is a popular paradigm in deep-learning.In this work, we investigated if this framework is capable of overfitting and what overfitting means in this context.We found that given enough epochs, unsupervised contrastive learning can indeed overfit to the training set.We showed that when the model does overfit, it loses the ability to bring positive pair’s representations closer to each other, resulting in the positive similarity being the driving factor behind the overfitting phenomenon.\n\n[1] Overfitting In Contrastive Learning? Overfitting in neural networks is commonly observed.When selecting how long to train a network, a simple choice is to train until overfitting occurs.One can detect overfitting in the classical supervised setting by observing that the training error decreases, while validation error begins to increase and diverge from the training error.This signals that the network has “overfit” to the training data.We ask if overfitting can occur with unsupervised contrastive learning, and if so, what does overfitting mean in this context.\n\n[2] Machine Learning Students Overfit to Overfitting Overfitting is the lack of generalization in a machine learning model (Murphy, 2022) (Bishop & Nasrabadi, 2006) (Goodfellow et al., 2016). This is usually evaluated over losses computed on train and validation split of the data, where the generalization gap can be estimated:Lgap=Lval−Ltrainsubscript𝐿gapsubscript𝐿valsubscript𝐿trainL_{\\text{gap}}=L_{\\text{val}}-L_{\\text{train}}(1)In general if Lgap>>0much-greater-thansubscript𝐿gap0L_{\\text{gap}}>>0, it is said that the model is overfitting. But there is normally a small difference between validation and training loss, the question is, how much difference should there be to declare overfitting. Regularization methods like Dropout can also have the effect of inverting the train and validation loss relationship (Srivastava et al., 2014). The typical view of overfitting is presented in Figure 1, where training loss decreases with epochs while validation loss increases, clearly indicating overfitting.\n\n[3] On overfitting and asymptotic bias in batch reinforcement learning with partial observability This paper provides an analysis of the tradeoff between asymptotic bias (suboptimality with unlimited data) and overfitting (additional suboptimality due to limited data) in the context of reinforcement learning with partial observability. Our theoretical analysis formally characterizes that while potentially increasing the asymptotic bias, a smaller state representation decreases the risk of overfitting. This analysis relies on expressing the quality of a state representation by bounding $L_1$ error terms of the associated belief states.  Theoretical results are empirically illustrated when the state representation is a truncated history of observations, both on synthetic POMDPs and on a large-scale POMDP in the context of smartgrids, with real-world data. Finally, similarly to known results in the fully observable setting, we also briefly discuss and empirically illustrate how using function approximators and adapting the discount factor may enhance the tradeoff between asymptotic bias and overfitting in the partially observable context.\n\n[4] Overfitting In Contrastive Learning? ℓ​(xi,xj)ℓsubscript𝑥𝑖subscript𝑥𝑗\\displaystyle\\ell(x_{i},x_{j})=−sim​(zi,zj)/τ+absentlimit-fromsimsubscript𝑧𝑖subscript𝑧𝑗𝜏\\displaystyle=-\\text{sim}(z_{i},z_{j})/\\tau+(3)log​∑k=12​N𝟙[k≠i​exp​(sim​(zi,zk)/τ)logsuperscriptsubscript𝑘12𝑁subscript1delimited-[𝑘𝑖expsimsubscript𝑧𝑖subscript𝑧𝑘𝜏\\displaystyle\\text{log}\\sum_{k=1}^{2N}\\mathbbm{1}_{[k\\neq i}\\text{exp}(\\text{sim}(z_{i},z_{k})/\\tau)\n\n[5] Low Frequency Names Exhibit Bias and Overfitting in Contextualizing Language Models Contextualization measures how much information from context a word incorporates. Words that incorporate information from context generalize well in contextualizing language models, whereas words which do not incorporate contextual information generalize poorly. To quantify contextualization, we measure intra-layer self-similarity across contexts. If a word is less contextualized, we seek to understand whether this is due to overfitting, or underfitting. How much processing a word undergoes in a model can help to indicate whether it is overfit or underfit. If the model changes the word significantly from initial representation, but fails to contextualize it, this suggests that the word is overfit, and processed to be similar to its embedding in few other contexts. If the model does not change the word much from its initial representation, and fails to contextualize it, this suggests that the model relies on a general representation, similar to what it sees in its embedding lookup matrix, indicating that the representation may be underfit. We measure inter-layer self-similarity to initial representation to determine whether poorly contextualized words are overfit or underfit.\n\n[6] Low Frequency Names Exhibit Bias and Overfitting in Contextualizing Language Models Infrequent and multiply tokenized names exhibit higher intra-layer self-similarity across language models, but lower inter-layer similarity to initial representation. Such names incorporate less information from context, and change more from initial representation, suggesting that disparities in contextualization are the result of overfitting.\n\n[7] Generalization despite overfitting in quantum machine learning models Generalization despite overfitting in quantum machine learning models The widespread success of deep neural networks has revealed a surprise in classical machine learning: very complex models often generalize well while simultaneously overfitting training data. This phenomenon of benign overfitting has been studied for a variety of classical models with the goal of better understanding the mechanisms behind deep learning. Characterizing the phenomenon in the context of quantum machine learning might similarly improve our understanding of the relationship between overfitting, overparameterization, and generalization. In this work, we provide a characterization of benign overfitting in quantum models. To do this, we derive the behavior of a classical interpolating Fourier features models for regression on noisy signals, and show how a class of quantum models exhibits analogous features, thereby linking the structure of quantum circuits (such as data-encoding and state preparation operations) to overparameterization and overfitting in quantum models. We intuitively explain these features according to the ability of the quantum model to interpolate noisy data with locally\"spiky\"behavior and provide a concrete demonstration example of benign overfitting. Introduction A long-standing paradigm in machine learning is the trade-off between the complexity of a model family and the model's ability to generalize: more expressive model classes contain better candidates to fit complex trends in data, but are also prone to overfitting noise,2. Interpolation, defined for our purposes as choosing a model with zero training error, was hence long considered bad practice. The success of deep learning -machine learning in a specific regime of extremely complex model families with vast\n\n[8] Understanding and Mitigating Overfitting in Prompt Tuning for Vision-Language Models Pretrained vision-language models (VLMs) such as CLIP have shown impressive generalization capability in downstream vision tasks with appropriate text prompts. Instead of designing prompts manually, Context Optimization (CoOp) has been recently proposed to learn continuous prompts using task-specific training data. Despite the performance improvements on downstream tasks, several studies have reported that CoOp suffers from the overfitting issue in two aspects: (i) the test accuracy on base classes first improves and then worsens during training; (ii) the test accuracy on novel classes keeps decreasing. However, none of the existing studies can understand and mitigate such overfitting problems. In this study, we first explore the cause of overfitting by analyzing the gradient flow. Comparative experiments reveal that CoOp favors generalizable and spurious features in the early and later training stages, respectively, leading to the non-overfitting and overfitting phenomena. Given those observations, we propose Subspace Prompt Tuning (Sub PT) to project the gradients in back-propagation onto the low-rank subspace spanned by the early-stage gradient flow eigenvectors during the entire training process and successfully eliminate the overfitting problem. In addition, we equip CoOp with a Novel Feature Learner (NFL) to enhance the generalization ability of the learned prompts onto novel categories beyond the training set, needless of image training data. Extensive experiments on 11 classification datasets demonstrate that Sub PT+NFL consistently boost the performance of CoOp and outperform the state-of-the-art CoCoOp approach. Experiments on more challenging vision downstream tasks, including open-vocabulary object detection and zero-shot semantic segmentation, also verify the effectiveness of the proposed method. Codes can be found at https://tinyurl.com/mpe64f89.\n\n[9] Generalization despite overfitting in quantum machine learning models The widespread success of deep neural networks has revealed a surprise in classical machine learning: very complex models often generalize well while simultaneously overfitting training data. This phenomenon of benign overfitting has been studied for a variety of classical models with the goal of better understanding the mechanisms behind deep learning. Characterizing the phenomenon in the context of quantum machine learning might similarly improve our understanding of the relationship between overfitting, overparameterization, and generalization. In this work, we provide a characterization of benign overfitting in quantum models. To do this, we derive the behavior of a classical interpolating Fourier features models for regression on noisy signals, and show how a class of quantum models exhibits analogous features, thereby linking the structure of quantum circuits (such as data-encoding and state preparation operations) to overparameterization and overfitting in quantum models. We intuitively explain these features according to the ability of the quantum model to interpolate noisy data with locally \"spiky\" behavior and provide a concrete demonstration example of benign overfitting.\n\n"}
{"case_id": "1f384c4d3942c46b676b5bfc66447192", "answer_text": "\n\nLarge Language Models (LLMs) have demonstrated significant potential in addressing traditional tabular machine learning (ML) tasks using in-context learning (ICL) [0][14], though their effectiveness varies based on several factors including the specific configurations of the tasks and data regimes.\n\n**In-Context Learning vs. Traditional Approaches**[5].\nICL leverages the ability of LLMs to learn and adapt to new tasks by providing examples directly in the input prompt, negating the need for parameter updates or extensive fine-tuning [14]. This method contrasts with traditional approaches like supervised learning, where models need to be trained on large datasets to perform well. The use of ICL is particularly advantageous in ultra low-data regimes, where fine-tuning can lead to overfitting. Studies have shown that ICL outperforms traditional fine-tuning in these settings, highlighting its robustness due to the reduced risk of overfitting [12].\n\nPerformance on Various ML Tasks demonstrates the broad capabilities of large language models (LLMs) across various domains, including tabular data management and low-data regimes [10]. These models exploit prior knowledge and can be fine-tuned or prompted to achieve impressive results on downstream tasks [0]. For example, LLMs can generate synthetic datasets that improve the performance of classifiers, particularly important for underrepresented subgroups [2]. In the context of tabular data learning, LLMs have shown competitive results when fine-tuned for specific tasks [1]. This highlights their potential to transform traditional machine learning approaches by leveraging extensive pre-trained knowledge and advanced tuning techniques [3]. However, the adaptability of LLMs also poses challenges, such as susceptibility to adversarial attacks and structural biases in data interpretation [7].\nLLMs, through ICL, have shown competitive performance on a range of tabular data tasks. For example, in tabular classification and regression, ICL has been effective in allowing LLMs like GPT-3 and its derivatives to approach or even surpass traditional tree-based models in certain few-shot learning scenarios [1][3]. This aligns with the findings where LLMs leveraging ICL demonstrated improved performance on underrepresented subgroups, significantly outperforming traditional generative models in data augmentation tasks [0][2].\n\nChallenges and Limitations\n\n[3] \nDespite the promising results, LLMs face challenges in fully comprehending and processing the intricacies of tabular data. One significant issue is structural bias; LLMs can struggle with the orientation of tables, such as correctly transposing table structures or dealing with shuffled rows and columns [7]. Furthermore, LLMs might not be as robust under adversarial perturbations in tabular data, such as changes in table headers or the introduction of correlated or masked columns [7]. This could limit their effectiveness in real-world applications where such variations are common.\n\nEnhancements through Hybrid Approaches[2]\nTo bridge the gap between the general knowledge encoded in LLMs and the specific skills required to handle tabular data, some studies have proposed hybrid approaches such as combining LLMs with structured knowledge graphs or dependency graphs to capture feature relationships better [6][9]. Additionally, methods like chain-of-thought prompting have been employed to enhance the LLMs’ abilities in complex reasoning tasks, which is essential for understanding and generating meaningful insights from tabular data [5].\n\n**Context-Aware Solutions**[8]\nContext-aware solutions have also been explored to enhance the performance of LLMs on tabular data tasks. For instance, the Context-Aware Automated Feature Engineering (CAAFE) approach uses LLMs to iteratively generate additional semantic features based on the dataset's description, improving the performance of traditional classifiers [8]. This approach underscores the ability of LLMs to provide interpretable and useful features for downstream ML tasks.\n\n\n\n**Empirical Success and Future Directions** [1][0][2][4][5][3]\n\n\nEmpirical evaluations have consistently supported the potential of LLMs to transform into effective tabular data models through ICL and related techniques. For example, deploying ICL strategies in generating synthetic tabular data or creating demonstrations tailored to the input queries has led to substantial improvements in model accuracy and data utility [14]. Future research might focus on further optimizing these hybrid approaches, addressing challenges like structural bias and adversarial robustness, and exploring the integration of LLMs into more automated machine learning workflows [1][7][10].\n\n\n**Conclusion**: As empirically shown, CLLM exploits the prior knowledge of LLMs along with our curation mechanism for improved downstream performance [0]. CLLM outperforms traditional generative models—most noticeably on underrepresented subgroups, for which data augmentation is of utmost importance [0]. Further improvements may be achieved through different tuning and prompting of the LLM, as shown in different domains [0].\n\nIn summary, LLMs utilizing ICL exhibit strong performance in solving traditional tabular ML tasks, especially in data-constrained environments [0]. Their effectiveness hinges on optimizing prompting techniques, combining structured knowledge, and enhancing context-aware learning [5]. While they face challenges like structural bias and robustness to adversarial attacks, ongoing research is actively addressing these issues [7]. The integration of LLMs into automated ML workflows and hybrid approaches holds promise for more comprehensive and robust applications in various data-intensive domains [8].\n\n\nReferences:\n[0] Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation in ultra low-data regimes We introduce CLLM, an approach for data augmentation in the low-data setting. CLLM  exploits the prior knowledge of LLMs along with our curation mechanism for improved downstream performance. As empirically shown, CLLM  outperforms traditional generative models—most noticeably on underrepresented subgroups, for which data augmentation is of utmost importance. CLLM  is grounded in the ICL capability of LLMs. Further improvements may be achieved through different tuning and prompting of the LLM, as shown in different domains (Meng et al., 2023; Liu et al., 2023). Improving LLM tuning and prompting is beyond the scope of our work, but we regard this as a promising avenue for future work.\n\n[1] From Supervised to Generative: A Novel Paradigm for Tabular Deep Learning with Large Language Models In our experiments, we employ LLaMA 2 (Touvron et al., 2023) as the base LLM, construct a large-scale data collection containing 115115115 public tabular datasets for generative tabular learning, and evaluate our approach on nine datasets (non-overlapping with our 115115115 datasets) built by Hegselmann et al. (2023).Concerning instruction following, our results show that our approach substantially improves the base LLaMA model and GPT-3.5 (Brown et al., 2020), outperforms TabLLM (Hegselmann et al., 2023) most of the time, and surprisingly, approaches or occasionally surpasses GPT-4 (OpenAI, 2023).In terms of fine-tuning for new tasks, our approach exhibits competitive performance compared to various state-of-the-art solutions for tabular learning.These results clearly demonstrate the significance of our approach in transforming LLMs into TabFMs.Moreover, we provide an in-depth discussion on our failure cases, limitations of the existing implementation, remaining challenges, and potential opportunities to explore.\n\n[2] Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation in ultra low-data regimes Our Approach. To address this challenge, we introduce CLLM, an approach for data augmentation in low-data regimes.As shown in Figure 1, CLLM  leverages LLMs to generate a synthetic dataset Dsynsubscript𝐷synD_{\\mathrm{syn}} using a small dataset Dtrainsubscript𝐷trainD_{\\mathrm{train}} (Sec. 2.1). It exploits the LLMs’ prior knowledge via in-context learning (ICL) and contextual information. CLLM  then curates Dsynsubscript𝐷synD_{\\mathrm{syn}} by analyzing the learning dynamics of samples in Dsynsubscript𝐷synD_{\\mathrm{syn}} based on predictive confidence and aleatoric (data) uncertainty. These metrics are obtained by training a supervised model on Dtrainsubscript𝐷trainD_{\\mathrm{train}}. We leverage them to define a curated dataset Dcuratedsubscript𝐷curatedD_{\\mathrm{curated}}, which is used to train a downstream classifier (Sec. 2.2).\n\n[3] From Supervised to Generative: A Novel Paradigm for Tabular Deep Learning with Large Language Models The remarkable success of LLMs, scaled to unprecedented sizes and trained on massive text corpus, has demonstrated their broad knowledge and incredible capabilities in transfer learning and instruction following (Brown et al., 2020; Ouyang et al., 2022). This success has motivated the adaptation of LLMs to tabular deep learning with the aims of 1) inheriting the broad world knowledge already learned, 2) enabling instruction following to support arbitrary tasks without tuning, and 3) effectively utilizing meta-information in tabular data, such as column names, task descriptions, and background knowledge. LIFT (Dinh et al., 2022) proposed language-interfaced fine-tuning, which fine-tuned GPT-3 (Brown et al., 2020) and GPT-J (Wang & Komatsuzaki, 2021) on multiple tabular learning datasets, finding that the performance of fine-tuned LLMs was roughly comparable to traditional solutions. TabLLM (Hegselmann et al., 2023), a subsequent study adopting T0 (Sanh et al., 2022) as the base LLM, reported competitive performance of fine-tuned LLMs in very few-shot scenarios but slight underperformance compared to classical tree models (Chen & Guestrin, 2016; Ke et al., 2017) and tabular networks (Hollmann et al., 2023) when more shots were available. In contrast to these studies that directly utilize LLMs pretrained on language data, our paper proposes to first bridge the gap between the general knowledge learned by LLMs and the specific abilities required to truly understand tabular data, such as numeracy skills, capturing feature dependencies, and understanding feature distributions. To achieve this, we introduce an additional learning stage on tabular data and develop several specific objectives to stimulate the learning of crucial abilities in understanding tabular data. Additionally, AnyPredict (Wang et al., 2023) also employed LLMs to build supplementary data for a specific target task, and TapTap (Zhang et al., 2023) used LLMs to generate synthetic tabular data, both of which differ from our approach.\n\n[4] Anomaly Detection of Tabular Data Using LLMs We fine-tune the LLMs by maximizing the conditional log-likelihood ∑b=1Blog⁡p​(Yb|Xb;θLoRA,θLLM)superscriptsubscript𝑏1𝐵𝑝conditionalsubscript𝑌𝑏subscript𝑋𝑏subscript𝜃LoRAsubscript𝜃LLM\\sum_{b=1}^{B}\\log p(Y_{b}|X_{b};\\theta_{\\text{LoRA}},\\theta_{\\text{LLM}}) of our simulated synthetic dataset {(Xb,Yb)}b=1Bsuperscriptsubscriptsubscript𝑋𝑏subscript𝑌𝑏𝑏1𝐵\\{(X_{b},Y_{b})\\}_{b=1}^{B} with respect to the learnable θLoRAsubscript𝜃LoRA\\theta_{\\text{LoRA}} while keeping LLM’s original parameter θLLMsubscript𝜃LLM\\theta_{\\text{LLM}} fixed.The conditional log-likelihood can be further factorized over the tokens {yib}i=1Lbsuperscriptsubscriptsubscriptsuperscript𝑦𝑏𝑖𝑖1subscript𝐿𝑏\\{y^{b}_{i}\\}_{i=1}^{L_{b}} of each response Ybsubscript𝑌𝑏Y_{b} in an auto-regressive fashion: ∑b=1B∑i=1Lblog⁡p​(yib|y<ib,Xb;θLoRA,θLLM)superscriptsubscript𝑏1𝐵superscriptsubscript𝑖1subscript𝐿𝑏𝑝conditionalsuperscriptsubscript𝑦𝑖𝑏superscriptsubscript𝑦absent𝑖𝑏subscript𝑋𝑏subscript𝜃LoRAsubscript𝜃LLM\\sum_{b=1}^{B}\\sum_{i=1}^{L_{b}}\\log p(y_{i}^{b}|y_{<i}^{b},X_{b};\\theta_{\\text{LoRA}},\\theta_{\\text{LLM}}).After optimization, θLoRAsubscript𝜃LoRA\\theta_{\\text{LoRA}} can be integrated into θLLMsubscript𝜃LLM\\theta_{\\text{LLM}} by an element-wise addition, which keeps the model size constant. More details are in Hu et al. (2022).\n\n[5] Large Language Models(LLMs) on Tabular Data: Prediction, Generation, and Understanding - A Survey Several key emergent abilities of LLMs are critical for data understanding and modeling including in-context learning, instruction following, and multi-step reasoning. In-context learning refers to designing large auto-regressive language models that generate responses on unseen task without gradient update, only learning through a natural language task description and a few in-context examples provided in the prompt. The GPT3 model (Brown et al., 2020) with 175 billion parameters presented an impressive in-context learning ability that was not seen in smaller models. LLMs have also demonstrated the ability to complete new tasks by following only the instructions of the task descriptions (also known as zero-shot prompts). Some papers also fine-tuned LLMs on a variety of tasks presented as instructions (Thoppilan et al., 2022). However, instruction-tuning is reported to work best only for larger-size models (Wei et al., 2022a; Chung et al., 2022). Solving complex tasks involving multiple steps have been challenging for LLMs. By including intermediate reasoning steps, prompting strategies such as chain-of-thought (CoT) has been shown to help unlock the LLM ability to tackle complex arithmetic, commonsense, and symbolic reasoning tasks (Wei et al., 2023). These new abilities of LLMs lay the groundwork for exploring their integration into intricate tasks extending beyond traditional NLP applications across diverse data types.\n\n[6] Are LLMs Naturally Good at Synthetic Tabular Data Generation? Since we consider autoregressive LLM-based generative models, employing the chain rule to sequentially produce each column of a table record 𝐭isubscript𝐭𝑖\\mathbf{t}_{i}, we can view each generationstep as approximating the joint distribution of the table columns as a product of conditional distributions (i.e., ℙ(ti,1,…,ti,m)≃∏j=1mℙ(ti,j|ti,1,..,ti,j−1)\\mathbb{P}(t_{i,1},...,t_{i,m})\\simeq\\prod_{j=1}^{m}\\mathbb{P}(t_{i,j}|t_{i,1},..,t_{i,{j-1}})).However, as the number of columns increases and the relationships between columns get more conditional, the likelihood of encountering training and sampling bias due to class imbalance also rises . To minimize such adverse effects, we can consider injecting knowledge of the pre-existing functional relationships among columns, to govern the autoregressive generation process. To infer such functional relationships, we leverage a learned dependency graph derived from functional dependency (FD) relations which enables us to effectively determine the appropriate training and sampling sequence. This, in turn, allows us to alleviate potential biases during training by establishing a generation curriculum leading to improved estimation accuracy of the joint distribution ℙ​(ti)ℙsubscriptt𝑖\\mathbb{P}(\\textbf{t}_{i}) in auto-regressive prediction ℙ(ti,k1,…,ti,km)≃∏j=1mℙ(ti,kj|ti,k1,..,ti,kj−1)\\mathbb{P}(t_{i,k_{1}},...,t_{i,{k_{m}}})\\simeq\\prod_{j=1}^{m}\\mathbb{P}(t_{i,k_{j}}|t_{i,k_{1}},..,t_{i,k_{j-1}}), where the ordering ti,k1​…,ti,kmsubscript𝑡𝑖subscript𝑘1…subscript𝑡𝑖subscript𝑘𝑚t_{i,k_{1}}\\dots,t_{i,k_{m}} is obtained by a feature order permutation function 𝐤=ϕ​(𝒟,𝒮)𝐤italic-ϕ𝒟𝒮\\mathbf{k}=\\phi(\\mathcal{D},\\mathcal{S}). We detail the requisite background and design of ϕ​(𝒟,𝒮)italic-ϕ𝒟𝒮\\phi(\\mathcal{D},\\mathcal{S}) in sections 3.2 and  3.3.\n\n[7] Large Language Models(LLMs) on Tabular Data: Prediction, Generation, and Understanding - A Survey Liu et al. (2023e) critically analyzed the robustness of GPT3.5 across structural perturbations in tables (transpose and shuffle). They find that LLMs suffer from structural bias in the interpretation of table orientations, and when tasked to transpose the table, LLMs performs miserably ( 50% accuracy). However, LLMs can identify if the first row or first column is the header (94-97% accuracy). Zhao et al. (2023e) investigated the effects of SOTA Table QA models on manipulations on the table header, table content and natural language question (phrasing).333For table headers, they explored synonym and abbreviation replacement perturbations. For table content, they explored five perturbations: (1) row shuffling, (2) column shuffling, (3) extending column names content into semantically equivalent expressions, (4) masking correlated columns (E.g. “Ranking” and “Total Points” can be inferred from one another), and (5) introducing new columns that are derived from existing columns. For the question itself, they perturbed questions at the word-level or sentence-level. They find that all examined Table QA models (TaPas, TableFormer, TaPEX, OmniTab, GPT3) are not robust under adversarial attacks.\n\n[8] LLMs for Semi-Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering As the field of automated machine learning (AutoML) advances, it becomes increasingly important to incorporate domain knowledge into these systems. We present an approach for doing so by harnessing the power of large language models (LLMs). Specifically, we introduce Context-Aware Automated Feature Engineering (CAAFE), a feature engineering method for tabular datasets that utilizes an LLM to iteratively generate additional semantically meaningful features for tabular datasets based on the description of the dataset. The method produces both Python code for creating new features and explanations for the utility of the generated features. Despite being methodologically simple, CAAFE improves performance on 11 out of 14 datasets - boosting mean ROC AUC performance from 0.798 to 0.822 across all dataset - similar to the improvement achieved by using a random forest instead of logistic regression on our datasets. Furthermore, CAAFE is interpretable by providing a textual explanation for each generated feature. CAAFE paves the way for more extensive semi-automation in data science tasks and emphasizes the significance of context-aware solutions that can extend the scope of AutoML systems to semantic AutoML. We release our code, a simple demo and a python package.\n\n[9] Are LLMs Naturally Good at Synthetic Tabular Data Generation? However, the use of language models (LLMs) for tabular data generation is still underexplored.Most modern LLMs are based on the transformer architecture   with parameters ranging from few millions to billions  , and researchers have developed creative ways to harness LLMs in traditional machine learning and data contexts. LIFT  initially transforms a table row into a sentence, such as ‘An Iris plant with sepal length 5.1cm, sepal width 3.5cm’, and employs an LLM as a learning model for table classification, regression, and generation tasks.GReaT  utilizes a GPT-2 model that has been fine-tuned using a specific corpus for synthetic data generation.They also show that even small-scale models such as Distill-GPT  have the potential for synthetic data generation . These models are specially viable for tabular generation given the lower compute costs of aligning smaller models to large and varied tabular datasets.A general benefit of utilizing LLMs is the promise of eliminating customized preprocessing pipelines.Following this paradigm, in this paper, we inspect just how well LLMs fare at generating traditional tabular data, the lingua franca of machine learning.\n\n[10] Large Language Models(LLMs) on Tabular Data: Prediction, Generation, and Understanding - A Survey This survey represents the first comprehensive investigation into the utilization of LLMs for modeling heterogeneous tabular data across various tasks, including prediction, data synthesis, question answering and table understanding. We delve into the essential steps required for tabular data to be ingested by LLM, covering serialization, table manipulation, and prompt engineering. Additionally, we systematically compare datasets, methodologies, metrics and models for each task, emphasizing the principal challenges and recent advancements in understanding, inferring, and generating tabular data. We provide recommendations for dataset and model selection tailored to specific tasks, aimed at aiding both ML researchers and practitioners in selecting appropriate solutions for tabular data modeling using different LLMs. Moreover, we examine the limitations of current approaches, such as susceptibility to hallucination, fairness concerns, data preprocessing intricacies, and result interpretability challenges. In light of these limitations, we discuss future directions that warrant further exploration in future research endeavors.\n\n[11] Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation in ultra low-data regimes Remark on ICL versus fine-tuning. Our results in Table 3 and Table 4 indicate that ICL is better than fine-tuning (GReaT baseline) in the low-data regime. This highlights the difficulty of fine-tuning in this regime, where it is easy to overfit to Dtrainsubscript𝐷trainD_{\\mathrm{train}}. As we increase the number of samples, this baseline, coupled with curation, improves to the level of CLLM  (GPT-4).\n\n[12] Dr.ICL: Demonstration-Retrieved In-context Learning In-context learning (ICL), teaching a large language model (LLM) to perform a task with few-shot demonstrations rather than adjusting the model parameters, has emerged as a strong paradigm for using LLMs. While early studies primarily used a fixed or random set of demonstrations for all test queries, recent research suggests that retrieving semantically similar demonstrations to the input from a pool of available demonstrations results in better performance. This work expands the applicability of retrieval-based ICL approaches by demonstrating that even simple word-overlap similarity measures such as BM25 outperform randomly selected demonstrations. Furthermore, we extend the success of retrieval-based ICL to instruction-finetuned LLMs as well as Chain-of-Thought (CoT) prompting. For instruction-finetuned LLMs, we find that although a model has already seen the training data at training time, retrieving demonstrations from the training data at test time yields better results compared to using no demonstrations or random demonstrations. Last but not least, we train a task-specific demonstration retriever that outperforms off-the-shelf retrievers.\n\n"}
{"case_id": "456d3c2c757f4b874abddb18a62f2ff8", "answer_text": "\n\nInsights into developer behavior on GitHub can be gleaned from analyzing both commits and pull requests, as these artifacts provide a rich source of data about how developers interact with projects, how they contribute code, and how their work is integrated into the main codebase [0]. This section will explore various perspectives on developer behavior obtained from different studies focusing on commits and pull requests. \n\n\n### 1. Casual Contributors vs. Regular Contributors\n\nCasual contributors are those who typically make a single or few contributions to a project and then disappear. Regular contributors, on the other hand, consistently contribute to a project over a longer period. This distinction is often recognized based on different metrics in the pull-based contribution model, such as the number of commits or merged pull requests [0].\n\nOne significant insight is the differentiation between casual and regular contributors. Casual contributors often make a single or few contributions before disappearing, while regular contributors have a more sustained involvement. Research indicates that using pull requests to identify casual contributors is more precise than using commits. Specifically, pull requests were found to identify a higher percentage (66%) of casual contributors compared to commits (48.98%) [0]. This discrepancy suggests that the method used to study contributions can significantly influence the findings.\n\n\n\n### 2. Developer Sentiment and Code Quality\n\nDeveloper emotion or sentiment in a software development environment has the potential to affect performance, and consequently, the software itself [2]. Sentiment analysis, conducted to analyze online collaborative artifacts, can derive effects of developer sentiment [2]. This study aims to understand how developer sentiment is related to bugs, by analyzing the difference of sentiment between regular and Fix-Inducing Changes (FIC) - changes to code that introduce bugs in the system [2]. To do so, sentiment is extracted from Pull Requests of 6 well known GitHub repositories, which contain both code and contributor discussion [2]. Sentiment is calculated using a tool specializing in the software engineering domain: SentiStrength-SE [2]. Next, FICs are detected from Commits by filtering the ones that fix bugs and tracking the origin of the code these remove [2]. Commits are categorized based on FICs and assigned separate sentiment scores (-4 to +4) based on different preceding artifacts - Commits, Comments and Reviews from Pull Requests [2]. The statistical result shows that FICs, compared to regular Commits, contain more positive Comments and Reviews [2]. Commits that precede an FIC have more negative messages [2]. Similarly, all the Pull Request artifacts combined are more negative for FICs than regular Commits [2].\n\n\nThe sentiment expressed in pull request discussions can affect the quality of the code. A study revealed that fix-inducing changes (FICs), which introduce bugs, often correlate with negative sentiments in preceding commits and overall more negative discussions in pull requests [2]. Conversely, comments and reviews associated with regular commits tend to be more positive. This finding highlights how emotional states and communication patterns can impact code quality and the likelihood of introducing errors.\n\n### 3. Technical Issues and Project Characteristics [1].\n\n\nBy analyzing the content of pull request discussions, researchers have identified common technical issues and the factors influencing the success and failure of pull requests. Eight frequently discussed technical topics include recursion and refactoring, database query execution, and OOP paradigm issues, among others [1][3]. Additionally, the success rate of pull requests is negatively influenced by the project's size and complexity, such as having a large number of forked projects or a high number of developers [1][3][5].\n\nAutomated tools are frequently used in social coding repositories to perform repetitive activities that are part of the distributed software development process [7]. Recently, GitHub introduced GitHub Actions, a feature providing automated workflows for repository maintainers. Understanding and anticipating the effects of adopting such kind of technology is important for planning and management. Our research investigates how projects use GitHub Actions, what the communities discuss about them, and how activity indicators change after their adoption [8]. \nThe introduction of automated tools such as GitHub Actions can change the dynamics of pull requests. Studies have found that after adopting GitHub Actions, projects tend to have more rejected pull requests, with accepted pull requests having fewer commits but more comments [7][9]. This suggests that while automation can streamline some aspects of development, it also brings new challenges, such as the need for more extensive discussions and potential integration difficulties.\n\n\n5. Behavior Across Multiple Platforms [13].\n\nDeveloper activities on GitHub can also be linked to their behavior on other platforms like StackOverflow. Research has shown that active issue committers on GitHub are also active question askers on StackOverflow, indicating a cross-platform engagement that spans both coding and community support [13]. Developers' concerns and the topics they discuss often align across these platforms, suggesting that their contributions are driven by consistent interests and challenges.\n\n6. Communication and Collaboration Dynamics [1].\nPull requests also offer insights into the collaborative dynamics of development teams. The success of pull requests is often tied to effective communication among developers. Large-scale analyses have demonstrated that projects with better communication in pull request discussions tend to have higher success rates of merging those requests [5]. Furthermore, the presence of explicit links within pull requests and issues can facilitate better coordination and understanding among team members [14].\n\nIn summary, the analysis of GitHub commits and pull requests provides valuable insights into various aspects of developer behavior, including the differentiation between casual and regular contributors [0], the impact of sentiment on code quality [2], common technical challenges [1], the influence of automated tools [7], cross-platform activity patterns [13], and the dynamics of collaboration and communication [11]. These insights can help project administrators make informed decisions about managing contributions, improving code quality, and fostering a collaborative development environment. \n\n\nReferences:\n[0] Pull Requests or Commits? Which Method Should We Use to Study Contributors' Behavior? Pull Requests or Commits? Which Method Should We Use to Study Contributors' Behavior? Social coding environments have been consistently growing since the popularization of the contribution model known as pull-based. This model has facilitated how developers make their contributions; developers can easily place a few pull requests without further commitment. Developers without strong ties to a project, the so-called casual contributors, often make a single contribution before disappearing. Interestingly, some studies about the topic use the number of commits made to identify the casual contributors, while others use the number of merged pull requests. Does the method used influence the results? In this paper, we replicate a study about casual contributors that relied on commits to identify and analyze these contributors. To achieve this goal, we analyzed the same set of GitHub-hosted software repositories used in the original paper. By using pull requests, we found an average of 66% casual contributors (in comparison to 48.98% when using commits), who were responsible for 12.5% of the contributions accepted (1.73% when using commits). We used a sample of 442 developers to investigate the accuracy of the method. We found that 11.3% of the contributors identified using the pull requests were misclassified (26.2% using commits). We also evidenced that using pull requests is more precise for determining the number of contributions, given that GitHub projects mostly follow the pull-based process. Our results indicate that the method used for mining contributors' data has the potential to influence the results. With this replication, it may be possible to improve previous results\n\n[1] An insight into the pull requests of GitHub and commit messages of source code repositories,6 for various software maintenace activities. Our work is closely related with the study by Hindle et al., where they extract the hidden topics from the commit comments of a code repository, and then map to different cross-project non-functional requirements in order to analyze the software maintenance activities. In this paper, we examine the pull request discussion texts along with project and developer specific information using a machine learning technique and then report the frequent technical issues and inefficiencies in the source code hosted at GitHub. We use MSR dataset, and collect information about 78,517 pull requests made to 78 base projects by 20,142 developers from 103,192 forked projects. We extract 100 underlying topics that the reported issues of 9,421 pull requests (containing pull request discussion) are based on. In order to extract the topics, Latent Dirichlet Allocation (LDA) with Gibbs sampling is used, and we manually label 64 topics. We identify eight frequently discussed technical topics, and manually analyze the pull request discussion texts for useful insights. From the analysis of project and developer specific information, our study reports that programming language and domain specific factors can influence the success and failure rates of the pull requests. More importantly, it finds out that success rate of pull requests for a project degrades comparatively with a large number developers (e.g., more than 4,000) or a large number of forked projects (e.g., more than 3,000). While the extracted frequent technical topics and language or domain specific insights can\n\n[2] Understanding the Effect of Developer Sentiment on Fix-Inducing Changes: An Exploratory Study on GitHub Pull Requests Developer emotion or sentiment in a software development environment has the potential to affect performance, and consequently, the software itself. Sentiment analysis, conducted to analyze online collaborative artifacts, can derive effects of developer sentiment. This study aims to understand how developer sentiment is related to bugs, by analyzing the difference of sentiment between regular and Fix-Inducing Changes (FIC) - changes to code that introduce bugs in the system. To do so, sentiment is extracted from Pull Requests of 6 well known GitHub repositories, which contain both code and contributor discussion. Sentiment is calculated using a tool specializing in the software engineering domain: SentiStrength-SE. Next, FICs are detected from Commits by filtering the ones that fix bugs and tracking the origin of the code these remove. Commits are categorized based on FICs and assigned separate sentiment scores (-4 to +4) based on different preceding artifacts - Commits, Comments and Reviews from Pull Requests. The statistical result shows that FICs, compared to regular Commits, contain more positive Comments and Reviews. Commits that precede an FIC have more negative messages. Similarly, all the Pull Request artifacts combined are more negative for FICs than regular Commits.\n\n[3] An insight into the pull requests of GitHub A number of existing studies focus on the analysis of email messages, bug reports, MSR papers, and commit messages of source code repositories, 6 for various software maintenace activities. Our work is closely related with the study by Hindle et al., where they extract the hidden topics from the commit comments of a code repository, and then map to different cross-project non-functional requirements in order to analyze the software maintenance activities.In this paper, we examine the pull request discussion texts along with project and developer specific information using a machine learning technique and then report the frequent technical issues and inefficiencies in the source code hosted at GitHub. We use MSR dataset, and collect information about 78,517 pull requests made to 78 base projects by 20,142 developers from 103,192 forked projects. We extract 100 underlying topics that the reported issues of 9,421 pull requests (containing pull request discussion) are based on. In order to extract the topics, Latent Dirichlet Allocation (LDA) with Gibbs sampling is used, and wemanually label 64 topics. We identify eight frequently discussed technical topics, and manually analyze the pull request discussion texts for useful insights. From the analysis of project and developer specific information, our study reports that programming language and domain specific factors can influence the success and failure rates of the pull requests. More importantly, it finds out that success rate of pull requests for a project degrades comparatively with a large number developers (e.g., more than 4,000) or a large number of forked projects (e.g., more than 3,000). While the extracted frequent technical topics and language or domain specific insights can help the developers with successful pull requests, project and developer specific insights can aid the GitHub project administrators with informed decision making in the management of pull requests, projects and developers involved.\n\n[4] An insight into the pull requests of GitHub An Insight into the Pull Requests of GitHub Given the increasing number of unsuccessful pull requests in GitHub projects, insights into the success and failure of these requests are essential for the developers. In this paper, we provide a comparative study between successful and unsuccessful pull requests made to 78 GitHub base projects by 20,142 developers from 103,192 forked projects. In the study, we analyze pull request discussion texts, project specific information (e.g., domain, maturity), and developer specific information (e.g., experience) in order to report useful insights, and use them to contrast between successful and unsuccessful pull requests. We believe our study will help developers overcome the issues with pull requests in GitHub, and project administrators with informed decision making. INTRODUCTION GitHub, a popular web-based source code hosting service, provides a convenient way for the software developers to collaborate on open source software development with one another. In order to contribute, a developer either creates her own repository or forks from a base repository, and continues her work. GitHub maintains the source code and associated content (e.g., committed code, commit comments) for both base and forked repositories separately. The idea is to allow the developer to continue her work without reporting every single commit instantly to the base repository. The approach helps her to avoid the frequent merge conflicts with other developers of the project, and also provides flexibility in the development. Once the developer completes a milestone (e.g., module) involving several commits to the forked repository, she makes a pull request to the owner (i.e.,\n\n[5] GitHub Actions: The Impact on the Pull Request Process GitHub Actions: The Impact on the Pull Request Process Automated tools are frequently used in social coding repositories to perform repetitive activities that are part of the distributed software development process. Recently, GitHub introduced GitHub Actions, a feature providing automated workflows for repository maintainers. Understanding and anticipating the effects of adopting such kind of technology is important for planning and management. Our research investigates how projects use GitHub Actions, what the communities discuss about them, and how activity indicators change after their adoption. Our results indicate that a considerable number of projects adopt GitHub Actions (almost 30% of our sample) and that developers frequently ask for help with them. Our findings also suggest that the adoption of GitHub Actions leads to more rejections of pull requests (PRs), more communication in accepted PRs and less in rejected PRs, fewer commits in accepted PRs and more in rejected PRs, and more time to accept a PR. We found similar results in the Utility Actions but we found fewer rejected PRs for the Code Quality Actions. Our results are especially relevant for practitioners to consider these effects when adopting GitHub Actions on their projects. Introduction Social coding platforms, such as GitHub, have changed the collaborative nature of open source software development by integrating mechanisms such as issue reporting and pull requests into distributed version control tools,15. This pull-based development workflow offers new opportunities for community engagement but at the same time increases the workload for repository maintainers, who need to communicate, review code, deal with contributor license agreement\n\n[6] How Do Software Developers Use GitHub Actions to Automate Their Workflows? Actions. These Actions are spread across 20 categories, including continuous integration, utilities, and deployment. We also analyzed the commit history of files related to GitHub Action workflows to understand how the use of predefined Actions evolves over time. Overall, we found that a typical Action is added two times, and never removed or modified. RQ2: How is the use of GitHub Actions discussed by developers? To gain an insight into how developers perceive GitHub Actions, we manually analyzed a set of 209 GitHub issues that discuss GitHub Actions. We found distinct categories of discussions related to GitHub Actions' maintenance and implementation, including switching other automation tools to Actions, suggestions to implement Actions, and problems and frustrations. RQ3: What is the impact of GitHub Actions? In this RQ, we investigate whether project activity indicators, such as the number of pull requests merged and nonmerged, number of comments, the time to close pull requests, and number of commits change after GitHub Actions adoption. We used a Regression Discontinuity Design to model the effect of Action adoption across 926 projects that had adopted GitHub Actions for at least 6 months. Our findings indicate that, on average, there are more rejected pull requests and fewer commits on merged pull requests after adopting GitHub Actions. In summary, we make the following contributions: (i) bringing attention to GitHub Actions, a relevant yet neglected resource that offers support for developers' tasks; (ii) characterizing the usage of GitHub Actions, and (iii) providing an understanding of how GitHub Actions' adoption impacts project activities and\n\n[7] GitHub Actions: The Impact on the Pull Request Process In this research question, we investigate whether project activity indicators, such as the number of pull requests, comments, commits, and time to close pull requests change after GitHub Actions adoption. We used a Regression Discontinuity Design (RDD) Thistlethwaite and Campbell (1960) to model the effect of Action adoption across 662 projects that had adopted GitHub Actions for at least 12 months. Our findings also suggest that the activity indicators change in opposite directions for accepted and rejected pull requests (PRs). Fewer pull requests are being accepted after adopting GitHub Actions, and these pull requests usually have more comments and fewer commits. In contrast, there are more rejected pull requests, with fewer comments and more commits.\n\n[8] GitHub Actions: The Impact on the Pull Request Process 973 unique predefined GitHub Actions were used within the workflows. Further, we collected and analyzed GitHub Actions related discussions and found that the majority of the threads were related to developers asking for help. These findings indicate that GitHub Actions can introduce additional issues related to debugging and contributing. By modeling the data around the introduction of GitHub Actions, we notice different results from merged pull requests and non-merged ones. For merged pull requests, the number of pull requests and commits decrease while comments increase, and for non-merged pull requests, the number of pull requests and commits increase while the number of comments decreases. Our findings bring to light how adopters are using, discussing, and being impacted by GitHub Actions. Learning from those adopters can provide insights to assist the open-source community to decide whether to use GitHub Actions and how to use them effectively. Future work includes the qualitative investigation of the effects of adopting a GitHub Actions and the expansion of our analysis for considering the effects of different types of GitHub Actions and activity indicators.\n\n[9] Mining Developer Behavior Across GitHub and StackOverflow Nowadays, software developers are increasingly involved in GitHub and StackOverflow, creating a lot of valuable data in the two communities. Researchers mine the information in these software communities to understand developer behaviors, while previous work mainly focuses on mining data within a single community. In this paper, we propose a novel approach to mining developer behaviors across GitHub and StackOverflow. This approach links the accounts from two communities using a CART decision tree, leveraging the features from usernames, user behaviors and writing styles. Then, it explores cross-site developer behaviors through T-graph analysis, LDA-based topics clustering and cross-site tagging. We conducted several experiments to evaluate this approach. The results show that the precision and F-Score of our identity linkage method are higher than previous methods in software communities. Especially, we discovered that (1) active issue committers are also active question askers; (2) for most developers, the topics of their contents in GitHub are similar to that of their questions and answers in StackOverflow; (3) developers’ concerns in StackOverflow shift over the time of their current participating projects in GitHub; (4) developers’ concerns in GitHub are more relevant to their answers than questions and comments in StackOverflow.\n\n[10] How Are Issue Units Linked? Empirical Study on the Linking Behavior in GitHub Issue reports and Pull Requests (PRs) are two important kinds of artifacts of software projects in GitHub. It is common for developers to leave explicit links in issues/PRs that refer to the other issues/PRs during discussions. Existing studies have demonstrated the value of such links in identifying complex bugs and duplicate issue reports. However, there are no broad examinations of why developers leave links within issues/PRs and the potential impact of such links on software development. Without such knowledge, practitioners and researchers may miss various opportunities to develop practical techniques for better solving bug-fixing or feature implementation related tasks. To fill this gap, we conducted the first empirical study to explore the characteristics of a large number of links within 642,281 issues/PRs of 16,584 popular (>50 stars) Python projects in GitHub. Specifically, we first constructed an Issue Unit Network (IUN, we refer to issue reports or PRs as issue units) by making use of the links between issue units. Then, we manually checked a sample of 1,384 links in the IUN and concluded six major kinds of linking relationships between issue units. For each kind of linking relationships, we presented some common patterns that developers usually adopted while linking issue units. By further analyzing as many as 423,503 links that match these common patterns, we found several interesting findings which indicate potential research directions in the future, including detecting cross-project duplicate issue reports, using IUN to help better identify influential projects and core issue reports.\n\n"}
{"case_id": "36d9a01cd11997e14deb22537f52f266", "answer_text": "\n\nProtecting software against vulnerabilities stemming from third-party libraries is a crucial aspect of software security [0][1][2]. A combination of automated tools, strategic practices, and community engagement is indispensable in maintaining robust security. Below are some of the best practices based on the existing literature [1]. \n\n1. **Automated Vulnerability Detection and Mitigation Tools**: [1]\n   Automated tools play a fundamental role in identifying and mitigating vulnerabilities within third-party libraries. Tools like PatchScout, VulFixMiner, VulCurator, TRACER, and PHunter leverage advanced techniques like deep learning and semantic analysis to improve the accuracy of vulnerability detection and the application of security patches [4]. Additionally, tools such as Snyk and npm-audit are effective for scanning JavaScript applications for vulnerable dependencies, and they can also suggest or automatically apply fixes to these vulnerabilities [13]. Utilizing these tools within a continuous integration/continuous delivery (CI/CD) pipeline ensures that vulnerabilities are identified and addressed promptly [12]. For example, in comparative studies, Snyk was shown to have a higher detection rate of vulnerabilities and to be more user-friendly than some of its counterparts [13].\n\n2. **Regular Updates and Reviews** [1]:\n   Regularly updating third-party libraries is a critical practice in mitigating security risks. Developers should ensure that libraries are consistently reviewed and updated to the latest secure versions. This practice not only helps in fixing known vulnerabilities but also enhances the overall security and stability of the software [1][8]. Tools like Up2Dep assist in keeping project dependencies up-to-date, thereby avoiding the usage of libraries with known security issues [3]. In evaluations, Up2Dep was shown to detect outdated dependencies promptly and provided seamless updates, significantly reducing the potential for security lapses [3]. **To ensure consistent updates, developers should:**\n- **Implement a regular review schedule** for assessing and updating third-party libraries [1].\n- **Use dependency management tools** that not only alert when updates are available but can also automate the update process if configured [3].\n\nDefine clear policies and procedures for version management, which include criteria for updating libraries (e.g., security patches, performance improvements) [1]. \n- **Incorporate update checks into the CI/CD pipeline**, so that each build includes the latest versions of dependencies [12]. \n - **Engage with the maintainers and communities** of third-party libraries to stay informed about upcoming changes and improvements [1]. \n\n3. **Isolation Techniques** [3]:\n   Isolating third-party libraries can effectively mitigate the impact of potential vulnerabilities. Techniques such as sandboxing and application isolation ensure that vulnerabilities in a third-party library do not compromise the entire application. For example, NativeGuard isolates native libraries in Android applications, preventing them from interacting with other sensitive components [6]. Similarly, BREAKAPP uses module boundaries to decompose applications into isolated units, enhancing security without compromising functionality [3]. \n\nHowever, there are also limitations to these techniques: [0]\n   - **Performance Overhead**: While empirical tests indicate that such isolation techniques incur only modest overhead, there can still be a performance trade-off depending on the complexity and number of isolation units [6].\n- **Compatibility Issues**: Some isolation techniques may not be compatible with all library functions, especially those requiring extensive inter-process communication or shared memory [3]. \n - **Implementation Effort**: Implementing such isolation techniques requires a careful assessment of the application architecture and may involve significant refactoring of the existing codebase [3]. \n\n4. **Validation Filters and Access Controls**: [0]\n   Incorporating validation filters and access controls can provide a strong defense against exploits targeting third-party libraries. By inserting validation filters between the main application and the vulnerable modules, developers can ensure that any input passed to the libraries is sanitized, thus preventing execution of malicious code [0]. For example, in a real-world scenario, a validation filter might be applied to sanitize user inputs before they reach a library known to have a vulnerability that can be exploited via crafted input attacks. Tools implementing ELF-based access control could then ensure that any interactions with the vulnerable module are mediated through these filters [0]. Additionally, implementing intra-process access control, as demonstrated by the Pyronia framework, restricts third-party library access to sensitive resources, mitigating data leak risks [5]. Pyronia has proven effective in experimental setups, successfully protecting sensitive data with minimal performance impact [5].\n\n5. **Formal Verification and Fuzzing** [11]:\n   Applying formal verification methods and conducting fuzz testing can significantly improve the security of third-party libraries. Formal verification methods, such as those used in Cryptol and PCL, rigorously examine the security properties of cryptographic protocols [11]. For instance, Cryptol has been employed to verify the correctness of cryptographic algorithms, ensuring they are free from logical flaws that could be exploited. Fuzzing approaches, such as the TLS-attacker framework, help in identifying vulnerabilities before they are exploited in the wild [11]. For example, the TLS-attacker framework successfully identified various issues in the OpenSSL library, such as the padding oracle attack in AES-NI CBC MAC check, which was subsequently patched to prevent potential exploitation [11]. Fuzzing tools like these continuously test libraries with random and crafted inputs to discover security weaknesses, often finding bugs that traditional testing methods might miss.\n\nCommunity Collaboration and Information Sharing [1]. \n   Strengthening cooperation and information sharing within the security community is pivotal. Joint efforts in responding to new security threats and disseminating information about vulnerabilities can enhance the collective security posture. Platforms facilitating such collaboration can help decrease the spread of vulnerable or outdated libraries by keeping developers informed and motivated to address security issues [1][8]. The effectiveness of community-driven platforms has been validated through quicker vulnerability disclosures and resolutions [8].\n\n7. **Developer Education and Awareness**: [1]\n   Improving developers' security training and awareness is essential for fostering a security-conscious development culture. Educating developers about common security vulnerabilities and best practices equips them to write more secure code and use third-party libraries responsibly [1]. This proactive approach reduces the likelihood of security oversights and strengthens the overall security of the software.\n\nIn conclusion, a multi-faceted approach combining automated tools, regular updates, isolation techniques, validation filters, formal verification, community collaboration, and developer education is essential in protecting software against vulnerabilities from third-party libraries [1][0][3][11]. By adopting these best practices, developers can significantly reduce security risks and ensure the robustness of their applications [1][9]. \n\n\n\nReferences:\n[0] Armor Within: Defending Against Vulnerabilities in Third-Party Libraries Armor Within: Defending Against Vulnerabilities in Third-Party Libraries Vulnerabilities in third-party software modules have resulted in severe security flaws, including remote code execution and denial of service. However, current approaches to securing such libraries suffer from one of two problems. First, they do not perform sufficiently well to be applicable in practice and incur high CPU and memory overheads. Second, they are also harder to apply to legacy and proprietary systems when the source code of the application is not available. There is, therefore, a dire need to secure the internal boundaries within an application to ensure vulnerable software modules are not exploitable via crafted input attacks. We present a novel approach to secure third-party software modules without requiring access to the source code of the program. First, using the foundations of language-theoretic security, we build a validation filter for the vulnerable module. Using the foundations of linking and loading, we present two different ways to insert that filter between the main code and the vulnerable module. Finally, using the foundations of ELF-based access control, we ensure any entry into the vulnerable module must first go through the filter. We evaluate our approaches using three known real-world exploits in two popular libraries-libpng and libxml. We were able to successfully prevent all three exploits from executing.\n\n[1] A Survey of Third-Party Library Security Research in Application Software To solve the challenges faced by third-party libraries, researchers can take the following measures: (1) Develop intelligent security tools to automatically detect and repair vulnerabilities in third-party libraries and respond to unknown threats. (2) Strengthen cooperation and information sharing in the security community, jointly respond to new security threats, and promptly disseminate information about vulnerabilities and risks. (3) Regularly update and review third-party libraries and promptly fix known vulnerabilities and security issues. (4) Ensure compliance with open source software licenses and avoid license conflicts. (5) Improve developers’ security training and awareness so that they are familiar with common security vulnerabilities and best practices to write more secure code and use third-party libraries correctly. These comprehensive measures will help improve software security and stability and protect applications from potential attacks and vulnerabilities.\n\n[2] A Survey of Third-Party Library Security Research in Application Software In today’s software development, third-party libraries play a crucial role, providing developers with rich functionality and convenient solutions. However, as the widespread use of third-party libraries continues, associated security risks and potential vulnerabilities are increasingly evident. Malicious attackers can exploit vulnerabilities in third-party libraries to infiltrate systems, execute unauthorized operations, or steal sensitive information, posing a serious threat to software security. To address this growing security challenge, hardening defense of third-party libraries in software becomes essential. Hardening defense aims to strengthen the security of third-party libraries, mitigate potential risks, and enhance the overall security and stability of software. By taking appropriate hardening measures, developers can effectively prevent and mitigate security threats that may arise from third-party libraries, protect user data and privacy, and avoid system attacks and losses.\n\n[3] A Survey of Third-Party Library Security Research in Application Software Dependency management is a crucial measure in software development to ensure system security. By effectively managing and controlling third-party libraries and components, developers can reduce the risks of version conflicts and security vulnerabilities, while improving code maintainability and scalability. In practice, researchers have proposed a series of methods and tools to optimize the dependency management process, ensuring the security and stability of software systems. Vasilakis introduced an automated application isolation technique called BREAKAPP, based on module boundaries, aiming to enhance the security and reliability of software systems. BREAKAPP decomposes applications into protected isolation units leveraging the presence of third-party modules and enforces security policies. It creates isolation units at runtime and maintains the semantics of the original application through remote procedure calls (RPC). It also allows users to adjust the type of isolation units, communication methods, and functionality restrictions using optional policy expressions, thereby balancing security and performance trade-offs according to different needs and scenarios. Nguyen et al. designed Up2Dep, a tool to assist Android developers in keeping project dependencies up-to-date and avoiding the use of third-party libraries with security issues. Up2Dep is an Android Studio extension that detects and updates third-party libraries in Android projects, enhancing project security and compatibility. It utilizes an offline database to store API and encryption information of third-party libraries, providing different update suggestions and quick-fix solutions, while collecting feedback from developers. Vasilakis et al. proposed a new method to eliminate vulnerabilities in software components using Active Learning and Reconstruction (ALR) technology. ALR explores the behavior of components in a controlled environment, learns their observable functional models from the client side, and then reconstructs a new version of the component using that model. They also introduced a Domain Specific Language (DSL) for capturing string computations and designed an inference algorithm based on the DSL. They implemented an ALR system called Harp, which automatically rebuilds vulnerable string libraries, including those written in JavaScript and C/C++. Experimental results demonstrate that Harp can rapidly complete the rebuilding process in most cases while maintaining compatibility and performance with the original library. They also showed that Harp can eliminate several large-scale software supply chain attacks targeting string libraries.\n\n[4] A Survey of Third-Party Library Security Research in Application Software Locating and fixing vulnerabilities in third-party libraries is an important step in ensuring software security and stability. With the continuous development of automated vulnerability mining methods such as PatchScout, VulFixMiner, VulCurator, TRACER, and PHunter, software developers and security experts can more efficiently find security patches and promptly eliminate potential security risks. These methods leverage techniques such as deep learning, semantic representation, and reference network construction to improve the accuracy of vulnerability fixes and the ability to discover unreported vulnerability patches. However, ongoing research and improvement are still needed to address challenges such as the efficiency of locating vulnerabilities in large-scale software libraries and dealing with cases where one vulnerability may have multiple fix commits. By continuously improving the level of vulnerability fixing techniques and tools, we can better protect software systems from security threats and enhance the overall security of the software ecosystem.\n\n[5] Pyronia: Intra-Process Access Control for IoT Applications such attacks in the wild, our experiments demonstrate that these dynamic features and open APIs place too much trust in third-party library developers, and can be misused for nefarious purposes. Thus, both dynamic language features and the capabilities of native libraries pose threats to the integrity of the application itself and the privacy of user data. Threat Model and Security Goals We aim to provide intra-process access control, which allows developers to prevent third-party code included in their IoT applications from leaking data. In particular, Pyronia protects sensitive OS resources, and restricts access to remote network destinations. Threat Model We assume that IoT device vendor, who usually also develop the device software, are trusted. As such, we also trust the underlying device hardware, the operating system, and the language runtime executing the IoT application. Yet, imported third-party code poses a risk to developers: library code is rarely inspected or readily accessible, so bugs or dynamic language features that leak sensitive data may go unnoticed. We do, however, assume that application developers do not intentionally include such vulnerable or malicious third-party code. While data leak vulnerabilities take many forms, Pyronia targets third-party code that aims to access arbitrary sensitive files or devices, or exfiltrate sensitive data to arbitrary remote network destinations. Pyronia does not seek to prevent any control flow (e.g., ROP) or side channel attacks (e.g., Spectre/Meltdown,38, or physical vectors). ROP defenses (e.g.,,16,31,63) may be used in a complementary fashion. Pyronia also does not prevent network-based attacks such as man-in-the-middle or denial-of-service\n\n[6] NativeGuard: protecting android applications from third-party native libraries Android applications often include third-party libraries written in native code. However, current native components are not well managed by Android's security architecture. We present NativeGuard, a security framework that isolates native libraries from other components in Android applications. Leveraging the process-based protection in Android, NativeGuard isolates native libraries of an Android application into a second application where unnecessary privileges are eliminated. NativeGuard requires neither modifications to Android nor access to the source code of an application. It addresses multiple technical issues to support various interfaces that Android provides to the native world. Experimental results demonstrate that our framework works well with a set of real-world applications, and incurs only modest overhead on benchmark programs.\n\n[7] Research on Third-Party Libraries in Android Apps: A Taxonomy and Systematic Literature Review The common delay response of library updating is due to the lack of timely information and incentives. We suggest that TPL vendors and app markets should help decrease the spread of vulnerable and outdated TPLs with an online platform for developers to tell if their apps may include vulnerable or outdated TPLs.At the same time, the app markets should set up a penalty system:TPL vulnerabilities will be reported to app developers should update the fixed version within a certain period; otherwise, they will be fined or their app will be deleted from the market. We believe this strategy can effectively decrease the risks of TPL vulnerabilities and outdatedness to users.\n\n[8] How well does LLM generate security tests? Developers often build software on top of third-party libraries (Libs) to improve programmer productivity and software quality. The libraries may contain vulnerabilities exploitable by hackers to attack the applications (Apps) built on top of them. People refer to such attacks as supply chain attacks, the documented number of which has increased 742% in 2022. People created tools to mitigate such attacks, by scanning the library dependencies of Apps, identifying the usage of vulnerable library versions, and suggesting secure alternatives to vulnerable dependencies. However, recent studies show that many developers do not trust the reports by these tools; they ask for code or evidence to demonstrate how library vulnerabilities lead to security exploits, in order to assess vulnerability severity and modification necessity. Unfortunately, manually crafting demos of application-specific attacks is challenging and time-consuming, and there is insufficient tool support to automate that procedure. In this study, we used ChatGPT-4.0 to generate security tests, and to demonstrate how vulnerable library dependencies facilitate the supply chain attacks to given Apps. We explored various prompt styles/templates, and found that ChatGPT-4.0 generated tests for all 55 Apps, demonstrating 24 attacks successfully. It outperformed two state-of-the-art security test generators -- TRANSFER and SIEGE -- by generating a lot more tests and achieving more exploits. ChatGPT-4.0 worked better when prompts described more on the vulnerabilities, possible exploits, and code context. Our research will shed light on new research in security test generation. The generated tests will help developers create secure by design and secure by default software.\n\n[9] Cryptography Vulnerabilities on HackerOne such libraries. What's worse, the software developers commonly cannot fix such vulnerabilities, and identifying them are beyond developer responsibility. Such flaws can be extremely dangerous as they expose thousands of software to security risks. Mitigation: Applying formal verification methods to examine the security properties of a cryptographic protocol can be considered, such as Cryptol and PCL. Furthermore, fuzzing approaches can also be part of library test suites or continuous integration in order to run before any versions are released, e.g., the TLS-attacker framework is being used in MatrixSSL and Botan libraries. On HackerOne: Security experts found 25 bugs in the OpenSSL library and in total, they received $24 500 award. Security researchers reported vulnerabilities specifically to the implementation of OpenSSL. For instance, a security researcher reported that there is a mismatch in accepting a nonce value for the AEAD cipher. Other vulnerabilities concerned about consuming excessive resources or exhausting memory, recovering key on Diffie-Hellman small subgroups, performing padding oracle in AES-NI CBC MAC check, heap corruption, out-of-bounds read, and denial of service attacks. D. HTTP/HTTPs mixed content The improper way of using HTTPS and mixing it with insecure network protocols, e.g., HTTP or WebSocket (WS), can bring about undesirable outcomes for the website. Attackers can eavesdrop and conduct complicated attacks, e.g., CRIME, JavaScript execution, cookie stealing, due to the unavoidable mistake of combining HTTPS and HTTP. Mitigation: All the traffic should go through secure channels and the features that enforce secure connections on browsers, devices, and servers must be utilized. There are\n\n[10] Efficient static checking of library updates Software engineering practices have evolved to the point where a developer writing a new application today doesn’t start from scratch, but reuses a number of open source libraries and components. These third-party libraries evolve independently of the applications in which they are used, and may not maintain stable interfaces as bugs and vulnerabilities in them are fixed. This in turn causes API incompatibilities in downstream applications which must be manually resolved. Oversight here may manifest in many ways, from test failures to crashes at runtime. To address this problem, we present a static analysis for automatically and efficiently checking if a library upgrade introduces an API incompatibility. Our analysis does not rely on reported version information from library developers, and instead computes the actual differences between methods in libraries across different versions. The analysis is scalable, enabling real-time diff queries involving arbitrary pairs of library versions. It supports a vulnerability remediation product which suggests library upgrades automatically and is lightweight enough to be part of a continuous integration/delivery (CI/CD) pipeline. To evaluate the effectiveness of our approach, we determine semantic versioning adherence of a corpus of open source libraries taken from Maven Central, PyPI, and RubyGems. We find that on average, 26% of library versions are in violation of semantic versioning. We also analyze a collection of popular open source projects from GitHub to determine if we can automatically update libraries in them without causing API incompatibilities. Our results indicate that we can suggest upgrades automatically for 10% of the libraries.\n\n[11] How well does LLM generate security tests? To mitigate supply chain attacks, researchers and engineers created a variety of tools to reveal vulnerable library dependencies in software applications (dep, 2020; npm, 2023; sny, 2023b; dep, 2023; gam, 2019; aud, 2023; ret, 2023; Ponta et al., 2020; Rahkema and Pfahl, 2022), and even suggest fixes for those vulnerabilities (npm, 2023; sny, 2023a; dep, 2023; Nguyen et al., 2020).For instance, snyk-test (sny, 2023b) and npm-audit (npm, 2023) are CLI commands that scan JavaScript (JS) applications for their package dependencies, compare those packages against the package lists in predefined vulnerability databases (e.g., CVE), and report a vulnerability for each found match. Snyk-fix (sny, 2023a) is another CLI command, which eliminates vulnerabilities by automatically applying recommended updates to the vulnerable package dependencies revealed by snyk-test.\n\n"}
{"case_id": "9b4f043c1561f830b2033f51ca87a648", "answer_text": "\n\nExternal events can significantly influence software projects by introducing unexpected challenges, costs, and delays, often necessitating adaptations in project planning and execution [7]. These events can stem from a variety of sources, including economic, political, social, environmental, and technological factors [11]. Several recent studies have highlighted how these factors play pivotal roles in shaping the success or failure of software projects [0]. \n\n**Economic Factors**: Economic factors, such as funding issues, can severely impact the sustainability of Open Source Software (OSS) projects. A lack of financial resources can lead to insufficient maintenance and the eventual abandonment of projects, thereby introducing bugs and security vulnerabilities to downstream projects that depend on these OSS components [0]. For instance, the Heartbleed vulnerability in the OpenSSL project, which emerged partly due to inadequate funding and manpower, is a stark example of how economic constraints can affect software projects. When key developers leave due to lack of funding or better opportunities elsewhere, the project may lose essential expertise, which is crucial for ongoing development and troubleshooting [0].\n\nThe study on the sustainability of OSS projects employed a combination of quantitative and qualitative methods to explore the effects of such external factors. This mixed-method approach allowed for a comprehensive analysis of the underlying mechanisms by which economic constraints impact OSS sustainability and offered tools to identify risk factors [0].\n\n**Political and Regulatory Factors**: Political and regulatory changes also affect software projects significantly. For example, the study on construction projects in the oil and gas sector in Yemen shows that political instability and regulatory changes can cause disruptions [7]. These risks can lead to delays and increased costs, as projects may need to comply with new laws or navigate the uncertainties brought about by political upheavals. The research highlighted that political risk factors had a strong negative effect on project success [7]. An example outside the oil and gas sector includes the General Data Protection Regulation (GDPR) implemented in the European Union, which forced many software projects worldwide to overhaul their data handling practices to comply with the new, stringent data protection and privacy standards.\n\nThis particular study used a Structural Equation Modeling (SEM) method and Partial Least Squares Structural Equation Modeling (PLS-SEM) approach to analyze data collected through structured surveys from project teams. The hierarchical model developed provided insights into the causative external risk factors and their effects on project success [7].\n\n**Social Factors**: Social factors, particularly user involvement and community engagement, are crucial for the success of software projects. A case study involving Project Management Information Systems in the Middle East found that user involvement in system development and effective communication channels were significant determinants of perceived usefulness and ease of use [1]. For instance, the adoption of PMIS in a Middle Eastern company showed that involving end-users in the development process and maintaining open communication channels increased user satisfaction and system effectiveness. When external users contribute indirectly by requesting features, reporting issues, or commenting, their engagement can influence the project's direction and development priorities [9]. However, poor handling of community contributions can lead to demotivation among quasi-contributors, who might stop contributing altogether if their input is consistently rejected or unacknowledged [5].\n\nThe study on user perceptions used a quantitative analysis of data collected from 128 users of Project Management Information System (PMIS). Partial Least Squares (PLS) supported the significant impact of various organizational, social, individual, and technological factors on users’ beliefs regarding the system's usefulness and ease of use [1].\n\n**Environmental Factors**: Natural disasters and environmental factors also pose risks. For instance, unexpected events such as natural disasters can disrupt the working environment, affecting both human resources and the technological infrastructure needed to maintain and develop software projects. These disruptions can lead to delays in milestones and missed deadlines, thereby affecting the overall project timeline [4]. The study on unmanned vessels indicates that external physical impact factors like data reliability and environmental conditions can restrict the software system's perception of the outside world, affecting its decision-making processes and overall performance [4]. For example, the Fukushima nuclear disaster in Japan led to significant delays and shifts in software development activities due to the evacuations and infrastructure breakdowns in the affected areas.\n\n**Technological Factors**: Technological advancements or trends also play a dual role in either mitigating or exacerbating the impact of external events. For example, the rise of cloud computing has provided software projects with flexibility and resilience, allowing teams to quickly scale resources up or down in response to fluctuating demands or economic constraints. Similarly, advancements in artificial intelligence (AI) and machine learning can help in predictive maintenance, identifying potential risks before they become critical issues [12]. Conversely, reliance on emerging technologies can also introduce new vulnerabilities — for instance, AI systems may require extensive and specific datasets which, if interrupted by political or regulatory changes, can severely hamper project progress.\n\nThe inclusion of AI and machine learning in software engineering presents both opportunities and challenges. These technologies can automate mundane tasks, provide smarter decision-making tools, and predict project risks with higher accuracy. However, they also bring ethical considerations and require significant expertise to manage effectively [12]. \n\n**Expert Involvement**: Furthermore, the influence of external consultants or developers can introduce complexity into software projects. A study highlighted that external developers might focus more on the technical aspects and overlook the social implications, leading to perceptions of them as agents of management rather than collaborators. This can cause friction and misunderstandings within the project team, ultimately affecting project outcomes [2]. Effective coordination between internal teams and external agents is crucial, and satisfaction of these coordination needs influences team performance and project success [6]. For example, the introduction of remote development consultants into a U.S.-based software development project led to initial resistance from the in-house team, necessitating additional efforts to foster collaboration and alignment of objectives.\n\nIn summary, external events such as economic constraints, political instability, social dynamics, environmental factors, technological advancements, and the involvement of external agents significantly influence software projects. These events introduce various challenges that require careful risk management and adaptation strategies [7]. By understanding and anticipating the impact of these external factors, project managers can better navigate the complexities they introduce, thereby enhancing the likelihood of project success [7].\n\n**Mitigation Strategies**: To further illustrate the impact, a quantitative analysis of the dataset from the Project Management Information System (PMIS) case study shows that organizational factors including User involvement and Communication channels have a significant path coefficient (β = 0.25, p < 0.01) on perceived usefulness [1]. Similarly, political risk factors in the Yemeni oil and gas construction projects were found to have a high coefficient of determination (R² = 0.743), emphasizing the substantial influence these external elements can have on project success [7].\n\nTo mitigate these external impacts, several strategies can be employed [11].\n\n1. **Economic Constraints:** Diversifying funding sources and establishing partnerships with commercial entities can provide more financial stability [5]. Regular audits and financial health checks can preempt funding crises. \n\n2. **Political and Regulatory Changes:** Keeping abreast of regulatory changes and involving legal experts in the project can help navigate political instability. Developing flexible project plans that can adapt to new regulations is also crucial [7].\n\n3. **Social Dynamics:** Enhancing community engagement through regular updates, feedback loops, and recognizing contributors' efforts can keep the community motivated. Implementing clear contribution guidelines can also reduce frustration among quasi-contributors [5].\n\n4. **Environmental Factors:** Developing robust backup and disaster recovery plans can minimize the impact of natural disasters [7]. Ensuring that project data and resources are distributed across multiple locations can also help [7].\n\n5. **Technological Factors:** Investing in scalable and resilient technological infrastructure, like cloud computing, can provide flexibility in managing resources [12]. Leveraging AI and machine learning for predictive maintenance and risk management can help mitigate potential disruptions [12].\n\n6. **External Consultants:** Promoting a culture of collaboration and inclusiveness within the team can mitigate resistance to external consultants [2]. Regular team-building activities and clear communication of the consultants' roles and objectives can align everyone's efforts. \n\nBy implementing these mitigation strategies, software projects can better withstand the uncertainties introduced by external events, thereby improving their chances of success [7].\n\n\nReferences:\n[0] External Factors in Sustainability of Open Source Software External Factors in Sustainability of Open Source Software Modern software development is heavily reliant on Open Source. It saves time and money, but, as any other non-commercial software, it comes on as-is basis. If not properly maintained or even abandoned by its community, Open Source Software (OSS) might impose extra costs or introduce bugs to the downstream projects. While software developers are well aware of these premises, existing techniques of mitigating such risks assume sustainability to be an intrinsic property of OSS, largely ignoring external factors. With plenty of examples of even high profile projects failing because of bugs in upstream dependencies, funding issues, lost competition or key developers left, it is important to understand the effect of these factors on OSS sustainability. Using a combination of quantitative and qualitative methods, this dissertation explores effects of external factors in OSS sustainability, the mechanisms behind them, and proposes tools to make certain risk factors more visible. The findings indicate that multiple external factors, such as reused libraries, dependent projects and organizational involvement, play a significant role in OSS projects sustainability. Projects serving end users and end programmers are particularly at risk to be overwhelmed by excessive number of requests, especially questions. We found, however, that there are simple heuristics that can help getting additional insight into the structure of effort demand in OSS. For example, since established users of software mostly report bugs and new adopters mostly ask questions, we can estimate project’s lifecycle stage and user base structure using already existing issue classification. Finally, this work\n\n[1] Investigating the Impact of External Factors on User Perceptions: A Case Study of Software Adoption in Middle East This paper identifies a set of external factors which may influence on users’ perceptions about an information system’s ease of use and usefulness underlying user beliefs. The research model was developed based on Davis's Technology Acceptance Model (TAM) along with extending a comprehensive list of organizational, social, individual, and technological factors as the determinants of users’ perceptions about a technology. The research model was tested through a quantitative analysis of a dataset collected from 128 users of Project Management Information System (PMIS), software for managing the flow of information across the entire company's projects. Partial Least Squares (PLS) supported the significant impact of the following factors on users’ beliefs in terms of Perceived Usefulness: organizational factors including User involvement in system development and Communication channels, social factors including Subjective norms of peers and informal groups and technological factors include Task-Technology Fit; and Quality of relationship with managers and peers; individual factors including Self-efficacy and Personal innovativeness; and technological factors including Compatibility were the most important antecedent of the belief variable in term of Perceived Ease of Use. Finally, the research implications and future directions are discussed.\n\n[2] Factors that affect software systems development project outcomes: A survey of research 2001b. Sarkkinen & Karsten highlight the difficulties that external developers or consultants can encounter during a project, particularly where the system significantly changes individuals' work practices, task division, and organizational status or authority. As outsiders to the organization, external developers or consultants may be unaware of the consequences associated with the new system or of any political undercurrents. They are more likely to focus on the technical aspects rather than the social aspects of the project. In doing so, they are likely to be perceived by users participating in the project as agents of management, forwarding their interests. Project Team All but the smallest of software systems projects are undertaken by a team that includes development personnel, user representatives, managers, and possibly external consultants. The composition of the project team, their collective expertise, their roles and relationships, can influence project outcomes through project team performance. For example, found that strong project team effectiveness improves project outcomes. Similarly, Wang et al. found that project team cohesiveness was significantly positively related to project performance. The size and composition of the project team can themselves influence the outcome of a project. For example, large-sized project teams and teams that have not worked together in the past have been suggested as project risk factors . Aladwani found that project team size was significantly negatively correlated with project team performance, with larger teams experiencing dissatisfaction among team members and decreased productivity and problem solving. Developers in the team interviewed by Linberg felt that small-sized teams improved communication,\n\n[3] A Comparative Assessment of Collision Risk of Manned and Unmanned Vessels represent the influence of the internal and external state of the intelligent decision-making system on its decision-making process. The contents are listed in Table 3. It can be seen from Figure 6 that the performance of the unmanned ship's software system is restricted by various internal and external factors. External PIFs refer to various impact factors outside the software system. Data reliability, environmental factors of navigation, and hardware factors will all restrict the software system's perception of the outside world, affect the ship's hydrodynamic performance, and finally effect the decision-making and control systems. Conditioning events and hidden faults are inevitable, especially in sailing. Even though the software system should have a response plan for the unpredictable hazards, unexpected or unknown disruptions are still one of the most important factors causing navigation risks and accidents. In contrast to external PIF, internal PIF is a direct factor that affects the running state of the software system. Some of these factors are due to defects in the software design and development stage, and some are due to the complexity of the current situation, which exceeds the capacity of the software system. The problems caused by software design defects are long-term and will affect the entire life cycle of unmanned ships. The problems caused by the current situation are short-term and will only have an impact in the current task. In this paper, the former type of internal PIFs is called knowledge base factors, and the latter type is called working memory. Both concepts are borrowed from cognition-based human\n\n[4] Almost There: A Study on Quasi-Contributors in Open-Source Software Projects Recent studies suggest that well-known OSS projects struggle to find the needed workforce to continue evolving—in part because external developers fail to overcome their first contribution barriers. In this paper, we investigate how and why quasi-contributors (external developers who did not succeed in getting their contributions accepted to an OSS project) fail. To achieve our goal, we collected data from 21 popular, non-trivial GitHub projects, identified quasi-contributors, and analyzed their pull-requests. In addition, we conducted surveys with quasi-contributors, and projects' integrators, to understand their perceptions about nonacceptance.We found 10,099 quasi-contributors — about 70% of the total actual contributors — that submitted 12,367 non-accepted pull-requests. In five projects, we found more quasi-contributors than actual contributors. About one-third of the developers who took our survey disagreed with the nonacceptance, and around 30% declared the nonacceptance demotivated or prevented them from placing another pull-request. The main reasons for pull-request nonacceptance from the quasi-contributors' perspective were \"superseded/duplicated pull-request\" and \"mismatch between developer's and team's vision/opinion.\" A manual analysis of a representative sample of 263 pull-requests corroborated with this finding. We also found reasons related to the relationship with the community and lack of experience or commitment from the quasi-contributors. This empirical study is particularly relevant to those interested in fostering developers' participation and retention in OSS communities.\n\n[5] Team‐external coordination in large‐scale software development projects High‐quality work is said to depend on team abilities. However, teams working in large‐scale projects often do not have all expertise to complete their tasks, which are also highly interdependent. Therefore, teams need to rely on coordination with other teams, experts, and supporting roles. In this paper, we explore teams' coordination needs and evaluate the impact of the satisfaction of these needs on team performance. We conducted an embedded multicase study with nine teams in two projects in two companies. We collected qualitative data through nine focus groups and 19 interviews and quantitative data using a questionnaire with 49 members from the studied teams. Our results suggest that project‐, team‐, and task‐related characteristics impact teams' coordination needs. Even in the same project, teams may have different expertise and work coordination needs. We found that the satisfaction of these needs seems to influence teams' performance, although our results are inconclusive and yield a closer look in future research. On the basis of our findings, we recommend the companies to cultivate a networking culture and support teams external coordination with other teams and experts, paying attention to their needs, for example, driven by a lack of experience or increased work complexity.\n\n[6] Assessment of the effect of external risk factors on the success of an oil and gas construction project Construction projects in the oil and gas sector are greatly affected by external risk factors, especially those related to the economy, politics, security and stability factors. Hence, this research aimed to investigate the fundamental relationship between the external risk factors and their effects on the construction project success using Structural Equation Modeling method and PLS-SEM approach.,Data collected through a structured survey distributed to projects teams in the oil and gas sectors in Yemeni companies involved in mega construction projects. A hierarchical model for assessing causative external risk factors and their effects on project success was developed and analyzed using Smart PLS 3 software of SEM.,The findings showed that economic, political, force majeure and security-related risk factors had a strong effect on project success. Besides, the Coefficient of Determination (R-squared value) equals 0.743, represented the proportion of variation in the dependent variable(s), which can be explained by one or more predictor variable. Moreover, the predictive relevance value Q2 is 0.375 above zero, which indicates that the conceptual model can predict the endogenous latent constructs. The calculated Goodness of Fit (GoF) Index of the model was 0.699, which shows that the developed model had substantial explanatory power to represent the relationship between the cause of external risk factors to and the effect on construction project success.,This research was limited to the oil and gas construction projects in Yemen as case study.,Practically, this study highlights the external risk factors that cause a negative effect on the success of oil and gas construction projects in Yemen. The research model of these factors is the first step in the risk management process to develop strategic responses for risks and explain the relationship between cause and effect on project success.,The model of external risks factors that cause the failure of construction projects helps develop response strategies for these risks, thereby increasing the chances of project success reflected in the oil and gas sector, which is a main tributary of the national economy in developing countries.,There is a need to improve the planning of economic and security performance as well as to mitigate political risk factors effects on project success and other risk factors discussed in this study, which effect on construction project success according to their priorities.\n\n[7] Painting the Landscape of Automotive Software in GitHub , based on their activities in automotive software. Internal users contribute directly to the development of a project by making changes to the actual software (commits) and moderating the decision to include/exclude the proposed changes (like merging and closing pull requests and closing issues). External users, on the other hand, contribute indirectly by requesting features, reporting issues, and commenting. We believe that investigating the distribution of internal and external users across projects indicates how a community works. For deeper insights, we also explore changes in contribution patterns, if any, across organization and user projects. A natural next step to understand developer contribution and collaboration patterns is to examine developer roles (e.g., maintainer, or reporter) and their distribution. However, given the small community size and limited development activities, it is infeasible to offer meaningful insights and conclusive statistical analyses. Therefore, we do not report collaboration patterns. Development activities: Development activities on GitHub can be broadly classified into commits, issues, and pull requests. Issue events indicate participation from the broader user base (beyond contributors) requesting additional features or indicating problems. Participation in issue events indicate how the users of the software interact with developers, influencing its development. The next group of development activities are pull requests which indicate a relatively stronger influence on the software by proposing changes for inclusion into the software system or its associated artifacts. These activities log the decisions to include or exclude proposed changes. Finally, a commit is an even more involved activity dealing with the technical aspects of creating desired changes in\n\n[8] Software Estimations Risk in Pakistan Software Industry expected. Guessing the effort is more accurate; how the business gets better and where the software project respects the commitment to budget, time and quality. (The Standish Group Report, 2014) III. Analysis and Approaches Risks related to Software Estimation Process. Risks are inevitable part of software development process and can occur at any phase of development. We are focusing on the risks that can occur in the software estimation process. We are categorizing these risks into six categories; A. Internal Risks: The risk that arise from the organization itself are known as Internal Risks. Internal risks are often predictable, so they can be avoided or minimized. Internal risk is usually generated by one (or a combination of) human, technical or physical factors. (Hooman Hoodat) B. External Risks: The risks that are not in control of organization are known as External risks. Many external risks are caused by legal, environmental and political changes. The impact of a major environmental disaster on the organization's supply is an external hazard. (Hooman Hoodat). Some other external risks could be; A. People are switching jobs frequently: We have seen that most of people switch their job very frequently. Generally people link themselves with the organizations not the projects. And due to that, the estimates are affected. Because when the people change, the effort and time required developing the software changes. Suppose, if an experienced personnel has provided the estimates and now he is leaving the job, which would cause the shift in estimates. B. Unexpected/Unwanted events or holidays: We as a\n\n[9] Generative Artificial Intelligence for Software Engineering - A Research Agenda The use of GenAI for SE brings implications and opportunities to other spheres beyond the technical aspects, such as market, business, organizations and society. This phenomenon can be observed with other technologies. For example, the advent of cloud computing, in which computing resources are hired on demand without the need for large upfront investments in infrastructure, allowed the cheap experimentation of new digital products , paving the way for methodologies for the development of startups focused on quick feedback from customers, such as Customer Development  and Lean Startup . A more recent example is the emergence of more powerful AI algorithms and their reliance on data, leading to a growing research interest in the ethical aspects of this use , i.e., AI ethics.\n\n"}
{"case_id": "f4cdd43e14e8ede7bb1a2b7776ff5a58", "answer_text": "\n\nWhen responding to critical vulnerabilities, developers rely on a variety of tools and data sources to meet their information needs [9]. These tools and data sources can be categorized based on their primary functionalities such as vulnerability detection, assessment, debugging, and remediation [7]. \n\n\nTools for Vulnerability Detection and Assessment [9].\n\n\n \n1. **Static Analysis Tools**: [2]\n\n- **Flawfinder**: Scans C/C++ code for security vulnerabilities, identifying known patterns of insecure coding practices [12].\n - **RATS (Rough Auditing Tool for Security)**: Provides a similar function to Flawfinder but supports multiple languages, including C, C++, Perl, Python, and PHP [2]. \n- **ITS4 (It's The Software, Stupid)**: Analyzes C and C++ code for security flaws [2].\n   These tools are recommended for identifying software vulnerabilities directly from the source code and are particularly useful in scanning for known security weaknesses [2][12]. For instance, Flawfinder was used in a project to scan for C/C++ security vulnerabilities, resulting in the identification and remediation of several critical issues before deployment.\n   - **Find Security Bugs**: A security-oriented static analysis tool for Java code that helps developers diagnose potential security vulnerabilities, answering questions about the source of tainted data, associated attacks, and fixes [7]. An example of its practical application includes its use in open-source projects to ensure that new commits do not introduce security flaws, leading to a more secure codebase over time.\n   Combining diverse static analysis tools, such as Flawfinder, RATS, and ITS4, can improve the coverage of vulnerability detection, although this might increase the number of false alarms [11]. For example, a comparative study involving WordPress plugins revealed that combining tools uncovered unique vulnerabilities missed by individual tools, albeit with increased false positives.\n\n2. **Deep Learning-Based Tools** [12]:\n   - **SySeVR (SySeVR-enabled BGRU)**: Utilizes deep learning to detect software vulnerabilities by accommodating semantic information induced by data and control dependencies. It has been shown to be more effective than traditional static analysis tools [12]. For instance, SySeVR was applied in a study to analyze vulnerabilities in the Xen hypervisor, successfully uncovering several previously undetected issues.\n\n3. **Machine Learning Models**: Machine learning models have been increasingly adopted in the field of software vulnerability detection. These models rely on publicly available datasets that include samples of both vulnerable and non-vulnerable code [14]. Employing techniques such as ensemble learning can achieve high predictive performance, especially when evaluated across different versions and projects [14]. The development of sophisticated machine learning (ML) models now allows for the prediction of security vulnerabilities in a system before it is deployed, providing a valuable tool for developers and security analysts [14]. However, the quality and comprehensiveness of the datasets used for training these models are crucial for their effectiveness [9]. Data-driven research in this area often encounters challenges due to the lack of diverse and well-labeled datasets, which affects the reliability of the ML approaches employed [9].\n   - Several machine learning models can predict security vulnerabilities using static source code metrics. These models are trained on publicly available vulnerability datasets and can evaluate vulnerabilities across different versions and projects [14]. For instance, an ML model was used to predict vulnerabilities in Apache Tomcat, achieving high accuracy and enabling proactive security measures across multiple versions of the software. These models often use features such as code complexity, historical vulnerability data, and code change metrics to train on historical vulnerability instances. They undergo rigorous validation processes, including cross-validation and testing on hold-out datasets to ensure robustness. These methods can sometimes outperform traditional static analysis tools by detecting more nuanced patterns associated with vulnerabilities, thus providing a more proactive approach to security.\n\n### Data Sources for Vulnerability Information [9]  \n\n\n1. **Common Vulnerabilities and Exposures (CVE) Records**: Publicly available records that provide detailed information on known vulnerabilities, essential for understanding the nature of vulnerabilities and tracking fixes [9].\n\n2. **Vulnerability Datasets**: Data-driven vulnerability research depends on the availability of datasets with samples of real-life vulnerable code and their fixes. Moreover, such datasets should encompass multiple levels of granularity, such as files, classes, functions, etc., and cover widely-used programming languages [9]. Finally, for reliable training and evaluation of machine learning (ML) approaches, we need comprehensive datasets that contain large numbers of diverse and labeled samples of both vulnerable and non-vulnerable code [9]. \n   - Datasets used in machine learning models for predicting vulnerabilities, which encompass multiple levels of granularity and cover widely-used programming languages [14]. For example, a comprehensive dataset compiled from various versions of the Apache Struts framework was used to train models for vulnerability prediction.\n\n3. **API and Web Platforms** [13]:\n   - **Exploitability Scores API**: Provides historical and predictive scores for vulnerabilities, helping prioritize which vulnerabilities need immediate attention. It allows for detailed exploration of scores by various criteria, such as publication date and affected products [13]. An example of its practical application is in emergency response scenarios where critical vulnerabilities in essential systems are prioritized and addressed promptly.\n\nTools for Debugging and Performance Analysis[3][6][8].\n\n1. **GLIMPS**: A tool that aids developers in debugging the performance of configurable software systems by providing information on influencing options, option hotspots, and the cause-effect chain. It is validated to be useful in speeding up the debugging process [0][6][8]. For instance, GLIMPS was used in a complex web application project to identify performance bottlenecks linked to specific configuration options, leading to significant performance improvements.\n\n2. **ActivitySpace**: A framework that helps tackle the problem of information fragmentation by tracking a developer’s activities across different applications. It provides various UIs to correlate and re-find cross-cutting information based on content, time, and place of activities, thereby supporting the information needs of developers in an integrated manner [5]. For example, ActivitySpace helped a development team in coordinating efforts across email, IDEs, and Q&A sites, improving overall productivity and information coherence.\n\n### Tools for Vulnerability Remediation [2][7][12][14][13][9] \n\n1. **Vulnerability Remediation Platforms**: [2][7][11][12][13][14]\n   - **CVEfixes**: Automates the collection of vulnerabilities and their corresponding fixes from open-source software, helping ensure that vulnerabilities are addressed promptly and effectively [9]. For instance, CVEfixes facilitated the timely patching of vulnerabilities in widely-used libraries, reducing the risk of exploitation.\n\n2. **Patch Management Systems:** \n\nPatch management systems are crucial for maintaining software security and performance. They involve the process of distributing and applying updates to software, ensuring that vulnerabilities are addressed and performance is optimized.\n\n\nThese systems help in the deployment of patches to fix vulnerabilities, ensuring that patches are applied systematically and verifying that vulnerabilities are indeed mitigated post-deployment [9].\n\n3. **Configuration Management Tools** [0].\n- Tools like **Ansible** and **Puppet**: Used to manage application configurations and ensure secure settings are enforced across environments, reducing the attack surface and ensuring consistent application of security policies [2]. \n\n### Scenario-Based Tools [1]\n\n1. **Scenario-Based Vulnerability Assessment Framework** [1]:\n\nReferences:  \n[1] Imagining and assessing future risks: A dynamic scenario‐base social vulnerability analysis framework for disaster planning and response. \n\nText: While social vulnerability assessments should play a crucial part in disaster management, there is a lack of assessment tools that retain sensitivity to the situation‐specific dynamics of vulnerabilities emerging in particular hazard scenarios. We developed a novel scenario‐based vulnerability assessment framework together with practitioners in crisis management and assessed the suitability of its components in three past crises and their scenario‐based derivations: a large‐scale power outage, the COVID‐19 pandemic, and a cyber‐attack. Rather than deterministically concluding about vulnerability based on prefixed factors, the framework guides relevant stakeholders to systematically think through categories of vulnerability pertinent to a scenario. We used a table‐top exercise, interviews, and focus groups to demonstrate how the framework broadens the crisis managers’ understanding of the scope of factors that may cause vulnerability, the related sources of information and enables to identify individuals burdened by certain vulnerability mixes. The new framework could be applied to different types of crises to enhance preparedness, demand‐driven relief and rescue during critical events. [FROM AUTHOR]\n\n\n   - Developed for disaster planning and response, this framework helps identify and assess social vulnerabilities in specific hazard scenarios. By guiding stakeholders through categories of vulnerability pertinent to a scenario, it enhances preparedness and tailored intervention strategies [1]. For instance, this framework was applied in a tabletop exercise for a cyber-attack scenario, leading to improved crisis management strategies.\n\n2. **Community-Based Adaptation (CBA) Tools** [4]:\n   - These tools support communities in adapting to climate change and assess and communicate climate information in locally relevant ways. They help in moving from vulnerability assessments to adaptation actions, addressing the critical challenges of tool selection and effective application [4].\n\nIntegration into the Software Development Lifecycle\n\nIntegration into the Software Development Lifecycle is crucial for improving software performance and debugging capabilities. Tools like GLIMPS, tailored information providers adapted from global and local performance-influence models, CPU profiling, and program slicing, have been designed to support developers in identifying influencing options, locating option hotspots, and tracing cause-effect chains [0].\n\n\n\nThe integration of these tools and data sources into the software development lifecycle is essential for maintaining secure and robust systems [9]. Here is how these tools are typically integrated:\n\n**Incorporating during Development**: Tools like static analysis (Flawfinder, RATS, ITS4) and deep learning-based tools (SySeVR) can be integrated into the development pipeline to continuously scan and detect vulnerabilities as code is written [2, 12]. This is often facilitated by incorporating these tools into Continuous Integration/Continuous Deployment (CI/CD) workflows, ensuring that each code commit is analyzed for potential vulnerabilities before it is merged into the main codebase. \n\n2. **Automated Testing and Validation**: Machine learning models and other predictive tools can be used to automatically test code changes and predict potential vulnerabilities [14]. This proactive approach allows developers to address issues before they become critical, reducing the risk of vulnerabilities being introduced into production environments.\n\n3. **Post-Deployment Monitoring**: Tools like GLIMPS [0] and ActivitySpace [5] can be used to monitor the performance and security of applications in production. These tools help identify performance bottlenecks and potential security issues that could arise from configuration changes or external factors.\n\n4. **Incident Response and Remediation**: During and after a security incident, tools like CVEfixes and patch management systems help automate the remediation process, ensuring that vulnerabilities are patched quickly and efficiently [9]. Scenario-based tools and frameworks can also guide the response efforts, ensuring that all aspects of the incident are addressed [1]. \n\nConclusion [3].\n\nOverall, the combination of these tools and data sources plays a pivotal role in enabling developers to effectively identify, assess, mitigate, and remediate critical vulnerabilities in their systems [9]. By leveraging these resources and integrating them into the software development lifecycle, developers can enhance the security and robustness of their software products [9]. \n\n\nReferences:\n[0] On Debugging the Performance of Configurable Software Systems: Developer Needs and Tailored Tool Support We aim to support developers inidentifying influencing options,locating option hotspots, andtracing the cause-effect chain.To this end, we design information providers, adapting information sources, to support the above needs.We implement the designed information providers in a tailoredand cohesiveprototype called GLIMPS (Velez et al., 2022), which can assist developers to debug the performance of configurable software systems.Table 1 shows which information needs are supported by the information sources global and local performance-influence models, CPU profiling, and program slicingthat we adapt for designing information providers.\n\n[1] Imagining and assessing future risks: A dynamic scenario‐base social vulnerability analysis framework for disaster planning and response While social vulnerability assessments should play a crucial part in disaster management, there is a lack of assessment tools that retain sensitivity to the situation‐specific dynamics of vulnerabilities emerging in particular hazard scenarios. We developed a novel scenario‐based vulnerability assessment framework together with practitioners in crisis management and assessed the suitability of its components in three past crises and their scenario‐based derivations: a large‐scale power outage, the COVID‐19 pandemic, and a cyber‐attack. Rather than deterministically concluding about vulnerability based on prefixed factors, the framework guides relevant stakeholders to systematically think through categories of vulnerability pertinent to a scenario. We used a table‐top exercise, interviews, and focus groups to demonstrate how the framework broadens the crisis managers’ understanding of the scope of factors that may cause vulnerability, the related sources of information and enables to identify individuals burdened by certain vulnerability mixes. The new framework could be applied to different types of crises to enhance preparedness, demand‐driven relief and rescue during critical events. [ FROM AUTHOR\n\n[2] Comparative Assessment of Static Analysis Tools for Software Vulnerability have presented comparative assessment of three static analysis tools for identifying software vulnerabilities using three open source applications for the purpose of aiding software developers to choose appropriate tools for their needs. The results show that Flawfinder, RATS, and ITS4 are recommended in the order of preference. Also, combinatory use of the tools is recommended to complement their weaknesses for a synergistic performance. We acknowledge that the selection of the applications may not have exposed the full functionality of the tools. We plan to expand this work by including a wide range of open source applications with more tools.\n\n[3] On Debugging the Performance of Configurable Software Systems: Developer Needs and Tailored Tool Support We identified the information needs – influencing options, option hotspots, and cause-effect chain – that developers have when debugging the performance of configurable software systems.Subsequently, we designed and implemented information providers, adapted from global and local performance-influence models, CPU profiling, and program slicing, that support the above needs.Two user studies, with a total of 202020 developers, validate and confirm that our designed information providers help developers debug the performance of complex configurable software systems.\n\n[4] Tools for CBA: Lessons from NGO Collaboration in Vanuatu Community-based adaptation (CBA) has emerged as an important component of the global response to climate change. Numerous tools have been developed to support communities to adapt, and to guide the agencies that they work with. Despite the proliferation of CBA tools, a number of critical challenges exist. For example, CBA tools often overlap, and there is confusion regarding when and how the tools should be applied. There is a lack of tools that communicate climate information in locally relevant ways; support the application of resilience as a unifying framework; support monitoring and evaluation of CBA; and provide technical guidance on the implementation of locally appropriate adaptation options. Additionally, guidance is needed to move from community-led vulnerability assessments to adaptation actions. This article explores these challenges, offers guidance on tool selection, tailoring, and development, and provides lessons learned based on the results from a multiyear CBA program by a consortium of organizations in Vanuatu. These lessons are relevant to the development and implementation of CBA activities globally.\n\n[5] ActivitySpace: A Remembrance Framework to Support Interapplication Information Needs Developers' daily work produces, transforms, and communicates cross-cutting information across applications, including IDEs, emails, Q&A sites, Twitter, and many others. However, these applications function independently of one another. Even though each application has their own effective information management mechanisms, cross-cutting information across separate applications creates a problem of information fragmentation, forcing developers to manually track, correlate, and re-find cross-cutting information across applications. In this paper, we present ActivitySpace, a remembrance framework that unobtrusively tracks and analyze a developer's daily work in separate applications, and provides various semantic and episodic UIs that help developers correlate and re-find cross-cutting information across applications based on information content, time and place of his/her activities. Through a user study of 8 participants, we demonstrate how ActivitySpace helps to tackle information fragmentation problem in developers' daily work.\n\n[6] On Debugging the Performance of Configurable Software Systems: Developer Needs and Tailored Tool Support In this paper, we take a human-centered approach (Myerset al., 2016; Farooquiet al., 2019) to identify, design, implement, and evaluate a solution to support developers in the process of debugging the performance of configurable software systems; particularly, in situations such as our example in Figure 1.Our human-centered research design consists of three steps, summarized in Figure 2:We first conduct an exploratory user study to identify the information needs that developers havewhen debugging the performance of configurable software systems.Ourstudy reveals that developers struggle to find relevant information to(a) identify influencing options; the options or interactions causing an unexpected performance behavior,(b) locate option hotspots; the methods where options affect the performance of the system, and(c) trace the cause-effect chain; how influencing options are used in the implementation to directly and indirectly affect the performance of option hotspots.Subsequently, we design and implement information providers to support developers’ needs, adapting and tailoring global and local performance-influence models, CPU profiling, and program slicing,in a tool called GLIMPS.Finally, we conducttwo user studies to validate and confirm that the designed information providers are useful to developers when debugging the performance of complex configurable software systems, in terms of supporting their information needs and speeding up the process.\n\n[7] Questions developers ask while diagnosing potential security vulnerabilities with static analysis Questions developers ask while diagnosing potential security vulnerabilities with static analysis Security tools can help developers answer questions about potential vulnerabilities in their code. A better understanding of the types of questions asked by developers may help toolsmiths design more effective tools. In this paper, we describe how we collected and categorized these questions by conducting an exploratory study with novice and experienced software developers. We equipped them with Find Security Bugs, a security-oriented static analysis tool, and observed their interactions with security vulnerabilities in an open-source system that they had previously contributed to. We found that they asked questions not only about security vulnerabilities, associated attacks, and fixes, but also questions about the software itself, the social ecosystem that built the software, and related resources and tools. For example, when participants asked questions about the source of tainted data, their tools forced them to make imperfect tradeoffs between systematic and ad hoc program navigation strategies.\n\n[8] On Debugging the Performance of Configurable Software Systems: Developer Needs and Tailored Tool Support In summary, we make the following contributions:•The information needs – influencing options, option hotspots, and cause-effect chain – that developers have when debugging the performance of configurable software systems;•The design of information providers, adapted from global and local performance-influence models, CPU profiling, and program slicing,to supportthe above needs;•Two empirical evaluations to demonstrate the usefulness of the designed information providers;•A prototype tool, GLIMPS, that implements the designed information providers to help developers debugthe performance of configurable software systems.\n\n[9] CVEfixes: automated collection of vulnerabilities and their fixes from open-source software and substantial outages of essential systems. Despite the continued efforts of the software engineering community to improve software quality and security by means of secure coding guidelines, software testing, and various forms of code review, the publicly available Common Vulnerabilities and Exposures (CVE) records reveal an increasing trend in the number of vulnerabilities that are discovered each year. The overall security of software is highly dependent on the effective and timely identification and mitigation of software vulnerabilities. However, this is not an easy task and requires experience and specialized skills that go beyond the expertise of the typical developer, resulting in many vulnerabilities that go unnoticed for a long time. Consequently, there has been considerable attention in academia and industry to the development of techniques and tools that can help developers identify, and possibly repair, security vulnerabilities in source code already in the development phase. Data-driven vulnerability research depends on the availability of datasets with samples of real-life vulnerable code and their fixes. Moreover, such datasets should encompass multiple levels of granularity,4, such as files, classes, functions, etc., and cover widelyused programming languages,6. Finally, for reliable training and evaluation of machine learning (ML) approaches, we need comprehensive datasets that contain large numbers of diverse and labeled samples of both vulnerable and non-vulnerable code. As we will see in the discussion of related work (Section 2), the currently available vulnerability datasets do not fulfill these requirements. To address their shortcomings and assist in data-driven vulnerability research, we propose to automatically collect and\n\n[10] On Combining Diverse Static Analysis Tools for Web Security: An Empirical Study On Combining Diverse Static Analysis Tools for Web Security: An Empirical Study Developers frequently rely on free static analysis tools to automatically detect vulnerabilities in the source code of their applications, but it is well-known that the performance of such tools is limited and varies from one software development scenario to another, both in terms of coverage and false positives. Diversity is an obvi-ous direction to take to improve coverage, as different tools usual-ly report distinct vulnerabilities, but this may come with an in-crease in the number of false alarms. In this paper, we study the problem of combining diverse static analysis tools to detect web vulnerabilities, considering four software development scenarios with different goals and constraints, ranging from low budget to high-end (e.g., business critical) applications. We conducted an experimental campaign with five free static analysis tools to detect vulnerabilities in a workload composed by 134 WordPress plugins. Results clearly show that the best solution depends on the development scenario. Furthermore, in some cases, a single tool performs better than the best combination of tools\n\n[11] SySeVR: A Framework for Using Deep Learning to Detect Software Vulnerabilities accommodating semantic information induced by data dependency and control dependency. We compare our most effective model BGRU with the commercial static vulnerability detection tool Checkmarx and open-source static analysis tools Flawfinder and RATS, because (i) these tools arguably represent the state-of-the-art static analysis for vulnerability detection; (ii) they are widely used for detecting vulnerabilities in C/C++ source code; (iii) they directly operate on the source code (i.e., no need to compile the source code); and (iv) they are available to us. We also consider the state-of-theart system VUDDY, which is particularly suitable for detecting vulnerabilities incurred by code cloning. We further consider VulDeePecker, and we consider all 4 kinds of SyVCs and data as well as control dependency for SySeVR. Table IV), by reducing 6.7% in FNR. In summary, Insight 6: SySeVR-enabled BGRU is much more effective than the state-of-the-art vulnerability detection methods. 5) Applying BGRU to Detect Vulnerabilities in Software Products: In order to show the usefulness of SySeVR in detecting software vulnerabilities in real-world software products, we apply SySeVR-enabled BGRU to detect vulnerabilities in 4 products: Libav, Seamonkey, Thunderbird, and Xen. Each of these products contains multiple targets programs, from which we extract their SyVCs, SeVCs, and vectors. For each product, we apply SySeVR-enabled BGRU to its 20 versions so that we can tell whether some vulnerabilities have been \"silently\" patched by the vendors when releasing a newer version. As highlighted in Table VIII, we detect 15 vulnerabilities that are not reported in NVD. Among them, 7 are unknown\n\n[12] Expected Exploitability: Predicting the Development of Functional Vulnerability Exploits that vulnerability as well as a sample of the artifacts used in computing its EE. One use-case for this tool is the investigation of critical vulnerabilities, as discussed in Section 7.3 -EE for critical vulnerabilities. The Score Comparison tool allows users to compare the scores across subsets of vulnerabilities of interest. Vulnerabilities can be filtered based on the publication date, type, targeted product or affected vendor. The results are displayed in a tabular form, where users can rank vulnerabilities according to various criteria of interest (e.g., the latest or maximum EE score, the score percentile among selected vulnerabilities, whether an exploit was observed etc.). One use-case for the tool is the discovery of critical vulnerabilities that need to be prioritized soon or for which exploitation is imminent, as discussed in Section 7.3 -EE for emergency response. B.3.2 API Client The API allows clients to download historical scores for a given vulnerability (using the /scores/cveid endpoint), or all the prediction scores for a particular model on a particular date (using the /scores/daily endpoint). The API documentation describes the endpoints and the parameters required for each call, and provides example code for clients to interact with the API. B.4 How to access The Web platform is available at https://exploitability.app/. The API and the client code are available at https://api.exploitability.app/. B.5 Installation The practitioner tools are available on the Web platform. To use the API, clone the code repository, point a terminal to that folder and run bash docker/run.sh. This will create the Docker container and spawn a Jupyter\n\n[13] Are Source Code Metrics \"Good Enough\" in Predicting Security Vulnerabilities? vulnerabilities in the source code. Security vulnerabilities in a system usually go unmarked until the system is delivered and deployed. Therefore, in this study, intending to help developers and security analysts, we construct several machine learning (ML) models to predict security vulnerabilities in the source code, relying on static source code metrics that are easy to gather from the source code. We rely on publicly available vulnerability information to build two vulnerability datasets. In this study, we answer the following four research questions: • RQ1: How do the individual learners perform while predicting the security vulnerabilities? • RQ2: Can we employ ensemble learning techniques to achieve a high predictive performance? • RQ3: How does the best performing model perform in cross-version prediction? • RQ4: How does the best performing model perform in cross-project prediction? Our main contributions include: • Two datasets comprising vulnerability information and static source code metrics from two open-source systems: Apache Tomcat (7 versions-v4.x to v10.x) and Apache Struts2-core (11 versions-six 2.3.x and five 2.5.x minor versions); • The best feature set comprising static source code metrics to predict security vulnerabilities; • A set of ML models to predict security vulnerabilities (type, severity, and title); and • Evaluation of the best performing predictive model in the within project, cross-version, and cross-project setup. The rest of the article is organized as follows: Section 2 presents background information relevant to the study, while Section 3 describes our research method. Section 4 presents the results of the experiments, and Section 5 provides the related work which\n\n"}
{"case_id": "baee287ff68f0fd60dcbd0d8b9b741b8", "answer_text": "\n\nAside from its primary role in preventing overfitting, dropout in deep neural networks has several additional side effects, both desirable and undesirable, that are noteworthy [8].\n\n**Desirable Side Effects**: Dropout is a technique for addressing the problem of overfitting in neural networks. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different \"thinned\" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods [8]. \n\n1. **Improved Generalization**: By randomly dropping units during training, dropout forces the network to become more robust and reduces its dependency on specific neurons, leading to better generalization across various tasks [8]. This essentially means that dropout helps the neural network perform better on unseen data, which is crucial for building practical machine learning models [8]. For example, Srivastava et al. (2014) demonstrate that applying dropout to deep neural networks significantly improves test accuracy across benchmark datasets such as CIFAR-10 and MNIST [0][8].\n\n2. **Reduced Co-adaptation**: Dropout prevents units from co-adapting too strongly, promoting the learning of more independent feature detectors [8]. This can potentially lead to a more diverse set of learned features, as the model cannot rely on the presence of any particular neurons during training, forcing it to distribute learning more evenly across the network [0][6]. For instance, research has shown that dropout reduces the cross-covariance of hidden activations, indicating less correlation among features and improved generalization [0].\n\n3. **Interpretability and Uncertainty Estimation**: Using dropout at test time (Monte Carlo dropout) allows for interpretability and uncertainty estimation in neural networks [1]. This technique enables the network to produce a distribution of predictions rather than a single deterministic output, which can be valuable for tasks requiring robust uncertainty estimation, such as medical diagnosis or autonomous driving [1][7]. For example, Monte Carlo dropout has been successfully applied to spatial interpolation in geochemistry, providing well-calibrated uncertainty estimates [7]. In medical applications, this uncertainty estimation helps radiologists and clinicians understand the confidence levels of AI-based diagnoses, aiding in better decision-making. In autonomous driving, the technique can help the system assess the reliability of its object detection and navigation decisions under varying environmental conditions, thereby improving safety [1][7].\n\n4. **Model Averaging**: Dropout inherently performs a form of model averaging by training an ensemble of \"thinned\" models, thus approximating the effects of averaging predictions from multiple networks [8]. This can lead to improved performance and stability in the predictions made by the neural network [8]. In practice, this has resulted in state-of-the-art performance on datasets like ImageNet and CIFAR-100 [8].\n\n**Undesirable Side Effects** [8]:\n\n1. **Increased Training Time**: Dropout adds randomness to the training process, which can slow down convergence and increase the overall training time [8]. This is because the network needs to learn to perform well despite the stochastic removal of neurons, which can require more epochs to achieve the same level of performance as a network trained without dropout [8]. For instance, it has been observed that networks with dropout may require up to twice as many training iterations to converge [8].\n\n2. **Complex Hyperparameter Tuning**: The effectiveness of dropout is highly dependent on the dropout rate, which can vary based on the architecture and the specific task. Finding the optimal dropout rate often requires extensive hyperparameter tuning, making the training process more complex and time-consuming [11][12]. For example, applying dropout to different layers (e.g., embedding layers versus recurrent layers) in models like LSTMs necessitates careful tuning to avoid overfitting or underfitting [14]. One effective strategy for hyperparameter tuning is to use a validation set to test various dropout rates systematically. Techniques like grid search or Bayesian optimization can also be employed to find the optimal dropout rate more efficiently [11][12]. Additionally, it is often beneficial to start with a moderate dropout rate (e.g., 0.2 to 0.5) and adjust based on the validation performance.\n\n3. **Potential Overfitting with High Dropout Rates**: Although dropout is designed to combat overfitting, inappropriate application (e.g., excessively high dropout rates) can lead to underfitting or even overfitting. This is particularly problematic in models with large embedding layers, where insufficient regularization can exacerbate overfitting [14]. Additionally, high dropout rates can lead to a significant reduction in the effective capacity of the network, making it difficult to learn complex patterns in the data [7]. For instance, researchers have noted that high dropout rates can reduce the model's ability to capture intricate relationships in data, leading to poorer performance on tasks like language modeling [14].\n\n4. **Training Instability in Recurrent Networks**: In the context of recurrent neural networks (RNNs), applying dropout can introduce instability in training due to the sequential nature of RNNs, where small changes can propagate and amplify through time steps. For instance, applying dropout to RNNs without considering time-step dependencies can result in degraded performance on sequence prediction tasks [13]. To mitigate this, variants like recurrent dropout have been proposed. Moon et al.’s variant of recurrent dropout, for example, modifies the internal cell state equation to include a constant dropout mask across all time steps, which helps maintain stability [13]. Despite these solutions, recurrent dropout still requires careful tuning to avoid negative impacts on model performance [10][13].\n\n5. **Reduced Model Capacity**: By dropping neurons randomly, dropout effectively reduces the capacity of the network during training. If not managed correctly, this can lead to a network that is less capable of capturing the underlying structure of the data, resulting in suboptimal performance [7][12]. For instance, studies have shown that overly aggressive dropout can impair the network's ability to learn detailed feature representations necessary for tasks such as image recognition [12].\n\nIn conclusion, while dropout is a powerful regularization technique that enhances generalization and robustness, it introduces several complexities and potential pitfalls that need careful consideration and tuning to balance its benefits against its drawbacks [8]. Understanding these trade-offs is essential for effectively leveraging dropout in deep learning models [7]. \n\n\nReferences:\n[0] Reducing Overfitting in Deep Networks by Decorrelating Representations at training time. Srivastava et al. (2014) show that this has a regularizing effect, leading to increased generalization and sparser, less correlated features. Notice that this is without explicitly encouraging decorrelation in hidden activations. To further investigate the relationship between hidden activation correlations and overfitting, we show in Fig. 1 two quantities from a CNN trained for image classification on CIFAR100 (Krizhevsky & Hinton, 2009) -(1) the amount of overfitting in the model (as measured by the gap between train and val accuracy), and (2) the amount of correlation in hidden activations (as measured by the Frobenius norm of the sample cross-covariance matrix computed from vectors of hidden activations; details in Section 2). Both these quantities of interest are reported as a function of amount of training data (x-axis) and with/without Dropout (left/right subplot). As expected, both increased training data and Dropout have a regularizing effect and lead to reduced overfitting. The figure also shows an interesting novel trend -as the amount of overfitting reduces, so does the degree of correlation in hidden activations. In essence, overfitting and co-adaptation seem to be correlated. The open question of course is -is the relationship causal? Overfitting and Cross-Covariance with Dropout Figure 1: Two principal ways to prevent overfitting in deep models are to train with more data (x axis) and to train with Dropout (right plot). As expected, both of these decrease validation error (left axis), but they also happen to decrease hidden activation cross-covariance (right axis). We investigate whether explicitly minimizing cross-covariance can lead to reduced overfitting.\n\n[1] Dropout as a Bayesian Approximation: Appendix dropout helps in avoiding over-fitting, a property which is not often observed in Bayesian models. Papers such as [Wager et al., 2013;Baldi and Sadowski, 2013 have suggested that dropout performs stochastic gradient descent on a regularised error function, or is equivalent to an L 2 regulariser applied after scaling the features by some estimate. Here we show that a deep neural network (NN) with arbitrary depth and non-linearities, with dropout applied before every weight layer, is mathematically equivalent to an approximation to the probabilistic deep Gaussian process model [Damianou and Lawrence, 2013 (marginalised over its covariance function parameters). We would like to stress that no simplifying assumptions are made on the use of dropout in the literature, and that the results derived are applicable to any network architecture that makes use of dropout exactly as it appears in practical applications. We show that the dropout objective, in effect, minimises the Kullback-Leibler divergence between an approximate distribution and the posterior of a deep Gaussian process (marginalised over its finite rank covariance function parameters). We survey possible applications of this new interpretation, and discuss insights shedding light on dropout's properties. This interpretation of dropout as a Bayesian model offers an explanation to some of its properties, such as its ability to avoid over-fitting. Further, our insights allow us to treat NNs with dropout as fully Bayesian models, and obtain uncertainty estimates over their features. In practice, this allows the introduction of the Bayesian machinery into existing deep learning frameworks in a principled way. Lastly, our analysis suggests straightforward\n\n[2] Continuous Dropout. Dropout has been proven to be an effective algorithm for training robust deep networks because of its ability to prevent overfitting by avoiding the co-adaptation of feature detectors. Current explanations of dropout include bagging, naive Bayes, regularization, and sex in evolution. According to the activation patterns of neurons in the human brain, when faced with different situations, the firing rates of neurons are random and continuous, not binary as current dropout does. Inspired by this phenomenon, we extend the traditional binary dropout to continuous dropout. On the one hand, continuous dropout is considerably closer to the activation characteristics of neurons in the human brain than traditional binary dropout. On the other hand, we demonstrate that continuous dropout has the property of avoiding the co-adaptation of feature detectors, which suggests that we can extract more independent feature detectors for model averaging in the test stage. We introduce the proposed continuous dropout to a feedforward neural network and comprehensively compare it with binary dropout, adaptive dropout, and DropConnect on Modified National Institute of Standards and Technology, Canadian Institute for Advanced Research-10, Street View House Numbers, NORB, and ImageNet large scale visual recognition competition-12. Thorough experiments demonstrate that our method performs better in preventing the co-adaptation of feature detectors and improves test performance.\n\n[3] Bayesian Deep Learning for Spatial Interpolation in the Presence of Auxiliary Information probability?-the overall effect of Monte Carlo dropout on the network as a whole is to turn our single neural network into a near-infinite self-contained ensemble. Each different configuration of dropped nodes realises a different function (or model) from the ensemble, so that rather than learning a single 'best' fit, our neural network learns a distribution over possible fits. The dropout rate relates to the variance we expect to see between different functions drawn from the ensemble-it acts as our prior distribution over functions. In general, a higher dropout rate will induce higher variance within the ensemble, as samples (or 'ensemble members') become less correlated. However, the dropout rate also affects the capacity of the neural network to represent complex functions; for example, a high dropout rate of 0.9 would on average leave only 10% of the nodes of the network active for any given sample, effectively causing the ensemble to be composed of much smaller neural networks which would likely be weaker learners (Srivastava et al. 2014). In the same way that theory does not dictate the optimal neural network architecture for a given task, so too the optimal dropout rate is task-(and architecture-)dependent. By manually tuning the dropout rate in order to minimise loss on the evaluation set, a well-calibrated posterior predictive distribution can be achieved. For our geochemical mapping application, we found a dropout rate of 0.2 for the fully connected layers, and a spatial dropout rate of 0.5 for the convolutional layers (in which filters, rather than nodes, are dropped) to be effective.\n\n[4] Dropout: a simple way to prevent neural networks from overfitting Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different \"thinned\" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.\n\n[5] Survey of Dropout Methods for Deep Neural Networks Recurrent dropout  is an alternative approach that can preserve memory in an LSTM while still generating different dropout masks for each input sample, as in standard dropout. This is done by only applying dropout to the part of the RNN that updates the hidden state and not the state itself. So, if an element is dropped, then it simply does not contribute to network memory, rather than erasing the hidden state. For an LSTM, the equations are the same as in the original LSTM except that the equation for 𝐜tsubscript𝐜𝑡\\mathbf{c}_{t} becomes𝐜t=𝐟t∘𝐜t−1+𝐢t∘𝐠t∘𝐦t,mt,i∼Bernoulli​(1−p).formulae-sequencesubscript𝐜𝑡subscript𝐟𝑡subscript𝐜𝑡1subscript𝐢𝑡subscript𝐠𝑡subscript𝐦𝑡similar-tosubscript𝑚𝑡𝑖Bernoulli1𝑝\\mathbf{c}_{t}=\\mathbf{f}_{t}\\circ\\mathbf{c}_{t-1}+\\mathbf{i}_{t}\\circ\\mathbf{g}_{t}\\circ{\\mathbf{m}}_{t},\\quad m_{t,i}\\sim\\text{Bernoulli}(1-p).(25)\n\n[6] A deep auto-encoder model for gene expression prediction learning model is to apply dropout that has been shown to be able to efficiently combine many different neural network architectures. In a neural network, the dropout strategy means that units are dropped out either at the hidden or visible layers to avoid overfitting and improve model performance. Specifically, to drop out a unit is to temporarily remove the unit from the network, along with all of its incoming and outgoing links in the network. A simple strategy for doing so is that each unit is kept in the network with a retention probability p independent of any other units. The probability p can chosen using a validation set, or naively set at 0.5. A pre-set of the probability p at 0.5, albeit simple, seems to be close to optimal for a variety of networks and tasks. One exception is that for the input units, the optimal probability of retention is usually close to 1 than 0.5. With the above ideas in mind, a dropout strategy for a deep learning model, just like the application of regularization to mitigate overfitting works as follows. First, remove units and their associated weights in the network by a retention probability p for certain training samples and train the network with dropout using backpropagation. Second, repeat the dropout procedure (i.e., remove any other random set of units and their connections) and train the model on the training samples. Finally, take the mean of the weights across all of these modified network structures with dropouts when conducting predictions on new\n\n[7] Dropout as a Regularizer of Interaction Effects We generate 1500 samples of 25 input features where Xi∼Unif​(−1,1)similar-tosubscript𝑋𝑖Unif11X_{i}\\sim\\text{Unif}(-1,1) and the target is generated according to one of three settings: (1) only main effects: Y∼N​(sin⁡(X0)+cos⁡(X1),σ2)similar-to𝑌Nsubscript𝑋0subscript𝑋1superscript𝜎2Y\\sim\\text{N}(\\sin(X_{0})+\\cos(X_{1}),\\sigma^{2}), (2) only pair effects: Y∼N​(sin⁡(X0)​cos⁡(X1),σ2)similar-to𝑌Nsubscript𝑋0subscript𝑋1superscript𝜎2Y\\sim\\text{N}(\\sin(X_{0})\\cos(X_{1}),\\sigma^{2}), and (3) only three-way effects: Y∼similar-to𝑌absentY\\sim N​(sin⁡(X0)​cos⁡(X1)​X2,σ2)Nsubscript𝑋0subscript𝑋1subscript𝑋2superscript𝜎2\\text{N}(\\sin(X_{0})\\cos(X_{1})X_{2},\\sigma^{2}).We optimize fully-connected NNs on these dataand measure effect sizes as described in Sec. 4.1.Results are shown in Fig. 6.The key findings are: 1) the rightmost column shows that NNs with low rates of Dropout tend to massively overfit due to a reliance on high-order interactions; 2) the different levels of Dropout have different steady-state optima; 3) because Dropout slows the learning of high-order effects, early stopping is doubly effective in combination with Dropout.NNs tend to learn simple functions earlier (regardless of Dropout usage), and Dropout slows the learning of high-order interactions; these factors combine to reduce the complexity of the learned function under early stopping.\n\n[8] A Theoretically Grounded Application of Dropout in Recurrent Neural Networks In comparison, Zaremba et al.’s dropout variant replaces 𝚣𝐱subscript𝚣𝐱\\mathtt{z}_{\\mathbf{x}} in eq. (7) with the time-dependent 𝚣𝐱tsuperscriptsubscript𝚣𝐱𝑡\\mathtt{z}_{\\mathbf{x}}^{t} which is sampled anew every time step (whereas 𝚣𝐡subscript𝚣𝐡\\mathtt{z}_{\\mathbf{h}} is removed and the recurrent connection 𝐡t−1subscript𝐡𝑡1\\mathbf{h}_{t-1} is not dropped):(𝐢¯𝐟¯𝐨¯𝐠¯)=(sigmsigmsigmtanh)​((𝐱t∘𝚣𝐱t𝐡t−1)⋅𝐖).matrix¯𝐢¯𝐟¯𝐨¯𝐠matrixsigmsigmsigmtanh⋅matrixsubscript𝐱𝑡superscriptsubscript𝚣𝐱𝑡subscript𝐡𝑡1𝐖\\displaystyle\\begin{pmatrix}\\underline{\\mathbf{i}}\\\\\\underline{\\mathbf{f}}\\\\\\underline{\\mathbf{o}}\\\\\\underline{\\mathbf{g}}\\end{pmatrix}=\\begin{pmatrix}\\text{sigm}\\\\\\text{sigm}\\\\\\text{sigm}\\\\\\text{tanh}\\end{pmatrix}\\bigg{(}\\begin{pmatrix}\\mathbf{x}_{t}\\circ\\mathtt{z}_{\\mathbf{x}}^{t}\\\\\\mathbf{h}_{t-1}\\end{pmatrix}\\cdot\\mathbf{W}\\bigg{)}.(8)On the other hand, Moon et al.’s dropout variant changes eq. (4.1) by adapting the internal cell𝐜t=𝐜t∘𝚣𝐜subscript𝐜𝑡subscript𝐜𝑡subscript𝚣𝐜\\displaystyle\\mathbf{c}_{t}=\\mathbf{c}_{t}\\circ\\mathtt{z}_{\\mathbf{c}}(9)with the same mask 𝚣𝐜subscript𝚣𝐜\\mathtt{z}_{\\mathbf{c}} used at all time steps. Note that unlike, by viewing dropout as an operation over the weights our technique trivially extends to RNNs and GRUs.\n\n[9] A Theoretically Grounded Application of Dropout in Recurrent Neural Networks Lastly, we tested the performance of Variational LSTM with different recurrent layer dropout probabilities, fixing the embedding dropout probability at either pE=0subscript𝑝𝐸0p_{E}=0 or pE=0.5subscript𝑝𝐸0.5p_{E}=0.5 (figs. 4(b)-4(c)). These results are rather intriguing. In this experiment all models have converged, with the loss getting near zero (not shown).Yet it seems that with no embedding dropout, a higher dropout probability within the recurrent layers leads to overfitting! This presumably happens because of the large number of parameters in the embedding layer which is not regularised. Regularising the embedding layer with dropout probability pE=0.5subscript𝑝𝐸0.5p_{E}=0.5 we see that a higher recurrent layer dropout probability indeed leads to increased robustness to overfitting, as expected. These results suggest that embedding dropout can be of crucial importance in some tasks.\n\n"}
{"case_id": "0348920a58979e759af9081a6225ee0d", "answer_text": "\n\nApplying bias mitigation algorithms at different stages of the model pipeline—pre-processing, training, and inference—offers unique advantages and limitations [6]. Understanding these characteristics is essential for developing robust and fair machine learning systems. \n\nPre-processing bias mitigation techniques are crucial as they can address unwanted biases before the training phase, thereby ensuring that models are trained on less biased data, and hence, are more likely to generalize better to unseen examples [10]. This stage involves manipulating the training data to remove biases, such as by re-sampling or re-weighting the data, or by using techniques like data augmentation [13]. Effective pre-processing can help models focus on learning the underlying task, rather than exploiting biases present in the dataset [0]. These methods are particularly valuable as they do not require any changes to the model architecture or the training process itself, making them adaptable and relatively easy to implement [6]. \n\n\n**Advantages:** [0]\n\n1. **Data Reformation**: Pre-processing techniques modify the input data to mitigate biases before training. This can help ensure that the model is trained on a more balanced dataset, which is particularly useful when protected attributes are underrepresented or overrepresented [11]. For instance, in a study by Wei et al. (2020), pre-processing approaches like re-sampling methods showed considerable promise in balancing the class distribution in datasets, thus improving fairness metrics.\n2. **Model Agnosticism**: These methods do not depend on the specific learning algorithm and can be applied to any machine learning model. This allows flexibility in model choice and ensures that any model trained on the pre-processed data benefits from bias mitigation [12]. Empirical results by Li et al. (2022a) demonstrated that pre-processing methods could be successfully applied across different models, achieving better fairness without a significant drop in performance.\n\n\n**Limitations:**\n\nSeveral recent studies have shown that strong natural language understanding (NLU) models are prone to relying on unwanted dataset biases without learning the underlying task, resulting in models that fail to generalize to out-of-domain datasets and are likely to perform poorly in real-world scenarios [0]. \n\nNatural Language Processing (NLP) models risk overfitting to specific terms in the training data, thereby reducing their performance, fairness, and generalizability [1]. \n\nIt has been demonstrated that neural models tend to rely on existing idiosyncratic biases in the datasets and leverage superficial correlations between the label and existing shortcuts in the training dataset to perform surprisingly well without learning the underlying task [5].\n\nExisting efforts towards the ethical development of AI systems have leveraged data science to mitigate biases in the training set or introduced fairness principles into the training process [7]. \n\nMoreover, the combination of pre- and post-processing methods has only been addressed four times [10].\n\n\n1. **Complexity in Identifying Bias**: Identifying the specific biases to address can be complex. The effectiveness of pre-processing methods highly depends on the accurate identification and reformation of biased samples [1][11]. For example, identifying gender or race biases in datasets without introducing new biases can be challenging and may require sophisticated statistical and manual methods. In practice, this could mean significant effort and domain expertise to properly preprocess datasets.\n2. **Potential for Over-Correction**: Some pre-processing methods may over-correct for bias, leading to a reduction in model representativeness and utility. This can happen if the methods fail to maintain the natural structure and distribution of the data [10][13]. In the work by Pentyala et al. (2022), over-sampling methods sometimes resulted in models overfitting to minority classes, reducing generalization capabilities. This over-correction can adversely affect the model’s performance on real-world, diverse data where the natural distribution of features might be different.\n\n **Specific Techniques [0][3][1][12]:** \n1. **Re-sampling Methods**: These involve oversampling the minority class or undersampling the majority class to balance the dataset [10].\n2. **Data Augmentation**: Techniques such as generating synthetic data to balance class distributions, which help in reducing bias while preserving data diversity [11].\n3. **Data Transformation**: Applying transformations to features that can neutralize the bias, like anonymizing protected attributes [13].\n\nTraining Stage (In-Processing) refers to techniques applied during the training phase to reduce bias in machine learning models. These methods aim to directly alter the learning procedure itself to ensure fairer outcomes. These techniques often involve modifying the loss function [1], using adversarial training [3], or incorporating bias-only models to focus training on less biased examples [0]. \n\n**Advantages:** Natural Language Processing (NLP) models risk overfitting to specific terms in the training data, thereby reducing their performance, fairness, and generalizability. For instance, neural hate speech detection models are strongly influenced by identity terms like gay, or women, resulting in false positives, severe unintended bias, and lower performance [1]. Most mitigation techniques use lists of identity terms or samples from the target domain during training. However, this approach requires a-priori knowledge and introduces further bias if important terms are neglected [1]. Instead, we propose a knowledge-free Entropy-based Attention Regularization (EAR) to discourage overfitting to training-specific terms [1]. An additional objective function penalizes tokens with low self-attention entropy [1]. We fine-tune BERT via EAR: the resulting model matches or exceeds state-of-the-art performance for hate speech classification and bias metrics on three benchmark corpora in English and Italian [1]. EAR also reveals overfitting terms, i.e., terms most likely to induce bias, to help identify their effect on the model, task, and predictions [1]. \n\nRecent neural models (Devlin et al., 2019; Radford et al., 2018; Chen et al., 2017) have achieved high and even near human-performance on several large-scale natural language understanding benchmarks. However, it has been demonstrated that neural models tend to rely on existing idiosyncratic biases in the datasets, and leverage superficial correlations between the label and existing shortcuts in the training dataset to perform surprisingly well, without learning the underlying task [5]. For instance, natural language inference (NLI) is supposed to test the ability of a model to determine whether a hypothesis sentence (There is no teacher in the room) can be inferred from a premise sentence (Kids work at computers with a teacher’s help) [5]. \n\n\n1. **Direct Bias Mitigation**: In-processing techniques can directly address biases during the model training process, allowing for more nuanced adjustments. Techniques like adversarial training and entropy-based attention regularization can dynamically adjust the model's learning to focus on less biased representations [1][3]. For instance, models trained with entropy-based regularization (EAR) were shown by [1] to match or exceed state-of-the-art performance in hate speech classification, with reduced bias.\n2. **Improved Generalization**: By directly addressing biases during training, these approaches can lead to models that generalize better to out-of-domain datasets, as the models are less likely to rely on dataset-specific artifacts and shortcuts [0][8]. Empirical evidence from Kaushik and Lipton (2018) showed that models applying bias mitigation during training performed significantly better on out-of-domain datasets.\n\n\n**Limitations:**\nRecent neural models have achieved high and even near human-performance on several large-scale natural language understanding benchmarks. However, it has been demonstrated that neural models tend to rely on existing idiosyncratic biases in the datasets, and leverage superficial correlations between the label and existing shortcuts in the training dataset to perform surprisingly well, without learning the underlying task (Kaushik and Lipton, 2018; Gururangan et al., 2018; Poliak et al., 2018; Schuster et al., 2019; McCoy et al., 2019b) [5].\n\n1. **Increased Complexity**: Implementing bias mitigation during training often increases the complexity of the training process. For example, methods like adversarial training require additional components such as adversarial networks, which can make the training process more computationally intensive and difficult to optimize [3][5]. Additionally, models with complex mitigation strategies may need more tuning and hyperparameter adjustments, which can be resource-intensive.\n2. **Irreversibility**: Once a model is trained with in-processing bias mitigation techniques, it often cannot be undone. This lack of flexibility might not be ideal for all scenarios, particularly when the application context or fairness requirements change over time [3]. A study by McCoy et al. (2019b) illustrated the challenges of this irreversibility in practical applications. This inflexibility can limit the model’s adaptability in dynamic application environments where retraining is not feasible.\n\nSpecific Techniques [0]. \n1. **Adversarial Training**: This involves training a model with an adversarial discriminator that tries to predict protected attributes, thereby forcing the model to learn representations independent of these attributes [3].\n2. **Regularization Techniques**: Using regularization terms in the loss function to penalize the model for focusing too much on specific biased features [1].\n3. **Bias-Only Models**: Employing auxiliary models that capture bias in the data and then adjusting the main model’s training to reduce reliance on these biased predictions [0].\n\nInference Stage (Post-Processing) [6][12]. \n\n\nAdvantages: \n\nNatural Language Processing (NLP) models risk overfitting to specific terms in the training data, thereby reducing their performance, fairness, and generalizability [1]. Bias mitigation algorithms can be classified into three categories based on the stage at which they intervene: pre-processing, in-processing, and post-processing [6]. Pre-processing approaches are advantageous because they adjust the data before training, which means there is no need to modify the model’s internal structure [10]. In-processing methods, such as adversarial training, make adjustments during the training process to impose debiasing by updating the model’s parameters [3]. Post-processing methods manipulate model predictions based on fairness constraints and can be applied in a black-box setting, meaning they do not require access to the model’s parameters and can be utilized for any machine learning model [12]. \n1. **Flexibility and Simplicity**: Post-processing methods can be applied to the model's predictions without needing to access the training data or adjust the model parameters. This makes them suitable for deployed models where re-training is not feasible [6][12]. An example is the Calibrated Equalized Odds approach by Pleiss et al., which can be applied post hoc to improve fairness metrics without altering the model.\n2. **Runtime Adaptability**: These methods can be adapted to various runtime environments and different fairness constraints, allowing for real-time adjustments based on the application requirements [6][7]. Fairness-aware adversarial perturbation (FAAP), for example, enables real-time mitigation of biases in deployed systems without modifying the underlying model [7].\n\n\n\n**Limitations:**\n\nRecent neural models (Devlin et al., 2019; Radford et al., 2018; Chen et al., 2017) have achieved high and even near human-performance on several large-scale natural language understanding benchmarks. However, it has been demonstrated that neural models tend to rely on existing idiosyncratic biases in the datasets, and leverage superficial correlations between the label and existing shortcuts in the training dataset to perform surprisingly well, without learning the underlying task (Kaushik and Lipton, 2018; Gururangan et al., 2018; Poliak et al., 2018; Schuster et al., 2019; McCoy et al., 2019b). For instance, natural language inference (NLI) is supposed to test the ability of a model to determine whether a hypothesis sentence (There is no teacher in the room) can be inferred from a premise sentence (Kids work at computers with a teacher’s help) (Dagan et al., 2006)[5].\n\n\n1. **Performance Trade-offs**: Post-processing methods often involve a trade-off between fairness and performance. Since these methods typically modify model outputs to meet fairness constraints, they can sometimes introduce errors or reduce the overall performance of the model [12][6]. Experiments with post-processing methods have shown that while fairness can be improved, the precision and recall of the model may suffer as a result [12]. For instance, real-world applications like credit scoring or hiring systems could face significant impacts if predictive accuracy is compromised.\n2. **Limited Scope**: While they can address certain fairness metrics like group fairness, post-processing methods might not effectively handle more complex or fine-grained biases such as individual fairness. This limitation arises because these methods often only adjust the final predictions rather than addressing the underlying biased representations [6][12]. Recent work by Schuster et al. (2019) highlights the limitations in addressing individual fairness using post-processing techniques alone, where the nuance of individual-specific biases cannot be fixed merely by adjusting outputs post-hoc.\n\n**Specific Techniques**:Post-processing algorithms are a common approach to bias mitigation, as they do not require access to the training process and can be applied in a black-box setting, making them suitable for any machine learning model [6]. Another method is parameter-efficient modularized bias mitigation via AdapterFusion, which encapsulates bias mitigation functionalities into separate adapters that can be added to the model on-demand [3]. Attention regularization, such as entropy-based attention regularization, is also an effective method, penalizing tokens with low self-attention entropy to reduce overfitting to specific terms in the training data [1]. Additionally, strategies that use counterfactual data augmentation, such as those in MABEL, can mitigate gender bias effectively and preserve task performance [9]. \n1. **Calibrated Equalized Odds**: Adjusting the decision thresholds for different groups to equalize false-positive and false-negative rates [6].\n2. **Output Perturbation**: Modifying the output probabilities to ensure fairer distribution across groups [14].\n3. **Fairness Constraints**: Applying constraints at the decision level, such as ensuring demographic parity in predictions [12].\n\nCombining Methods for Optimization[10].\n\nAdvantages: [12]\n1. **Comprehensive Bias Mitigation**: Combining pre-processing, in-processing, and post-processing methods can address different sources and types of biases more effectively. While pre-processing can handle data imbalances, in-processing can adjust the model learning process, and post-processing can fine-tune predictions for fairness. This holistic approach ensures that various facets of bias are tackled at multiple stages, providing a more robust fairness solution [10].\n2. **Flexibility and Robustness**: This combined approach allows for greater flexibility and adaptability. For instance, if pre-processing addresses data imbalance and in-processing tackles learning biases, post-processing can target any residual biases in model predictions, ensuring a more comprehensive and adaptable solution to evolving fairness requirements [14]. Studies like those by Wei et al. (2020) and Li et al. (2022a) highlight the benefits of combining approaches to maintain fairness across different scenarios without compromising performance.\n\n**Limitations:**\nSeveral recent studies have shown that strong natural language understanding (NLU) models are prone to relying on unwanted dataset biases without learning the underlying task, resulting in models that fail to generalize to out-of-domain datasets and are likely to perform poorly in real-world scenarios [0][1]. We propose two learning strategies to train neural models, which are more robust to such biases and transfer better to out-of-domain datasets [0]. The biases are specified in terms of one or more bias-only models, which learn to leverage the dataset biases. During training, the bias-only models’ predictions are used to adjust the loss of the base model to reduce its reliance on biases by down-weighting the biased examples and focusing the training on the hard examples [0]. We experiment on large-scale natural language inference and fact verification benchmarks, evaluating on out-of-domain datasets that are specifically designed to assess the robustness of models against known biases in the training data [0]. Results show that our debiasing methods greatly improve robustness in all settings and better transfer to other textual entailment datasets [0]. Our code and data are publicly available in https://github.com/rabeehk/robust-nli [0]. \n\n1. **Increased Complexity in Implementation**: While combining methods can lead to comprehensive bias mitigation, it also introduces additional complexity in implementation. Coordinating multiple stages of bias mitigation requires careful planning, additional resources, and expertise in various techniques [10]. This can be particularly challenging in resource-constrained environments where the overhead of implementing and maintaining multiple bias mitigation strategies might be prohibitive.\n2. **Potential for Inconsistent Outcomes**: The interaction between different stages of bias mitigation may lead to inconsistent outcomes if not properly managed. For instance, a pre-processing method may inadvertently counteract the effects of an in-processing technique, or vice versa, leading to suboptimal fairness improvements [10]. Therefore, it is crucial to carefully design and test the combined approach to ensure that all methods work synergistically to improve fairness without unintended consequences.\n\nConclusion\nReferences:\n[0] Title: End-to-End Bias Mitigation by Modelling Biases in Corpora Text: Several recent studies have shown that strong natural language understanding (NLU) models are prone to relying on unwanted dataset biases without learning the underlying task, resulting in models that fail to generalize to out-of-domain datasets and are likely to perform poorly in real-world scenarios [0]. We propose two learning strategies to train neural models, which are more robust to such biases and transfer better to out-of-domain datasets. The biases are specified in terms of one or more bias-only models, which learn to leverage the dataset biases [0]. During training, the bias-only models’ predictions are used to adjust the loss of the base model to reduce its reliance on biases by down-weighting the biased examples and focusing the training on the hard examples [0]. We experiment on large-scale natural language inference and fact verification benchmarks, evaluating on out-of-domain datasets that are specifically designed to assess the robustness of models against known biases in the training data [0]. Results show that our debiasing methods greatly improve robustness in all settings and better transfer to other textual entailment datasets [0]. Our code and data are publicly available in https://github.com/rabeehk/robust-nli.\n\n\nBias mitigation strategies must be chosen based on the specific requirements and constraints of the application. Pre-processing offers broad applicability but requires accurate bias identification. In-processing allows for nuanced and dynamic bias mitigation but at the cost of increased complexity and irreversibility. Post-processing provides flexibility and simplicity but might not address deeper or more nuanced biases effectively. Combining these methods can optimize bias mitigation by addressing different sources of bias at various stages, ensuring a more comprehensive and robust solution. Each stage has its unique set of advantages and limitations, and often, a combination of these methods may be employed to achieve optimal fairness and performance [10][14][6].\n\n\nReferences:\n[0] End-to-End Bias Mitigation by Modelling Biases in Corpora Several recent studies have shown that strong natural language understanding (NLU) models are prone to relying on unwanted dataset biases without learning the underlying task, resulting in models that fail to generalize to out-of-domain datasets and are likely to perform poorly in real-world scenarios. We propose two learning strategies to train neural models, which are more robust to such biases and transfer better to out-of-domain datasets. The biases are specified in terms of one or more bias-only models, which learn to leverage the dataset biases. During training, the bias-only models’ predictions are used to adjust the loss of the base model to reduce its reliance on biases by down-weighting the biased examples and focusing the training on the hard examples. We experiment on large-scale natural language inference and fact verification benchmarks, evaluating on out-of-domain datasets that are specifically designed to assess the robustness of models against known biases in the training data. Results show that our debiasing methods greatly improve robustness in all settings and better transfer to other textual entailment datasets. Our code and data are publicly available in https://github.com/rabeehk/robust-nli.\n\n[1] Entropy-based Attention Regularization Frees Unintended Bias Mitigation from Lists Natural Language Processing (NLP) models risk overfitting to specific terms in the training data, thereby reducing their performance, fairness, and generalizability. E.g., neural hate speech detection models are strongly influenced by identity terms like gay, or women, resulting in false positives, severe unintended bias, and lower performance.Most mitigation techniques use lists of identity terms or samples from the target domain during training. However, this approach requires a-priori knowledge and introduces further bias if important terms are neglected.Instead, we propose a knowledge-free Entropy-based Attention Regularization (EAR) to discourage overfitting to training-specific terms. An additional objective function penalizes tokens with low self-attention entropy.We fine-tune BERT via EAR: the resulting model matches or exceeds state-of-the-art performance for hate speech classification and bias metrics on three benchmark corpora in English and Italian.EAR also reveals overfitting terms, i.e., terms most likely to induce bias, to help identify their effect on the model, task, and predictions.\n\n[2] Parameter-efficient Modularised Bias Mitigation via AdapterFusion Large pre-trained language models contain societal biases and carry along these biases to downstream tasks. Current in-processing bias mitigation approaches (like adversarial training) impose debiasing by updating a model’s parameters, effectively transferring the model to a new, irreversible debiased state. In this work, we propose a novel approach to develop stand-alone debiasing functionalities separate from the model, which can be integrated into the model on-demand, while keeping the core model untouched. Drawing from the concept of AdapterFusion in multi-task learning, we introduce DAM (Debiasing with Adapter Modules) – a debiasing approach to first encapsulate arbitrary bias mitigation functionalities into separate adapters, and then add them to the model on-demand in order to deliver fairness qualities. We conduct a large set of experiments on three classification tasks with gender, race, and age as protected attributes. Our results show that DAM improves or maintains the effectiveness of bias mitigation, avoids catastrophic forgetting in a multi-attribute scenario, and maintains on-par task performance, while granting parameter-efficiency and easy switching between the original and debiased models.\n\n[3] End-to-End Bias Mitigation by Modelling Biases in Corpora Recent neural models (Devlin et al., 2019; Radford et al., 2018; Chen et al., 2017) have achieved high and even near human-performance on several large-scale naturallanguage understanding benchmarks. However, it has been demonstrated that neural models tend to rely on existing idiosyncratic biases in the datasets,and leverage superficial correlations between the label and existing shortcuts in the training dataset to perform surprisingly well,111We use biases, heuristics or shortcuts interchangeably.without learning the underlying task (Kaushik and Lipton, 2018; Gururangan et al., 2018; Poliak et al., 2018; Schuster et al., 2019; McCoy et al., 2019b). For instance, natural languageinference (NLI) is supposed to test the ability of a model to determine whether a hypothesis sentence (There is no teacher in the room) can be inferred from a premise sentence (Kids work at computers with a teacher’s help) (Dagan et al., 2006).222Thegiven sentences are in the contradictory relation, and the hypothesis cannot be inferred from the premise. However, recent work has demonstrated that large-scale NLI benchmarks contain annotationartifacts; certain words in the hypothesis that are highly indicative of inference class and allow models that do not consider the premise to perform unexpectedly well (Poliak et al., 2018; Gururangan et al., 2018). As an example, in someNLI benchmarks, negation words such as “nobody”, “no”, and “not” in the hypothesis are often highly correlated with the contradiction label.\n\n[4] Bias Mitigation Post-processing for Individual and Group Fairness such notion being disparate impact. Individual fairness, in its broadest sense, seeks for similar individuals to be treated similarly. Checking for group fairness is a fairly straightforward computation of statistical metrics, but checking for individual fairness is more computationally involved when there are many protected attributes with many values and scoring samples using a model is expensive,10. Unified metrics for both group and individual fairness have recently been proposed based on inequality indices. Machine learning pipelines contain three possible points of intervention to mitigate unwanted bias: the training data, the learning procedure, and the output predictions, with three corresponding classes of bias mitigation algorithms: pre-processing, in-processing, and post-processing. Advantages of post-processing algorithms are that they do not require access to the training process and are thus suitable for run-time environments. Moreover, post-processing algorithms operate in a black-box fashion, meaning that they do not need access to the internals of models, their derivatives, etc., and are therefore applicable to any machine learning model (or amalgamation of models). The vast majority of bias mitigation algorithms address group fairness, but a few address individual fairness,16. Some preprocessing algorithms address both group and individual fairness,18,6, but to the best of our knowledge, all existing postprocessing algorithms are only for group fairness,19,20,21. Our main contribution in this paper is to propose a post-processing bias mitigation algorithm that considers both group and individual fairness. Moreover, unlike the previous work, our proposal does not require any ground truth class labels in the validation\n\n[5] Fairness-aware Adversarial Perturbation Towards Bias Mitigation for Deployed Deep Models Prioritizing fairness is of central importance in artificial intelligence (AI) systems, especially for those societal applications, e.g., hiring systems should recommend applicants equally from different demographic groups, and risk assessment systems must eliminate racism in criminal justice. Existing efforts towards the ethical development of AI systems have leveraged data science to mitigate biases in the training set or introduced fairness principles into the training process. For a deployed AI system, however, it may not allow for retraining or tuning in practice. By contrast, we propose a more flexible approach, i.e., fairness-aware adversarial perturbation (FAAP), which learns to perturb input data to blind deployed models on fairness-related features, e.g., gender and ethnicity. The key advantage is that FAAP does not modify deployed models in terms of param-eters and structures. To achieve this, we design a discriminator to distinguish fairness-related attributes based on latent representations from deployed models. Meanwhile, a perturbation generator is trained against the discriminator, such that no fairness-related features could be extracted from perturbed inputs. Exhaustive experimental evaluation demonstrates the effectiveness and superior performance of the proposed FAAP. In addition, FAAP is validated on real-world commercial deployments (inaccessible to model pa-rameters), which shows the transferability of FAAP, foreseeing the potential of black-box adaptation.\n\n[6] End-to-End Bias Mitigation by Modelling Biases in Corpora We evaluate our models on challenging benchmarks in textual entailment and fact verification, including HANS (Heuristic Analysis for NLI Systems) (McCoy et al., 2019b), hard NLI sets (Gururangan et al., 2018) of Stanford Natural Language Inference (SNLI) (Bowman et al., 2015) and MultiNLI (MNLI) (Williams et al., 2018), and FEVER Symmetric test set (Schuster et al., 2019). The selected datasets are highly challenging and have been carefully designed to be unbiased to allow proper evaluation of the out-of-domain performance of the models. We additionally construct hard MNLI datasets from MNLI development sets to facilitate the out-of-domain evaluation on this dataset.333Removing the need to submit to an online evaluation system for MNLI hard test sets.We show that including our strategies on training baseline models, including BERT (Devlin et al., 2019), provides a substantial gain on out-of-domain performance in all the experiments.\n\n[7] MABEL: Attenuating Gender Bias using Textual Entailment Data Pre-trained language models encode undesirable social biases, which are further exacerbated in downstream use. To this end, we propose MABEL (a Method for Attenuating Gender Bias using Entailment Labels), an intermediate pre-training approach for mitigating gender bias in contextualized representations. Key to our approach is the use of a contrastive learning objective on counterfactually augmented, gender-balanced entailment pairs from natural language inference (NLI) datasets. We also introduce an alignment regularizer that pulls identical entailment pairs along opposite gender directions closer. We extensively evaluate our approach on intrinsic and extrinsic metrics, and show that MABEL outperforms previous task-agnostic debiasing approaches in terms of fairness. It also preserves task performance after fine-tuning on downstream tasks. Together, these findings demonstrate the suitability of NLI data as an effective means of bias mitigation, as opposed to only using unlabeled sentences in the literature. Finally, we identify that existing approaches often use evaluation settings that are insufficient or inconsistent. We make an effort to reproduce and compare previous methods, and call for unifying the evaluation settings across gender debiasing methods for better future comparison.\n\n[8] Bias Mitigation for Machine Learning Classifiers: A Comprehensive Survey Moreover, the combination of pre- and post-processing methods has only been addressed four times (Zhang et al., 2018b; Wei et al., 2020; Pentyala et al., 2022; Li et al., 2022a).The number of classification models considered by these four publications range from 1 to 3.This is a promising combination of approaches, as one can perform experiments with bias mitigation methods at two different stages (i.e., before and after training) on various classification models and thereby collect extensive empirical evidence for fairness improvements.Additionally, we found several publications that applied multiple bias mitigation methods of the same type (e.g., two pre-processing methods). Six of these applied multiple pre-processing methods and 19 applied multiple in-processing methods (Table 7).However, we found no publication that applied multiple post-processing methods.\n\n[9] Detecting and Mitigating Bias in Data Using Machine Learning with Pre-Training Metrics Detecting and Mitigating Bias in Data Using Machine Learning with Pre-Training Metrics In this paper, the proposed algorithm to detect the bias from the datasets and to mitigate the bias in the datasets was observed. The consequences of this work shows that not only bias in a model can be decreased without forfeiting model performance rate, but improving the performance. Class imbalance, KL divergence, sample disparity and Kolmogorov-Smirnov (KS) are the pre-training metrics used in the work. Each metric is given weightage and the features are detected based on the maximum weightage. The model is trained to learn the unbiased data and shows the significant improvement in the performance of the system. ROC curve, False Positive Rate and False Negative Rate is used for bias trade-off. The comparison between FPR and FNR before mitigating bias and after mitigating bias is performed and its results are significantly improved.\n\n[10] End-To-End Bias Mitigation: Removing Gender Bias in Deep Learning these NLP tasks by over 40%, without reducing the model's predictive accuracy more than 1%. E. Post-Processing: Bias Mitigation Through Prediction Constraints Other researchers have proposed post-processing as a method for bias mitigation,,-. This method generally works by manipulating model predictions based on a fairness constraint. The primary benefit of this method is, like pre-processing, it does not require access to model parameters, so it can be applied in a black-box setting. This also means it can theoretically be utilized for any kind of machine learning model to increase run-time fairness. Some approaches to postprocessing do not even necessitate access to the input features and can be applied to the joint distribution over labels Y and model predictionsŶ . However, this technique can result in a significant loss in performance and has been shown to be suboptimal. Furthermore, it increases fairness strictly with respect to the specified constraint -not necessarily to any other notions of fairness. We further explore this technique by detailing an example approach. Pleiss et al. propose Calibrated Equalized Odds as a post-processing algorithm. This technique attempts to retain a model's calibrated probability estimates while minimizing error disparities across different groups. In this method, a cost function is defined to penalize a model for disparities in falsenegative rates, false-positive rates, or a weighted combination of these across different groups such as women and men. The choice of cost function condition is problem-specific and up to practitioners. For example, in experimenting with the Adult Dataset, uses\n\n[11] Epistemic Uncertainty-Weighted Loss for Visual Bias Mitigation Most current state-of-the-art bias mitigation techniques directly use bias-informing metadata to either adjust the data before training, the model during training, or the predictions at inference time. Oversampling techniques aim to balance out the minority samples in the labelled data either before or during training to create the appearance of having a balanced set. The fairness through blindness approach forces the model to simultaneously learn the target task and to ignore, or “learn to not learn” a protected variable. However, these methods are susceptible to redundant encoding, where combinations of other non-protected variables act as a proxy for the protected variable. In contrast, fairness through awareness approaches encode and explicitly mitigate all protected features. A model is taught all subgroup and target class combinations and at inference time these relationships are removed. A recent approach titled domain independent training aims to remove class-bias correlations by averaging class decision boundaries.\n\n[12] Are Bias Mitigation Techniques for Deep Learning Effective? Assuming access to the test distribution for model selection is unrealistic and can result in models being right for the wrong reasons . Rather, it is ideal if the methods can generalize without being tuned on the test distribution and we study this ability by comparing models selected through varying tuning distributions. To control the tuning distribution, we define a generalization of the mean per group accuracy (MPG) metric, that can interpolate within as well as extrapolate beyond the train and test distributions:A​c​c​(α)=∑g=1|G|pgα​A​c​cg∑g=1|G|pgα.𝐴𝑐𝑐𝛼superscriptsubscript𝑔1𝐺superscriptsubscript𝑝𝑔𝛼𝐴𝑐subscript𝑐𝑔superscriptsubscript𝑔1𝐺superscriptsubscript𝑝𝑔𝛼\\displaystyle Acc(\\alpha)=\\frac{\\sum_{g=1}^{|G|}p_{g}^{\\alpha}Acc_{g}}{\\sum_{g=1}^{|G|}p_{g}^{\\alpha}}.Here, pgsubscript𝑝𝑔p_{g} denotes the ratio of samples present in group g𝑔g,G|𝐺|G| is the total number of groups and α𝛼\\alpha is used to control the group prior. When α=0𝛼0\\alpha=0, A​c​c​(α=0)𝐴𝑐𝑐𝛼0Acc(\\alpha=0) yields the MPG i.e., a balanced distribution where all groups are weighed equally. When α=1𝛼1\\alpha=1, then the weights reflect the train priors/biases. When 0<α<10𝛼10<\\alpha<1, it interpolates between biased (train) priors and unbiased group weights. When α<0𝛼0\\alpha<0, minority groups are weighed more and when α>1𝛼1\\alpha>1, majority groups are weighed more i.e., it amplifies the train bias.\n\n"}
{"case_id": "323e85c9052082358fc0c045fe20a537", "answer_text": "\n\nDifferent fairness metrics can exhibit various degrees of correlation with each other across different datasets and model architectures [7]. This topic has been studied through empirical analyses and statistical tests, yielding several key insights into how these correlations manifest and their implications for fairness evaluations in machine learning [2]. \n\nClustering of Fairness Metrics [2][6][7][11].\n\nTo begin with, several studies have shown that fairness metrics often cluster together, indicating that they may measure similar underlying phenomena. For example, over multiple datasets and model architectures, complementary metrics such as accuracy and worst-group accuracy, and demographic parity difference and equalized odds difference, have been found to have strong correlations [5]. This implies that selecting one metric from each pair could be sufficient to gauge fairness in a given context.\n\nFairness-Performance Trade-offs[0]. \n\nMoreover, the correlation between fairness metrics can vary depending on the number of fairness-enhancing interventions applied. Studies have shown that the relationship between fairness metrics and performance metrics like the F1 score changes as the number of interventions increases. Initially, high fairness metrics tend to correlate negatively with high performance metrics, but this negative correlation weakens with more interventions, suggesting an improved balance between fairness and performance [0]. This observation underscores the importance of considering multiple fairness metrics simultaneously to achieve a more nuanced understanding of model behavior.\n\n\n### Dimensionality Reduction Techniques [2]\n\n\nIn terms of methodological approaches, Principal Component Analysis (PCA) has been utilized to visualize and understand the multidimensional nature of fairness metrics. By reducing the dimensions of multiple fairness metrics into two or three components, researchers demonstrate that the variance between metrics can be explained with a small number of principal components [2]. This approach helps in identifying which metrics are most representative of overall fairness for a model within a specific context.\n\n\nInconsistencies Among Fairness Metrics [0]. \n\n\nHowever, the correlation between different fairness metrics is not always perfect, and discrepancies can arise due to their inherently different definitions and objectives. Pearson and Spearman correlation coefficients have been used to assess these relationships quantitatively. For example, it has been found that some fairness metrics may rank subsets of data differently, hinting at potential inconsistencies or inaccuracies in the metrics themselves [1]. Such findings underscore the potential limitations of relying solely on one metric, as it might lead to misleading conclusions about model fairness.\n\nAdvanced Statistical Approaches\n\nAdvanced statistical methods such as Bayesian Inference have been proposed to assess the quality and reliability of fairness metrics, emphasizing the importance of credible interval estimates and mean absolute error as indicators of metric accuracy [3]. Specifically, Bayesian calibration has shown to produce significantly more accurate estimates of group fairness metrics, such as FPR and TPR differences, compared to frequentist approaches. This method has proven particularly useful in settings with limited labeled data, where traditional methods may fall short. Studies indicate that Bayesian Calibration (BC) consistently provides more accurate estimates than Beta-Binomial (BB) across various datasets and model types, demonstrating its effectiveness in mitigating inaccuracies [3].\n\nContextual Factors in Fairness Evaluation[0].\n\nIn the domain of contextualized language models, studies have found that aligning evaluation datasets can modestly improve the correlation between fairness metrics. For instance, the correlation of StereoSet with B-Regard increased from −0.08 to 0.19, and with B-Stereo from -0.18 to 0.08, suggesting the source and nature of evaluation data play a significant role in how metrics correlate with one another [12].\n\nSubgroup Robustness[5].\n\nA recent empirical investigation into subgroup robustness showed that certain metrics, such as CVaR and CVaR DORO, exhibit almost perfect correlations across different datasets and model architectures. However, the study also highlighted that these correlations can vary significantly based on the specifics of the datasets and model types used [5].\n\nPractical Implications\n\nFrom a practical standpoint, understanding these correlations and their context-dependence has vital implications for model deployment and evaluation. Organizations must carefully select fairness metrics that align with their specific fairness goals and the demographics of their user base [2]. Moreover, practitioners should be aware that achieving fairness in one aspect does not guarantee fairness across all dimensions [0]. This nuanced approach can help in making informed decisions when deploying models in sensitive applications such as hiring, lending, and law enforcement, where fairness is not merely a technical metric but a crucial ethical requirement [4].\n\nConclusion\n\nIn conclusion, the correlations between different fairness metrics are complex and context-dependent, varying across datasets and model architectures [6]. Employing a combination of multiple fairness metrics alongside advanced statistical methods provides a more comprehensive and reliable measure of model fairness [2]\n\n\nReferences:\n[0] Cascaded Debiasing: Studying the Cumulative Effect of Multiple Fairness-Enhancing Interventions metrics. We start off by grouping all experimental data across datasets by the number of interventions. For each group, we compute the Spearman correlation coefficient between different fairness metrics and F1 score as shown in Table 1. Here, we have used absolute values for group fairness metrics. We observe that there is a significant positive correlation between fairness metrics and F1 score for the baseline case. For all fairness metrics except for the consistency metric, a higher value means more bias (less fairness). Hence, a positive correlation suggests that F1 score and fairness are negatively linked. In other words, interventions with high F1 score generally result in poor fairness and vice versa. This observation is in line with existing literature which discusses a tradeoff between accuracy and fairness for individual interventions. As the number of interventions increases, we observe a steady decline in the correlation coefficient across fairness metrics. Here, the correlation coefficient for consistency moves in the opposite direction as unlike all other fairness metrics higher values means more fairness. The decrease in correlation suggests that the likelihood of attaining a high F1 score along with high fairness increases with higher numbers of interventions. As an example, we plot the bivariate relation between Statistical parity difference and F1 score for different numbers of interventions (see Figure 9). The decrease in correlation coefficient is evident from the decrease in the slope of the regression line as we move towards higher numbers of interventions. If the reduction in F1 score caused by different interventions was in\n\n[1] Measuring Fairness with Biased Rulers: A Survey on Quantifying Biases in Pretrained Language Models For each set of attributes 𝒫fsubscript𝒫𝑓\\mathcal{P}_{f} and 𝒫msubscript𝒫𝑚\\mathcal{P}_{m} we generate 20 subsets {a1,…,a20}subscript𝑎1…subscript𝑎20\\{a_{1},...,a_{20}\\} by randomly sampling 10 professions. We refer to the set of these subsets as 𝒜f={af​1,…,af​20}subscript𝒜𝑓subscript𝑎𝑓1…subscript𝑎𝑓20\\mathcal{A}_{f}=\\{a_{f1},...,a_{f20}\\} and 𝒜m={am​1,…,am​20}subscript𝒜𝑚subscript𝑎𝑚1…subscript𝑎𝑚20\\mathcal{A}_{m}=\\{a_{m1},...,a_{m20}\\}, for female and male professions respectively such that af​i={xf​1,…,xf​10}subscript𝑎𝑓𝑖subscript𝑥𝑓1…subscript𝑥𝑓10a_{fi}=\\{x_{f1},...,x_{f10}\\}, where xf​i∈𝒫fsubscript𝑥𝑓𝑖subscript𝒫𝑓x_{fi}\\in\\mathcal{P}_{f} and af​i∈Afsubscript𝑎𝑓𝑖subscript𝐴𝑓a_{fi}\\in A_{f}.We expect that some subsets will show higher levels of bias than others and that given two “accurate” fairness metrics ℳ1subscriptℳ1\\mathcal{M}_{1} and ℳ2subscriptℳ2\\mathcal{M}_{2}, if ℳ1subscriptℳ1\\mathcal{M}_{1} ranks three subsets as a1<a2<a3subscript𝑎1subscript𝑎2subscript𝑎3a_{1}<a_{2}<a_{3}, indicating that a1subscript𝑎1a_{1} contains less bias than a2subscript𝑎2a_{2} which in turn contains less bias than a3subscript𝑎3a_{3}, ℳ2subscriptℳ2\\mathcal{M}_{2} should likewise rank the three subsets in the same order of fairness.If there is any deviation in this ranking, we can draw two conclusions, either ℳ1subscriptℳ1\\mathcal{M}_{1} or ℳ2subscriptℳ2\\mathcal{M}_{2} is inaccurate, or both ℳ1subscriptℳ1\\mathcal{M}_{1} and ℳ2subscriptℳ2\\mathcal{M}_{2} are inaccurate.Caliskan et al. (2017); May et al. (2019); Lauscher et al. (2021); Tan and Celis (2019) used a similar approach to calculate distributional properties and perform statistical tests.Using this idea, we conduct a number of correlation analysis experiments with some of the popular fairness evaluation techniques in order to probe for (in)consistencies that ensue from using such techniques.We use Pearson correlation coefficients to carry out these investigations.\n\n[2] Addressing multiple metrics of group fairness in data-driven decision making Addressing multiple metrics of group fairness in data-driven decision making The Fairness, Accountability, and Transparency in Machine Learning (FAT-ML) literature proposes a varied set of group fairness metrics to measure discrimination against socio-demographic groups that are characterized by a protected feature, such as gender or race.Such a system can be deemed as either fair or unfair depending on the choice of the metric. Several metrics have been proposed, some of them incompatible with each other.We do so empirically, by observing that several of these metrics cluster together in two or three main clusters for the same groups and machine learning methods. In addition, we propose a robust way to visualize multidimensional fairness in two dimensions through a Principal Component Analysis (PCA) of the group fairness metrics. Experimental results on multiple datasets show that the PCA decomposition explains the variance between the metrics with one to three components.\n\n[3] Can I Trust My Fairness Metric? Assessing Fairness with Unlabeled Data and Bayesian Inference In this section we systematically evaluate the quality of different estimation approaches by repeating the same type of experiment as in Section 3 and Figure 3 across different amounts of labeled data nLsubscript𝑛𝐿n_{L}.In particular, for each value of nLsubscript𝑛𝐿n_{L} we randomly sample sets of labeled datasets of size nLsubscript𝑛𝐿n_{L}, generate point estimates of a metric ΔΔ\\Delta of interest for each labeled dataset for each of the BB and BC estimation methods, and compute the mean absolute error (MAE) between the point estimates and the true value (computed on the full labeled test set). The frequency-based estimates are not shown for clarity—they are almost always worse than both BB and BC. As an example, Figure 4 illustrates the quality of estimation where ΔΔ\\Delta is the FPR group difference ΔΔ\\Delta for the MLP classification model, evaluated across 8 different dataset-group pairs. Each y-value is the average of 100 different randomly sampled sets of nLsubscript𝑛𝐿n_{L} instances, where nLsubscript𝑛𝐿n_{L} is the corresponding x-axis value.The BC method dominates BB across all datasets indicating that the calibrated scores are very effective at improving the accuracy in estimating group FPR. This is particularly true for small amounts of labeled data (e.g., up to nL=100subscript𝑛𝐿100n_{L}=100) where the BB Method can be highly inaccurate, e.g., MAEs on the order of 10 or 20% when the true value of ΔΔ\\Delta is often less than 10%.\n\n[4] OpenXAI: Towards a Transparent Evaluation of Model Explanations Fairness.To measure fairness of explanation methods, we compute the average metric values (for each of the aforementioned faithfulness and stability metrics) for different subgroups (e.g., male and female) in the dataset and compare them. Larger gaps between the metric values for different subgroups indicates higher disparities (unfairness).Without loss of generality, we present results using the PGU (see Section 2 and Appendix A) metric.Results for LR models in Figures  2 and 3 provide two key findings.First, the fairness analysis in Figures  2 and 3 shows that there are disparities in the faithfulness of explanations (see Section 2) output by several methods (Vanilla Gradients, Integrated Gradients, and SmoothGrad).Second, Gradient x Input results in the least amount of disparity across both the datasets.These results also suggest a trade-off between various evaluation metrics. For instance, Gradient x Input method underperforms on faithfulness and stability metrics, but outperforms other methods (+8.9%) when it comes to fairness metrics.Given such trade-offs, practitioners can leverage the OpenXAI leaderboards (Figure 1) to select an explanation method that best meets application-specific needs. Results with NN models and other fairness metrics are included in the Appendix E.7.\n\n[5] Subgroup Robustness Grows On Trees: An Empirical Baseline Investigation Our results show that, over all models and datasets, metrics which we refer to as “complementary” – those measuring the same event on different subsets, or with different conditioning – are strongly correlated with each other: accuracy and worst-group accuracy; CVaR and CVaR DORO; and Demographic Parity Difference and Equalized Odds Difference all show strong correlation with each model class ℱℱ\\mathcal{F}; we show these correlations on Adult in Figure 5. The median ρ𝜌\\rho values over all datasets and algorithms are 0.87, 0.99, 0.79 for the three pairs of metrics, respectively (we show the complete set of correlations for each model and metric in Figure 18). These results show that model selection based on one metric from each pair (Overall Accuracy) is likely to lead to a model with strong performance for the other metric in the pair (e.g. Worst-Group Accuracy).\n\n[6] On the Choice of Fairness: Finding Representative Fairness Metrics for a Given Context propose an automatic framework for identifying a small subset of fairness metrics that are representative of other metrics. In summary, we make the following contributions: • We propose the problem of using the correlations between different fairness metrics, for a given context (specified by training data and a model type), to find a small subset of metrics that represent other fairness metrics. To the best of our knowledge, we are the first to propose this problem. • We design a sampling-based Monte-Carlo method to estimate the correlations between the fairness metrics. • We develop an efficient approach for sampling models with different fairness metric values, which enables estimating the correlation between the fairness metrics. • We adapt and extend the existing work in order to specify the small subset of representative metrics, using the correlations between fairness metrics. • In order to evaluate our findings, we conduct comprehensive experiments, using real-world benchmark datasets, multiple types of classifiers, and a comprehensive set of fairness metrics. Our experiment results, verify the effectiveness of our approach for choosing fairness metrics. In the following, we first provide a brief explanation of the fairness metrics and then propose our technical solution. Fairness Model Consider a training dataset D consisting of n data points denoted by vectors of (X, S, Y ). Let X be the set of nonsensitive attributes of dimension d, S be the set of sensitive attributes, and Y be the response variable. In a classification setting Y = 1, . . . , K with K being the\n\n[7] On the Intrinsic and Extrinsic Fairness Evaluation Metrics for Contextualized Language Representations We conduct a study on gender, race, and religion domains (see the Appendix A for the list of protected groups on each domain).We conduct correlation analysis on the variance of group metric scores across protected groups, as it captures score disparities across protected groups for each domain. For example, for M𝑀M = CEAT, we define SMrace=Var​(sAsian,sWhite,sBlack,…)subscript𝑆subscript𝑀raceVarsubscript𝑠Asiansubscript𝑠Whitesubscript𝑠Black…S_{M_{\\textrm{race}}}=\\textrm{Var}(s_{\\textrm{Asian}},s_{\\textrm{White}},s_{\\textrm{Black}},...).A less-biased model would have smaller variance score.Thus, if two metrics are correlated, we would see a positive correlation, as reducing the disparity between groups in one metric, as measured by variance would reduce that in the other.\n\n[8] On the Choice of Fairness: Finding Representative Fairness Metrics for a Given Context f16 f1 f2 f3 f4 f5 f6 f7 f8 f9 f10 f11 f12 f13 f14 f15 Figure 8, we illustrate a graph with nodes representing the fairness metrics, orange nodes indicating the representative metrics of each subset, and edges showing the subsets of metrics that are highly correlated. We used τ = 0.5 for this plot. The graph confirms that the number of subsets are smaller using Logit model than that using NN, as previously discussed. Similarly, Figure 9 shows the representative metrics for Credit dataset using Logit and NN models. Discovery Ratio, Predictive Parity, Equality of Opportunity, and Average Odd Difference are selected as representative metrics for Credit dataset when we use Logit classifier. Related work Algorithmic fairness have been studied extensively in recent years (Corbett-Davies et al. 2017;Kleinberg et al. 2018). Various fairness metrics have been defined in the literature to address the inequalities of the algorithmic decision making from different perspectives. (Barocas, Hardt, and Narayanan 2017) and (Verma and Rubin 2018) define different fairness notions in details. Majority of works focus on the fairness consideration in different stages of predictive modeling including pre-processing (Feldman et al. 2015;Kamiran and Calders 2012;Calmon et al. 2017), in-processing (Calders and Verwer 2010;Zafar et al. 2015;Asudeh et al. 2019), and post-processing (Pleiss et al. 2017;Feldman et al. 2015;Stoyanovich, Yang, and Jagadish 2018;Hardt, Price, and Srebro 2016) to mitigate bias of the outcome. Furthermore, the proposed interventions are tied to a specific notion of fairness; statistical parity (Calders and Verwer 2010), equality of opportunity (Hardt, Price, and Srebro 2016),\n\n[9] On the Intrinsic and Extrinsic Fairness Evaluation Metrics for Contextualized Language Representations To align the stereotype metrics, we use data from the StereoSet intersentence dataset, which consists of a one-sentence context followed by a relevant stereotyped sentence, to compute BOLD metrics. Specifically, we use the context sentence for BOLD-like generation (see Appendix F for generation examples). We test StereoSet with the new B-Stereo on the race domain and find that the correlation score increase from −0.180.18-0.18 to 0.020.020.02 (p-value 0.98).This indicates that aligning the evaluation dataset source has a modest impact on improving correlation between metrics.\n\n"}
{"case_id": "46eecf06447c43657abf042004cdc115", "answer_text": "\n\n\nDoping plays a crucial role in integrated circuit (IC) design, influencing various device characteristics such as threshold voltage, subthreshold slope, drive current, and overall device performance. These effects vary based on the type of device and the doping strategy employed [1][2].\n\n\nThreshold Voltage Manipulation[14]\n\nDoping significantly affects the threshold voltage (VTH) of transistors, which is essential for proper device operation and power efficiency. For instance, in vertical Impact Ionization MOSFETs (I-MOS), increased source-drain doping concentrations lead to a decrease in VTH due to a higher electric field and impact generation rates [14]. Similarly, doping in MoS2-FETs can be controlled by inserting an aluminum interlayer to modulate VTH for wafer-scale IC fabrication [12]. This capability to adjust the threshold voltage is critical for optimizing the performance of digital logic circuits, such as inverters and more complex logic blocks.\n\nSubthreshold Slope and Leakage Currents[1][5][14] \n\nThe subthreshold slope (SS) is another parameter influenced by doping and plays a significant role in determining the switching efficiency of transistors. For instance, in tunneling field-effect transistors (TFETs), source doping impacts both the on-state and off-state performance of the devices. Separating the doping from the heterostructure can improve the subthreshold swing, essential for achieving low-power operation [1][2]. Additionally, doping strategies, such as embedding a drain pocket, can improve both SS and the drive current (ION) by suppressing leakage currents and enhancing the electric field at the source-channel tunnel junction [2].\n\nDrive Current Enhancement can be significantly improved through careful doping engineering of semiconductor devices [2]. For instance, embedding an opposite N+ doping layer next to the P+ source, or inserting a δ doping layer, can enhance the electric field at the source-channel tunnel junction and improve drive current (ION). This finding is supported by research showing that adjusting the doping density in specific regions of a device can lead to substantial improvements in performance, addressing limitations in source-channel tunneling probability or source-to-drain tunneling leakage [2].\n\nEnhancing the drive current is crucial for high-performance ICs, and doping can be instrumental in achieving this. The use of p-type and n-type doping in quantum-dot semiconductor optical amplifiers, for example, can significantly enhance the optical gain and saturation density, thereby improving the overall performance of the device [3]. Likewise, in organic field-effect transistors (OFETs), doping with organic salts like N,N-dimethylanilinium tetrakis(pentafluorophenyl)borate (DTB) has been shown to increase hole mobility and reduce threshold voltage, thus improving performance [6].\n\nMinimizing Short Channel Effects[11]. \n\nAs MOSFETs are downscaled to improve circuit density and speed, they encounter challenges related to short channel effects (SCEs). Doping techniques help mitigate these effects. For example, multigate MOSFETs or FinFETs offer better control over the drain current than single gate MOSFETs, largely due to their improved gate control influenced by precise doping levels [11]. This better control is essential for maintaining performance as device dimensions shrink.\n\n### Material-Specific Doping Effects\n\nThe impact of doping also varies depending on the material used. For example, doping aluminum into cadmium sulfide (CdS) films results in changes in electrical properties, such as decreased resistivity and work function, which affect the diode behavior in ITO/CdS:Al/NiOx/Ni/Au heterojunctions [10]. Similarly, boron doping in metal oxide transistors (TFTs) suppresses defects responsible for electron trapping and enhances dielectric properties, leading to improved device performance [0].\n\n### Optical and Thermoelectric Properties\n\n[9]\n\nIn addition to electronic properties, doping can influence the optical and thermoelectric characteristics of materials used in ICs. For instance, p-type doping in PEDOT enhances its thermoelectric properties by optimizing carrier concentration and reducing trap states, which is critical for applications needing efficient thermal management [9].\n\nConclusion\n\nWe report the results of a study to enhance metal oxide (MO) thin-film transistor (TFT) performance by doping both the semiconductor (In2O3) and gate dielectric (Al2O3) layers with boron (yielding IBO and ABO, respectively) and provide the first quantitative analysis of how B doping affects charge transport in these MO dielectric and semiconducting matrices (0). The impact of 1-9 atom % B doping on MO microstructure, morphology, oxygen defects, charge transport, and dielectric properties is analyzed together, in detail, by complementary experimental (microstructural, electrical) and theoretical (ab initio MD, DFT) methods (0). The results indicate that B doping frustrates In2O3 crystallization while suppressing defects responsible for electron trapping and carrier generation (0). In the adjacent Al2O3 dielectric, B doping increases the dielectric constant and refractive index while reducing leakage currents (0). Furthermore, optimized solution-processed TFTs combining IBO channels with 6 atom % B and ABO dielectrics with 10 atom % B exhibit field effect mobilities as high as 11 cm2 V-1 s-1, current on/off ratios >105, threshold voltages = 0.6 V, and superior bias stress durability (0). \n\nIn summary, doping is a versatile and powerful tool in IC design, allowing for the fine-tuning of various device parameters to achieve desired performance metrics. The ability to control threshold voltage, subthreshold slope, drive current, and minimize leakage currents and short channel effects through tailored doping strategies is essential for developing high-performance, low-power, and scalable integrated circuits [11].\n\n\nReferences:\n[0] Synergistic Boron Doping of Semiconductor and Dielectric Layers for High-Performance Metal Oxide Transistors: Interplay of Experiment and Theory. Synergistic Boron Doping of Semiconductor and Dielectric Layers for High-Performance Metal Oxide Transistors: Interplay of Experiment and Theory. We report the results of a study to enhance metal oxide (MO) thin-film transistor (TFT) performance by doping both the semiconductor (In2O3) and gate dielectric (Al2O3) layers with boron (yielding IBO and ABO, respectively) and provide the first quantitative analysis of how B doping affects charge transport in these MO dielectric and semiconducting matrices. The impact of 1-9 atom % B doping on MO microstructure, morphology, oxygen defects, charge transport, and dielectric properties is analyzed together, in detail, by complementary experimental (microstructural, electrical) and theoretical (ab initio MD, DFT) methods. The results indicate that B doping frustrates In2O3 crystallization while suppressing defects responsible for electron trapping and carrier generation. In the adjacent Al2O3 dielectric, B doping increases the dielectric constant and refractive index while reducing leakage currents. Furthermore, optimized solution-processed TFTs combining IBO channels with 6 atom % B and ABO dielectrics with 10 atom % B exhibit field effect mobilities as high as 11 cm2 V-1 s-1, current on/off ratios >105, threshold voltages = 0.6 V, and superior bias stress durability.\n\n[1] Impact of source doping on the performance of vertical InAs/InGaAsSb/GaSb nanowire tunneling field-effect transistors Impact of source doping on the performance of vertical InAs/InGaAsSb/GaSb nanowire tunneling field-effect transistors In this paper, we analyze experimental data from state-of-the-art vertical InAs/InGaAsSb/GaSb nanowire tunneling field-effect transistors (TFETs) to study the influence of source doping on their performance. Overall, the doping level impacts both the off-state and on-state performance of these devices. Separation of the doping from the heterostructure improved the subthreshold swing of the devices. The best devices reached a point subthreshold swing of 30 mV/dec at 100 x higher currents than previous Si-based TFETs. However, separation of doping from the heterostructure had a significant impact on the on-state performance of these devices due to effects related to source depletion. An increase in the doping level helped to improve the on-state performance, which also increased the subthreshold swing. Thus, further optimization of doping incorporation with the heterostructure will help to improve vertical InAs/InGaAsSb/GaSb nanowire TFETs.\n\n[2] Exploring channel doping designs for high-performance tunneling FETs Exploring channel doping designs for high-performance tunneling FETs Future high-performance low-power integrated circuits require compact logic devices with both steep subthreshold swing (SS) and large drive current (ION). Tunneling field-effect transistors (TFETs) can meet the first requirement but their ION is severely limited either by the low source-channel tunneling probability or by the high source-to-drain tunneling leakage. One of the methods that can be employed to boost ION is doping engineering. In particular (1)lowering the drain doping density elongates the drain depletion region and thus suppresses the leakage leading to improved SS (and ION). This scheme, however, is not scalable as a long drain length is needed to reach charge neutrality; (2) embedding an opposite N+ doping layer next to the P+ source, i.e., the source-pocket (SP) design, or inserting a δ doping layer, can enhance the electric field at the source-channel tunnel junction and improve ION. It can be shown that the improvement increases as the pocket doping density (Np) increases, but in practice doping density has an upper limit. In this paper, we show that, (1) embedding a P+ drain pocket can also improve the SS (and ION) and it is more scalable than lowering the drain doping; (2) by resorting to P+ channel, we can further improve ION of the SP design without having to increase Np.\n\n[3] Effect of Doping on the Optical Characteristics of Quantum-Dot Semiconductor Optical Amplifiers Effect of Doping on the Optical Characteristics of Quantum-Dot Semiconductor Optical Amplifiers The influence of p-type and n-type doping on the optical characteristics of a quantum-dot semiconductor optical amplifier (SOA) is studied using a rate equation model that takes into account the effect of the multidiscrete energy levels and the charge neutrality relation. Our calculations show that the amplifier optical gain can be greatly enhanced through p-type doping where the doping concentration should not exceed the certain level. We find that increasing the acceptor concentration increases the unsaturated optical gain but at the same time decreases the saturation density and the effective relaxation lifetime. Also our calculation reveals that the use of p-type doping will be associated with an increase in the transparency current where the increase in the transparency current depends on the incident photon energy. On the other hand, we find that it is possible to increase the saturation density and enhance the linearity of the SOA by using n-type doping.\n\n[4] Doping Concentrations Analysis on The Performance of Vertical Strained-SiGe Impact Ionization MOSFET Incorporating Dielectric Pocket (VESIMOS-DP) Doping Concentrations Analysis on The Performance of Vertical Strained-SiGe Impact Ionization MOSFET Incorporating Dielectric Pocket (VESIMOS-DP) The Vertical Strained SiGe Impact Ionization MOSFET incorporating Dielectric Pocket (VESIMOS-DP) has been successfully developed and analyzed in this paper. The effect of doping concentration for both Source and Drain (S/D) as well as body doping concentration to the performance of VESIMOS-DP in terms of subthreshold slope (S), threshold voltage (VTH) and drain current has been observed in this paper. An inverse proportional of S and VTH value was found when S/D doping concentration increased. It is notable that for S/D doping concentration above 1019 atoms/cm3, there is a significant increase in S values which is not recommended as the switching speed getting higher distracting performance of the device. However, too low S/D doping concentration is not essential as it didn’t show any significant improvement on the performance of the device. It is also revealed that with low body doping concentration, the device suffers tremendous Parasitic Bipolar Transistor (PBT) effect that prevents the device from switched off. Thus, optimum doping concentration is imperative to obtain superb device characteristic. Due to the DP layer, a stable VTH =1.35V obtained due to the vicinity of DP layer near the drain end has reduced charge sharing between the source and drain. The slight different and consistency of VESIMOS-DP subthreshold value (S = 19 mV/dec) has given advantages for incorporating DP layer near the drain end. In many aspects, it is revealed that the incorporation of DP has enhanced the electrical performance and suppressed\n\n[5] Pursuing High‐Performance Organic Field‐Effect Transistors through Organic Salt Doping Pursuing High‐Performance Organic Field‐Effect Transistors through Organic Salt Doping Doping is an effective strategy for controlling the charge density and device performance of thin‐film electronics. Herein, a new doping system is reported for organic electronics using the organic salt p‐dopant N,N‐dimethylanilinium tetrakis(pentafluorophenyl)borate (DTB) to significantly improve the device performance of indacenodithiophene‐co‐benzothia‐diazole (IDT‐BT) organic field‐effect transistors (OFETs). With optimized doping ratios, the hole mobility increases almost fourfold from 0.32 to 1.15 cm2 V–1 s–1 and the threshold voltage reduces from −38 to 0 V. Moreover, systematical electrical characterizations demonstrate that the contact resistance and activation energy dramatically reduce in the doped devices. Such reductions are ascribed to the shift of the Fermi energy level closer to the transport level and the lowered density of trap states in doped semiconductors, as revealed by ultraviolet photoelectron spectroscopy and low‐frequency noise measurements, respectively. This study also demonstrates that the trap density increases when the doping ratio is high, explaining the device performance degradation at high doping ratios. This is the first time that DTB organic salt is used as an efficient dopant to improve the performance of OFETs, demonstrating a promising route for employing organic salt dopants to achieve high‐performance OFETs.\n\n[6] Unravelling Doping Effects on PEDOT at the Molecular Level: From Geometry to Thermoelectric Transport Properties. Tuning carrier concentration via chemical doping is the most successful strategy to optimize the thermoelectric figure of merit. Nevertheless, how the dopants affect charge transport is not completely understood. Here we unravel the doping effects by explicitly including the scattering of charge carriers with dopants on thermoelectric properties of poly(3,4-ethylenedioxythiophene), PEDOT, which is a p-type thermoelectric material with the highest figure of merit reported. We corroborate that the PEDOT exhibits a distinct transition from the aromatic to quinoid-like structure of backbone, and a semiconductor-to-metal transition with an increase in the level of doping. We identify a close-to-unity charge transfer from PEDOT to the dopant, and find that the ionized impurity scattering dominates over the acoustic phonon scattering in the doped PEDOT. By incorporating both scattering mechanisms, the doped PEDOT exhibits mobility, Seebeck coefficient and power factors in very good agreement with the experimental data, and the lightly doped PEDOT exhibits thermoelectric properties superior to the heavily doped one. We reveal that the thermoelectric transport is highly anisotropic in ordered crystals, and suggest to utilize large power factors in the direction of polymer backbone and low lattice thermal conductivity in the stacking and lamellar directions, which is viable in chain-oriented amorphous nanofibers.\n\n[7] Effects of aluminum doping upon properties of cadmium sulfide thin films and its effect on ITO/CdS:Al/NiOx/Ni/Au diodes Effects of aluminum doping upon properties of cadmium sulfide thin films and its effect on ITO/CdS:Al/NiOx/Ni/Au diodes We analyze the Al doping effect in CdS films by chemical bath deposition and how it impacts this doping effect in a heterojunction, for the case of ITO/CdS:Al/NiOx/Ni/Au. The results show that the doping effect of aluminum on the CdS films by chemical bath deposition does slightly affect the structural and morphological properties. The impact of the doping is related to the electrical properties; first, with a decrement in the resistivity from 108 to 107 Ω cm, the work function decreases from 4.7 to 4.3 eV, the bandgap and the valence band suffers a small increment of 0.1 eV. The change in the electrical properties impacts the diode behavior, and the increment in the work function leads to an increment of the Fermi energy level difference between NiOx and CdS:Al; this difference was reflected in a wider depletion region and a reduction of the leakage current.\n\n[8] The Effect of Doping on Different FET Structures: MOSFET, TFET and FinFET The Effect of Doping on Different FET Structures: MOSFET, TFET and FinFET MOSFET have been scaled down over the past few years in order to give rise to high circuit density and increase the speed of circuit. But scaling of MOSFET leads to issues such as poor control gate over the current which depends on gate voltage. Many short channel effects (SCE) influence the circuit performance and leads to the indeterminist response of drain current. These effects can be decreased by gate excitation or by using multiple gates and by offering better control gate the device parameters. In Single gate MOSFET, gate electric field decreases but multigate MOSFET or FinFET provides better control over drain current. In this paper, different FET structures such as MOSFET, TFET and FINFET are designed at 22nm channel length and effect of doping had been evaluated and studied. To evaluate the performance donor concentration is kept constant and acceptor concentration is varied. I. INTRODUCTION: In modern day integrated circuits, MOS transistors have gained popularity over conventional transistors because of its properties like low power consumption, high speed and high input impedance. Moore's Law led to extensive research in downscaling of transistor parameters. Downscaling of transistors parameters had led to increase in operating speed, low power consumption and reduction in die area of prevailing integrated circuits. To cope up with this law, the channel length of MOSFETs has been lowered gradually in past few decades. However, lowering of channel length beyond a certain level leads to degraded performance of MOS transistors.\n\n[9] Engineering Top Gate Stack for Wafer-Scale Integrated Circuit Fabrication Based on Two-Dimensional Semiconductors. Engineering Top Gate Stack for Wafer-Scale Integrated Circuit Fabrication Based on Two-Dimensional Semiconductors. In recent years, two-dimensional (2D) semiconductors have attracted considerable attention from both academic and industrial communities. Recent research has begun transforming from constructing basic field-effect transistors (FETs) into designing functional circuits. However, device processing remains a bottleneck in circuit-level integration. In this work, a non-destructive doping strategy is proposed to modulate precisely the threshold voltage (VTH) of MoS2-FETs in a wafer scale. By inserting an Al interlayer with a varied thickness between the high-k dielectric and the Au top gate (TG), the doping could be controlled. The full oxidation of the Al interlayer generates a surplus of oxygen vacancy (Vo) in the high-k dielectric layer, which further leads to stable electron doping. The proposed strategy is then used to optimize an inverter circuit by matching the electrical properties of the load and driver transistors. Furthermore, the doping strategy is used to fabricate digital logic blocks with desired logic functions, which indicates its potential to fabricate fully integrated multistage logic circuits based on wafer-scale 2D semiconductors.\n\n[10] Effects of S/D doping concentrations on strained SiGe vertical I-MOS characteristics Effects of S/D doping concentrations on strained SiGe vertical I-MOS characteristics This paper reports the effects of source and drain doping concentration on the device characteristics of strained SiGe vertical Impact Ionization MOSFET (I-MOS). Silvaco 2-D Atlas simulations were done to examine the device characteristics. It was found that the source-drain doping concentrations had a significant impact on the threshold voltage of the device. With increasing doping concentrations, the threshold voltage values decreased. This can be explained by the higher electric field and the impact generation rates, with increasing doping concentrations. A threshold voltage of 0.9V was obtained for a drain bias of 1.75V, with S/D doping concentration of 10. The effects of S/D doping on subthreshold slopes was also examined with the help of TCAD device simulations.\n\n"}
{"case_id": "b73090602419bdedaebd2a9d40e5a199", "answer_text": "\n\nSampling techniques play a crucial role in yield estimation for automated electronic circuit design. These techniques aim to efficiently and accurately predict the performance and reliability of circuits under varying conditions, which can involve substantial computational effort [1]. Below is an overview of the primary sampling techniques used in this field, categorized based on their methods and efficiency improvements.\n\nTraditional Monte Carlo (MC) Simulation [1]. \nMonte Carlo simulation is one of the most commonly used techniques for yield estimation in electronic circuit design. It involves generating a large number of random samples from the parameter space and running simulations to estimate the circuit's performance and yield. While MC is straightforward and versatile, it is computationally expensive, especially for high-yield circuits that require a massive number of simulations to achieve statistically significant results [1][9]. For instance, traditional MC methods are known to be inefficient for high-yield NoCs, requiring extensive runtimes [1].\n\nQuasi-Monte Carlo (QMC) Methods. [0] \nTo address the inefficiency of traditional MC simulations, Quasi-Monte Carlo (QMC) methods have been developed. QMC techniques use low-discrepancy sequences to generate sample points that are more uniformly distributed over the parameter space, leading to faster convergence rates and improved estimation accuracy. One example is the use of QMC combined with behavioral models of circuits, which has shown to accelerate the yield estimation process significantly, providing up to an 8 times speed-up compared to conventional methods [2]. However, QMC can suffer from performance degradation if the number of effective variables becomes too large, which is often the case for realistic analog circuits [0].\n\nLatin Hypercube Sampling (LHS) [14].\nLatin Hypercube Sampling (LHS) aims to improve the efficiency of yield estimation by ensuring that the parameter space is more evenly sampled compared to traditional random sampling methods. Each sample point is chosen such that the entire range of each parameter is explored, minimizing the variance in the estimation. Studies have shown that LHS can provide substantial speedups over primitive Monte Carlo methods while maintaining high accuracy. Specifically, LHS combined with trimmed-sample MC methodology achieves 29× to 54× speedup with yield estimation errors smaller than 1% [3][14]. However, LHS can be complex to implement and may require additional steps to ensure that samples are well-distributed for high-dimensional problems.\n\n Importance Sampling (IS) [7]. \nImportance Sampling (IS) enhances the efficiency of yield estimation by focusing on the most critical regions of the parameter space that contribute most to the yield. This technique assigns higher sampling rates to these critical regions, thus reducing the number of required simulations for an accurate yield estimate. The ISLE (Importance Sampling based on Stochastic Logical Effort) method is an example that combines importance sampling with logical effort to achieve high accuracy with fewer simulations, demonstrating a reduction in the number of full circuit simulations while maintaining high estimator accuracy [5][7][8]. However, IS requires accurate knowledge of the critical regions for effective sampling, which may not always be easily obtainable.\n\n\n### Gibbs Sampling [1]\n\nGibbs Sampling is another approach that can effectively generate failed instances as simulation samples, significantly reducing the number of required simulation runs [1]. This technique is useful in network-on-chip (NoC) yield estimation, where it has been shown to cut down simulation runtime by up to 100 times compared to traditional MC methods while maintaining the same level of accuracy [1]. The main limitation of Gibbs Sampling is that it may be less effective in highly complex or non-linear systems where conditional probabilities are difficult to compute accurately.\n\nHybrid Sampling Techniques[4]. \nCombining different sampling methods can further enhance the efficiency and accuracy of yield estimation. For example, a hybrid approach that integrates Latin Hypercube Sampling (LHS) and random sampling has been proposed to minimize the Monte Carlo processing time during the optimization loop for analog IC sizing [4]. This methodology can reduce computational effort and improve the search efficiency in the design space. In another instance, the AYLeSS method combines Quasi-Monte Carlo (QMC) sampling with Latin Supercube Sampling (LSS) to address yield estimation in high-dimensional problems. The combination yields approximately a twofold speed enhancement compared to state-of-the-art methods [0][6]. These hybrid approaches benefit from the strengths of each individual method, though they can be complex to implement and may require careful tuning to achieve optimal performance.\n\nComputational Intelligence Techniques [12]. \nAdvanced computational intelligence techniques, such as Gaussian process classification and ordinal optimization, have also been applied to yield estimation. These methods use machine learning models to guide the sampling process, focusing computational resources on the most promising design points. For instance, the ordinal optimization (OO)-based random-scale differential evolution (ORDE) algorithm integrates OO in the first stage to allocate sufficient samples to promising solutions and avoid unnecessary Monte Carlo simulations for non-critical solutions [12][13]. This method uses differential evolution for global searches combined with random-scale mutations for fine-tuning, achieving up to a tenfold improvement in computational effort [12][13]. Additionally, Gaussian process classification models design preferences and guides the optimization process more efficiently, achieving up to twelvefold speed-up without significant loss of accuracy [11]. However, the effectiveness of these techniques depends on the quality of the underlying machine learning models, which may require substantial training data and computational resources to develop.\n\n\n\n### Varied Applications\n\nAnalog integrated circuit yield estimation methods have diversified to address the unique challenges posed by high-dimensional problems. A novel method called AYLeSS uses Latin Supercube Sampling (LSS) to estimate yield, showing approximately a 2 times speed enhancement compared with the best state-of-the-art method [0]. \n\nFast yield estimation for analog circuits has also been achieved by combining the behavioral model of a circuit with the Quasi-Monte Carlo (QMC) sampling technique, significantly reducing time for yield estimation [2]. \n\nNetwork-on-Chip (NoC) yield analysis benefits from Gibbs sampling, which reduces the number of required simulation runs significantly compared to traditional methods [1].\n\nImproved yield optimization for analog circuits via Gaussian process classification and varying-sigma sampling achieves up to 12× speed-up without loss of accuracy [11]. \n\nThe trimmed-sample Monte Carlo methodology can speed up yield estimation by 29× to 54× over traditional MC analysis, maintaining errors smaller than 1% [3]. \n\n\nThe sampling techniques discussed here find applications across various domains of electronic circuit design beyond analog IC sizing and NoC yield estimation. For example, in microwave integrated circuits, Latin Hypercube Sampling has proved effective in yield analysis of small signal amplifiers, improving accuracy and reducing simulation runs [14]. The ISLE method is particularly advantageous in timing yield estimation for circuits under process parameter variations and environmental fluctuations [9]. The Scaled Sigma Sampling technique is utilized to assess design sensitivity to process excursions, which is critical in mitigating risks related to product yield due to discrepancies between design models and actual silicon production [10]. These varied applications demonstrate the versatility and importance of advanced sampling techniques in optimizing circuit performance and reliability.\n\nConclusion: Yield estimation for analog integrated circuits remains a time-consuming operation in variation-aware sizing. State-of-the-art statistical methods such as ranking-integrated Quasi-Monte-Carlo (QMC), suffer from performance degradation if the number of effective variables is large (as typically is the case for realistic analog circuits) [0]. To address this problem, a new method, called AYLeSS, is proposed to estimate the yield of analog circuits by introducing Latin Supercube Sampling (LSS) technique from the computational statistics field. Firstly, a partitioning method is proposed for analog circuits, whose purpose is to appropriately partition the process variation variables into low-dimensional sub-groups fitting for LSS sampling [0]. Then, randomized QMC is used in each sub-group [0]. In addition, the way to randomize the run order of samples in Latin Hypercube Sampling (LHS) is used for the QMC sub-groups [0]. AYLeSS is tested on 4 designs of 2 example circuits in 0.35μm and 90nm technologies with yield from about 50% to 90% [0]. Experimental results show that AYLeSS has approximately a 2 times speed enhancement compared with the best state-of-the-art method [0]. \nIn summary, a variety of sampling techniques are employed in yield estimation for automated electronic circuit design, each offering different trade-offs between computational efficiency and estimation accuracy. Traditional MC methods provide a baseline [1], while QMC [2], LHS [14], and importance sampling [7] offer significant improvements in efficiency. Hybrid techniques [4] and computational intelligence methods [12] further push the boundaries, enabling faster and more accurate yield estimation in complex, high-dimensional design spaces. \n\nFor high-dimensional problems, methods like AYLeSS with its hybrid QMC and LSS approach are particularly effective. In scenarios requiring focus on critical regions, Importance Sampling (IS) and ISLE methods are advantageous. For large-scale, high-yield NoC designs, Gibbs Sampling provides substantial runtime reductions. Computational intelligence techniques, such as those using Gaussian process classification or ordinal optimization, excel in scenarios where machine learning models can be effectively trained to guide yield estimation. The specific performance results, such as the 5×-100× runtime reduction by Gibbs Sampling [1] and up to 54× speedup by trimmed-sample Monte Carlo [3], underscore the advantages of these advanced techniques in optimizing circuit performance and reliability in modern electronic design.\n\n\nReferences:\n[0] A fast analog circuit yield estimation method for medium and high dimensional problems Yield estimation for analog integrated circuits remains a time-consuming operation in variation-aware sizing. State-of-the-art statistical methods such as ranking-integrated Quasi-Monte-Carlo (QMC), suffer from performance degradation if the number of effective variables is large (as typically is the case for realistic analog circuits). To address this problem, a new method, called AYLeSS, is proposed to estimate the yield of analog circuits by introducing Latin Supercube Sampling (LSS) technique from the computational statistics field. Firstly, a partitioning method is proposed for analog circuits, whose purpose is to appropriately partition the process variation variables into low-dimensional sub-groups fitting for LSS sampling. Then, randomized QMC is used in each sub-group. In addition, the way to randomize the run order of samples in Latin Hypercube Sampling (LHS) is used for the QMC sub-groups. AYLeSS is tested on 4 designs of 2 example circuits in 0.35μm and 90nm technologies with yield from about 50% to 90%. Experimental results show that AYLeSS has approximately a 2 times speed enhancement compared with the best state-of-the-art method.\n\n[1] An Efficient Network-on-Chip Yield Estimation Approach Based on Gibbs Sampling An Efficient Network-on-Chip Yield Estimation Approach Based on Gibbs Sampling A network-on-chip (NoC), a redundancy-rich and thus relatively robust system-chip, is still vulnerable to defects due to its large-scale integration. Thus, it is desirable to analyze the NoC yield in an early design phase. A Monte Carlo (MC) approach was proposed for the NoC yield analysis at the system level; however, it is inefficient due to the requirement of a large number of simulation runs. In this paper, we propose a Gibbs sampling approach, which can efficiently generate failed NoC instances as simulation samples, for yield estimation. This approach significantly reduces the number of required simulation runs for obtaining an accurate yield estimation. Implementation issues, such as initial sample selection, calculation of conditional distributions, and stop criterion, to customize Gibbs sampling for the NoC yield analysis are discussed. Potential optimization opportunities to further improve Gibbs sampling's efficiency are also explored. Compared to the MC approach, our experimental results show that the proposed approach can reduce the simulation runtime by 5×-100× for a high-yield NoC (a failure rate at 10-2-10-5), while achieving the same level of accuracy for yield estimation.\n\n[2] A Fast Yield Estimation Approach Considering Foundry Variation for Analog Design A Fast Yield Estimation Approach Considering Foundry Variation for Analog Design Herein, we propose a fast yield estimation approach for analog circuits design in which we combine the behavioral model of circuit and the Quasi-Monte Carlo (QMC) sampling technique to accelerate yield estimation process. The behavioral model is constructed in Verilog-A based on the simulation results which are done at transistor-level; then, the accuracy of the model is verified by experimental testing on a specific analog circuit. Furthermore, instead of using random circuit samples, in this work, QMC circuit samples are adopted to obtain faster convergence rates for the yield prediction process. In conventional analog design stage, designers repeat a number of yield estimation process to select the optimal design point. Each yield estimation effort is a time-consuming process since designers have to simulate on a large number of circuits. Unlike the conventional method, in this work, we build a look-up table for constructing behavioral model of any given circuit; then, this table can be reused in repeating the yield-estimation processes. Therefore, the proposed method can significantly reduce the time for the yield estimation process. Experimental results show that the proposed approach can speed-up the yield estimation process 8 times compared to conventional simulation-based methods with a reasonable drop in accuracy (less than 5%).\n\n[3] Efficient trimmed-sample Monte Carlo methodology and yield-aware design flow for analog circuits Efficient trimmed-sample Monte Carlo methodology and yield-aware design flow for analog circuits This paper proposes efficient trimmed-sample Monte Carlo (TSMC) methodology and novel yield-aware design flow for analog circuits. This approach focuses on “trimming simulation samples” to speedup MC analysis. The best possible yield and the worst performance are provided “before” MC simulations such that designers can stop MC analysis and start improving circuits earlier. Moreover, this work can combine with variance reduction techniques or low discrepancy sequences to reduce the MC simulation cost further. Using Latin Hypercube Sampling as an example, this approach gives 29× to 54× speedup over traditional MC analysis and the yield estimation errors are all smaller than 1%. For analog system designs, the proposed flow is still efficient for high-level MC analysis, as demonstrated by a PLL system.\n\n[4] A Hybrid Sampling Method for In-the-Loop Yield Estimation of Analog ICs in an Optimization Process A Hybrid Sampling Method for In-the-Loop Yield Estimation of Analog ICs in an Optimization Process Analog integrated circuit sizing can be modeled as an optimization problem and solved by optimization heuristics. The resulting solution is dependent on the modeling strategy and on the performance estimation, which is done, in general, by electrical simulations. However, the optimized solution falls on the border of the design space, where a small variation in the device parameters affects the circuit performance. In order to address this issue during sizing steps, a Monte Carlo simulation is included in the optimization loop, leading in a computational effort increasing. This work analyses both Latin Hypercube and the traditional random sampling methods in order to reduce the number of Monte Carlo runs for minimizing the necessary time to estimate the resulting yield at each iteration of the optimization process. Based on these analysis a hybrid sampling method is proposed to lower the Monte Carlo processing time. The methodology is applied for the sizing of a two-stage Miller operational transconductance amplifier, showing advantages in terms of processing time and circuit performance while producing a more efficient search in the design space.\n\n[5] Fast Monte Carlo Estimation of Timing Yield: Importance Sampling with Stochastic Logical Effort (ISLE) The estimation of loss based on the STD-MC estimator in(16) will obviously be more accurate than the onebased on the SLE-MC estimator in (18), but much morecostly. We use the cheap SLE-MC estimator not by itself for yieldestimation, by in a novel approach to constructing an IS-based lossestimator with reduced variance. This approach is called ISLE(Importance Sampling based on Stochastic Logical Effort) and providesthe same accuracy as the STD-MC estimator but at a cost of much fewernumber of full circuit simulations.\n\n[6] A fast analog circuit yield estimation method for medium and high dimensional problems A fast analog circuit yield estimation method for medium and high dimensional problems Yield estimation for analog integrated circuits remains a time-consuming operation in variation-aware sizing. State-of-the-art statistical methods such as ranking-integrated Quasi-Monte-Carlo (QMC), suffer from performance degradation if the number of effective variables is large (as typically is the case for realistic analog circuits). To address this problem, a new method, called AYLeSS, is proposed to estimate the yield of analog circuits by introducing Latin Supercube Sampling (LSS) technique from the computational statistics field. Firstly, a partitioning method is proposed for analog circuits, whose purpose is to appropriately partition the process variation variables into low-dimensional sub-groups fitting for LSS sampling. Then, randomized QMC is used in each sub-group. In addition, the way to randomize the run order of samples in Latin Hypercube Sampling (LHS) is used for the QMC sub-groups. AYLeSS is tested on 4 designs of 2 example circuits in 0.35μm and 90nm technologies with yield from about 50% to 90%. Experimental results show that AYLeSS has approximately a 2 times speed enhancement compared with the best state-of-the-art method.\n\n[7] Fast Monte Carlo Estimation of Timing Yield: Importance Sampling with Stochastic Logical Effort (ISLE) The technique we propose aims to improve the accuracy of the yieldestimates obtained from a given number of Monte Carlosimulations. Alternatively, our improved Monte Carlo estimatorachieves the same accuracy as the standard Monte Carlo estimator, butat a cost of much fewer number of full circuit simulations. This ismade possible by using a variance reduction technique calledimportance sampling that we combine in a novel manner with astochastic generalization of the logical effort formalism originallyproposed by Sutherland et. al .Logical effort is a method for quicklyestimating and optimizing the path delays in a circuit. We use thestochastic logical effort formalism to guide the generation andselection of sample points in the parameter probability space in atransistor-level simulation based Monte Carlo method for timing yieldestimation.\n\n[8] Fast Monte Carlo Estimation of Timing Yield: Importance Sampling with Stochastic Logical Effort (ISLE) A key benefit of the ISLE approach is that circuit-level simulationsare avoided for discarded samples, i.e., when Xisubscript𝑋𝑖X_{i} results in an SLEcircuit delay estimate smaller than Tcϵsuperscriptsubscript𝑇𝑐italic-ϵT_{c}^{\\epsilon}. The improvementbrought about by ISLE, however, goes significantly beyond this. Forthe same number of samples N𝑁N, the ISLE estimator in (21)provides a much more accurate (with significantly reduced variance)loss estimate than the STD-MC estimator in (16). Were itpossible to use the ideal biasing function f~i​d​e​a​lsubscript~𝑓𝑖𝑑𝑒𝑎𝑙\\tilde{f}_{ideal}, azero-variance estimator would have been obtained with a single sample.The ISLE approach makes it possible to explore the space betweenstandard MC and this ideal. Using an f~~𝑓\\tilde{f} that approximatesf~i​d​e​a​lsubscript~𝑓𝑖𝑑𝑒𝑎𝑙\\tilde{f}_{ideal} as closely as possible, ISLE both reduces the numberof full circuit simulations required and improves upon standardMC in the estimator accuracy achieved for the same number of fullcircuit simulations. The next section makes this discussion moreprecise.\n\n[9] Fast Monte Carlo Estimation of Timing Yield: Importance Sampling with Stochastic Logical Effort (ISLE) problem of estimating timing yield for a circuit under statistical process parameter variations and environmental fluctuations by proposing a novel and improved Monte Carlo method based on transistor-level circuit simulations. In conventional Monte Carlo yield estimation, a number of samples in the parameter probability space are generated. The overall delay of the circuit for each sample point is determined by performing transistor-level timing simulations. An estimator for timing yield is obtained by considering the fraction of samples for which the timing constraint is satisfied. Because of the computational cost of determining circuit delay for each sample, the number of samples one has to work with is limited. This adversely affects the accuracy of the yield estimator; which has a large error for a small number of samples. This is a weakness of the conventional Monte Carlo method and has prevented it from finding widespread use for practical yield estimation. The technique we propose aims to improve the accuracy of the yield estimates obtained from a given number of Monte Carlo simulations. Alternatively, our improved Monte Carlo estimator achieves the same accuracy as the standard Monte Carlo estimator, but at a cost of much fewer number of full circuit simulations. This is made possible by using a variance reduction technique called importance sampling that we combine in a novel manner with a stochastic generalization of the logical effort formalism originally proposed by Sutherland et. al. Logical effort is a method for quickly estimating and optimizing the path delays in a circuit. We use the stochastic logical\n\n[10] Rapid Assessment of Design Sensitivity to Process Excursions via Scaled Sigma Sampling Rapid Assessment of Design Sensitivity to Process Excursions via Scaled Sigma Sampling Spiraling costs of a product revision demand that we mitigate risks to product yield due to unintended disconnects between SPICE models used for design and production silicon, and intentional process retargeting for product performance optimization. This often necessitates product robustness to about +/-4.0-sigmas or about 60 ppm. However, the computational costs of even the most advanced simulation techniques are so prohibitive for many of the large circuit design problems that one often cannot obtain visibility to circuit behavior below about 5000 ppm. In this paper, we build on the scaled sigma sampling (SSS) foundation presented earlier and develop a formalism for efficient assessment of circuit yield exposure to low probability tails, including estimation of its confidence interval and optimization of process sigma scale factors and sample sizes used for the SPICE simulations. We illustrate the efficacy of SSS through an extended suite of circuit yield estimation examples including one that investigates the yield dependencies of a normality capable metric on process shifts and multiplicity.\n\n[11] An efficient yield optimization method for analog circuits via gaussian process classification and varying-sigma sampling An efficient yield optimization method for analog circuits via gaussian process classification and varying-sigma sampling This paper presents an efficient yield optimization method for analog circuits via Gaussian process classification and varying-sigma sampling. To quickly determine the better design, yield estimations are executed at varying sigma of process variations. Instead of regression methods requiring accurate yield values, a Gaussian process classification method is applied to model these preference information of designs with binary comparison results, and the preferential Bayesian optimization framework is implemented to guide the search. Additionally, a multi-fidelity surrogate model is adopted to learn the yield correlation at different sigmas. Compared with the state-of-the-art methods, the proposed method achieves up to 12× speed-up without loss of accuracy.\n\n[12] Efficient and Accurate Statistical Analog Yield Optimization and Variation-Aware Circuit Sizing Based on Computational Intelligence Techniques Efficient and Accurate Statistical Analog Yield Optimization and Variation-Aware Circuit Sizing Based on Computational Intelligence Techniques In nanometer complementary metal-oxide-semiconductor technologies, worst-case design methods and response-surface-based yield optimization methods face challenges in accuracy. Monte-Carlo (MC) simulation is general and accurate for yield estimation, but its efficiency is not high enough to make MC-based analog yield optimization, which requires many yield estimations, practical. In this paper, techniques inspired by computational intelligence are used to speed up yield optimization without sacrificing accuracy. A new sampling-based yield optimization approach, which determines the device sizes to optimize yield, is presented, called the ordinal optimization (OO)-based random-scale differential evolution (ORDE) algorithm. By proposing a two-stage estimation flow and introducing the OO technique in the first stage, sufficient samples are allocated to promising solutions, and repeated MC simulations of non-critical solutions are avoided. By the proposed evolutionary algorithm that uses differential evolution for global search and a random-scale mutation operator for fine tunings, the convergence speed of the yield optimization can be enhanced significantly. With the same accuracy, the resulting ORDE algorithm can achieve approximately a tenfold improvement in computational effort compared to an improved MC-based yield optimization algorithm integrating the infeasible sampling and Latin-hypercube sampling techniques. Furthermore, ORDE is extended from plain yield optimization to process-variation-aware single-objective circuit sizing.\n\n[13] Efficient and Accurate Statistical Analog Yield Optimization and Variation-Aware Circuit Sizing Based on Computational Intelligence Techniques In nanometer complementary metal-oxide-semiconductor technologies, worst-case design methods and response-surface-based yield optimization methods face challenges in accuracy. Monte-Carlo (MC) simulation is general and accurate for yield estimation, but its efficiency is not high enough to make MC-based analog yield optimization, which requires many yield estimations, practical. In this paper, techniques inspired by computational intelligence are used to speed up yield optimization without sacrificing accuracy. A new sampling-based yield optimization approach, which determines the device sizes to optimize yield, is presented, called the ordinal optimization (OO)-based random-scale differential evolution (ORDE) algorithm. By proposing a two-stage estimation flow and introducing the OO technique in the first stage, sufficient samples are allocated to promising solutions, and repeated MC simulations of non-critical solutions are avoided. By the proposed evolutionary algorithm that uses differential evolution for global search and a random-scale mutation operator for fine tunings, the convergence speed of the yield optimization can be enhanced significantly. With the same accuracy, the resulting ORDE algorithm can achieve approximately a tenfold improvement in computational effort compared to an improved MC-based yield optimization algorithm integrating the infeasible sampling and Latin-hypercube sampling techniques. Furthermore, ORDE is extended from plain yield optimization to process-variation-aware single-objective circuit sizing.\n\n[14] A novel approach to efficient yield estimation for microwave integrated circuits A novel approach to efficient yield estimation for microwave integrated circuits Latin Hypercube Sampling (LHS) is proposed as an alternative to Primitive Monte Carlo (PMC) for yield analysis. It is shown that the LHS method improves accuracy and confidence in yield estimate and yet requires smaller number of simulations compared to PMC technique. Yield analysis of a 8-12 GHz microwave small signal amplifier demonstrate the viability of the proposed approach.\n\n"}
{"case_id": "2345ff0aa0b13fc87a46d5528be46060", "answer_text": "\n\nThe field of compressing phylogenetic and other types of trees has seen substantial development, with various algorithms emerging, each with its strengths and limitations in terms of computational complexity, efficiency, and space savings [2]. Here, we compare several key algorithms in this domain, focusing on their computational complexity and operational efficiency.\n\nTreeZip is the most efficient algorithm for compressing homogeneous tree collections [0]. To the best of our knowledge, no other domain-based compression algorithm exists for large heterogeneous tree collections or enables their rapid analysis [1]. Our experimental results indicate that TreeZip averages 89.03 percent (72.69 percent) space savings on unweighted (weighted) collections of trees when the level of heterogeneity in a collection is moderate [1]. The organization of the TRZ file allows for efficient computations over heterogeneous data [1]. Combining TreeZip compressed (TRZ) file with general-purpose compression yields average space savings of 97.34 percent (81.43 percent) on unweighted (weighted) collections of trees [1]. TreeZip will prove invaluable in the efficient archival of tree collections, and enables scientists to develop novel methods for relating heterogeneous collections of trees [1]. \nTreeZip, introduced for compressing large collections of phylogenetic trees, is noted for its high efficiency. Originally designed for unweighted trees, TreeZip has been extended to handle weighted trees as well [1][2]. The algorithm achieves significant space savings—up to 98% for unweighted and 73% for weighted phylogenetic trees [2]. The combined use of TreeZip with general-purpose compressors like 7zip further enhances compression, achieving up to 99% and 92% space savings for unweighted and weighted trees, respectively [4]. One of the key advantages of TreeZip is its ability to perform operations such as filtering and consensus tree computation directly on the compressed format without decompression, significantly reducing computational overhead [4]. However, the time complexity of TreeZip grows with the number of trees, indicating that as the dataset size increases, so does the time required for compression [6].\n\n**Top Tree Compression** [5] \nTop tree compression proposes a novel approach for tree compression by leveraging repetitive tree patterns, similar to tree grammars, while supporting navigational queries on the compressed representation in logarithmic time [5]. It provides a worst-case compression performance that is theoretically better than classical methods like Directed Acyclic Graph (DAG) compression. Top tree compression can compress exponentially better than DAG compression in certain cases, maintaining a compression ratio that is always at least \\(\\log_\\sigma^{0.19} n\\), compared to the information-theoretic bound of \\(\\log_\\sigma n\\) [5]. This method is particularly effective for trees with repetitive structures and offers a balance between complexity and compression efficacy, making it a versatile option for large datasets.\n\n**Branch-and-Bound (BB) Tree Compression**[3].\n\nThe tree compression problem (TCP) for BB trees is more specific, targeting the reduction of BB trees used in integer programming [3]. The complexity of this compression involves identifying new disjunctions or removing leaves to obtain a smaller tree with the same certification strength. The exact computational complexity can be high, especially for large trees, due to the need for extensive post-hoc analysis. Heuristic methods have been proposed to make this process more tractable, but they are still computationally intensive, often involving exact and heuristic algorithms [3].\n\n\nTree Compression with Top Trees [5].\n\nThis method also supports efficient navigational queries in logarithmic time, making it suitable for applications requiring frequent tree traversals. The worst-case complexity is better than traditional methods, providing a compression ratio that can outperform DAG compression by substantial margins [11]. It's particularly effective in cases with high tree pattern repetition, offering both space efficiency and operational speed.\n\nComparative Analysis\n- **Complexity and Efficiency:** TreeZip demonstrates high efficiency in terms of space savings and operational speed, especially when combined with 7zip. Its extension to weighted trees and the ability to operate directly on compressed files add to its practical utility [1][2][4][6]. Top tree compression and Tree Compression with Top Trees are theoretically more advanced, providing better worst-case performance and supporting logarithmic-time queries, making them suitable for applications requiring frequent navigational queries [5][11].\n- **Operational Flexibility:** TreeZip's strength lies in its operational flexibility, enabling set operations and decompression directly on the compressed format without needing full decompression [4]. This feature is crucial for biological datasets where frequent retrieval and analysis of specific tree subsets are required.\n- **Scalability:** While TreeZip's performance scales with the size of the dataset, top tree compression methods offer theoretical guarantees on performance, maintaining efficiency even for very large datasets with repetitive structures [5][11].\n\nIn conclusion, the choice of a tree compression algorithm largely depends on the specific requirements of the task, including the need for navigational queries, the size of the dataset, and the balance between compression efficiency and computational complexity. TreeZip and top tree compression methods offer distinct advantages, making them suitable for different types of tree compression tasks in computational phylogenetics and beyond [0][5].\n\nReferences:\n[0] Heterogeneous Compression of Large Collections of Evolutionary Trees Heterogeneous Compression of Large Collections of Evolutionary Trees Compressing heterogeneous collections of trees is an open problem in computational phylogenetics. In a heterogeneous tree collection, each tree can contain a unique set of taxa. An ideal compression method would allow for the efficient archival of large tree collections and enable scientists to identify common evolutionary relationships over disparate analyses. In this paper, we extend TreeZip to compress heterogeneous collections of trees. TreeZip is the most efficient algorithm for compressing homogeneous tree collections. To the best of our knowledge, no other domain-based compression algorithm exists for large heterogeneous tree collections or enable their rapid analysis. Our experimental results indicate that TreeZip averages 89.03 percent (72.69 percent) space savings on unweighted (weighted) collections of trees when the level of heterogeneity in a collection is moderate. The organization of the TRZ file allows for efficient computations over heterogeneous data. For example, consensus trees can be computed in mere seconds. Lastly, combining the TreeZip compressed (TRZ) file with general-purpose compression yields average space savings of 97.34 percent (81.43 percent) on unweighted (weighted) collections of trees. Our results lead us to believe that TreeZip will prove invaluable in the efficient archival of tree collections, and enables scientists to develop novel methods for relating heterogeneous collections of trees.\n\n[1] Heterogeneous Compression of Large Collections of Evolutionary Trees. Compressing heterogeneous collections of trees is an open problem in computational phylogenetics. In a heterogeneous tree collection, each tree can contain a unique set of taxa. An ideal compression method would allow for the efficient archival of large tree collections and enable scientists to identify common evolutionary relationships over disparate analyses. In this paper, we extend TreeZip to compress heterogeneous collections of trees. TreeZip is the most efficient algorithm for compressing homogeneous tree collections. To the best of our knowledge, no other domain-based compression algorithm exists for large heterogeneous tree collections or enable their rapid analysis. Our experimental results indicate that TreeZip averages 89.03 percent (72.69 percent) space savings on unweighted (weighted) collections of trees when the level of heterogeneity in a collection is moderate. The organization of the TRZ file allows for efficient computations over heterogeneous data. For example, consensus trees can be computed in mere seconds. Lastly, combining the TreeZip compressed (TRZ) file with general-purpose compression yields average space savings of 97.34 percent (81.43 percent) on unweighted (weighted) collections of trees. Our results lead us to believe that TreeZip will prove invaluable in the efficient archival of tree collections, and enables scientists to develop novel methods for relating heterogeneous collections of trees.\n\n[2] An efficient and extensible approach for compressing phylogenetic trees. Biologists require new algorithms to efficiently compress and store their large collections of phylogenetic trees. Our previous work showed that TreeZip is a promising approach for compressing phylogenetic trees. In this paper, we extend our TreeZip algorithm by handling trees with weighted branches. Furthermore, by using the compressed TreeZip file as input, we have designed an extensible decompressor that can extract subcollections of trees, compute majority and strict consensus trees, and merge tree collections using set operations such as union, intersection, and set difference. On unweighted phylogenetic trees, TreeZip is able to compress Newick files in excess of 98%. On weighted phylogenetic trees, TreeZip is able to compress a Newick file by at least 73%. TreeZip can be combined with 7zip with little overhead, allowing space savings in excess of 99% (unweighted) and 92%(weighted). Unlike TreeZip, 7zip is not immune to branch rotations, and performs worse as the level of variability in the Newick string representation increases. Finally, since the TreeZip compressed text (TRZ) file contains all the semantic information in a collection of trees, we can easily filter and decompress a subset of trees of interest (such as the set of unique trees), or build the resulting consensus tree in a matter of seconds. We also show the ease of which set operations can be performed on TRZ files, at speeds quicker than those performed on Newick or 7zip compressed Newick files, and without loss of space savings. TreeZip is an efficient approach for compressing large collections of phylogenetic trees. The semantic and compact nature of the TRZ file allow it to be operated upon directly and quickly, without a need to decompress the original Newick file. We believe that TreeZip will be vital for compressing and archiving trees in the biological community.\n\n[3] Compressing Branch-and-Bound Trees Compressing Branch-and-Bound Trees A branch-and-bound (BB) tree certifies a dual bound on the value of an integer program. In this work, we introduce the tree compression problem (TCP): Given a BB tree T that certifies a dual bound, can we obtain a smaller tree with the same (or stronger) bound by either (1) applying a different disjunction at some node in T or (2) removing leaves from T? We believe such post-hoc analysis of BB trees may assist in identifying helpful general disjunctions in BB algorithms. We initiate our study by considering computational complexity and limitations of TCP. We then conduct experiments to evaluate the compressibility of realistic branch-and-bound trees generated by commonly-used branching strategies, using both an exact and a heuristic compression algorithm. Introduction Consider an integer linear programming (IP) problem min{c x : x ∈ P ∩ Z n }, where c ∈ Q n and P := {x ∈ R n : Ax ≤ b} for A ∈ Q m×n and b ∈ Q m . Primal bounds on (1) can be certified by integer feasible solutions z ∈ P ∩ Z n . Dual bounds on (1), on the other hand, are typically certified using branchand-bound (BB) trees. A BB tree is a graph-theoretical tree T where each node v corresponds to a polyhedron Q(v), with the root corresponding to P. Moreover, v is either a leaf, or it has exactly two children corresponding to the polyhedra defined by applying a disjunction (π x ≤ π 0 ) ∨ (π x ≥\n\n[4] An efficient and extensible approach for compressing phylogenetic trees to reconstruct the true tree, phylogenetic searches can easily return tens of thousands to hundreds of thousands of candidate evolutionary trees for biologists to consider. To help biologists handle these large collections of trees, we extend our previous TreeZip algorithm in several significant ways. First, the Tree-Zip algorithm is augmented to allow for the compression of trees with weighted branches. Second, we offer an extensible decompressor which allows for filtering and extraction of sets of trees of interest. Lastly, TreeZip can perform fast set operations directly on its compressed TRZ file. Our experimental evaluation of TreeZip shows that it compresses a Newick file into a plain text TRZ representation that is at least 73% smaller than the original file on weighted trees and over 98% smaller on unweighted trees. When combined with 7zip, the TreeZip+7zip file achieves an average space savings of 92% on the weighted case, and a space savings of over 99% on the unweighted case. Our results also show that TreeZip's performance is robust to different Newick representations of the same phylogenetic tree. The space savings achieved by 7zip, on the other hand, decreases as the number of different Newick representations for the same phylogenetic tree increases. However, TreeZip's most powerful advantage arises from its flexible compressed file format. Since the TRZ file is in plain text, we can easily design extensible decompressors that extract the relevant phylogenetic tree information of interest. In this paper, we illustrate two decompression applications (identifying the unique set of trees in a file and computing consensus\n\n[5] Tree Compression with Top Trees We propose a new compression scheme for labeled trees, which we call top tree compression. To the best of our knowledge, this is the first compression scheme for trees that (i) takes advantage of tree pattern repeats (like tree grammars) but (ii) simultaneously supports navigational queries on the compressed representation in logarithmic time (like DAG compression).In the worst case, we show that (iii) the compression ratio of top tree compression is always at least logσ0.19⁡nsuperscriptsubscript𝜎0.19𝑛\\log_{\\sigma}^{0.19}n (compared to the information-theoretic bound of logσ⁡nsubscript𝜎𝑛\\log_{\\sigma}n). This is in contrast to both tree grammars and DAG compression that have not yet been proven to have worst-case compression performance comparable to the information-theoretic bound. Finally, we compare the performance of top tree compression to DAG compression. We show that top tree compression (iv) can compress exponentially better than DAG compression, and (v) is never worse than DAG compression by more than a log⁡n𝑛\\log n factor.\n\n[6] An efficient and extensible approach for compressing phylogenetic trees our TreeZip algorithm in two primary ways: space savings and by using Results and discussion In this section, we explore the compression and decompression performance of 7zip, TreeZip, and TreeZip+7zip. Our previous results show that 7zip is the best generalpurpose compressor in comparison to gzip and bzip. The TreeZip+7zip compressed format is the TreeZip (TRZ) format which is then fed to 7zip for further compression. Moreover, our previous study showed that TreeZip outperforms TASPI. Since no implementation of TASPI is available and since none of the trees we had available that were used in the TASPI experiments had branch lengths, we could not compare TreeZip to TASPI in the context of this study. Finally, each point in the plots represents the average performance over three runs. Figure 4 shows the performance of TreeZip's compression algorithm. Figure 4(a) shows run-time information, and Figure 4(b) shows space savings results. On the freshwater and angiosperms datasets, TreeZip is faster than 7zip. However, as the number of trees under consideration increases in size, so does the amount of time needed for compression. In terms of size, the TRZ file by itself is larger than the 7zip file. However, we obtain an average of 75% space savings on our weighted collections, and about 99% space savings on our unweighted collection. The discrepancy in space savings between the weighted and unweighted cases underlines the complexity of compressing branch lengths. However, we note that when the TRZ file is combined with 7zip, the TRZ +7zip file has space savings on average of\n\n[7] Tree Compression with Top Trees 𝖠𝖼𝖼𝖾𝗌𝗌​(x)𝖠𝖼𝖼𝖾𝗌𝗌𝑥\\mathsf{Access}(x):Return the label associated with node x𝑥x.𝖣𝖾𝖼𝗈𝗆𝗉𝗋𝖾𝗌𝗌​(x)𝖣𝖾𝖼𝗈𝗆𝗉𝗋𝖾𝗌𝗌𝑥\\mathsf{Decompress}(x):Return the tree T​(x)𝑇𝑥T(x).𝖯𝖺𝗋𝖾𝗇𝗍​(x)𝖯𝖺𝗋𝖾𝗇𝗍𝑥\\mathsf{Parent}(x):Return the parent of node x𝑥x.𝖣𝖾𝗉𝗍𝗁​(x)𝖣𝖾𝗉𝗍𝗁𝑥\\mathsf{Depth}(x):Return the depth of node x𝑥x.𝖧𝖾𝗂𝗀𝗁𝗍​(x)𝖧𝖾𝗂𝗀𝗁𝗍𝑥\\mathsf{Height}(x):Return the height of node x𝑥x.𝖲𝗂𝗓𝖾​(x)𝖲𝗂𝗓𝖾𝑥\\mathsf{Size}(x):Return the number of nodes in T​(x)𝑇𝑥T(x).𝖥𝗂𝗋𝗌𝗍𝖼𝗁𝗂𝗅𝖽​(x)𝖥𝗂𝗋𝗌𝗍𝖼𝗁𝗂𝗅𝖽𝑥\\mathsf{Firstchild}(x):Return the first child of x𝑥x.𝖭𝖾𝗑𝗍𝖲𝗂𝖻𝗅𝗂𝗇𝗀​(x)𝖭𝖾𝗑𝗍𝖲𝗂𝖻𝗅𝗂𝗇𝗀𝑥\\mathsf{NextSibling}(x):Return the sibling immediately to the right of x𝑥x.𝖫𝖾𝗏𝖾𝗅𝖠𝗇𝖼𝖾𝗌𝗍𝗈𝗋​(x,i)𝖫𝖾𝗏𝖾𝗅𝖠𝗇𝖼𝖾𝗌𝗍𝗈𝗋𝑥𝑖\\mathsf{LevelAncestor}(x,i):Return the ancestor of x𝑥x whose distance from x𝑥x is i𝑖i.𝖭𝖢𝖠​(x,y)𝖭𝖢𝖠𝑥𝑦\\mathsf{NCA}(x,y):Return the nearest common ancestor of the nodes x𝑥x and y𝑦y.\n\n"}
{"case_id": "01a5d21763c1d38dd26ec52bd719e5ee", "answer_text": "\n\nThe Range Minimum Query (RMQ) problem is a fundamental task in computer science, involving the preprocessing of an array to efficiently answer queries about the minimum value within any subarray [0]. Over time, various data structures have been developed to address the RMQ problem, each offering different trade-offs in terms of preprocessing time, query time, space complexity, and suitability for dynamic updates. This discussion will outline some of the most common data structures used for RMQ, along with their advantages, limitations, and applications. \n\nSparse tables are a fundamental data structure for efficiently answering range minimum queries (RMQs) in constant time with linear space complexity. Sparse tables can precompute RMQs for all ranges that are powers of two, allowing any RMQ to be answered in constant time using a combination of these precomputed values.\n\nReferences:\n[1] Title: A Simple Linear-Space Data Structure for Constant-Time Range Minimum Query Text: that provide O(1)-time RMQ (e.g.,13, see Section 2). These solutions typically require a transformation or invoke a property that enables the volume of stored precomputed data to be reduced while allowing constant-time access and RMQ computation. Each such solution is a conceptual organization of the data into a compact table for efficient reference; essentially, the algorithm reduces to a clever table lookup.\n\nThe sparse table is one of the most straightforward data structures for answering RMQs efficiently. It preprocesses the array in \\(O(n \\log n)\\) time and allows answering queries in \\(O(1)\\) time [0][4]. The preprocessing involves constructing a table where each entry \\(st[i][j]\\) represents the minimum value in the subarray starting at index \\(i\\) with length \\(2^j\\). The query is answered by combining the precomputed minimums of overlapping segments.\n\nAdvantages: The state-of-the-art range minimum query (RMQ) data structures provide O(1)-time query responses while using O(n) space, which is essential for efficiently handling fundamental statistics in data analysis [1]. These data structures can transform or invoke a property that enables the reduction of stored precomputed data volume while maintaining constant-time access and computation [7]. Recent advances also offer succinct solutions, achieving optimal space usage for random inputs, demonstrating the importance of encoding and compact data structuring [12]. Additionally, the reduction to lowest common ancestor (LCA) queries allows for efficient query time solutions [10]. These RMQ solutions, leveraging properties like Cartesian trees, are vital in various applications, such as stringology and tree-based computations [4]. \n- Fast query time of \\(O(1)\\) [0].\n- Simple to implement [0]. \n\nLimitations:\n- High preprocessing time and space complexity of \\(O(n \\log n)\\) [9]. \nNot suitable for dynamic arrays where updates are frequent, as the entire table may need to be reconstructed [1]. \n\nApplications [7]. \nThis data structure is particularly useful in applications where the array does not change frequently, such as processing static datasets in computational biology for efficient sequence analysis [0].\n\n### Segment Tree\n\nSegment Trees are a versatile data structure used to solve range queries efficiently. These trees provide a way to preprocess an array, allowing various range queries to be answered in logarithmic time. Each node in a segment tree represents a segment (or interval) of the array, and operations like sum, minimum, and maximum within a range can be performed efficiently. The construction of a segment tree can be done in linear time, and the space complexity is also linear.\n\nSegment Tree is said to combine the ideas from various previous data structures, such as van Emde Boas trees and resizable arrays, to achieve efficient range minimum query computation in linear space and constant time. [0]\n\nThe segment tree can be used to preprocess an array for answering range minimum queries (RMQs). Given an array A of n items, the tree is constructed such that each node represents the minimum element in a segment of A. This allows RMQs to be answered in constant time after linear time and space preprocessing. [2]\nSegment trees are another widely used data structure for RMQs, allowing \\(O(\\log n)\\) query time with \\(O(n \\log n)\\) preprocessing time. Segment trees are highly flexible and can handle dynamic updates to the array [7]. Each node in the segment tree represents a segment of the array, and the tree is built such that each internal node stores the minimum value of its child segments.\n\n**Advantages:**\n\nReferences:\n[0] Title: A Simple Linear-Space Data Structure for Constant-Time Range Minimum Query Text: A Simple Linear-Space Data Structure for Constant-Time Range Minimum Query We revisit the range minimum query problem and present a new O(n)-space data structure that supports queries in O(1) time. Although previous data structures exist whose asymptotic bounds match ours, our goal is to introduce a new solution that is simple, intuitive, and practical without increasing costs for query time or space. Motivation Along with the mean, median, and mode of a multiset, the minimum (equivalently, the maximum) is a fundamental statistic of data analysis for which efficient computation is necessary. Given a list A : n − 1 of n items drawn from a totally orderered set, a range minimum query (RMQ) consists of an input pair of indices (i, j) for which the minimum element of A[i : j must be returned. The objective is to preprocess A to construct a data structure that supports efficient response to one or more subsequent range minimum queries, where the corresponding input parameters (i, j) are provided at query time. Although the complete set of possible queries can be precomputed and stored using Θ(n 2 ) space, practical data structures require less storage while still enabling efficient response time. For all i, if i = j, then a range query must report A[i. Consequently, any range query data structure for a list of n items requires Ω(n) storage space in the worst case. This leads to a natural question: how quickly can an O(n)-space data structure answer a range minimum query? Previous O(n)-space data structures exist\n\nSupports efficient updates and queries in \\(O(\\log n)\\) time.\n- Can handle dynamic arrays [1]. \n\n**Limitations:** The RMQ problem has a classic “textbook” solution using O(n) space and answering each RMQ query in constant time, due to Gabow, Bentley, Tarjan [GBT84] and Harel, Tarjan [HT84]. To build this data structure, one first constructs the Cartesian tree of the input array A. [4] \nHigher space complexity of \\(O(n \\log n)\\) compared to sparse tables [9].\nMore complex to implement and maintain[6].\n\n**Applications:**\n\nSegment trees are commonly used in real-time data analytics and online competitive programming where the array elements may frequently update [7].\n\nBinary Indexed Tree (Fenwick Tree) [4]. \nAlthough commonly used for range sum queries, binary indexed trees can be adapted for RMQs as well. The key advantage of binary indexed trees is that they allow both updates and queries in \\(O(\\log n)\\) time, making them suitable for scenarios where the array is frequently updated [14].\n\n**Advantages:**\nWe revisit the range minimum query problem and present a new O(n)-space data structure that supports queries in O(1) time [0]. These solutions typically require a transformation or invoke a property that enables the volume of stored precomputed data to be reduced while allowing constant-time access and RMQ computation [1]. The range-minimum query (RMQ) problem is a fundamental data structuring task with numerous applications [3]. Despite the fact that succinct solutions with worst-case optimal 2n+o(n) bits of space and constant query time are known, it has been unknown whether such a data structure can be made adaptive to the reduced entropy of random inputs [3]. The Cartesian tree is a rooted binary tree of n nodes, hence the number of different inputs is the n-th Catalan number [9]. In many applications that deal with storing and indexing massive data, the values in A have no intrinsic significance and A can be discarded after pre-processing [12]. It is well known that the RMQ problem is equivalent to the problem of supporting lowest common ancestor (LCA) queries on a binary tree, the Cartesian tree of A [12]. The answer to RMQ(i, j) is the label of the LCA of the nodes labeled by i and j [12]. Farzan and Munro showed that an n-node binary tree can be represented in 2n + o(n) bits and support constant-time RMQs [12]. \n\n- Efficient updates and queries in \\(O(\\log n)\\) time\n- Relatively simple implementation [0]. \n\n**Limitations:** [4]\n\nMore complex adaptation is needed for RMQs compared to range sum queries [6].\n\n- Less efficient for RMQs compared to specialized structures like sparse tables and segment trees [0][4]. \n\n**Applications:**\n\nThe RMQ problem is sometimes defined such that a query returns only the index of the minimum element instead of the minimum element itself (e.g., [0], [1]). The classical solution to the RMQ problem uses O(n) space and allows each query to be answered in constant time [4]. Advanced data structures for RMQ have reduced space usage while maintaining efficient query times [2]. Encoding RMQ results can facilitate efficient in-memory processing of massive data and reduce space requirements [12]. The RMQ problem also has applications in areas such as the longest common extension problem on strings and computing suffix links in suffix trees [14]. \nIdeal for financial applications, such as keeping track of stock prices or other financial metrics where efficient updates and queries are critical [14].\n\nThe Cartesian tree is an n-node binary tree, where the nodes correspond to the entries of A. The root A[i] has the minimum value in A, and its left and right subtrees are recursively constructed on A..i−1 and A[i+1..n respectively. It turns out that RMQ(a, b) is exactly the lowest common ancestor (LCA) of nodes A[a] and A[b] in this tree. The LCA problem admits an O(n) space and O(1) query time solution, based on a reduction to the ±1 RMQ problem [4].\nA Cartesian tree is a binary tree that uniquely represents the array such that an in-order traversal of the tree yields the original array, and each parent node is the minimum value of its subtree [1]. The RMQ problem is reduced to finding the Lowest Common Ancestor (LCA) of the indexes corresponding to the query range in the Cartesian tree.\n\n**Advantages:** [1]\nEfficient preprocessing in \\(O(n)\\) time with \\(O(1)\\) query time [0].\n- Provides a clear structural representation of the array [1].\n\n**Limitations:** The range minimum query (RMQ) problem is a fundamental task in data structuring with a broad array of applications. Despite various solutions, achieving an optimal combination of space efficiency and query time remains a challenge. For instance, classical solutions requiring O(n) space and O(1) query time often fall short of the lower bounds for space usage, which can be as low as 2n + o(n) bits [5]. Moreover, while solutions exist for encoding RMQs that achieve these lower bounds, they can be complex and less intuitive to implement compared to more straightforward but less space-efficient methods [0]. Significant efforts have been directed towards balancing these trade-offs, aiming to develop practical yet theoretically sound data structures for efficient RMQ computation [3].   \n\n- Overall complexity becomes \\(O(n + q \\log n)\\) when combined with LCA techniques [10].\nMore complex to implement and understand [12]. \n\n**Applications:**\nHighly utilized in text processing applications, for instance, in computing suffix arrays and other string matching algorithms [1].\n\nSuccinct Data Structures[4][5].\nFor applications requiring minimal space usage, succinct data structures are employed. They use space close to the information-theoretic lower bound while maintaining fast query times. For example, Fischer and Heun developed a structure that uses around 2n bits and supports constant-time RMQ queries [5].\n\nAdvantages: Range Minimum Query (RMQ) data structures have been extensively studied due to their fundamental role in data analysis and have numerous applications, such as graph problems, text processing, and other areas of computer science [11]. \nVery space-efficient, using close to the theoretical minimum space [2].\n- Supports constant-time queries [0]. \n\n\n\n**Limitations:**\nResearchers have designed data structures that outperform the best worst-case solution on inputs with certain structures [6]. \n\nDespite the fact that the Range Minimum Query problem has drawn a significant amount of attention from the algorithm community, to the best of our knowledge, no lower bound is known [13].\n\n\n- Complexity in construction and implementation [8].\n\n- Typically higher preprocessing time compared to other structures [8].\n\n\n**Applications:** The range minimum query (RMQ) problem is a fundamental data structuring task with numerous applications [3]. It is used in various fields such as graph problems, text processing, and other areas of computer science [11]. Additionally, RMQ data structures are crucial for finding lowest common ancestors in trees, solving the longest common extension problem on strings, and computing suffix links in suffix trees [3]. Furthermore, efficient RMQ solutions are essential for indexing and querying large databases, where fast access to specific data subsets is required [12]. The problem also plays a significant role in dynamic data structures, streaming algorithms, and succinct data structures [13]. \n\nCritical in large-scale information retrieval systems where space efficiency is paramount, such as in search engines and database indexing [5].\n\nEntropy-Based Structures [3].\nRecent advancements include entropy-based approaches that take advantage of the reduced entropy of random inputs. These structures can further reduce the space requirements while maintaining constant-time queries. For instance, a compressed data structure for binary trees that can store RMQs in asymptotically optimal space for average-case scenarios has been developed. This structure significantly compresses the data by leveraging the entropy of the input, leading to a reduction in space usage [3].\n\n\n**Advantages:**\n\nGiven a list A of n items drawn from a totally ordered set, a range minimum query (RMQ) consists of an input pair of indices (i, j) for which the minimum element of A[i : j] must be returned. The objective is to preprocess A to construct a data structure that supports efficient response to one or more subsequent range minimum queries, where the corresponding input parameters (i, j) are provided at query time [0]. Although the complete set of possible queries can be precomputed and stored using Θ(n^2) space, practical data structures require less storage while still enabling efficient response time [0]. For all i, if i = j, then a range query must report A[i]. Consequently, any range query data structure for a list of n items requires Ω(n) storage space in the worst case [2]. This leads to a natural question: how quickly can an O(n)-space data structure answer a range minimum query [5]? Previous O(n)-space data structures exist that provide O(1)-time RMQ (e.g., [0]), but these solutions typically require a transformation or invoke a property that enables the volume of stored precomputed data to be reduced while allowing constant-time access and RMQ computation [1].\n\n- Extremely space-efficient, especially for random inputs [3].\nMaintains constant-time queries [0].\n\n**Limitations:**\n\nGiven an integer array \\( Z \\) of length \\( n \\), there is a Range Minimum Query (RMQ) data structure taking \\( 2n+o(n) \\) bits of space that can answer an RMQ query on \\( Z \\) in constant time [7]. This data structure can be constructed in \\( O(n) \\) time with \\( o(n) \\) bits of additional working space [7].\n\nThe range-minimum query (RMQ) problem is a fundamental data structuring task with numerous applications [3]. Despite the fact that succinct solutions with worst-case optimal \\( 2n+o(n) \\) bits of space and constant query time are known, it has been unknown whether such a data structure can be made adaptive to the reduced entropy of random inputs [3]. We construct a succinct data structure with the optimal \\( 1.736n+o(n) \\) bits of space on average for random RMQ instances, settling this open problem [3].\n\nEncoding RMQs can facilitate the efficient in-memory processing of massive data [12]. It is well known that the RMQ problem is equivalent to the problem of supporting lowest common ancestor (LCA) queries on a binary tree [12]. \n\nOur solution combines new ideas with techniques from various previous data structures, achieving a \\( O(n) \\) space and \\( O(1) \\) query time solution based on a reduction to the \\( \\pm 1 \\) RMQ problem [10]. The Cartesian tree of the input array precisely captures all the information about the answers to all RMQ queries [9]. The Cartesian tree is a rooted binary tree of \\( n \\) nodes [9]. The information-theoretical optimal number of bits needed to store the input is \\( 2n-1.5\\log n+O(1) \\) [9]. However, the above classical solution consumes \\( O(n) \\) words, which is approximately \\( O(n\\log n) \\) bits [9].\n\nSadakane showed that it is possible to achieve \"truly\" linear space and constant query time, proposing a data structure using approximately \\( 4n \\) bits of space and supporting RMQ queries in \\( O(1) \\) time, assuming the word-size is \\( \\Omega(\\log n) \\) [10][4]. The space bound matches the best possible in the leading constant of \\( 2n+O(n \\log \\log n / \\log n) \\) [10]. The lower bounds for succinct range minimum query data structures indicate that this is the most efficient representation achievable for practical purposes [6][10]. \n\n\n- Complex to implement and understand[7]. \n- May not always provide significant benefits for non-random inputs [3].\n\n\nGiven a list A : n − 1 of n items drawn from a totally ordered set, a range minimum query (RMQ) consists of an input pair of indices (i, j) for which the minimum element of A[i : j] must be returned [0]. The objective is to preprocess A to construct a data structure that supports efficient response to one or more subsequent range minimum queries, where the corresponding input parameters (i, j) are provided at query time [0]. Although the complete set of possible queries can be precomputed and stored using Θ(n^2) space, practical data structures require less storage while still enabling efficient response time [2]. Consequently, any range query data structure for a list of n items requires Ω(n) storage space in the worst case [0].\n\nUseful in big data applications where storage efficiency directly impacts performance and cost, such as in large-scale sensor networks and genomic data storage [3].\n\nComparison of Trade-offs\n\n[3] The range-minimum query (RMQ) problem is a fundamental data structuring task with numerous applications. Despite the fact that succinct solutions with worst-case optimal $2n+o(n)$ bits of space and constant query time are known, it has been unknown whether such a data structure can be made adaptive to the reduced entropy of random inputs (Davoodi et al. 2014). We construct a succinct data structure with the optimal $1.736n+o(n)$ bits of space on average for random RMQ instances, settling this open problem. Our solution relies on a compressed data structure for binary trees that is of independent interest. It can store a (static) binary search tree generated by random insertions in asymptotically optimal expected space and supports many queries in constant time.\n\n[8] An important facet of every data structure is its construction time. Table 1 presents the construction times (and space usage) for several RMQ algorithms or their configurations, for the input array of size n=1​G. We can see that the plain BbST is clearly the fastest, about 40 times faster than the fastest solution with constant worst-case time queries, SDSL-SCT. Note also that in the construction time for SDSL-SCT over 1​G elements we can build BbST and answer from about 100​M to 400​M queries. Our two-level variant, BbST2, is still very fast in construction. The hybrids, however, must require more time to build than SDSL-REC, which is their component. ST, as clearly the most memory-demanding data structure, is also the slowest to build.\n\n\n | Data Structure | Preprocessing Time [14] | Query Time [2] | Space Complexity [0] | Dynamic Updates | Ease of Implementation |\n\n|----------------|---------------------|------------|------------------|-----------------|-----------------------|\n| Sparse Table   | \\(O(n \\log n)\\)     | \\(O(1)\\)   | \\(O(n \\log n)\\)  | No              | Simple                |[0][1][10] \nSegment Tree   | \\(O(n \\log n)\\)     | \\(O(\\log n)\\)| \\(O(n \\log n)\\)| Yes             | Moderate              |\n| Binary Indexed Tree | \\(O(n)\\)      | \\(O(\\log n)\\)| \\(O(n)\\)        | Yes             | Moderate              | [8]\n| Cartesian Tree | \\(O(n)\\) [4] | \\(O(1)\\) [4] | \\(O(n)\\) [4] | No | Complex |\n\nSuccinct Data Structures | \\(O(n)\\) | \\(O(1)\\)    | \\(2n \\text{ bits}\\)| No             | Complex [2][5]. \n\nEntropy-Based Structures \\(O(n)\\) [0] \\(O(1)\\) [0] Optimal average-case space [3] No Complex [0]. \n\nIn summary, the choice of data structure for RMQ depends on the specific needs of the application, including query speed, update frequency, and space constraints. Sparse tables and segment trees are generally favored for their balanced trade-offs between preprocessing time, query time, and space complexity, while more specialized structures like Cartesian trees, succinct data structures, and entropy-based approaches offer optimized solutions for specific scenarios [0][1][5][3]. By understanding the strengths and limitations of each approach, practitioners can select the most appropriate data structure to meet their specific application needs.\n\n\nReferences:\n[0] A Simple Linear-Space Data Structure for Constant-Time Range Minimum Query A Simple Linear-Space Data Structure for Constant-Time Range Minimum Query We revisit the range minimum query problem and present a new O(n)-space data structure that supports queries in O(1) time. Although previous data structures exist whose asymptotic bounds match ours, our goal is to introduce a new solution that is simple, intuitive, and practical without increasing costs for query time or space. Motivation Along with the mean, median, and mode of a multiset, the minimum (equivalently, the maximum) is a fundamental statistic of data analysis for which efficient computation is necessary. Given a list A : n − 1 of n items drawn from a totally orderered set, a range minimum query (RMQ) consists of an input pair of indices (i, j) for which the minimum element of A[i : j must be returned. The objective is to preprocess A to construct a data structure that supports efficient response to one or more subsequent range minimum queries, where the corresponding input parameters (i, j) are provided at query time. Although the complete set of possible queries can be precomputed and stored using Θ(n 2 ) space, practical data structures require less storage while still enabling efficient response time. For all i, if i = j, then a range query must report A[i. Consequently, any range query data structure for a list of n items requires Ω(n) storage space in the worst case. This leads to a natural question: how quickly can an O(n)-space data structure answer a range minimum query? Previous O(n)-space data structures exist\n\n[1] A Simple Linear-Space Data Structure for Constant-Time Range Minimum Query that provide O(1)-time RMQ (e.g.,13, see Section 2). These solutions typically require a transformation or invoke a property that enables the volume of stored precomputed data to be reduced while allowing constant-time access and RMQ computation. Each such solution is a conceptual organization of the data into a compact table for efficient reference; essentially, the algorithm reduces to a clever table lookup. In this paper our objective is not to minimize the total number of bits occupied by the data structure (our solution is not succinct) but rather to present a simpler and more intuitive method for organizing the precomputed data to support RMQ efficiently. Our solution combines new ideas with techniques from various previous data structures: van Emde Boas trees, resizable arrays, range mode query, one-sided RMQ, and a linear-space data structure that supports RMQ in O( √ n) time. The resulting RMQ data structure matches previous optimal bounds of O(n) space and O(1) query time. Our data structure stores efficient representations of the data to permit direct lookup without requiring the indirect techniques employed by previous solutions, such as transformation to a lowest common ancestor query, Cartesian trees, Eulerian tours, and the Four Russians speedup (e.g.,13). The RMQ problem is sometimes defined such that a query returns only the index of the minimum element instead of the minimum element itself (e.g.,). In this paper we require that the actual minimum element be returned. As we discuss in Section 2, several succint data structures exist that support O(1)-time\n\n[2] Encoding Range Minimum Queries Encoding Range Minimum Queries We consider the problem of encoding range minimum queries (RMQs): given an array A..n of distinct totally ordered values, to pre-process A and create a data structure that can answer the query RMQ(i,j), which returns the index containing the smallest element in A[i..j, without access to the array A at query time. We give a data structure whose space usage is 2n + o(n) bits, which is asymptotically optimal for worst-case data, and answers RMQs in O(1) worst-case time. This matches the previous result of Fischer and Heun [SICOMP, 2011, but is obtained in a more natural way. Furthermore, our result can encode the RMQs of a random array A in 1.919n + o(n) bits in expectation, which is not known to hold for Fischer and Heun's result. We then generalize our result to the encoding range top-2 query (RT2Q) problem, which is like the encoding RMQ problem except that the query RT2Q(i,j) returns the indices of both the smallest and second-smallest elements of A[i..j. We introduce a data structure using 3.272n+o(n) bits that answers RT2Qs in constant time, and also give lower bounds on the effective entropy} of RT2Q. Introduction Given an array A..n of elements from a totally ordered set, the range minimum query (RMQ) problem is to pre-process A and create a data structure so that the query RMQ(i, j), which takes two indices 1 ≤ i ≤ j ≤ n and returns argmin i≤k≤j A[k, is supported efficiently (both in terms of space and time). We consider\n\n[3] Entropy Trees and Range-Minimum Queries In Optimal Average-Case Space Entropy Trees and Range-Minimum Queries In Optimal Average-Case Space The range-minimum query (RMQ) problem is a fundamental data structuring task with numerous applications. Despite the fact that succinct solutions with worst-case optimal $2n+o(n)$ bits of space and constant query time are known, it has been unknown whether such a data structure can be made adaptive to the reduced entropy of random inputs (Davoodi et al. 2014). We construct a succinct data structure with the optimal $1.736n+o(n)$ bits of space on average for random RMQ instances, settling this open problem. Our solution relies on a compressed data structure for binary trees that is of independent interest. It can store a (static) binary search tree generated by random insertions in asymptotically optimal expected space and supports many queries in constant time. Using an instance-optimal encoding of subtrees, we furthermore obtain a\"hyper-succinct\"data structure for binary trees that improves upon the ultra-succinct representation of Jansson, Sadakane and Sung (2012). Introduction The range-minimum query (RMQ) problem is the following data structuring task: Given an array A. .n of comparable items, construct a data structure at preprocessing time that can answer subsequent queries without inspecting A again. The answer to the query rmq(i, j), for 1 ≤ i ≤ j ≤ n, is the index (in A) of the 1 minimum in A[i..j, i.e., rmq(i, j) = arg min i≤k≤j A [k. RMQ data structures are fundamental building blocks to find lowest common ancestors in trees, to solve the longest common extension problem on strings, to compute suffix links in suffix trees,\n\n[4] Lower bound for succinct range minimum query The RMQ problem has a classic “textbook” solution using O​(n)𝑂𝑛O(n) space and answering each RMQ query in constant time, due to Gabow, Bentley, Tarjan [GBT84 and Harel, Tarjan [HT84.To build this data structure, one first constructs the Cartesian tree of the input array A𝐴A.The Cartesian tree is an n𝑛n-node binary tree, where the nodes correspond to the entries of A𝐴A.The root A​[i𝐴delimited-[𝑖A[i has the minimum value in A𝐴A, and its left and right subtrees are recursively constructed on A..i−1A..i-1 and A[i+1..nA[i+1..n respectively.It turns out that RMQ(a𝑎a, b𝑏b) is exactly the lowest common ancestor (LCA) of nodes A​[a𝐴delimited-[𝑎A[a and A​[b𝐴delimited-[𝑏A[b in this tree.The LCA problem admits an O​(n)𝑂𝑛O(n) space and O​(1)𝑂1O(1) query time solution, based on a reduction to the ±1plus-or-minus1\\pm 1RMQ problem.\n\n[5] Encoding range minima and range top-2 queries Encoding range minima and range top-2 queries We consider the problem of encoding range minimum queries (RMQs): given an array A..n of distinct totally ordered values, to pre-process A and create a data structure that can answer the query RMQ(i,j), which returns the index containing the smallest element in A[i..j, without access to the array A at query time. We give a data structure whose space usage is 2n+o(n) bits, which is asymptotically optimal for worst-case data, and answers RMQs in O(1) worst-case time. This matches the previous result of Fischer and Heun, but is obtained in a more natural way. Furthermore, our result can encode the RMQs of a random array A in 1.919n+o(n) bits in expectation, which is not known to hold for Fischer and Heun’s result. We then generalize our result to the encoding range top-2 query (RT2Q) problem, which is like the encoding RMQ problem except that the query RT2Q(i,j) returns the indices of both the smallest and second smallest elements of A[i..j. We introduce a data structure using 3.272n+o(n) bits that answers RT2Qs in constant time, and also give lower bounds on the effective entropy of the RT2Q problem.\n\n[6] Lower bound for succinct range minimum query The state-of-the-art333Due to the wide application, special cases of the problem are also interesting.Researchers have designed data structures that outperform the best worst-case solution on inputs with certain structures [ACN13, BLR+15, GJMW19. RMQ data structure [Pǎt08b, NS14, FH11, DRS17 uses 2​n+n/(log⁡nt)Ω​(t)+O~​(n3/4)2𝑛𝑛superscript𝑛𝑡Ω𝑡~𝑂superscript𝑛342n+n/(\\frac{\\log n}{t})^{\\Omega(t)}+\\tilde{O}(n^{3/4}) bits of space and answers queries in O​(t)𝑂𝑡O(t) time, for any parameter t>1𝑡1t>1 and word-size Θ​(log⁡n)Θ𝑛\\Theta(\\log n).444Their data structure was originally stated as 2​n+n/logt⁡n2𝑛𝑛superscript𝑡𝑛2n+n/\\log^{t}n space and O​(t)𝑂𝑡O(t) time for constant t𝑡t.It naturally generalizes to the above trade-off.See Appendix C.In particular, one can achieve n/poly​log⁡n𝑛poly𝑛n/\\mathrm{poly}\\log n redundancy and constant query time for any poly​log⁡npoly𝑛\\mathrm{poly}\\log n.On the other hand, despite the fact that the Range Minimum Query problem has drawn a significant amount of attention from the algorithm community, to the best of our knowledge, no lower bound is known.\n\n[7] Compact Data Structures for Shortest Unique Substring Queries [i, j ⊂, n, the range minimum query RmQ Z (i, j) (resp. the range maximum query RMQ Z (i, j)) asks for the index p of a minimum element (resp. a maximum element) of the subarray Z[i..j, i.e., p ∈ arg min i≤k≤j Z[k, or respectively p ∈ arg max i≤k≤j Z[k. We use the following well-known data structure to handle these kind of queries: ). Given an integer array Z of length n, there is an RmQ (resp. RMQ) data structure taking 2n+o(n) bits of space that can answer an RmQ (resp. RMQ) query on Z in constant time. This data structure can be constructed in O(n) time with o(n) bits of additional working space. Suffix Array, Inverse Suffix Array and LCP Array We define the three integer arrays SA T. for every i with 2 ≤ i ≤ n, where lcp(P, Q) denotes the length of the longest common prefix of P and Q for two given strings P and Q. Computing MUSs in Compact Space For computing SUSs efficiently, it is advantageous to have a data structure available that can retrieve MUSs starting or ending at specific positions, as the following lemma gives a crucial connections between MUSs and SUSs: Lemma 2). Every point SUS contains exactly one MUS. Fig. 2 gives an overview of our introduced data structure and shows the connections between this section and the following sections that focus on our two SUS problems. For our data structure retrieving MUSs, we propose a compact representation and an\n\n[8] Faster range minimum queries An important facet of every data structure is its construction time.Table 1 presents the construction times (and space usage)for several RMQ algorithms or their configurations,for the input array of size n=1​G𝑛1Gn=1\\textrm{G}.We can see that the plain BbST is clearly the fastest,about 40 times faster than the fastest solution with constant worst case time queries,SDSL-SCT.Note also that in the construction time for SDSL-SCT over1​G1G1\\textrm{G} elements we can build BbST and answerfrom about 100​M100M100\\textrm{M} to 400​M400M400\\textrm{M} queries.Our two-level variant, BbST2, is still very fast in construction.The hybrids, however, must require more time to build than SDSL-REC,which is their component.ST, as clearly the most memory-demanding data structure,is also the slowest to build.\n\n[9] Nearly Tight Lower Bounds for Succinct Range Minimum Query A trend in the theory of data structures is the succinct data structure .From the perspective of the succinct data structure, the classical solution is far from being optimal.It turns out that the Cartesian tree  of the input array A​​…​n𝐴delimited-[1…𝑛A\\ldots n precisely captures all the information about the answers to all RMQ queries.The Cartesian tree is a rooted binary tree of n𝑛n nodes, hence the number of different inputs is the n𝑛n-th Catalan number Cn≜(2​nn)/(n+1)≜subscript𝐶𝑛binomial2𝑛𝑛𝑛1C_{n}\\triangleq\\binom{2n}{n}/(n+1), and the information-theoretical optimal number of bits needed to store the input is 2​n−1.5​log⁡n+O​(1)2𝑛1.5𝑛𝑂12n-1.5\\log n+O(1).However the above classical solution consumes O​(n)𝑂𝑛O(n) words ≈O​(n​log⁡n)absent𝑂𝑛𝑛\\approx O(n\\log n) bits.\n\n[10] Lower bound for succinct range minimum query the minimum value in A, and its left and right subtrees are recursively constructed on A..i − 1 and A[i + 1..n respectively. It turns out that RMQ(a, b) is exactly the lowest common ancestor (LCA) of nodes A[a and A [b in this tree. The LCA problem admits an O(n) space and O(1) query time solution, based on a reduction to the ±1RMQ problem. In terms of the space usage, this data structure is in fact suboptimal. The only information needed to answer all RMQ queries on A is the Cartesian tree of A, which is a rooted binary tree with n nodes. It is well known that the number of such binary trees is equal to the n-th Catalan number C n = 1 n+1 2n n . Hence, the information theoretical space lower bound for this problem is in fact, log 2 C n = 2n − Θ(log n) bits, whereas the above data structure uses O(n) words of space. Sadakane [Sad07b showed that it is possible to achieve \"truly\" linear space and constant query time. He proposed a data structure using ∼ 4n bits of space and supporting RMQ queries in O(1) time, assuming the word-size is Ω(log n). 1 Later, the space is further improved to 2n + O(n log log n/ log n) by Fischer and Heun [FH07,FH11. The space bound matches the best possible in the leading constant. Such data structures that use H + r bits, for problems that require H bits of space 2 and for r\n\n[11] Nearly Tight Lower Bounds for Succinct Range Minimum Query Give an array of distinct integers A​​…​n𝐴delimited-[1…𝑛A\\ldots n, the Range Minimum Query (RMQ) problem requires us to construct a data structure from A𝐴A, supporting the RMQ query: given an interval [a,b⊆,n𝑎𝑏1𝑛[a,b\\subseteq,n, return the index of the minimum element in subarray A​[a​…​b𝐴delimited-[𝑎…𝑏A[a\\ldots b, i.e. returnRMQ​(a,b)≜arg​mini∈[a,b⁡A​[i.≜RMQ𝑎𝑏subscriptargmin𝑖𝑎𝑏𝐴delimited-[𝑖\\texttt{RMQ}(a,b)\\triangleq\\operatorname*{arg\\,min}_{i\\in[a,b}A[i.The data structures to RMQ problem play significant roles in numerous areas of computer science, such as graph problems , 3, 16, 2, 27, text processing , 32, 12, 41, 42, 48, 6, 13, 20, 9 and other areas of computer science , 46, 7.\n\n[12] Encoding Range Minimum Queries the encoding version of this problem: after pre-processing A, the data structure should answer RMQs without access to A; in other words, the data structure should encode all the information about A needed to answer RMQs. In many applications that deal with storing and indexing massive data, the values in A have no intrinsic significance and A can be discarded after pre-processing (for example, A may contain scores that are used to determine the relative order of documents returned in response to a search query). As we now discuss, the encoding of A for RMQs can often take much less space than A itself, so encoding RMQs can facilitate the efficient in-memory processing of massive data. It is well known that the RMQ problem is equivalent to the problem of supporting lowest common ancestor (LCA) queries on a binary tree, the Cartesian tree of A. The Cartesian tree of A is a binary tree with n nodes, in which the root is labeled by i where A[i is the minimum element in A; the left subtree of of the root is the Cartesian tree of A..i − 1 and the right subtree of the root is the Cartesian tree of A[i + 1..n. The answer to RMQ(i, j) is the label of the LCA of the nodes labeled by i and j. Thus, knowing the topology of the Cartesian tree of A suffices to answer RMQs on A. Farzan and Munro showed that an n-node binary tree can be represented in 2n +\n\n[13] Lower bound for succinct range minimum query One of the most widely used technique in proving data structure lower bounds is the cell-sampling.It has applications to dynamic data structures [Lar12a, CGL15, LWY18, streaming lower bounds [LNN15, static data structures with low space usage [PTW10, Lar12b, GL16, Yin16, as well as succinct data structures [GM07.To prove a lower bound using cell-sampling, one samples a small fraction of the memory cells, and argues that if the query time is low, then many queries can still be answered using only the sampled cells.Finally, one proves that too much information about the input is revealed by the answers to those queries, yielding a contradiction.In order to apply this technique, the problem is usually required to have the property that the answers to a set of n1.1superscript𝑛1.1n^{1.1} random queries almost determine the entire input data, which the RMQ problem does not have (as one could keep getting indices with small values).\n\n[14] Data structures for range minimum queries in multidimensional arrays Data structures for range minimum queries in multidimensional arrays Given a d-dimensional array A with N entries, the Range Minimum Query (RMQ) asks for the minimum element within a contiguous subarray of A. The 1D RMQ problem has been studied intensively because of its relevance to the Nearest Common Ancestor problem and its important use in stringology. If constant-time query answering is required, linear time and space preprocessing algorithms were known for the 1D case, but not for the higher dimensional cases. In this paper, we give the first linear-time preprocessing algorithm for arrays with fixed dimension, such that any range minimum query can be answered in constant time.\n\n"}
{"case_id": "01d02d3984b60367c275972613992a64", "answer_text": "\n\nPre-training models on table-specific tasks involves various datasets and methods that can be categorized based on data sources, types of datasets, and pre-training objectives. The datasets used can be broadly divided into human-created data, machine-synthesized data, and publicly available structured tables [0][2]. The methods employed for pre-training generally focus on understanding table structures, table-text interactions, and leveraging different model architectures for specific downstream tasks [4][5]. \n\n\n\n### Datasets Used for Table Pre-training\n\nHuman-created data usually show higher quality than web-crawled ones which might require careful prepossessing for their size, diversity and noises. It is common to manually add extra labels for existing dataset [0]. E.g., ToTTo, a well-labeled dataset for table-to-text with NL descriptions and corresponding web tables, was used by StruG for pre-training. Also, human-created labels can be collected in smart ways [1]. E.g., ForTaP extracted existing formulas from a large web-crawled spreadsheet corpus and extracted numerical reference and calculation relationships from them for pre-training [0]. And large fine-grained labeled datasets were also used for pre-training, e.g., ToTTo, a well-labeled dataset for table-to-text with NL descriptions and corresponding web tables, was used by StruG for pre-training [0][1].\n\n\n\nHuman-created data usually show higher quality than web-crawled ones which might require careful prepossessing for their size, diversity and noises. It is common to manually add extra labels for existing dataset. E.g., ToTTo, a well-labeled dataset for table-to-text with NL descriptions and corresponding web tables, was used by StruG for pre-training [0].\n- **ToTTo:** A well-labeled dataset for table-to-text generation, containing natural language (NL) descriptions and corresponding web tables, has been extensively used for pre-training models like StruG [0]. However, human-created datasets are often limited in size and can be costly to produce.\n- **Crowd-Sourced Datasets:** For example, the Korean-specific dataset for table question answering comprises 1.4 million tables from Korean Wikipedia and 70,000 pairs of questions and answers generated by crowd-sourced workers [7][14]. These datasets are valuable for training models that need to perform question-answering tasks over tables. Nevertheless, they may suffer from variability in label quality and potential biases introduced by the crowd-sourcing process.\n\n**2. Machine-Synthesized Data:** Synthesized data are more targeted and controllable, but require careful designs to ensure meaningfulness and diversity [1]. \n- **SQL Synthesis:** GraPPa uses a synchronous context-free grammar (SCFG) to generate synthetic sentence-SQL pairs over tables, providing a controlled and targeted data set for pre-training [1][5]. While synthesized data ensures a high level of control and diversity, they may lack the nuances of real-world data and require careful design to ensure meaningfulness.\n- **Text-Table Synthesis:** Some methods use human-written templates to create NL sentences for existing tables, although this requires careful design to ensure the meaningfulness and diversity of the synthesized data [3][13]. The challenge here is the labor-intensive process of creating diverse templates and the potential for limited coverage of real-world scenarios.\n\n**3. Publicly Available Structured Tables**: Previous work has shown the potential of using publicly available semi-structured tables for pre-training tasks. For instance, TaPEx works effectively with a corpus of high-quality tables from existing public datasets, such as nearly 1,500 tables from the WikiTableQuestions dataset [2]. Additionally, large corpora such as the WDC Web Table Corpus, Dresden Web Tables Corpus, WebTables, and WikiTables, which contain millions of tables, have been identified as valuable sources for table pre-training [9]. \n- **WikiTableQuestions:** A dataset containing semi-structured tables, which has been leveraged for pre-training models like TaPEx. This dataset comprises nearly 1,500 high-quality tables selected from public sources [2]. Public datasets often come with challenges such as noise, redundancy, and a lack of domain-specific data.\n- **Large Web-Table Corpora:** These include datasets like the WDC Web Table Corpus, Dresden Web Tables Corpus, and WebTables, which contain millions of tables mined from the web. These large corpora are used for diverse table-related pre-training tasks [9]. However, the raw data mined from the web can be extremely noisy and requires complicated heuristics to clean.\n\nMethods for Table Pre-Training [0].\n\nPre-Training Objectives: To best leverage table characteristics while maintaining capabilities to understand text within/out of tables, various Tabular Language Models (TaLMs) are proposed for table pre-training [4]. For example, TaBERT encoded tables and text by concatenating a row-wise transformer with column-wise vertical attention layers, pre-trained with Masked Language Models (MLM) and Masked Column Prediction (MCP), and achieved SOTA results on benchmarks of table QA [4]. TaPas pioneeringly proposed an end-to-end table-text joint reasoning framework using transformers without explicitly generating logical forms for table QA, and TaPas also employed MLM for pre-training [4]. TURL was the first work to learn entity representations from relational tables to enhance table knowledge matching and table augmentation, and it restricted each cell to aggregating information from the located row and column via masked attention [4]. TUTA then extended the success of table pre-training to generally structured tables using tree-based attention and tree-based positional encoding based on a novel unified bi-tree structure, achieving new SOTA results on five representative table structure understanding datasets [4]. Far different from previous pre-training objectives, TaPEx explored to learn a neural SQL executor and demonstrated surprising effectiveness on table-text joint reasoning [4]. Recently, UnifiedSKG explored to directly fine-tune T5 on 21 datasets across 6 tasks and achieved promising and even SOTA results [4]. \n- **Masked Language Modeling (MLM):** Commonly used in models like TaBERT and TaPas, MLM involves predicting masked tokens within tables to learn better tabular representations [4]. This approach helps models understand the context within a table by focusing on individual elements and their relationships. However, MLM may not fully capture the structural dependencies unique to tabular data.\n- **Masked Column Prediction (MCP):** Proposed by TaBERT, MCP aims to predict the names and data types of masked columns, promoting an understanding of columnar data [4]. By masking entire columns and having the model predict them, MCP emphasizes the importance of understanding both the structure and the content of tables. A potential limitation is its reliance on the availability of well-structured tables.\n- **SQL Execution Task:** TaPEx introduces a novel pre-training task that involves executing synthetic SQL queries, which has shown significant improvements in table-based question answering and fact verification [3][10]. This task mimics real-world database operations, training the model to understand and manipulate table data accurately and efficiently. The main challenge here is the complexity in accurately simulating realistic SQL queries.\n\n**2. Model Architectures:** To best leverage table characteristics while maintaining capabilities to understand text within/out of tables, various Tabular Language Models (TaLMs) are proposed for table pre-training. For example, TaBERT encoded tables and text by concatenating a row-wise transformer with column-wise vertical attention layers, pre-trained with Masked Language Models (MLM) and Masked Column Prediction (MCP), and achieved SOTA results on benchmarks of table QA. TaPas pioneeringly proposed an end-to-end table-text joint reasoning framework using transformers without explicitly generating logical forms for table QA, and TaPas also employed MLM for pre-training. TURL was the first work to learn entity representations from relational tables to enhance table knowledge matching and table augmentation, and it restricted each cell to aggregating information from the located row and column via masked attention. TUTA then extended the success of table pre-training to generally structured tables using tree-based attention and tree-based positional encoding based on a novel unified bi-tree structure. TUTA achieved new SOTA results on five representative table structure understanding datasets. Far different from previous pre-training objectives, TaPEx explored to learn a neural SQL executor and demonstrated surprising effectiveness on table-text joint reasoning. Recently, UnifiedSKG explored to directly fine-tune T5 on 21 datasets across 6 tasks and achieved promising and even SOTA results [4]. \n- **Row-Wise and Column-Wise Attention:** Models like TaBERT use a combination of row-wise transformers and column-wise vertical attention layers to encode tables and text jointly, allowing robust interaction between table structures and NL queries [4]. This architecture ensures that the model captures both horizontal (row) and vertical (column) dependencies within tables. However, the increased complexity in architecture may lead to higher computational costs.\n- **Tree-Based Attention:** TUTA employs a tree-based attention mechanism to extend table pre-training to generally structured tables, achieving state-of-the-art results in various table structure understanding tasks [4]. This method leverages hierarchical relationships in tables, offering a more nuanced understanding of table data. The challenge lies in constructing and processing tree structures which can be computationally intensive.\n- **End-to-End Table-Text Reasoning:** TaPas integrates transformers for joint table-text reasoning without explicitly generating logical forms, focusing on the end-to-end prediction for table question answering [4]. This approach simplifies the workflow by directly linking input tables and textual questions to output answers, streamlining the reasoning process. However, it may struggle with complex logical reasoning tasks that require explicit representations of logic.\n\nAdditionally, methods like **T5 fine-tuning** as used in UnifiedSKG, demonstrate the utility of leveraging pre-trained language models across diverse tasks, enhancing adaptability and performance in table-specific applications [4]. While methods like TapTap showcase the potential for generating high-quality synthetic tables for specific tasks like privacy protection and missing value imputation, emphasizing the versatility of pre-trained models in creating synthetic data [8].\n\nDownstream Tasks and Their Impact [1].\n\n- **Table Question Answering (TableQA):** This task involves understanding and reasoning over tables to answer questions formulated in natural language. Models like GraPPa and TAPEX have demonstrated significant performance improvements on benchmarks by leveraging pre-training techniques. For instance, TaPEx achieved an absolute improvement of 19.5% over BART on the WikiTableQuestions dataset, showcasing the effectiveness of its SQL execution-based pre-training task [10]. However, the model’s performance may still depend on the quality and diversity of the pre-training data.\n- **Table Fact Verification (TableFV):** This task checks the veracity of statements against tabular data. Pre-trained models on table-specific tasks, like those using synthetic SQL data, have shown strong results in these applications. TaPEx, for example, has set new state-of-the-art results on several benchmarks, significantly outperforming previous approaches [10]. One challenge is ensuring the model’s robustness to varied and potentially deceptive data.\n- **Table-to-Text Generation:** This task involves generating logical and coherent text from tables. PLOG, for instance, focuses on generating logical-level summaries from tables without explicit logical forms and has outperformed previous models like LOGIC2TEXT in human evaluation for logical reasoning fidelity [11]. However, the complex nature of logical text generation remains a significant challenge.\n\nIn summary, table-specific model pre-training involves a diverse range of datasets and innovative methods that together aim to enhance models' understanding and reasoning over tabular data [1]. This multi-faceted approach results in improved performance across various downstream tasks, including question answering, fact verification, and table-to-text generation [10]. However, each approach comes with its own set of limitations and challenges that need to be addressed for continued advancement in this field.\n\n\nReferences:\n[0] Table Pre-training: A Survey on Model Architectures, Pretraining Objectives, and Downstream Tasks Human-created:Human-created data usually show higher quality than web-crawled ones which might require careful prepossessing for their size, diversity and noises.It is common to manually add extra labels for existing dataset. E.g., ToTTo, a well-labeled dataset for table-to-text with NL descriptions and corresponding web tables, was used by StruG for pre-training.Also, human-created labels can be collected in smart ways. E.g., ForTaP extracted existing formulas from a large web-crawled spreadsheet corpus and extracted numerical reference and calculation relationships from them for pre-training.And large fine-grained labeled datasets were also used for pre-training, e.g., ToTTo, a well-labeled dataset for table-to-text with NL descriptions and corresponding web tables, was used by StruG for pre-training.\n\n[1] Table Pre-training: A Survey on Model Architectures, Pretraining Objectives, and Downstream Tasks NL descriptions and corresponding web tables, was used by StruG for pretraining. Also, human-created labels can be collected in smart ways. E.g., ForTaP extracted existing formulas from a large web-crawled spreadsheet corpus and extracted numerical reference and calculation relationships from them for pre-training. And large fine-grained labeled datasets were also used for pretraining, e.g., ToTTo, a well-labeled dataset for table-to-text with NL descriptions and corresponding web tables, was used by StruG for pre-training. Machine-synthesized: Synthesized data are more targeted and controllable, but require careful designs to ensure meaningfulness and diversity. GraPPa proposed an SCFG (synchronous context-free grammar) and applied it to synthesize sentence-SQL pairs over tables. Downstream Tasks As shown in Figure 2, tasks of table understanding often have intersections with domains like NL, programming language, and computer vision, and thus prefer different capabilities of table modeling. For example, table question answering is a prevalent cross-domain task that requires models to understand tables and NL questions jointly, and to enable robust reasoning over tables, semantic parsing becomes a widely-studied task of parsing NL questions to programming languages such as SQLs, logical forms, or python code. Question answering over visual documents is a cross-domain task of computer vision, tabular data, and NL. We think that classifying tasks by domains presents a fresh perspective to future works on multi-modal modeling, but it is not an absolutely strict or static categorization, e.g., table QA can involve programming languages via semantic parsing , while it can also use endto-end prediction [Herzig et al., 2020 without explicitly using a programming language.\n\n[2] TAPEX: Table Pre-training via Learning a Neural SQL Executor Table Source  Following previous work by Yin et al. (2020), we choose publicly available semi-structured tables as the table source.However, rather than requiring millions of raw tables in (Yin et al., 2020), TaPEx works well even with only a few thousand tables.Therefore, instead of fetching noisy tables from the Web and then heuristically filtering them, we pick high-quality tables right from existing public datasets.Concretely, we randomly select nearly 1,50015001,500 tables from the training set of WikiTableQuestions (Pasupat & Liang, 2015) as the table source for our pre-training corpus.Notice that there is no overlap between the tables used in our pre-training and the tables used in the dev and test sets of all downstream tasks, so there is no data leakage problem.\n\n[3] TAPEX: Table Pre-training via Learning a Neural SQL Executor Table Pre-training  The work most related to ours is table pre-training whose key factors include the pre-training corpus and the pre-training task.As for the pre-training corpus, most of previous works almost collect NL-table data to perform table pre-training.They either mined a large corpus of tables and their NL sentence contexts (Yin et al., 2020; Herzig et al., 2020), leveraged human-annotated parallel NL-table datasets for pre-training (Deng et al., 2021; Yu et al., 2021a), or synthesized a NL-table corpus using human-written templates (Yu et al., 2021a; Eisenschlos et al., 2020).Our work is different from theirs because we are the first to use pure synthetic SQL-table data for table pre-training, which allows us to automatically synthesize a diverse, large-scale, and high-quality pre-training corpus.As for the pre-training task, existing works proposed several pre-training tasks, such as Mask Column Prediction (Yin et al., 2020), Multi-choice Cloze at the Cell Level (Wang et al., 2021b) and Structure Grounding (Deng et al., 2021).Different from all of them, we present a novel SQL execution task to perform table pre-training.\n\n[4] Table Pre-training: A Survey on Model Architectures, Pretraining Objectives, and Downstream Tasks To best leverage table characteristics while maintaining capabilities to understand text within/out of tables, various Tabular Language Models (TaLMs) are proposed for table pre-training. For example, TaBERT Yin et al. (2020) encoded tables and text by concatenating a row-wise transformer with column-wise vertical attention layers, pre-trained with Masked Language Models (MLM) and Masked Column Prediction (MCP), and achieved SOTA results on benchmarks of table QA. TaPas Herzig et al. (2020) pioneeringly proposed an end-to-end table-text joint reasoning framework using transformers without explicitly generating logical forms for table QA, and TaPasalso employed MLM for pre-training.TURL Deng et al. (2020) was the first work to learn entity representations from relational tables to enhance table knowledge matching and table augmentation, and it restricted each cell to aggregating information from the located row and column via masked attention.TUTA Wang et al. (2020) then extended the success of table pre-training to generally structured tables using tree-based attention and tree-based positional encoding based on a novel unified bi-tree structure. TUTA achieved new SOTA results on five representative table structure understanding datasets.Far different from previous pre-training objectives, TaPEx Liu et al. (2021) explored to learn a neural SQL executor and demonstrated surprising effectiveness on table-text joint reasoning. Recently, UnifiedSKG Xie et al. (2022) explored to directly fine-tune T5 on 21 datasets across 6 tasks and achieved promising and even SOTA results.\n\n[5] GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing We present GraPPa, an effective pre-training approach for table semantic parsing that learns a compositional inductive bias in the joint representations of textual and tabular data. We construct synthetic question-SQL pairs over high-quality tables via a synchronous context-free grammar (SCFG) induced from existing text-to-SQL datasets. We pre-train our model on the synthetic data using a novel text-schema linking objective that predicts the syntactic role of a table field in the SQL for each question-SQL pair. To maintain the model's ability to represent real-world data, we also include masked language modeling (MLM) over several existing table-and-language datasets to regularize the pre-training process. On four popular fully supervised and weakly supervised table semantic parsing benchmarks, GraPPa significantly outperforms RoBERTa-large as the feature representation layers and establishes new state-of-the-art results on all of them.\n\n[6] Korean-Specific Dataset for Table Question Answering Korean-Specific Dataset for Table Question Answering Existing question answering systems mainly focus on dealing with text data. However, much of the data produced daily is stored in the form of tables that can be found in documents and relational databases, or on the web. To solve the task of question answering over tables, there exist many datasets for table question answering written in English, but few Korean datasets. In this paper, we demonstrate how we construct Korean-specific datasets for table question answering: Korean tabular dataset is a collection of 1.4M tables with corresponding descriptions for unsupervised pre-training language models. Korean table question answering corpus consists of 70k pairs of questions and answers created by crowd-sourced workers. Subsequently, we then build a pre-trained language model based on Transformer and fine-tune the model for table question answering with these datasets. We then report the evaluation results of our model. We make our datasets publicly available via our GitHub repository and hope that those datasets will help further studies for question answering over tables, and for the transformation of table formats. Introduction The task of question answering is to correctly answer to given questions, which requires a high level of language understanding and machine reading comprehension abilities. As pre-trained language models on Transformer (Vaswani et al., 2017) have brought significant improvements in performance in many natural language processing tasks, there have been many studies in machine reading comprehension (MRC) and question answering (QA) tasks (Devlin et al., 2018;Lan et al., 2019;Yang et al., 2019;Yamada et al., 2020;Liu et al.,\n\n[7] Generative Table Pre-training Empowers Models for Tabular Prediction Recently, the topic of table pre-training has attracted considerable research interest. However, how to employ table pre-training to boost the performance of tabular prediction remains an open challenge. In this paper, we propose TapTap, the first attempt that leverages table pre-training to empower models for tabular prediction. After pre-training on a large corpus of real-world tabular data, TapTap can generate high-quality synthetic tables to support various applications on tabular data, including privacy protection, low resource regime, missing value imputation, and imbalanced classification. Extensive experiments on 12 datasets demonstrate that TapTap outperforms a total of 16 baselines in different scenarios. Meanwhile, it can be easily combined with various backbone models, including LightGBM, Multilayer Perceptron (MLP) and Transformer. Moreover, with the aid of table pre-training, models trained using synthetic data generated by TapTap can even compete with models using the original dataset on half of the experimental datasets, marking a milestone in the development of synthetic tabular data generation. The codes are available at https://github.com/ZhangTP1996/TapTap.\n\n[8] Table Pre-training: A Survey on Model Architectures, Pretraining Objectives, and Downstream Tasks Web TablesLarge corpora include WDC Web Table Corpus 111http://webdatacommons.org/framework/ (233M tables), Dresden Web Tables Corpus Eberius et al. (2015) (174M tables), WebTables Cafarella et al. (2008) (154M tables), and WikiTables (1.6M tables). More details are summarized by  Zhang and Balog (2020).\n\n[9] TAPEX: Table Pre-training via Learning a Neural SQL Executor TaPEx is conceptually simple and easy to implement.In this paper, we regard the pre-training as a sequence generation task and employ an encoder-decoder model.Specifically, we employ the pre-trained encoder-decoder language model BART (Lewis et al., 2020) as the backbone.Furthermore, we examine the effectiveness of TaPEx via two fundamental downstream tasks: table-based question answering (TableQA) and table-based fact verification (TableFV).To enable fine-tuning of downstream tasks to take full advantage of TaPEx, we reformulate these tasks using the encoder-decoder sequence generation paradigm.We evaluate TaPEx using four well-known benchmark datasets.Experimental results clearly demonstrate that TaPEx can bring significant and consistent improvements on these datasets.For example, TaPEx obtains an absolute improvement of 19.5%percent19.519.5\\% over BART in the WikiTableQuestions dataset.Furthermore, TaPEx yields strong results even with a small pre-training corpus, demonstrating its high efficiency.Finally, TaPEx achieves new state-of-the-art results on all experimental benchmarks, outperforming previous approaches by a large margin, including complicated table pre-training approaches with several heuristics in data processing.We will make our code, model, and data publicly available to facilitate future research.\n\n[10] PLOG: Table-to-Logic Pretraining for Logical Table-to-Text Generation table-to-text generation, with Wikipedia tables and human-annotated logical descriptions. Chen et al. (2021) proposed a deconfounded variational encoder-decoder model to encourage the model to generate non-surface-level predictions; however, the logical reasoning process is not explicitly considered, leading to low fidelity scores on human evaluation. Chen et al. (2020b) proposed to annotate logical forms to guide the generation and released a LOGIC2TEXT dataset. In this work, we focus on direct logical table-to-text generation without any explicit logical forms. Another related line of datasets are ToTTo (Parikh et al., 2020) and HiTab (Cheng et al., 2021), which incorporate highlighted cells to promote controllable generation. The CONTLOG dataset we propose is similar to the task settings of these datasets but differs in that we focus on logical-level generation. At the same time, only a small portion of examples in ToTTo and HiTab involve logical reasoning. ROTOWIRE (Wiseman et al., 2017) and Numeric-NLG (Suadaa et al., 2021) also involve numerical reasoning over table records, while they focus on document-level table summarization instead of sentence generation. los et al., 2020;Liu et al., 2021b;Dong et al., 2022;Iida et al., 2021) has been popular for table understanding tasks such as Table Question Answering (TableQA) (Zhong et al., 2017;Pasupat and Liang, 2015) and Table Fact Verification (TableFV) (Chen et al., 2019). With large-scale pretraining corpora, the table pretraining models can learn a better joint understanding of tabular and textual data through well-defined pretraining objectives. Most table pretraining works are based on table-text corpora, while TAPEX (Liu et al., 2021b) learns from synthetic SQL programs,\n\n[11] TAPEX: Table Pre-training via Learning a Neural SQL Executor To apply pre-training techniques on structured tabular data, there exist two key challenges: (i) where to obtain a large-scale pre-training corpus with high quality, and (ii)how to design an efficient pre-training task for table pre-training.For the first challenge, existing works generally collect parallel data including NL sentences and tables as the pre-training corpus, since downstream tasks often involve a joint reasoning over both free-form NL sentences and tables.They either crawled tables and their surrounding NL sentences from the Web (Herzig et al., 2020; Yin et al., 2020; Deng et al., 2021), or synthesized NL sentences on available tables (Yu et al., 2021a; Shi et al., 2021a).However, as pointed by Yin et al. (2020), the raw data mined from the Web is extremely noisy and requires complicated heuristics to clean.Conversely, the synthesis method is easier to control the data quality, but it usually requires experts to write hundreds of templates, which is both costly and often lacking diversity.Regarding the pre-training task, existing works often employ different variants of Masked Language Modeling (MLM) (Devlin et al., 2019) to guide LMs to learn better representations of tabular data.For example, TaPaS (Herzig et al., 2020) used MLM with whole word masking, and TaBERT (Yin et al., 2020) proposed Masked Column Prediction (MCP) to encourage the model to recover the names and data types of masked columns.Despite their success, they still largely treat tabular data as a structural format of text, which leads to the need of an extremely large corpus for their table pre-training.All of these hinder the progress of table pre-training.\n\n[12] Korean-Specific Dataset for Table Question Answering model performance according to question difficulty and different table conversion formats are also reported in this study. We summarize our main contributions as follows: • We construct two Korean-specific datasets: Korean Wikipedia Tabular dataset (KorWikiTabular) consisting of 1.4M tables that are converted into sentence strings containing tabular structural information, and Korean Wikipedia Table Questions dataset (KorWikiTQ) including 70k pairs of questions and answers generated according to question difficulty by paid crowdsourced workers. • We introduce approaches to converting tabular data into sentence strings, which allows models to represent table structural properties and information. • We present KO-TaBERT, a pre-trained language model that learns syntactic and lexical information and as well structural information from tables. • We build table question answering models based on the pre-trained language model using our datasets. Our model is evaluated on the tablerelated subset of the KorQuAD 2.0 corpus, and they can be treated as baselines for future research. Related Works There have been many studies to reason the correct answer to a given question over tables. A semantic parsing task is applied for question answering over tables, which translates natural language questions to logical forms such as SQL. Several works (Hwang et al., 2019;Lyu et al., 2020;Lin et al., 2020;Cao et al., 2021) introduce approaches that leverage BERT (Devlin et al., 2018), a pre-trained language model (PLM), in the text-to-SQL task, since PLMs with a deep contextualized word representation have led noticeable improvements in many NLP challenges. Word contextualization has contributed to improving accuracy for the generation of logical forms, but\n\n"}
{"case_id": "183575a6b64714da860ca0dd4b2d1f48", "answer_text": "\n\nAdapting transformer-based models for tabular data representation presents several significant challenges[2]. These challenges stem from the intrinsic differences between tabular data and other types of data like text or images, for which transformers were originally designed[0]. Below, we provide a comprehensive overview of these challenges based on the literature. \n\n1. **Heterogeneity of Tabular Data**: Tabular data often contains a mix of categorical, continuous, and sometimes missing values. This heterogeneity poses a problem for transformer models that are typically designed to handle homogeneous text data. As a result, there is a need to develop preprocessing methods to transform heterogeneous tabular data into a uniform representation suitable for transformers. Specific preprocessing techniques include the use of embedding layers for categorical data and normalization techniques for continuous data to ensure they can be effectively combined. For instance, models like TabTransformer use contextual embeddings for categorical features and concatenate them with continuous features before feeding them into a model [7]. However, this approach might lose important correlations between different data types [7]. Empirical results from studies on models like the TabTransformer highlight how these models specifically handle categorical data through contextual embeddings, significantly improving the performance over traditional methods [0]. Another technique involves quantizing continuous values using methods like K-Means clustering to create ordered embeddings [4].\n\n2. **Pre-training and Fine-tuning with Different Column Sets**: Another challenge is that the set of columns available during pre-training may differ significantly from those available during fine-tuning or inference. This scenario is common in real-world datasets where column names and features can vary [3][13]. Some models, such as **TransTab** and **TabLLM**, attempt to address this by creating column-by-column representations or converting rows into sequences of tokens. Specifically, TransTab generates representations for each row based on the column name and value, facilitating the transfer to unseen columns without relying heavily on column descriptions [3]. On the other hand, TabLLM converts rows into sequences of tokens, feeding them into a pre-trained language model, assuming semantic correspondences between column descriptions [3]. This assumption, however, can be faulty in cases where column descriptions are non-informative or absent. Empirical evidence shows that models like **TabRet**, which do not rely on column descriptions, achieve better transferability to different column sets, enhancing their robustness and applicability [3].\n\n3. **Scalability and Computational Costs**: Transformer models inherently have quadratic complexity with respect to sequence length due to their multi-head self-attention mechanism, making them resource-intensive. When dealing with high-dimensional tabular data, this complexity can become prohibitive [14]. Efficient approximations of multi-head self-attention (MHSA) and model distillation methods can alleviate some of these issues, but the problem persists, particularly for large datasets [14]. Studies have shown that implementations like **SAINT**, which introduce row and column attention, provide a more scalable approach with competitive performance [7].\n\n4. **Encoding Numerical Features**: Properly encoding numerical data for transformer models is another hurdle. Some approaches convert numerical values into textual representations, which can sometimes be counter-intuitive but have shown promising results [12]. However, these textual encodings can introduce inefficiencies and inaccuracies. Smarter encoding strategies might involve normalizing numerical values and applying embeddings similar to those used in NLP tasks. For example, approaches such as min-max normalization or z-score normalization can standardize numerical values, making them more comparable within the dataset. Moreover, newer techniques like using ordered embeddings—which place numerical values into discrete bins before transforming them into embeddings—allow models to better interpret the inherent order of numerical features [4][12]. The effectiveness of these strategies varies; therefore, the selection of encoding techniques can significantly influence the performance of transformer models on tabular datasets. Experimental results with models like **GReaT** confirm that such textual encodings can be effective, but also highlight the potential for further improvements through more sophisticated encoding strategies [12].\n\n5. **Incorporating Domain-specific Knowledge**: While transformers have shown success by leveraging large text corpora to build contextual embeddings, incorporating domain-specific knowledge into tabular transformers still presents challenges. Examples of methodologies used to address this issue include leveraging knowledge graphs and domain-specific metadata. For instance, some models utilize knowledge graph information to enhance their understanding of the tabular data structure [1][8]. The multi-encoder Metadata framework is one such example that extracts entity linking, column type, and property information from knowledge graphs and integrates this information into the transformer model through a process called Knowledge Fusion [1]. Additionally, Distribution Fusion techniques are employed to represent column distributions effectively, further improving model performance [1]. Using these methods, transformers can achieve greater accuracy in tasks like metadata inference and entity recognition, demonstrating the importance of domain-specific knowledge integration.\n\n6. **Limited Benchmarks and Evaluations**: There is also a lack of standardized benchmarks for evaluating transformer models on tabular data. While some efforts have been made to create benchmarks, such as the open-source benchmarks proposed by Xu et al. for deep learning models on tabular data (Sec. V-B) [6], the field still lacks widespread adoption of these standards. This absence hampers the ability to measure and compare the performance of different models effectively. Establishing common benchmarks and evaluation frameworks is critical for advancing the field [6]. For instance, open-source benchmarks and standardized evaluation frameworks proposed by Xu et al. have facilitated more consistent comparison and identification of progress in the field [6].\n\nIn summary, while transformer-based models hold significant promise for tabular data representation, several challenges related to data heterogeneity, scalability, encoding techniques, incorporation of domain-specific knowledge, and standardized evaluation need to be addressed [6][14]. Ongoing research is focused on tackling these issues to make transformers more effective and efficient for tabular data applications [2][8][9]. \n\n\nReferences:\n[0] TabTransformer: Tabular Data Modeling Using Contextual Embeddings Furthermore, many existing deep learning models for tabular data are designed for supervised learning scenario but few are for semi-supervised leanring (SSL). Unfortunately, the state-of-art SSL models developed in computer vision (Voulodimos et al. 2018; Kendall and Gal 2017) and NLP (Vaswani et al. 2017; Devlin et al. 2019) cannot be easily extended to tabular domain. Motivated by such challenges, we exploit pre-training methodologies from the language models and propose a semi-supervised learning approach for pre-training Transformers of our TabTransformer model using unlabeled data.\n\n[1] Inferring Tabular Analysis Metadata by Infusing Distribution and Knowledge Information graph (KG In order to get supervision signals for metadata inference (challenge 1), we collect a large corpus and use smart supervision from downstream tasks, the manual labels through crowdsourcing, the existing information attached to tables( §4). This corpus is collected from spreadsheet datasets, public web table datasets, and public synthetic datasets. Recently, the pre-training strategy has achieved significant success in natural language processing, and some works,34 try to use it on tabular data. However, existing tabular pre-training works seldom consider the existing knowledge (challenge 2) and column distribution (challenge 3). To fill this gap and better solve the metadata problem, we propose a multi-encoder Metadata framework ( §5), which can be applied to almost all the transformer-based pre-training tabular models. To represent column distribution, the framework extracts data features and proposes Distribution Fusion to infuse them with the tabular model. To utilize extra knowledge graph information, the framework extracts KG information (entity linking, column type, and property) for each table and proposes Knowledge Fusion to infuse them with the tabular model. We conduct several experiments ( §6) to illustrate the importance of Metadata tasks and the effectiveness of our Metadata model. Our model has accuracy = 98%, hit rate at top-1 > 67%, accuracy > 80%, and accuracy = 88% for the four analysis metadata inference tasks, respectively, and outperforms baselines varying from rule-based, traditional machine learning methods to pre-trained tabular models. And in especial, Distribution fusion improves one of the common field roles tasks by 3.64% and Knowledge fusion improves one of the\n\n[2] Transformers for Tabular Data Representation: A Survey of Models and Applications In the last few years, the natural language processing community has witnessed advances in neural representations of free texts with transformer-based language models (LMs). Given the importance of knowledge available in tabular data, recent research efforts extend LMs by developing neural representations for structured data. In this article, we present a survey that analyzes these efforts. We first abstract the different systems according to a traditional machine learning pipeline in terms of training data, input representation, model training, and supported downstream tasks. For each aspect, we characterize and compare the proposed solutions. Finally, we discuss future work directions.\n\n[3] TabRet: Pre-training Transformer-based Tabular Models for Unseen Columns Despite the popularity, most Transformer-based methods have not adapted to the case where pre-training and fine-tuning tasks have different column sets.To the best of our knowledge, TransTab (Wang & Sun, 2022), TabLLM (Hegselmann et al., 2022), and LIFT (Dinh et al., 2022) are the few exceptions that can train on tabular datasets with different column sets.TransTab creates column-by-column representations of each row from the column name and value, allowing pre-trained models to be applied to unseen columns.TabLLM and LIFT convert a row into a sequence of tokens and feed them to a pre-trained language model (Sanh et al., 2022; Wang & Komatsuzaki, 2021).They assumed there were semantic correspondences of column descriptions between different columns.In practice, however, there are many cases where column descriptions are not informative (e.g., random alphabet) or do not exist.Since TabRet does not explicitly use the column description information, it can transfer to a different column set in such a situation.\n\n[4] TabMT: Generating tabular data with masked transformers To construct each ordered embedding matrix O, we first we quantize the values of the continuous field. Our default quantizer is K-Means.We consider the maximum number of clusters a hyper-parameter.Let v∈ℝkvsuperscriptℝ𝑘\\textbf{v}\\in\\mathbb{R}^{k} be the vector of ordered cluster centers. We construct an vector of ratios r∈ℝkrsuperscriptℝ𝑘\\textbf{r}\\in\\mathbb{R}^{k} using min max normalization such thatri=vi−min⁡(v)max⁡(v)−min⁡(v).subscriptr𝑖subscriptv𝑖vvv\\textbf{r}_{i}=\\frac{\\textbf{v}_{i}-\\min(\\textbf{v})}{\\max(\\textbf{v})-\\min(\\textbf{v})}.(4)We use the ratio vector r to construct each ordered embedding in O:Oi=Ei+ri⋅l+(1−ri)⋅h.subscriptO𝑖subscriptE𝑖⋅subscriptr𝑖l⋅1subscriptr𝑖h\\textbf{O}_{i}=\\textbf{E}_{i}+\\textbf{r}_{i}\\cdot\\textbf{l}+(1-\\textbf{r}_{i})\\cdot\\textbf{h}.(5)This structure allows the transformer to both take advantage of the ordering of the properties and add unordered embedding information to each cluster.The unordered embeddings are useful in attention, multi-modal distributions, and encoding semantic separation between close values. We use this same structure toconstruct a dynamic linear layer at the output during prediction. This can be converted to a traditional linear layer once the model is trained.\n\n[5] Deep Neural Networks and Tabular Data: A Survey for a standardized benchmarking procedure, which allows to identify significant progress with respect to the state of the art. With this work, we also propose an open-source benchmark for deep learning models on tabular data. For tabular data generation tasks, Xu et al. proposes a sound evaluation framework with artificial and real-world data sets (Sec. V-B), but researchers need to agree on common benchmarks in this subdomain as well. Tabular Data Preprocessing. Many of the challenges for deep neural networks on tabular data are related to the heterogeneity of the data (e.g., categorical and sparse values). Therefore, some deep learning solutions transform them into a homogeneous representation more suitable to neural networks. While the additional overhead is small, such transforms can boost performance considerably and should thus be among the first strategies applied in real-world scenarios. Architectures for Deep Learning on Tabular Data. Architecture-wise, there has been a clear trend towards transformer-based solutions (Section IV-B2) in recent years. These approaches offer multiple advantages over standard neural network architectures, for instance, learning with attention over both categorical and numerical features. Moreover, self-supervised or unsupervised pre-training that leverages unlabelled tabular data to train parts of the deep learning model is gaining popularity, not only among transformerbased approaches. Performance-wise, multiple independent evaluations demonstrate that deep neural network methods from the hybrid (Sec. IV-B1) and transformers-based (Sec. IV-B2) groups exhibit superior predictive performance compared to plain deep neural networks on various data sets,,,. This underlines the importance of special-purpose architectures for tabular data. Regularization Models\n\n[6] SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training , is trained on semi-structured test data to perform language-specific tasks. Several other studies utilize tabular data, but their problem settings are outside of our scope,21,31,32,35. Transformer models for more general tabular data include TabTransformer, which uses a transformer encoder to learn contextual embeddings only on categorical features. The continuous features are concatenated to the embedded features and fed to an MLP. The main issue with this model is that continuous data do not go through the self-attention block. That means any information about correlations between categorical and continuous features is lost. In our model, we address that issue by projecting continuous features and categorical features to the higher dimensional embedding space and passing them both through the transformer blocks. In addition, we propose a new type of attention to explicitly allow data points to attend to each other to get better representations. Axial Attention Ho et al. are the first to propose row and column attention in the context of localized attention in 2-dimensional inputs (like images) in their Axial Transformer. This is where for a given pixel, the attention is computed only on the pixels that are on the same row and column, rather than using all the pixels in the image. The MSA Transformer extends this work to protein sequences and applies both column and row attention across similar rows (tied row attention). TABBIE is an adaptation of axial attention that applies self-attention to rows and columns separately, then averages the representations and passes them as input to\n\n[7] Named Entity Recognition in Industrial Tables using Tabular Language Models Named Entity Recognition in Industrial Tables using Tabular Language Models Specialized transformer-based models for encoding tabular data have gained interest in academia. Although tabular data is omnipresent in industry, applications of table transformers are still missing. In this paper, we study how these models can be applied to an industrial Named Entity Recognition (NER) problem where the entities are mentioned in tabular-structured spreadsheets. The highly technical nature of spreadsheets as well as the lack of labeled data present major challenges for fine-tuning transformer-based models. Therefore, we develop a dedicated table data augmentation strategy based on available domain-specific knowledge graphs. We show that this boosts performance in our low-resource scenario considerably. Further, we investigate the benefits of tabular structure as inductive bias compared to tables as linearized sequences. Our experiments confirm that a table transformer outperforms other baselines and that its tabular inductive bias is vital for convergence of transformer-based models. Introduction There has been growing interest in developing special model designs intended to capture tabular structure (Deng et al., 2020;Yin et al., 2020;Herzig et al., 2020;Wang et al., 2021). A recent survey named these models tabular language models (TaLMs) and provided an overview of the different architectures and pretraining objectives (Dong et al., 2022). One of the downstream tasks where TaLMs are applicable is table interpretation (TI) with its sub-tasks: entity linking, column type annotation and relation extraction (Deng et al., 2020). Most TaLMs for TI use BERT as the backbone language model (LM) for encoding the content of * Equal Contribution. table cells and aggregate\n\n[8] Transformers for Tabular Data Representation: A Tutorial on Models and Applications Transformers for Tabular Data Representation: A Tutorial on Models and Applications In the last few years, the natural language processing community witnessed advances in neural representations of free texts with transformer-based language models (LMs). Given the importance of knowledge available in relational tables, recent research efforts extend LMs by developing neural representations for tabular data. In this tutorial, we present these proposals with two main goals. First, we introduce to a database audience the potentials and the limitations of current models. Second, we demonstrate the large variety of data applications that benefit from the transformer architecture. The tutorial aims at encouraging database researchers to engage and contribute to this new direction, and at empowering practitioners with a new set of tools for applications involving text and tabular data.\n\n[9] Language Models are Realistic Tabular Data Generators approach. Since we convert heterogeneous tabular data into text (Sec. 3), continuous and discrete numerical values are represented as character sequences. While this might seem counter-intuitive at first, multiple independent studies have shown that transformer-based models are capable of understanding and processing numerical data encoded in such a way (Wallace et al., 2019;Brown et al., 2020). This is even true in the oneshot setting for inference tasks (Dinh et al., 2022). The impressive performance of GReaT that encompasses the numerical features aligns with these previous observations. Having said that, smarter encodings for numerical values can also be considered a possible path to further improvement. Unification of multiple modalities through transformer models. Along with the usual numerical values, tabular data frequently contains textual metadata, e.g., feature names, named categories (\"male\", \"female\"), and open text features (e.g., remarks). The recent evolution of transformer neural networks now permits to holistically unite this information, which was processed separately in the past, and learn context-specific, robust, and meaningful representations. In the case of tabular data, information comes from the textual and numerical data modalities. The GReaT approach and the results presented in this work provide an initial clue of the range of possibilities and opportunities that may lie in this line of research. CONCLUSION In our recent work, we investigate how state-of-the-art generative language models can be leveraged to synthesize highly realistic tabular data samples. Instead of following the usual path of encoding heterogeneous tabular data in a numerical format, we devise a textual encoding strategy and represent it with sentences\n\n[10] TabRet: Pre-training Transformer-based Tabular Models for Unseen Columns In pre-training, suppose we have a table consisting of a finite set of columns 𝒞𝒞\\mathcal{C}. Let 𝒳csubscript𝒳𝑐\\mathcal{X}_{c} be the space where column c∈𝒞𝑐𝒞c\\in\\mathcal{C} takes its value. A row 𝒙𝒙{\\bm{x}} is then defined on the product space 𝒳​(𝒞)=∏c∈𝒞𝒳c𝒳𝒞subscriptproduct𝑐𝒞subscript𝒳𝑐\\mathcal{X}(\\mathcal{C})=\\prod_{c\\in\\mathcal{C}}\\mathcal{X}_{c}. Suppose a downstream task is defined as a supervised task on an input-output pair (𝒙′,y)superscript𝒙′𝑦({\\bm{x}}^{\\prime},y). Here we consider the case where the input 𝒙′∈𝒳​(𝒞′)superscript𝒙′𝒳superscript𝒞′{\\bm{x}}^{\\prime}\\in\\mathcal{X}(\\mathcal{C}^{\\prime}) is a row of another column set 𝒞′≠𝒞superscript𝒞′𝒞\\mathcal{C}^{\\prime}\\neq\\mathcal{C} (Figure 1 a). As an example, consider healthcare records where 𝒞={𝚊𝚐𝚎,𝚐𝚎𝚗𝚍𝚎𝚛,𝚠𝚎𝚒𝚐𝚑𝚝}𝒞𝚊𝚐𝚎𝚐𝚎𝚗𝚍𝚎𝚛𝚠𝚎𝚒𝚐𝚑𝚝\\mathcal{C}=\\{\\mathtt{age},\\mathtt{gender},\\mathtt{weight}\\} are given as a column set in pre-training and 𝒞′={𝚐𝚎𝚗𝚍𝚎𝚛,𝙱𝙼𝙸}superscript𝒞′𝚐𝚎𝚗𝚍𝚎𝚛𝙱𝙼𝙸\\mathcal{C}^{\\prime}=\\{\\mathtt{gender},\\mathtt{BMI}\\} are in fine-tuning. In this case, the column 𝚐𝚎𝚗𝚍𝚎𝚛𝚐𝚎𝚗𝚍𝚎𝚛\\mathtt{gender} appears in common, but other columns {𝚊𝚐𝚎,𝚠𝚎𝚒𝚐𝚑𝚝,𝙱𝙼𝙸}𝚊𝚐𝚎𝚠𝚎𝚒𝚐𝚑𝚝𝙱𝙼𝙸\\{\\mathtt{age},\\mathtt{weight},\\mathtt{BMI}\\} appear only in either pre-training or fine-tuning. We assume that the pre-training table is inaccessible during fine-tuning.\n\n[11] Revisiting Deep Learning Models for Tabular Data Limitations. FT-Transformer requires more resources (both hardware and time) for training than simple models such as ResNet and may not be easily scaled to datasets when the number of features is “too large” (it is determined by the available hardware and time budget).Consequently, widespread usage of FT-Transformer for solving tabular data problems can lead to greater CO2 emissions produced by ML pipelines, since tabular data problems are ubiquitous.The main cause of the described problem lies in the quadratic complexity of the vanilla MHSA with respect to the number of features.However, the issue can be alleviated by using efficient approximations of MHSA (Tay et al., 2020).Additionally, it is still possible to distill FT-Transformer into simpler architectures for better inference performance.We report training times and the used hardware in supplementary.\n\n"}
{"case_id": "3927ad11a6f7001173bfb2bbb9e8729e", "answer_text": "\n\nSafe navigation in partially observable environments with moving obstacles is a complex problem that has garnered significant attention in recent years. Different approaches have been developed, each leveraging unique strategies to address the challenges posed by dynamic and partially known environments. These techniques can broadly be categorized into hierarchical planning frameworks [2], probabilistic methods [3], belief-space planning [3], hybrid strategies [9], and emerging methods [12].\n\nHierarchical Planning Frameworks[2]. \n\nHierarchical planning frameworks integrate high-level task planners with low-level motion planners to provide formal safety guarantees and robust navigation in dynamic and partially observable environments. For example, one such framework employs linear temporal logic (LTL) for reactive game synthesis between the robot and its environment, ensuring navigation safety and task completion [4]. The high-level planner estimates the location of dynamic obstacles using a belief abstraction model, which helps reduce conservatism in long-horizon navigation actions. These high-level plans are translated into specific locomotion actions that the low-level planner executes while ensuring dynamic balance and collision avoidance [4][5].\n\n#### Effectiveness and Limitations\n\nEffectiveness and limitations of motion planning in partially observable environments have been widely studied, particularly in the context of ensuring safe navigation and dynamic obstacle avoidance. Planning methods such as temporal-logic-based motion planning have been demonstrated to be effective in environments with static obstacles and simple robot dynamics, but challenges arise when dealing with dynamic obstacles visible only within a limited range [0]. The effectiveness of such frameworks is evident in hierarchical planning structures that integrate safety at both the high-level task planner and the low-level motion planner, which are crucial for dynamic balancing, navigation safety, and waypoint tracking [1]. Belief-space planning, which allows the controller to infer the locations of dynamic obstacles, enhances the framework by providing more informed decisions, leading to less conservative strategies and safe navigation even when static obstacles obscure parts of the environment [3]. Additionally, integrating special algorithms like LTL-based reactive navigation games with belief abstraction helps in dynamically expanding the navigation choices available, thereby effectively tackling the issue of partially observable environments [2].\nHierarchical planning frameworks are highly effective as they provide structured solutions that can be formally verified for safety and task completion [4]. Their multi-layered approach allows for detailed planning at various levels, ensuring that each aspect of navigation, from high-level task planning to low-level motion control, adheres to safety protocols [2]. However, they can be computationally intensive and may struggle with real-time adaptability due to their reliance on predefined models and plans. Scalability to large and complex environments remains an ongoing challenge as the computational overhead increases with the complexity of the environment [5].\n\nProbabilistic methods have been extensively used for collision avoidance and navigation in partially observable environments [3]. These methods leverage partially observable Markov decision processes (POMDPs), probabilistic velocity obstacle modeling, and object occlusion cost metrics to plan safe paths for robots [3]. One of the key techniques involves belief-space planning, which provides additional information to the controller about potential dynamic obstacles outside the robot's current field of view [3]. This approach not only enhances safety but also reduces conservative navigation strategies by explicitly tracking which non-visible areas are likely free of obstacles [3]. Experimental evaluations confirm the effectiveness of these probabilistic methods in real-world robotic navigation applications [3]. \n\nProbabilistic methods such as Partially Observable Markov Decision Processes (POMDPs) and Kalman filters are widely used to handle uncertainties in dynamic environments. POMDPs provide a formal framework for decision-making under uncertainty, enabling the robot to make informed decisions based on probabilistic estimates of the environment [3]. Kalman filters, on the other hand, are used to predict the trajectory of dynamic obstacles, incorporating uncertainty in the obstacle’s motion to generate safe navigation strategies [10]. These probabilistic approaches allow for more flexible and adaptive navigation, capable of adjusting to changes and uncertainties in real-time.\n\n#### Effectiveness and Limitations [3][5][8][14] \nProbabilistic methods are effective in dealing with uncertainties and providing adaptive navigation strategies. By maintaining probabilistic estimates, these methods continuously update their understanding of the environment, allowing for dynamic adjustments to navigation plans [10]. However, their high computational demands can be a limitation, particularly in real-time applications or resource-constrained systems. Scaling these methods to larger environments with numerous dynamic obstacles can exacerbate these computational challenges, potentially affecting their real-time effectiveness [10].\n\nBelief-Space Planning [3][5][6][12]. \n\nBelief-space planning involves maintaining a belief (a probabilistic representation) of the environment, including the positions of both visible and non-visible obstacles. This technique enables robots to plan actions that are safe even when parts of the environment are not directly observable [3][6]. By explicitly tracking the belief about the non-visible parts of the environment, robots can reduce the conservatism of their navigation strategies, allowing for more efficient path planning while still ensuring safety [6]. For instance, a belief abstraction method can expand the robot's navigation choices by estimating which regions of the environment are likely free of obstacles [3][4].\n\n#### Effectiveness and Limitations\nNavigation in partially observable environments presents unique challenges, particularly in terms of collision avoidance and dynamic obstacle management [3]. Traditional motion planning techniques like POMDPs and probabilistic velocity obstacle modeling have been used extensively for collision avoidance [3]. However, these approaches can be overly conservative, assuming that dynamic obstacles could appear at any moment and thus limiting the robot's efficiency in navigating dynamic environments [3]. Advanced frameworks integrate belief-space planning to offer navigation strategies that are both safe and efficient by predicting non-visible obstacles [3]. These methods enhance the capability of robots to move through complex environments while maintaining safety [4]. \n\n\nBelief-space planning is highly effective in reducing conservatism and enabling efficient path planning [3]. It enhances the robot's decision-making capabilities by maintaining a detailed probabilistic model of the environment, which includes both observed and unobserved elements [6]. However, maintaining and updating the belief state can be computationally intense, and the effectiveness of the planning heavily relies on the accuracy of the belief model [4]. Scaling to larger environments with more extensive non-visible areas can further challenge the computational feasibility of this approach, as the complexity of maintaining an accurate belief representation grows [8].\n\n\n\nHybrid strategies for navigation combine both global path planning and reactive approaches to achieve safe navigation [9]. These hybrid approaches provide a better solution for the safe navigation problem by attaining the advantages of both global planning and reactive navigation to overcome the shortcomings of relying purely on one [9]. Such advantages include escaping potential trapping situations and reacting quickly to perceived unknown and dynamic obstacles [9].\n\n\n\nHybrid strategies combine global path planning with reactive local navigation to achieve safe navigation in dynamic and partially observable environments. These approaches leverage the strengths of both global and local methods to overcome their respective limitations. For example, combining Rapidly-exploring Random Tree (RRT) algorithms for global path planning with reactive control laws allows a robot to quickly adapt to newly detected obstacles while following an overall safe trajectory [12]. Another hybrid strategy integrates Model Predictive Control (MPC) with Dynamic Control Barrier Functions (DCBF) to manage safety-critical tasks and ensure collision-free trajectories [11]. This method incorporates real-time obstacle detection and trajectory prediction to enable quick and safe navigation adjustments.\n\n#### Effectiveness and Limitations\nMotion planning in complex environments has been extensively studied, with a spectrum of approaches in the literature, 18, 19, 20, 21. Reactive methods for motion planning with formal guarantees are widely studied with the methods of safety barrier certificates, artificial potential functions, and more recent extensions, 24 [7]. While the work in provides convergence guarantees and obstacle avoidance in geometrically complicated unknown environments, it is restricted to static obstacles and has only been demonstrated on a fully actuated particle, or a simple unicycle model for a quadruped. Whereas the framework we present here is able to generate safe locomotion plans reacting to environmental events that include multiple, possibly adversarial, dynamic obstacles with formal guarantees on task completion and safety [7]. Moreover, our framework is validated on a bipedal robot Digit [7].\nHybrid strategies are effective in providing a balance between global path optimization and local reactivity. By combining the broad view and optimization capabilities of global planners with the quick responsiveness of local planners, these approaches ensure that the robot can navigate safely through dynamic environments [9]. However, the integration of multiple planning methods can increase system complexity and computational overhead. Scalability can be a concern as the environment complexity and the number of dynamic obstacles grow, necessitating more advanced computational resources and optimization techniques. \n\nEmerging techniques in autonomous navigation often integrate hierarchical planning frameworks to ensure safety and efficiency. For example, a recent study proposes a hierarchically integrated framework for safe task and motion planning (TAMP) for bipedal locomotion in environments with dynamic obstacles and uneven terrain, offering formal safety guarantees for both high-level task planning and low-level motion control [4]. Another approach focuses on dynamic control barrier functions combined with model predictive control to navigate safely in dynamic environments, leveraging advanced obstacle detection and trajectory prediction methods [10]. Belief abstraction methods are also employed to enable robots to make informed navigation decisions in partially observable environments, further enhancing their ability to avoid collisions with dynamic obstacles [5]. These techniques collectively push the boundaries of safe navigation in complex and unpredictable environments. \n\nEmerging techniques focus on leveraging advanced machine learning and optimization algorithms to enhance navigation in partially observable environments. For example, uncertainty-aware learning-based dynamics models and self-supervised traversability estimation models are employed to quantify and minimize uncertainties in control and perception [13]. Additionally, frameworks like FASTER, which enable high-speed trajectory planning by optimizing in both known and unknown spaces, represent advancements in ensuring safety without sacrificing speed [14]. These techniques often integrate with traditional methods to provide more robust and adaptive navigation capabilities.\n\n#### Effectiveness and Limitations\n\nTemporal-logic-based motion planning has been widely studied for mobile robot navigation in partially observable domains through exploration, re-synthesis when encountering unexpected obstacles, and receding-horizon planning [0]. These approaches are better suited for guaranteeing successful navigation and collision avoidance in environments with static obstacles and simple robot dynamics. On the contrary, our framework takes into account dynamic obstacles that are only visible within a limited range [3]. We devise a variant of the approach via a combined top-down and bottom-up strategy to design the navigation strategy in a partially observable environment while guaranteeing collision avoidance [4]. \nEmerging techniques are at the forefront of achieving state-of-the-art performance in dynamic and complex environments. They offer robust adaptivity and efficiency. These methods leverage the computational power and flexibility of modern machine learning algorithms to handle complex navigational tasks. However, they often require significant computational resources and may demand extensive training data and validation to ensure reliability [8]. Scaling these techniques to cover larger and more complex environments remains a significant challenge, given the increased computational and data requirements [10]. \n\n\n\nReal-world applications of autonomous navigation systems in partially observable environments have shown significant advancements. Temporal-logic-based motion planning has been widely studied for mobile robot navigation in partially observable domains through exploration,re-synthesis when encountering unexpected obstacles, and receding-horizon planning. These approaches are better suited for guaranteeing successful navigation and collision avoidance in environments with static obstacles and simple robot dynamics [0]. On the contrary, our framework takes into account dynamic obstacles that are only visible within a limited range. We devise a variant of the approach via a combined top-down and bottom-up strategy to design the navigation strategy in a partially observable environment while guaranteeing collision avoidance [1]. This work generalizes our previous work on temporal-logic-based locomotion[1]. A key component to achieving safe navigation in dynamic environments is the integration of global path planning algorithms with reactive control laws. For instance, a hybrid navigation approach combines a global path planning algorithm with a reactive control law to provide quick reflex-like reactions to newly detected obstacles, thereby enabling safe autonomous operations in partially unknown and dynamic environments[12]. \n\n\nSeveral frameworks and strategies have been validated through extensive simulations and real-world experiments. For instance, frameworks employing hierarchical task and motion planning have been successfully tested on bipedal robots like Cassie and Digit, demonstrating safe and robust navigation in complex environments with dynamic obstacles [4][5]. Probabilistic methods have been validated in real-world scenarios where robots navigate through environments with dynamic obstacles using POMDPs, such as in service robots operating in crowded spaces [3]. Hybrid strategies have shown effectiveness in UAV navigation, where high-speed trajectories need to be calculated in real-time with limited sensing capabilities [14]. Additionally, mobile robots employing MPC and DCBF have proven effective in avoiding various types of dynamic obstacles in real-world experiments, showcasing the robustness and safety of these approaches [11].\n\n### Summary\nReferences:\n\nTemporal-logic-based motion planning has been widely studied for mobile robot navigation in partially observable domains through exploration, re-synthesis when encountering unexpected obstacles, and receding-horizon planning [0]. These approaches are better suited for guaranteeing successful navigation and collision avoidance in environments with static obstacles and simple robot dynamics. On the contrary, our framework takes into account dynamic obstacles that are only visible within a limited range. We devise a variant of the approach via a combined top-down and bottom-up strategy to design the navigation strategy in a partially observable environment while guaranteeing collision avoidance [0]. This study proposes an integrated task and motion planning method for dynamic locomotion in partially observable environments with multi-level safety guarantees [5]. A belief abstraction at the task planning level enables belief estimation of dynamic obstacle locations and guarantees navigation safety with collision avoidance [5]. The high-level task planner, i.e., a two-level navigation planner, employs linear temporal logic for a reactive game synthesis between the robot and its environment while incorporating low-level safe keyframe policies into formal task specification design [5]. The synthesized task planner commands a series of locomotion actions including walking step length, step height, and heading angle changes, to the underlying keyframe decision-maker, which further determines the robot center-of-mass apex velocity keyframe [5]. The low-level phase-space planner uses a reduced-order locomotion model to generate non-periodic trajectories meeting balancing safety criteria for straight and steering walking [5]. These criteria are characterized by constraints on locomotion keyframe states, and are used to define keyframe transition policies via viability kernels [5]. Simulation results of a Cassie bipedal robot designed by Agility Robotics demonstrate locomotion maneuvering in a three-dimensional, partially observable environment consisting of dynamic obstacles and uneven terrain [5].\n\n\nIn summary, leading techniques for safe navigation in partially observable environments with moving obstacles include hierarchical planning frameworks, probabilistic methods, belief-space planning, hybrid strategies, and emerging techniques [1][3][9]. Each of these approaches has specific advantages and limitations. Hierarchical frameworks provide structured, verifiable solutions but can be computationally intensive and face scalability issues [1]. Probabilistic methods handle uncertainties well but may be demanding in terms of computation, especially in larger environments [3]. Belief-space planning reduces conservatism but requires accurate belief models and scalability can be a concern [4]. Hybrid strategies balance global and local planning but increase system complexity and computational overhead [9]. Emerging techniques offer robust adaptability but often need extensive computational resources and training. Combining these methods can lead to comprehensive and effective solutions for navigating complex and dynamic environments. \n\n\nReferences:\n[0] Towards Safe Locomotion Navigation in Partially Observable Environments with Uneven Terrain Temporal-logic-based motion planning has been widely studied for mobile robot navigation in partially observable domains through exploration,re-synthesis when encountering unexpected obstacles, and receding-horizon planning. These approaches are better suited for guaranteeing successful navigation and collision avoidance in environments with static obstacles and simple robot dynamics.On the contrary, our framework takes into account dynamic obstacles that are only visible within a limited range. We devise a variant of the approach in via a combined top-down and bottom-up strategy to design the navigation strategy in a partially observable environment while guaranteeing collision avoidance. This work generalizes our previous work on temporal-logic-based locomotion, 18, 19.\n\n[1] Towards Safe Locomotion Navigation in Partially Observable Environments with Uneven Terrain The main contributions of this study are fourfold:(i) design two-level safety criteria for locomotion motion planning, which guarantees the simultaneous dynamic balancing and navigation safety as well as waypoint tracking. (ii) devise a keyframe transition map based on low-level motion planer constraints and design a keyframe policy for locomotion safe navigation.(iii) employ a belief abstraction method for a reactive navigation gameto expand navigation choices in a partially observable environment.(iv) design a hierarchical planning structure that integrates safety for the high-level task planner and low-level motion planner cohesively.\n\n[2] Integrated Task and Motion Planning for Safe Legged Navigation in Partially Observable Environments The main contributions of this study are as follows:•Design a hierarchically integrated planning framework that provides formal safety guarantees simultaneously for the high-level task planner and middle-level ROM-based motion planner, which enables safe locomotion and navigation involving steering walking.•Design safe sequential composition of controllable regions for robust locomotion in the presence of perturbations, and sampling-based keyframe decision maker for accurate waypoint tracking to facilitate middle-level navigation safety.•Synthesize an LTL-based reactive navigation game for safe legged navigation and employ a belief abstraction method to expand navigation decisions in partially observable environments.•Experimental evaluation of the proposed framework on a bipedal robot Digit to navigate safely in a complex environment with dynamic obstacles.\n\n[3] Integrated Task and Motion Planning for Safe Legged Navigation in Partially Observable Environments Collision avoidance with dynamic obstacles in partially observable environments has been achieved through approaches such as partially observable Markov decision processes (POMDPs), probabilistic velocity obstacle modeling, and object occlusion cost metrics. The authors in guarantee passive motion safety by avoiding braking Inevitable Collision States (ICS) at all times via a braking ICS-checking algorithm. While these solutions provide collision avoidance guarantees, they assume dynamic obstacles could appear at any time and result in an overly conservative strategy. Our method investigates belief-space planning to provide the controller with additional information on when and where dynamic obstacles may appear in the robot’s visible range to inform the synthesized strategy if navigation actions are guaranteed to be safe, even when static obstacles occlude the robot’s view of adjacent environment locations. We have devised a variant of the approach in to explicitly track a belief of which non-visible environment locations are obstacle free, reducing the conservativeness of a guaranteed collision-free strategy. This belief tracking method is then integrated into our hierarchical TAMP framework.\n\n[4] Integrated Task and Motion Planning for Safe Legged Navigation in Partially Observable Environments This study proposes a hierarchically integrated framework for safe task and motion planning (TAMP) of bipedal locomotion in a partially observable environment with dynamic obstacles and uneven terrain. The high-level task planner employs linear temporal logic for a reactive game synthesis between the robot and its environment and provides a formal guarantee on navigation safety and task completion. To address environmental partial observability, a belief abstraction model is designed by partitioning the environment into multiple belief regions and employed at the high-level navigation planner to estimate the dynamic obstacles' location. This additional location information of dynamic obstacles offered by belief abstraction enables less conservative long-horizon navigation actions beyond guaranteeing immediate collision avoidance. Accordingly, a synthesized action planner sends a set of locomotion actions to the middle-level motion planner while incorporating safe locomotion specifications extracted from safety theorems based on a reduced-order model (ROM) of the locomotion process. The motion planner employs the ROM to design safety criteria and a sampling algorithm to generate nonperiodic motion plans that accurately track high-level actions. At the low level, a foot placement controller based on an angular-momentum linear inverted pendulum model is implemented and integrated with an ankle-actuated passivity-based controller for full-body trajectory tracking. To address external perturbations, this study also investigates the safe sequential composition of the keyframe locomotion state and achieves robust transitions against external perturbations through reachability analysis. The overall TAMP framework is validated with extensive simulations and hardware experiments on bipedal walking robots Cassie and Digit designed by Agility Robotics.\n\n[5] Towards Safe Locomotion Navigation in Partially Observable Environments with Uneven Terrain This study proposes an integrated task and motion planning method for dynamic locomotion in partially observable environments with multi-level safety guarantees. This layered planning framework is composed of a high-level symbolic task planner and a low-level phase-space motion planner. A belief abstraction at the task planning level enables belief estimation of dynamic obstacle locations and guarantees navigation safety with collision avoidance. The high-level task planner, i.e., a two-level navigation planner, employs linear temporal logic for a reactive game synthesis between the robot and its environment while incorporating low-level safe keyframe policies into formal task specification design. The synthesized task planner commands a series of locomotion actions including walking step length, step height, and heading angle changes, to the underlying keyframe decision-maker, which further determines the robot center-of-mass apex velocity keyframe. The low-level phase-space planner uses a reduced-order locomotion model to generate non-periodic trajectories meeting balancing safety criteria for straight and steering walking. These criteria are characterized by constraints on locomotion keyframe states, and are used to define keyframe transition policies via viability kernels. Simulation results of a Cassie bipedal robot designed by Agility Robotics demonstrate locomotion maneuvering in a three-dimensional, partially observable environment consisting of dynamic obstacles and uneven terrain.\n\n[6] Towards Safe Locomotion Navigation in Partially Observable Environments with Uneven Terrain The belief structure enables the robot to navigate around obstacles while still guaranteeing that the dynamic obstacle is not in the immediate non-visible vicinity. Fig. (7(a)) depicts a snapshot of a simulation where the robot must navigate around such an obstacle to reach its goal states. A successful strategy can be synthesized only when using belief region abstraction.Without explicitly tracking possible non-visible obstacle locations, the task planner believes the obstacle could be in any non-visible gray cell when it is out of sight. The planneris not able to synthesize a strategy that would allow the robot to advance, because it can not guarantee that the obstacle isn’t immediately behind the wall.Fig. (7(b)) depicts a potential collision that could occur.This comparison underlines the significance of the belief abstraction approach.\n\n[7] Integrated Task and Motion Planning for Safe Legged Navigation in Partially Observable Environments Motion planning in complex environments has been extensively studied, with a spectrum of approaches in the literature, 18, 19, 20, 21. Reactive methods for motion planning with formal guarantees are widely studied with the methods of safety barrier certificates, artificial potential functions, and more recent extensions, 24. While the work in provides convergence guarantees and obstacle avoidance in geometrically complicated unknown environments, it is restricted to static obstacles and has only been demonstrated on a fully actuated particle, or a simple unicycle model for a quadruped. Whereas the framework we present here is able to generate safe locomotion plans reacting to environmental events that include multiple, possibly adversarial, dynamic obstacles with formal guarantees on task completion and safety. Moreover, our framework is validated on a bipedal robot Digit.\n\n[8] Safe Motion Planning under Partial Observability with an Optimal Deterministic Planner Safe Motion Planning under Partial Observability with an Optimal Deterministic Planner This paper derives a principled framework for efficiently and safely navigating partially observable multi-agent systems using an optimal deterministic planner. This is accomplished by decoupling the navigation problem into independent collision avoidance and guidance problems and by providing mechanisms for solving both efficiently. While the framework does forgo global optimality in order to compute solutions, we argue that such optimality is unattainable in practice due to intractability, so nothing is actually sacrificed. An example solution is demonstrated for a novel graph traversal problem using a deterministic, single-agent velocity profile planner in a partially observable, multi-agent setting.\n\n[9] A Hybrid Strategy for Safe Navigation of Mobile Robots in Partially-Known Dynamic Environments A Hybrid Strategy for Safe Navigation of Mobile Robots in Partially-Known Dynamic Environments This paper proposes a new hybrid strategy for the navigation of mobile robots. It considers vehicles operating in partially-known/dynamic environments. The proposed strategy combines both global path planning and reactive approaches to achieve safe navigation. This hybrid approach provides a better solution for the safe navigation problem by attaining the advantages of both global planning and reactive navigation to overcome the shortcomings of relying purely on one of them. Such advantages are escaping potential trapping situations and reacting quickly to perceived unknown and dynamic obstacles. Computer simulations are carried out to confirm the performance of the proposed approach.\n\n[10] Dynamic Control Barrier Function-based Model Predictive Control to Safety-Critical Obstacle-Avoidance of Mobile Robot This paper proposes a safety-critical method for static and dynamic obstacles avoidance based on LiDAR sensor. Firstly, we adopt a robust local perception module, which perceives and parameterizes obstacles to MBEs, and predicts obstacles trajectory considering uncertainty based on Kalman filter. Then, we combine Dynamic Control Barrier Function and Model Predictive Control framework to generate a safe collision-free trajectory for robot navigation in dynamic environment.Real-world experiments avoiding different types of obstacles validate that our method is robust, safe, and allows running all algorithms on-board.Moreover, we compare our method with four baseline approaches (MPC, MPC-CBF, MPC-KF, and MPC-CBF-curvefit) in simulation. The experimental results show that our method is more safe and efficient than other methods in the process of obstacle avoidance.\n\n[11] Dynamic Control Barrier Function-based Model Predictive Control to Safety-Critical Obstacle-Avoidance of Mobile Robot To address these issues, this paper presents an onboard lidar-based approach for safe navigation of vehicles in a dynamic environment. We detect obstacles based on real-time point cloud, perform the clustering operation, and then parameterize the obstacles as MBEs. To deal with the uncertainty of MBE, we summarize the influence of MBE shape parameters on MBE center position parameters, and use KF to realize the stable estimation of obstacles. Uncertainty is used to extend the obstacle to enhance safety. For motion planning in a dynamic environment, we propose D-CBF-based MPC to obtain a safe collision-free trajectory.\n\n[12] A Hybrid Approach for Autonomous Collision-Free UAV Navigation in 3D Partially Unknown Dynamic Environments In the past decades, unmanned aerial vehicles (UAVs) have emerged in a wide range of applications. Owing to the advances in UAV technologies related to sensing, computing, power, etc., it has become possible to carry out missions autonomously. A key component to achieving this goal is the development of safe navigation methods, which is the main focus of this work. A hybrid navigation approach is proposed to allow safe autonomous operations in three-dimensional (3D) partially unknown and dynamic environments. This method combines a global path planning algorithm, namely RRT-Connect, with a reactive control law based on sliding mode control to provide quick reflex-like reactions to newly detected obstacles. The performance of the suggested approach is validated using simulations.\n\n[13] Safe Navigation in Unstructured Environments by Minimizing Uncertainty in Control and Perception This paper proposes a framework for autonomous vehicle navigation in unstructured environments. To this end, we employ an uncertainty-aware learning-based dynamics model and a self-supervised traversability estimation model. The vehicle dynamics model is trained through a model-based reinforcement learning framework that quantifies epistemic uncertainty. After learning the dynamics model, we generate an uncertainty-aware traversability cost map. Using the self-supervised traversability cost of terrains derived from interaction experiences, the traversability model is capable of online adaptation to minimize uncertainty. We integrate the dynamics model and traversability cost map with a sampling-based model predictive controller. The controller allows for safe navigation by optimizing the trajectory to avoid regions and actions with high uncertainty.\n\n[14] FASTER: Fast and Safe Trajectory Planner for Navigation in Unknown Environments FASTER: Fast and Safe Trajectory Planner for Navigation in Unknown Environments Planning high-speed trajectories for UAVs in unknown environments requires algorithmic techniques that enable fast reaction times to guarantee safety as more information about the environment becomes available. The standard approaches that ensure safety by enforcing a\"stop\"condition in the free-known space can severely limit the speed of the vehicle, especially in situations where much of the world is unknown. Moreover, the ad-hoc time and interval allocation scheme usually imposed on the trajectory also leads to conservative and slower trajectories. This work proposes FASTER (Fast and Safe Trajectory Planner) to ensure safety without sacrificing speed. FASTER obtains high-speed trajectories by enabling the local planner to optimize in both the free-known and unknown spaces. Safety is ensured by always having a safe back-up trajectory in the free-known space. The MIQP formulation proposed also allows the solver to choose the trajectory interval allocation. FASTER is tested extensively in simulation and in real hardware, showing flights in unknown cluttered environments with velocities up to 7.8m/s, and experiments at the maximum speed of a skid-steer ground robot (2m/s). I. INTRODUCTION D ESPITE its numerous applications, high-speed UAV navigation through unknown environments is still an open problem. The desired high speeds together with partial observability of the environment and limits on payload weight make this task especially challenging for aerial robots. Safe operation, in addition to flying fast, is also critical but difficult to guarantee since the vehicle must repeatedly generate collision-free, dynamically feasible trajectories in real-time with limited sensing. Similar to\n\n"}
{"case_id": "40ae2e68a52e2d5788f3da27f5d9decd", "answer_text": "\n\nRRT-based (Rapidly-exploring Random Tree) path planning algorithms have been extensively employed for robot navigation due to their ability to efficiently explore high-dimensional configuration spaces [1]. However, their performance can degrade significantly in environments containing narrow passages [1]. To address this challenge, several approaches have been proposed to enhance the efficiency of RRT-based methods in such scenarios [0]. \n\n**1. Adaptive Sampling Strategies** [14]\nOne common approach to improve RRT performance in narrow passages is to adapt the sampling strategy. The adaptive roadmap guided Multi-RRT strategy allocates computational resources proportionally to the geometric complexity of the local region. It employs an improved Bridge Test algorithm to identify global important roadmaps in narrow passages, allowing multiple RRTs to grow from these points and explore difficult sub-regions [8]. This adaptive sampling reduces the problem of aimless path searching and increases the likelihood of finding a path through narrow passages. However, this method might require significant computational resources to perform the dynamic adjustments needed for effective sampling.\n\n**2. Hybrid Sampling Methods**[1][3] \n\n\nHybrid sampling strategies combine different techniques to optimize the performance of RRT in complex environments. The multiple RRTs-based path planning framework, for instance, integrates the Randomized Star Builder (RSB) method and uniform sampling. The RSB efficiently recognizes narrow passage regions, avoiding unnecessary samples in corners and generating milestones for growing multiple local trees from these regions. Uniform sampling, on the other hand, helps in capturing global connectivity by generating global RRT trees [1][3]. According to [3], this approach, when applied to 3D flying robots, resulted in significant improvements in path-finding efficiency and success rates. A potential limitation of this hybrid approach could be its dependency on the proper tuning of parameters for both RSB and uniform sampling to balance local exploration and global connectivity.\n\n**3. Deterministic Planning for Narrow Passages** [2]\nAnother effective approach is switching to deterministic planning methods when dealing with narrow passages. This method utilizes the advantages of deterministic algorithms like A* in small and narrow regions. For example, the MIS-BiRRT (Multiple Importance Sampling Bidirectional RRT) integrates multiple importance sampling with bidirectional RRT. The transition to a deterministic planning method is triggered when the RRT growth path reaches the entrance of a narrow passage. At this critical juncture, the methodology switches to a deterministic path-planning algorithm such as A* to quickly and efficiently navigate through the narrow passage [5][6]. This approach not only expedites the search process but also improves path optimality in complex environments by leveraging the strengths of both stochastic and deterministic planning methods. The limitation here is the potential computational overhead of switching between algorithms, which might affect performance in real-time applications.\n\n**4. Enhanced Sampling and Path Growth Techniques**\n\nSampling-based methods such as Rapidly Exploring Random Trees (RRT) and Probabilistic Road Maps (PRM) have been recognized as effective tools to solve the path planning problem for both ground mobile robots and flying robots in high-dimensional configuration space. However, the efficiency of the RRT planner will be decreased in complex environments with narrow passages. [1] The RSB method can efficiently recognize narrow passage regions while avoiding unnecessary samples in the corners and dead ends, and generates milestones for growing multiple local trees from narrow passages. [1]\n\nThe concept of dynamic sampling space is also proposed, where the growth of the RRT tree is concentrated within this sampling space to increase the probability of finding the exit of the narrow channel. Although the improvements in waypoint sampling could assist the RRT methodologies to plan a path through a narrow passage. [0] \nSeveral enhancements to the sampling and path growth techniques have also been proposed. For instance, the Fast-RRT algorithm introduces a Fast-Sampling strategy that samples only in the unreached space of the random tree, improving search speed and stability. Additionally, a Random Steering strategy is employed to address performance issues in narrow channels by allowing the random tree to pivot around narrow regions and efficiently explore them [7]. Similarly, the RJ-RRT algorithm proposes a greedy sampling space reduction strategy to decrease redundant expansion and introduces a new narrow passage judgment method that generates multiple sub-trees within the passage. These sub-trees can then be merged into the main tree, facilitating efficient exploration [13]. According to [7], Fast-RRT demonstrated a 20-fold increase in speed over RRT* in benchmark tests. However, these methods may still struggle in environments with extremely complex and dense obstacle configurations.\n\n**5. Closed-Form Geometric Representations** [10]\nUsing closed-form geometric representations can also facilitate efficient path planning in narrow passages. By parameterizing configuration-space obstacles geometrically using strictly convex bodies (e.g., superquadrics) and combining this with stochastic sampling strategies, planners can avoid sampling infeasible configurations and ensure collision-free robot transitions [10]. Specific case studies, such as the one involving a humanoid robot navigating cluttered environments, demonstrate these benefits. Benchmarks indicated that this method not only reduced computational time but also increased the success rate in navigating narrow corridors compared to traditional sampling-based planners [10]. The potential drawback is the complexity of deriving and implementing such closed-form geometric representations for diverse and irregularly shaped environments.\n\n**6. Multi-RRT and Multi-Query Approaches** [1].\nMulti-RRT approaches, such as Spark PRM, use multiple trees or roadmaps to explore complex environments. This method sparks the growth of RRTs from narrow passage samples generated by a PRM, allowing for efficient exploration of narrow passages until they are fully mapped. This multi-RRT framework can be applied to various PRM variants and has shown significant speedups in computation time over traditional RRT methods [11]. Specifically, Spark PRM demonstrated notable speed improvements over other RRT and PRM variants in high-dimensional scenarios. However, the overhead of managing and synchronizing multiple RRTs or roadmaps might introduce additional complexity and computational demands.\n\n\n**Comparative Analysis with Other Popular Path-Planning Methods** [1].\n\nWhile the discussed techniques significantly enhance the efficiency of RRT-based planners in narrow passages, comparing them with other popular path-planning methods provides broader context. Probabilistic Roadmaps (PRMs), for example, also struggle with narrow passages but benefit from strategies like Spark PRM, which leverages RRT within PRM to improve narrow passage exploration [11]. Meanwhile, A* and D* Lite algorithms, known for grid-based deterministic planning, excel in small, well-defined spaces but often lack efficiency in high-dimensional or dynamically changing environments. By contrast, RRT-based methods, especially with the discussed enhancements, strike a balance between global exploration and local optimization, making them versatile for diverse and complex settings [5][6]. However, deterministic planners like A* provide optimality guarantees in less complex scenarios, which is a trade-off to consider when selecting an approach.\n\nIn conclusion, enhancing RRT-based path planning to handle narrow pathways involves a variety of strategies, including adaptive and hybrid sampling methods [8], switching to deterministic algorithms in critical regions [2], improving sampling and path growth techniques [1], utilizing closed-form geometric representations [10], and employing multi-RRT approaches [5]. These methods collectively address the inherent challenges posed by narrow passages, improving the efficiency and success rate of RRT-based planners in complex environments [4]. However, each method has its own limitations, and careful consideration of these potential drawbacks is crucial for effective implementation. \n\nIncluding diagrams or visual aids explaining the strategies could aid understanding of complex concepts like adaptive sampling and multi-RRT methods [1].\n\n\nReferences:\n[0] Efficient Path Planning for a Microrobot Passing through Environments with Narrow Passages RRT path in growing toward the narrow passages. These works could successfully plan a path passing through the rooms where only the entrance of each room is considered a narrow passage. However, when a narrow passage is thin and long, these related works may still hard find a path through this kind of narrow passage. Due to the characteristics of RRT uniform The related works of RRT improvement mentioned above can assist the RRT path in growing toward the narrow passages. These works could successfully plan a path passing through the rooms where only the entrance of each room is considered a narrow passage. However, when a narrow passage is thin and long, these related works may still hard find a path through this kind of narrow passage. Due to the characteristics of RRT uniform sampling, the search range easily affects the path-searching efficiency. The adaptive RRT draws several points around a waypoint of the RRT tree, which is close to a narrow passage, to find the feasible extension of the RRT tree through the narrow passage. The principal component analysis could be utilized to sample the space around a narrow passage for efficiently expanding the RRT tree through it. The concept of dynamic sampling space is also proposed, where the growth of the RRT tree is concentrated within this sampling space to increase the probability of finding the exit of the narrow channel. Although the improvements in waypoint sampling could assist the RRT methodologies to plan a path through a narrow passage,\n\n[1] Extended RRT-based path planning for flying robots in complex 3D environments with narrow passages Sampling-based methods such as Rapidly Exploring Random Trees (RRT) and Probabilistic Road Maps (PRM) have been recognized as effective tools to solve the path planning problem for both ground mobile robots and flying robots in high-dimensional configuration space. However, the efficiency of the RRT planner will be decreased in complex environments with narrow passages. This paper presents a multiple RRTs-based path planning framework to improve the above mentioned problem. The key ingredient of the framework is a hybrid sampling strategy which takes advantage of the Randomized Star Builder (RSB) and uniform sampling. The RSB method can efficiently recognize narrow passage regions while avoiding unnecessary samples in the corners and dead ends, and generates milestones for growing multiple local trees from narrow passages. Moreover, uniform sampling is used to generate global RRT trees in order to capture global connectivity. Simulation results of 3D flying robots demonstrate the effectiveness of the proposed method. Comparisons between the proposed method and other RRT-based planners are also presented.\n\n[2] Efficient Path Planning for a Microrobot Passing through Environments with Narrow Passages the process of iterative sampling through a long narrow passage would be quite time-consuming. We guided the RRT algorithm to quickly generate such a path to a narrow passage and then used an effective method to deterministically plan a path through it. Since the A* algorithm generates good path-planning results in small and narrow regions, the path-planning algorithm employed here was switched to this kind of deterministic method for efficiently finding a path in the narrow passage. In addition to the problem of crossing narrow passages, the path-searching process of traditional RRT methodology is often aimless due to its use of random sampling in complex environments. Even if the last generated waypoint is close to the target point, random sampling of the entire space could easily result in the next waypoint being far away from the target. This kind of aimless sampling during path searching makes path planning inefficient and consumes considerable time, especially in vast and complex environments. Therefore, several improvements were proposed for the waypoint sampling of RRT so that unnecessary searching and computational time can be reduced. The rapidly exploring random disjointed tree* (RRdT*) establishes a Markov chain to consider the direction of path growth according to the growth of the previous waypoint. Peng et al. utilized particle swarm optimization (PSO) to adjust waypoint sampling according to the path growth of RRT and apply it to the path planning of multi-UAVs. In addition, multi-RRT incorporated a sampling mechanism named Goal Bias, which considers the position of the target point and\n\n[3] Extended RRT-based path planning for flying robots in complex 3D environments with narrow passages Extended RRT-based path planning for flying robots in complex 3D environments with narrow passages Sampling-based methods such as Rapidly Exploring Random Trees (RRT) and Probabilistic Road Maps (PRM) have been recognized as effective tools to solve the path planning problem for both ground mobile robots and flying robots in high-dimensional configuration space. However, the efficiency of the RRT planner will be decreased in complex environments with narrow passages. This paper presents a multiple RRTs-based path planning framework to improve the above mentioned problem. The key ingredient of the framework is a hybrid sampling strategy which takes advantage of the Randomized Star Builder (RSB) and uniform sampling. The RSB method can efficiently recognize narrow passage regions while avoiding unnecessary samples in the corners and dead ends, and generates milestones for growing multiple local trees from narrow passages. Moreover, uniform sampling is used to generate global RRT trees in order to capture global connectivity. Simulation results of 3D flying robots demonstrate the effectiveness of the proposed method. Comparisons between the proposed method and other RRT-based planners are also presented.\n\n[4] Efficient Path Planning in Narrow Passages for Robots With Ellipsoidal Components Path planning has long been one of the major research areas in robotics, with probabilistic roadmap (PRM) and rapidly-exploring random trees (RRT) being two of the most effective classes of planners. Though generally very efficient, these sampling-based planners can become computationally expensive in the important case of “narrow passages.” This article develops a path planning paradigm specifically formulated for narrow passage problems. The core is based on planning for rigid-body robots encapsulated by unions of ellipsoids. Each environmental feature is represented geometrically using a strictly convex body with a $\\mathcal {C}^{1}$ boundary (e.g., superquadric). The main benefit of doing this is that configuration-space obstacles can be parameterized explicitly in closed form, thereby allowing prior knowledge to be used to avoid sampling infeasible configurations. Then, by characterizing a tight volume bound for multiple ellipsoids, robot transitions involving rotations are guaranteed to be collision free without needing to perform traditional collision detection. Furthermore, by combining with a stochastic sampling strategy, the proposed planning framework can be extended to solving higher dimensional problems, in which the robot has a moving base and articulated appendages. Benchmark results show that the proposed framework often outperforms the sampling-based planners in terms of computational time and success rate in finding a path through narrow corridors for both single-body robots and those with higher dimensional configuration spaces. Physical experiments using the proposed framework are further demonstrated on a humanoid robot that walks in several cluttered environments with narrow passages.\n\n[5] Efficient Path Planning for a Microrobot Passing through Environments with Narrow Passages accelerates the path growth of RRT toward the target point. Multi-RRT also draws multiple path growth directions simultaneously to increase the search range. Liu et al. sampled new waypoints to avoid obstacles by detecting the obstacle edge when it was close to the growth path. In this paper, we integrated the concepts of several waypoint sampling mechanisms to guide the path planning of a microrobot. Specifically, this paper proposes a multiple importance sampling bidirectional rapidly exploring random tree (MIS-BiRRT) to remedy the problem of RRT path planning through narrow passages that suffer from aimless path searching. The environmental map is first considered as an image for detecting the feature points of obstacles and narrow passages. The multiple importance sampling mechanism is used to draw the waypoints of a growing RRT path toward the target point, toward the entrance of a narrow passage, or so as to avoid obstacles. When the path grows into the entrance of a narrow passage, the RRT methodology switches to the deterministic path planning method to quickly obtain the optimal path through the narrow passage. The rest of this paper is structured as follows. The path planning problem is first formulated, and the path planning plus image feature detection for narrow passages is explained in Section 2. Section 3 describes the multiple importance sampling mechanism and multiple new waypoints generation of the proposed MIS-BiRRT. The simulation results and comparisons are presented in Section 4 to verify the efficiency of the proposed MIS-BiRRT. Finally, the conclusions are given in Section 5. Problem\n\n[6] Efficient Path Planning for a Microrobot Passing through Environments with Narrow Passages Efficient Path Planning for a Microrobot Passing through Environments with Narrow Passages This paper presents an efficient path-planning algorithm for microrobots attempting to pass through environments with narrow passages. Because of the extremely small size of a microrobot, it is suitable for work in this kind of environment. The rapidly exploring random tree (RRT) algorithm, which uses random sampling points, can quickly explore an entire environment and generate a sub-optimal path for a robot to pass through it; however, the RRT algorithm, when used to plan a path for a microrobot passing through an environment with narrow passages, has the problem of being easily limited to local solutions when it confronts with a narrow passage and is unable to find the final path through it. In light of this, the objectives of the considered path planning problem involve detecting the narrow passages, leading the path toward an approaching narrow passage, passing through a narrow passage, and extending the path search more efficiently. A methodology was proposed based on the bidirectional RRT in which image processing is used to mark narrow passages and their entrances and exits so that the bidirectional RRT can be quickly guided to them and combined with the deterministic algorithm to find paths through them. We designed the methodology such that RRT generates the sampling points for path growth. The multiple importance sampling technique is incorporated with bidirectional RRT, named MIS-BiRRT, to make the path grow faster toward the target point and narrow passages while avoiding obstacles. The proposed algorithm also considers multiple\n\n[7] Fast-RRT: A RRT-Based Optimal Path Finding Method As a sampling-based pathfinding algorithm, Rapidly Exploring Random Trees (RRT) has been widely used in motion planning problems due to the ability to find a feasible path quickly. However, the RRT algorithm still has several shortcomings, such as the large variance in the search time, poor performance in narrow channel scenarios, and being far from the optimal path. In this paper, we propose a new RRT-based path find algorithm, Fast-RRT, to find a near-optimal path quickly. The Fast-RRT algorithm consists of two modules, including Improved RRT and Fast-Optimal. The former is aims to quickly and stably find an initial path, and the latter is to merge multiple initial paths to obtain a near-optimal path. Compared with the RRT algorithm, Fast-RRT shows the following improvements: (1) A Fast-Sampling strategy that only samples in the unreached space of the random tree was introduced to improve the search speed and algorithm stability; (2) A Random Steering strategy expansion strategy was proposed to solve the problem of poor performance in narrow channel scenarios; (3) By fusion and adjustment of paths, a near-optimal path can be faster found by Fast-RRT, 20 times faster than the RRT* algorithm. Owing to these merits, our proposed Fast-RRT outperforms RRT and RRT* in both speed and stability during experiments.\n\n[8] An adaptive roadmap guided Multi-RRTs strategy for single query path planning An adaptive roadmap guided Multi-RRTs strategy for single query path planning During the past decade, Rapidly-exploring Random Tree (RRT) and its variants are shown to be powerful sampling based single query path planning approaches for robots in high-dimensional configuration space. However, the performance of such tree-based planners that rely on uniform sampling strategy degrades significantly when narrow passages are contained in the configuration space. Given the assumption that computation resources should be allocated in proportion the geometric complexity of local region, we present a novel single-query Multi-RRTs path planning framework that employs an improved Bridge Test algorithm to identify global important roadmaps in narrow passages. Multiple trees can grown from these sampled roadmaps to explore sub-regions which are difficult to reach. The probability of selecting one particular tree for expansion and connection, which can dynamically updated by on-line learning algorithm based on the historic results of exploration, guides the tree through narrow passage rapidly. Experimental results show that the proposed approach gives substantial improvement in planning efficiency over a wide range of single-query path planning problems.\n\n[9] Efficient Path Planning in Narrow Passages via Closed-Form Minkowski Operations Efficient Path Planning in Narrow Passages via Closed-Form Minkowski Operations Path planning has long been one of the major research areas in robotics, with PRM and RRT being two of the most effective classes of path planners. Though generally very efficient, these sampling-based planners can become computationally expensive in the important case of “narrow passages”. This paper develops a path planning paradigm specifically formulated for narrow passage problems. The core is based on planning for rigid-body robots encapsulated by unions of ellipsoids. The environmental features are enclosed geometrically using convex differentiable surfaces (e.g., superquadrics). The main benefit of doing this is that configuration-space obstacles can be parameterized explicitly in closed form, thereby allowing prior knowledge to be used to avoid sampling infeasible configurations. Then, by characterizing a tight volume bound for multiple ellipsoids, robot transitions involving rotations are guaranteed to be collision-free without traditional collision detection. Furthermore, combining the stochastic sampling strategy, the proposed planning framework can be extended to solving higher dimensional problems in which the robot has a moving base and articulated appendages. Benchmark results show that, remarkably, the proposed framework outperforms the popular sampling-based planners in terms of computational time and success rate in finding a path through narrow corridors and in higher dimensional configuration spaces. I. INTRODUCTION Sampling-based planners such as PRM and RRT (and a multitude of their extensions, e.g.,,) have demonstrated remarkable success in solving complex robot motion planning problems. These frameworks generate state samples randomly and perform explicit collision detection to assess their feasibility. These\n\n[10] Spark PRM: Using RRTs within PRMs to efficiently explore narrow passages Probabilistic RoadMaps (PRMs) have been successful for many high-dimensional motion planning problems. However, they encounter difficulties when mapping narrow passages. While many PRM sampling methods have been proposed to increase the proportion of samples within narrow passages, such difficult planning areas still pose many challenges. We introduce a novel algorithm, Spark PRM, that sparks the growth of Rapidly-expanding Random Trees (RRTs) from narrow passage samples generated by a PRM. The RRT rapidly generates further narrow passage samples, ideally until the passage is fully mapped. After reaching a terminating condition, the tree stops growing and is added to the roadmap. Spark PRM is a general method that can be applied to all PRM variants. We study the benefits of Spark PRM with a variety of sampling strategies in a wide array of environments. We show significant speedups in computation time over RRT, Sampling-based Roadmap of Trees (SRT), and various PRM variants.\n\n[11] RJ-RRT: Improved RRT for Path Planning in Narrow Passages RJ-RRT: Improved RRT for Path Planning in Narrow Passages As a representative of sampling-based planning algorithms, rapidly exploring random tree (RRT), is extensively welcomed in solving robot path planning problems due to its wide application range and easy addition of nonholonomic constraints. However, it is still challenging for RRT to plan the path for configuration space with narrow passages. As a variant algorithm of RRT, rapid random discovery vine (RRV) gives a better solution, but when configuration space contains more obstacles instead of narrow passages, RRV performs slightly worse than RRT. In order to solve these problems, this paper re-examines the role of sampling points in RRT. Firstly, according to the state of the random tree expanding towards the current sampling point, a greedy sampling space reduction strategy is proposed, which decreases the redundant expansion of the random tree in space by dynamically changing the sampling space. Secondly, a new narrow passage judgment method is proposed according to the environment around of sampling point. After the narrow passage is identified, the narrow passage is explored by generating multiple subtrees inside the passage. The subtrees can be merged into the main tree that expands in a larger area by subsequent sampling. These improvements further enhance the value of sampling points. Compared with the existing RRT algorithms, the adaptability for different environments is improved, and the planning time and memory usage are saved.\n\n[12] An Adaptive Rapidly-Exploring Random Tree Sampling-based planning algorithms play an important role in high degree-of-freedom motion planning (MP) problems, in which rapidly-exploring random tree (RRT) and the faster bidirectional RRT (named RRT-Connect) algorithms have achieved good results in many planning tasks. However, sampling-based methods have the inherent defect of having difficultly in solving planning problems with narrow passages. Therefore, several algorithms have been proposed to overcome these drawbacks. As one of the improved algorithms, Rapidly-exploring random vines (RRV) can achieve better results, but it may perform worse in cluttered environments and has a certain environmental selectivity. In this paper, we present a new improved planning method based on RRT-Connect and RRV, named adaptive RRT-Connect (ARRT-Connect), which deals well with the narrow passage environments while retaining the ability of RRT algorithms to plan paths in other environments. The proposed planner is shown to be adaptable to a variety of environments and can accomplish path planning in a short time.\n\n"}
{"case_id": "71b0b99a7954bf3dadd8cfd30ece359e", "answer_text": "\n\nVisio-tactile, piezoelectric, and triboelectric tactile sensors each offer unique advantages and face distinct challenges, influencing their suitability for various robotic applications [1]. Below, we provide a comprehensive overview of the advantages and disadvantages of each type of sensor, drawing from multiple sources, along with specific applications for each sensor type.\n\n\n\n### Visio-Tactile Sensors\n\n**Visio-tactile sensors** are essential for enhancing the interactive capabilities of robots, particularly in mimicking human-like tactile perception and interaction with the environment. \n\nTactile sensors, which endue industrial robots with the ability of tactile sensation, are vital components to make them more intelligent [1]. \n\nFlexible tactile sensors based on piezoelectric mechanisms have high sensitivity and fast response ability, so they are widely used to detect dynamically changing tactile signals [5]. \n\nAmong them, flexible tactile sensors based on piezoelectric mechanisms have high sensitivity and fast response ability, so they are widely used to detect dynamically changing tactile signals [12]. \n\nIn addition, piezoelectric tactile sensors can convert external mechanical pressure into electrical energy, which makes piezoelectric tactile sensors particularly important in self-powered applications [5].\n\n\n**Advantages:**\n\nPiezoelectric-based tactile sensors are highly sensitive, have a high dynamic range, and a high frequency response. They can be successfully down-scaled through advanced micro-electro-mechanical systems (MEMS) technology [0]. \n\nThese sensors are widely used in robotic applications due to their fast response ability and ability to convert external mechanical pressure into electrical energy, making them particularly important in self-powered applications [5].\n\nMoreover, piezoelectric materials such as polyvinylidene fluoride (PVDF) exhibit good flexibility, stable chemical properties, and low cost, which are advantageous for fabricating tactile sensors [5].\n\nAdditionally, piezoelectric sensors are characterized by high resolution, low power consumption, and ease of fabrication, making them suitable for overlapping large areas and curved surfaces [8]. \n\nFurthermore, because piezoelectric tactile sensors are immune to electromagnetic interference, they can be effectively integrated with minimally invasive surgical tools and magnetic resonance imaging (MRI) environments [6]. \n\n\n1. **High Spatial Resolution and Sensitivity:** Visio-tactile sensors achieve high spatial resolution and sensitivity by utilizing cameras and image processing techniques to monitor contact areas. This high resolution allows for detailed texture mapping and object recognition, making these sensors well-suited for complex tasks requiring precision [3][9].\n2. **Immunity to Electromagnetic Disturbances:** These sensors are immune to electromagnetic interference, which makes them suitable for use in environments where other sensors might face performance issues [3].\n3. **Versatility:** The use of cameras allows visio-tactile sensors to integrate seamlessly with systems already employing visual input, thereby enhancing multi-modal sensing capabilities. This versatility makes them ideal for applications requiring data fusion from multiple sensory modalities [9].\n\n**Disadvantages:**\n\nHowever, piezoelectric tactile sensors are not able to detect static forces, because the impedance of a piezoelectric film is related to the decay of a developed charge under an applied force [0]. Additionally, robustness and sensitivity to temperature are lacking [2].\n1. **Complexity and Cost:** The integration of cameras and advanced image processing algorithms increases the complexity and cost of these sensors, which might limit their adoption in cost-sensitive applications [9].\n2. **Latency and Processing Lag:** The time required for image capture and processing can introduce latency, which might not be acceptable for high-speed or real-time applications [3].\n3. **Lighting Dependence:** These sensors often depend on adequate lighting conditions for accurate image capture, which can be a limitation in dim or variable lighting environments [9].\n\nApplications: require both accurate measurement of applied force, and the possibility of direct attachment onto the surface of objects such as organs or skin for wearable and implantable devices [0]. Multiple approaches to artificial tactile sensors were investigated, using methods based on piezoresistive, capacitive, inductive, and piezoelectric effects [1]. Each of the sensing methodologies has advantages and disadvantages in tactile sensing; however, among the sensing approaches, piezoelectric-based tactile sensors have particularly high sensitivity, a high dynamic range, and a high frequency response [0]. Piezoelectric tactile sensors can be successfully down-scaled through advanced micro-electro-mechanical systems (MEMS) technology [0]. A high piezoelectric constant and a low dielectric constant are essential to achieve high sensitivity for piezoelectric MEMS-based devices [0]. Among various piezoelectric thin films, such as aluminum nitride (AlN), ZnO, BaTiO3, lead zirconate titanate (Pb(Zr,Ti)O3, PZT) and polyvinylidenefluoride (PVDF), PZT is the most popular due to its high piezoelectric constant, high energy density, and large electrical-mechanical coupling coefficients [0]. However, the pressure/touch sensitivity of thin film PZT with regard to voltage response is slightly smaller than that of AlN [0]. \n\n- **Robotic Manipulation:** Due to their high spatial resolution, visio-tactile sensors can be utilized in robotic hands and grippers for precise manipulation tasks [3]. \n- **Object Recognition:** Detailed texture mapping facilitates better object recognition, useful in search-and-rescue missions [2]. \n- **Medical Robotics:** These sensors enhance precision during minimally invasive surgical procedures[6].\n\nPiezoelectric Tactile Sensors[^0]. [^1]. [^2]. [^4]. [^5]. [^7]. [^10]. [^11] \n\n**Advantages:** Piezoelectric-based tactile sensors have particularly high sensitivity, a high dynamic range, and a high frequency response [0]. The frequency response of piezoelectric tactile sensors is good, and their measuring range is large [1]. Piezoelectric tactile sensors are flexible, sensitive, and suitable for large-area applications, which is important for the fabrication of industry tactile sensors [2]. They are also self-powered, converting external mechanical pressure into electrical energy [5]. Moreover, piezoelectric tactile sensors possess high sensitivity, improved hysteresis, excellent repeatability, and high durability [12]. Their advantages include high temperature resistance and insensitivity to external electric and magnetic fields [14].\n1. **High Sensitivity and Frequency Response:** Piezoelectric sensors exhibit high sensitivity and quick response times, making them ideal for detecting dynamic changes and rapid tactile events. These characteristics are crucial for applications such as robotic manipulation and human-robot interaction [0][1][4].\n2. **Self-Powered Operation:** These sensors can convert mechanical pressure directly into electrical signals, enabling self-powered operation and reducing the need for external power sources. This feature is particularly valuable in autonomous or portable robotic systems [1][12].\n3. **Compact and Flexible:** Advances in materials such as polyvinylidene fluoride (PVDF) and lead zirconate titanate (PZT) have enabled the development of compact, flexible piezoelectric sensors that can be easily integrated into various robotic components [0][5][12].\n\n\n**Disadvantages:**\n\nPiezoelectric tactile sensors are not able to detect static forces because the impedance of a piezoelectric film is related to the decay of a developed charge under an applied force. In addition, the spatial resolution of piezoelectric tactile sensors using typical bulky piezoelectric ceramics is not as high as that of piezoresistive sensors due to the complexity of the fabrication process, which limits sensors with low operating power consumption [0]. The robustness and sensitivity to temperature are lacking in piezoelectric tactile sensors [2]. Piezoelectric tactile sensors also have inherent drawbacks, the greatest one of which is the inability to measure static contact forces because the induced charge in piezoelectric materials dissipates very quickly. This means piezoelectric tactile sensors cannot be effective in static environments [6].\n\n1. **Inability to Detect Static Forces:** Piezoelectric sensors cannot measure static forces effectively, as the induced charge dissipates quickly. This limitation restricts their use in applications requiring the measurement of sustained pressures [0][2][6].\n2. **Temperature Sensitivity and Lack of Robustness:** Piezoelectric materials can be sensitive to temperature changes, which can affect measurement accuracy and reliability. Additionally, these sensors might lack the robustness needed for harsh environments [2][6].\n3. **Complex Fabrication Requirements:** The fabrication of piezoelectric sensors can be complex, requiring precise material deposition and layering processes, which can increase production costs and complexity [0]. \n\n\nApplications for flexible tactile sensors encompass a wide range of fields, including wearable and implantable devices, soft robotics, human-robot interaction, and industrial robots. Specifically, piezoelectric-based tactile sensors are notable for their high sensitivity, dynamic response, and self-powered capabilities. They are used in applications requiring accurate force measurements and direct attachment to surfaces such as organs or skin [0]. Additionally, these sensors are employed in the development of artificial skins for robots, enhancing their ability to interact with their environment [5].\n\n- **Dynamic Force Sensing:** Ideal for applications requiring rapid detection of force changes, such as robotic hands in manufacturing lines.[5]\n - **Wearable Robotics:** Due to their flexibility and compact size, they are used in exoskeletons and wearable robots for real-time feedback [5]. \n - **Human-Robot Interaction:** Piezoelectric sensors enable high-fidelity touch sensing in service robots to ensure safe and efficient interaction with humans [1]. \n\n\n\n### Triboelectric Tactile Sensors [13].\n\n\n\nAdvantages:\n\nPiezoelectric-based tactile sensors have particularly high sensitivity, a high dynamic range, and a high frequency response [0]. They are also known for their ability to convert external mechanical pressure into electrical energy, making them important for self-powered applications [5]. Additionally, piezoelectric materials used for tactile sensors are often flexible, which makes them suitable for use on complex curved surfaces [5]. In terms of other benefits, these sensors can have low power consumption and can be fabricated through advanced micro-electro-mechanical systems (MEMS) technology, achieving small sizes [0]. Furthermore, piezoelectric sensors are resistant to high temperatures and external electric and magnetic fields, which adds to their robustness [14].\n1. **Wide Range of Materials:** The triboelectric effect allows for the use of a wide range of materials, including flexible polymers, which can be tailored to specific applications. This adaptability makes triboelectric sensors suitable for diverse robotic applications [13][14].\n2. **High Sensitivity and Energy Harvesting:** Triboelectric sensors are highly sensitive to small changes in force and can also harvest energy from mechanical movements, providing dual functionality. This feature is advantageous for developing self-powered sensor networks in robots [14].\n\n**Disadvantages:**\n\nPiezoelectric tactile sensors are not able to detect static forces because the induced charge in piezoelectric materials dissipates very quickly [2]. The spatial resolution of piezoelectric tactile sensors using typical bulky piezoelectric ceramics is not as high as that of piezoresistive sensors due to the complexity of the fabrication process, which limits sensors with low operating power consumption [0]. Additionally, robustness and sensitivity to temperature are lacking in piezoelectric tactile sensors [2]. Optical tactile sensors, while resistant to external interference and possessing high spatial resolution, often suffer from complexity in their design and effective integration [4]. Piezoresistive tactile sensors, although having good robustness and a wide measuring range, are impressionable to hysteresis [1]. Capacitive tactile sensors provide great spatial resolution and low power consumption but lack robustness to interference [1]. \n\n\n1. **Complex Fabrication and Integration:** The complex fabrication processes required for triboelectric sensors can increase production costs and complicate integration into existing robotic systems [13].\n2. **Performance Variability:** The performance of triboelectric sensors can vary significantly depending on material properties and environmental conditions, such as humidity and temperature. This variability complicates calibration and reliability [13].\n3. **Wear and Tear:** As triboelectric sensors rely on contact and separation between materials, they are susceptible to wear and tear over time, which can degrade performance and longevity [13].\n\n\n\n**Applications:**\n\nApplications of flexible tactile sensors are diverse and span across several fields due to their adaptability, sensitivity, and versatility. In robotics, these sensors play a crucial role in enabling robots to interact safely and effectively with their environment by providing essential tactile feedback [4]. In the biomedical field, they are used in wearable and implantable devices for monitoring physiological conditions and movements [12]. Additionally, flexible tactile sensors have been integrated into human-machine interfaces, enhancing their capabilities by mimicking human touch [5]. These sensors are also pivotal in industrial settings for enhancing the precision and control of automated systems [1].\n\n\n- **Self-Powered Sensor Networks:** Suitable for autonomous robots in remote or inaccessible areas where battery replacement is impractical [13]. \n- **Soft Robotics:** The flexibility of materials used in triboelectric sensors makes them ideal for soft robotic structures that need to conform to varied surfaces [13]. \n- **Environmental Monitoring:** Their high sensitivity allows these sensors to detect minute changes in force, making them suitable for precise environmental monitoring tasks [0]. \n\nComparison with Other Popular Tactile Sensor Technologies[0][1][2][5][4][6][13] \nApart from visio-tactile, piezoelectric, and triboelectric tactile sensors, other popular tactile sensing technologies include capacitive and piezoresistive sensors [1]. Here's a brief comparison:\n- **Capacitive Sensors:** Offer high spatial resolution and low power consumption but are less robust against electromagnetic interference [1][7].\n- **Piezoresistive Sensors:** Known for their large measuring range and good robustness, but they are prone to hysteresis and can be influenced by temperature fluctuations [1][7].\n\nIn summary, visio-tactile sensors offer high spatial resolution and immunity to electromagnetic disturbances, making them suitable for detailed texture mapping and object recognition, albeit at higher costs and complexity [3]. Piezoelectric sensors are highly sensitive with fast response times and self-powering capabilities, but they cannot detect static forces and may be sensitive to temperature changes [0]. Triboelectric sensors, while adaptable and capable of energy harvesting, face challenges in fabrication, integration, and performance consistency [13]. Selecting the appropriate sensor type depends on the specific requirements and constraints of the robotic application. \n\n\nReferences:\n[0] Integrated Piezoelectric AlN Thin Film with SU-8/PDMS Supporting Layer for Flexible Sensor Array applications require both accurate measurement of applied force, and the possibility of direct attachment onto the surface of objects such as organs or skin for wearable and implantable devices. Multiple approaches to artificial tactile sensors were investigated, using methods based on piezoresistive, capacitive, inductive, and piezoelectric effects. Each of the sensing methodologies has advantages and disadvantages in tactile sensing; however, among the sensing approaches, piezoelectric-based tactile sensors have particularly high sensitivity, a high dynamic range, and a high frequency response. Nevertheless, piezoelectric tactile sensors are not able to detect static forces, because the impedance of a piezoelectric film is related to the decay of a developed charge under an applied force. In addition, the spatial resolution of piezoelectric tactile sensors using typical bulky piezoelectric ceramics is not as high as that of piezoresistive sensors due to the complexity of the fabrication process, which limits sensors with low operating power consumption. Piezoelectric tactile sensors can be successfully down-scaled through advanced micro-electro-mechanical systems (MEMS) technology,13. A high piezoelectric constant and a low dielectric constant are essential to achieve high sensitivity for piezoelectric MEMS-based devices. Among various piezoelectric thin films, such as aluminum nitride (AlN), ZnO, BaTiO3, lead zirconate titanate (Pb(Zr,Ti)O3, PZT) and polyvinylidenefluoride (PVDF), PZT is the most popular due to its high piezoelectric constant, high energy density, and large electrical-mechanical coupling coefficients,20. However, the pressure/touch sensitivity of thin film PZT with regard to voltage response is slightly smaller than that of AlN\n\n[1] Common Sensors in Industrial Robots: A Review to perceive multiple properties of external objects and interact with the environment, industrial robots also need the sense of touch to cognize the outer world. Therefore, tactile sensors, which endue industrial robots with the ability of tactile sensation, are vital components to make them more intelligent. Tactile sensors have been developed for several decades, and they are gradually used in various robots now. The types of tactile sensors mainly include capacitive, piezoelectric, piezo-resistive and optical. The capacitive tactile sensor utilizes the change of capacitance to measure the contact force. It has great spatial resolution and low power consumption, but its robustness to interference is poor. The piezoelectric tactile sensor depends on the theory of piezoelectric effect which means that the electrical charge will be emerged on the surface of piezoelectric materials when exterior load is exerted. The frequency response of it is good and its measuring range is large, but its resolution is not very satisfying. The piezo-resistive tactile sensor bases on the principle of piezo-resistive effect which refers to the variance of resistance when outer force is implemented. It has large measuring range and good robustness, however, it is impressionable to hysteresis. The optical tactile sensor can transduce the external contact information into the variance of parameters of light. It is resistant to the external interference and possesses great spatial resolution. Furthermore, some researchers have developed artificial skins to imitate human's skin, which may be the development trend of the future. Multiple sensors are deployed on each cell of the\n\n[2] Recent Progress in Technologies for Tactile Sensors according to an extended gate configuration. These two sensors are flexible, sensitive, and suitable for large-area applications, which is important for the fabrication of industry tactile sensors. Moreover, in comparison with conventional organic FETs, these sensors can detect fast varying contact forces due to their much higher mobility. film and tactile sensors. Reprinted from, copyright (2014), with permission from Elsevier. In addition to detecting external forces, procuring information about surface texture is also an important part in both human and robotic tactile sensing, as it is critical for distinguishing contact objects. Roughness is considered the most critical feature of a surface texture. Liu et al. proposed a piezoelectric tactile sensor array with PVDF for roughness detection utilizing the interval of response time between adjacent sensor units and the principal frequency of vibration at different scanning velocities. By measuring principal frequency and scanning velocity, the surface spatial period can be calculated; meanwhile, another characteristic variable texture amplitude can be extracted according to the amplitude of the output charge in the measurable range. Thus, the roughness of a stimuli's surface texture can be completely determined. However, piezoelectric tactile sensors also have inherent drawbacks, the greatest one of which is the inability to measure static contact forces because the induced charge in piezoelectric materials dissipates very quickly. This means piezoelectric tactile sensors cannot be effective in static environments. Additionally, robustness and sensitivity to temperature are lacking. Optical Tactile Sensors Optical tactile sensors obtain tactile information by analyzing changes of internal or output light. In optical\n\n[3] Dynamic-Vision-Based Force Measurements Using Convolutional Recurrent Neural Networks of tactile sensors for the safe human robot interaction, soft robotics, and object recognition. Different types of tactile sensing methods for dexterous robot hands are reviewed in, which shows the advantages of optical tactile sensors in terms of high spatial resolution, sensitivity, repeatability, and immunity to electromagnetic disturbances. Optical tactile sensors aim to extract tactile information by emitting light waves and study behaviourbehavior of back-scattered waves in a flexible membrane (fingertip). Use of different materials within the flexible membrane allows optical tactile sensors to achieve high sensitivity as illustrated in. Furthermore, multi-modal optical tactile sensors combined with electrical tactile sensors demonstrate a successful application for materials classification and proximity range detection. Vision-based sensors are a subcategory of optical sensors that utilizuse cameras and image processing techniques for measurement and sensing purposes. Advancement in camera technology, machine vision techniques, and processing power, increase the number of Vision-Based Measurement (VBM) systems significantly. The fundamental approach of VBM sensors for tactile sensing is to monitor the contact area by camera and acquire information about physical properties as well as objects characteristics. For example, multiple Kalman filter is considered to observe the contact area deformation to estimate the contact force continuously. Another approach in proposed a technique to estimate the contact force from frustrated total internal reflection using a camera. One of the most popular designs is to attach markers or pins on the elastomer surface which can be tracked by computer vision algorithms to determine the contact area properties reviewed\n\n[4] Large-Scale Integrated Flexible Tactile Sensor Array for Sensitive Smart Robotic Touch vision, touch, hearing, smell and taste, which play indispensable roles for humans to interact with the environment. From science fictions to realworld applications, it has been long anticipated for robots to feel and act like humans. To achieve that, it is crucial to equip them with a variety of bio-mimicking sensors, which have been an active research and development frontier for decades. 1 Inspiringly, image sensors and computer vision have made tremendous progress in the past years, and can now perform even better than human eyes in many applications. 2 It also largely enriches the sensing abilities of robots with the help of machine learning. By contrast, as a critical sense during physical interactions providing subtle information that vision cannot capture, the developments of tactile sensors and their applications on robots are falling behind. For robots to safely grasp and manipulate objects as humans routinely do, such as opening a bottle without breaking it, high-performance tactile sensors are expected to provide key information, including shape, size, weight, stiffness, etc. 3 Therefore, to make robots physically helpful and intelligent, it is highly desired to develop flexible and durable tactile sensors with high sensitivity, high spatial resolution, fast response and large pressure detection range. In literature, there have been extensive studies on various pressure sensors to mimic the tactile sense of human skin. According to their working mechanisms, flexible pressure sensors can be mainly categorized into three types: piezoelectric, 4-8 capacitive, 9,10 and piezoresistive. In general, piezoelectric tactile sensors can have a small size and can operate\n\n[5] Contact Pattern Recognition of a Flexible Tactile Sensor Based on the CNN-LSTM Fusion Algorithm is an essential tactile information acquisition medium of the robot sensor system, and it has special advantages in detecting target surface texture and physical properties, which is conducive to establishing a more secure and reliable humanmachine interaction system,5. A flexible tactile sensor is a flexible device that senses the force on the surface. With its good flexibility and stretchability, it can be attached to the surface of irregular objects, and it is widely used in the research of robotic bionic skin. Therefore, researchers improve the performance of sensors from the aspects of materials, multifunctional integration, and self-energy. At present, flexible tactile sensors based on capacitive, piezoresistive,21, piezoelectric,23, and other mechanisms have been continuously proposed, and they have shown good application prospects. Among them, flexible tactile sensors based on piezoelectric mechanisms have high sensitivity and fast response ability, so they are widely used to detect dynamically changing tactile signals. In addition, piezoelectric tactile sensors can convert external mechanical pressure into electrical energy, which makes piezoelectric tactile sensors particularly important in self-powered applications. Piezoelectric materials used to fabricate tactile sensors are mainly divided into inorganic piezoelectric materials, organic piezoelectric materials and their copolymers. On the one hand, inorganic piezoelectric materials such as PZT,26 hinder their application on complex curved surfaces due to their lack of flexibility. On the other hand, the organic piezoelectric material represented by polyvinylidene fluoride (PVDF) and its copolymer shows good flexibility, stable chemical properties, and low cost, so PVDF piezoelectric material is\n\n[6] Recent Progress in Technologies for Tactile Sensors Liu et al. proposed a piezoelectric tactile sensor array with PVDF for roughness detection utilizing the interval of response time between adjacent sensor units and the principal frequency of vibration at different scanning velocities. By measuring principal frequency and scanning velocity, the surface spatial period can be calculated; meanwhile, another characteristic variable texture amplitude can be extracted according to the amplitude of the output charge in the measurable range. Thus, the roughness of a stimuli's surface texture can be completely determined. However, piezoelectric tactile sensors also have inherent drawbacks, the greatest one of which is the inability to measure static contact forces because the induced charge in piezoelectric materials dissipates very quickly. This means piezoelectric tactile sensors cannot be effective in static environments. Additionally, robustness and sensitivity to temperature are lacking. Optical Tactile Sensors Optical tactile sensors obtain tactile information by analyzing changes of internal or output light. In optical tactile sensors, optical fibers are utilized as a medium for transmitting light, so the advantages of optical fibers can be regarded as merits of optical tactile sensors to some degree. Advantages of optical-based tactile sensors include a light weight, physical flexibility, chemical inertness, a fast response, and a small size. In addition, optical tactile sensors are immune to electromagnetic interference, which means they are compatible with magnetic resonance imaging (MRI) and can be integrated with minimally invasive surgical (MIS) manipulation tools. As for optical tactile sensors in the form of arrays, there is no interconnection between optical fibers and no parasitic disturbance due to\n\n[7] Recent Advances in Flexible Tactile Sensors for Intelligent Systems the essential performances of high sensitivity, high resolution, fast response time, and wide detection range, etc., are the vital factors for guaranteeing the ability to sense tiny pressure precisely and produce a high-resolution mapping matrix. On the other hand, in order to apply the flexible tactile sensor into extensive practical applications, the multifunction, such as self-powered, visualization, biocompatibility, and self-healing, are considered as the significant factors. In this section, we display the recent development of representative high-resolution, high sensitivity, self-powered, and visualization to enhance the comprehensive performance, which also point out the key development direction of flexible tactile sensing in the future. Additionally, other performances and functions of flexible tactile sensors have also been discussed. High-Resolution Tactile Sensing High-resolution flexible electronic devices promote the advance of robot technology, human-machine interfaces, and wearable electronics. Tactile sensing devices with high resolution could identify the position, direction, and appearance of complex objects more accurately,28,85. Therefore, this performance of flexible tactile sensors is significant, and can be applied in smart signatures, electronic skins, and other directions. As discussed above, tactile sensors can be roughly divided into piezoresistive, capacitive, piezoelectric, and triboelectric. Tactile sensors based on piezoresistive or capacitive have been widely investigated, however, their resolution is generally in the millimeter level, not comparable to micrometer of human skin,87. As an important type of tactile perception, piezoelectricbased tactile nanosensors can convert the external pressure into the internal polarization of the nanostructured material, achieving a higher resolution level, as shown in Figure 2. zoelectric, and triboelectric. Tactile sensors based on\n\n[8] Recent Progress in Technologies for Tactile Sensors external force/pressure. Thus, piezoelectric materials used for dielectric materials with a certain thickness Although magnetic induction tactile sensors a good choice for artificial skin in robot application because they are flexible, sensitive, and easy to fabricate, there are still drawbacks that should be taken into consideration. For instance, electromagnetic induction tactile sensors consume more power, in comparison with magnetic tactile sensors that use permanent magnets, because of the difference in current in the excitation coils. Additionally, the eddy current effect and stray capacitance make sensors less reliable and decrease the sensor's performance in some degree. Piezoelectric Tactile Sensors Tactile sensors based on piezoelectric principles transduce external force or pressure to output voltage proportionally. The most essential elements of this kind of tactile sensors are piezoelectric materials, which generate charges under the circumstances of being subjected to external force/pressure. Thus, piezoelectric materials used for dielectric materials with a certain thickness and area can be regarded as capacitances approximately when dynamic forces are applied to them. Polyvinylidene fluoride (PVDF) and its copolymers are most widely used as piezoelectric materials in tactile sensors due to their light weight, low power consumption, simplicity of fabrication, and flexibility, properties suitable for overlapping large areas and curved surfaces. Moreover, PVDF films that can measure pressure directly are able to generate a dynamic response with high sensitivity in dynamic environments with which robotic tactile applications are always confronted and in which the bandwidth of the response frequency is substantially high, about 0-1 kHz. In human skin, many tactile corpuscles, such as Pacinian\n\n[9] HiVTac: A High-Speed Vision-Based Tactile Sensor for Precise and Real-Time Force Reconstruction with Fewer Markers of perception between creatures and the real world. For many years, efforts have been made to develop robots that are more human-like, and doing so requires tactile perception. Intrinsically, tactile perception is an interactive process between mechanics and the nervous system. Inspired by that, to begin with, research into tactile sensors focuses on converting mechanical signals to electrical signals, just as the nervous system does, by elaborate circuits with multiple electronic components, such as resistive,2, piezoelectric,4, capacitive,6, magnetic,8, optoelectronic,10, and triboelectric,12 components. The common downsides to all the above approaches are: (1) designing such dedicated circuits can be time-consuming, (2) such complicated circuit structures with too many electronic components reduce the robustness of the whole system, and (3) electromagnetic interference is introduced as an extra problem. In recent decades, thanks to the advances in the semiconductor industry and in computer vision techniques, obtaining tactile information in a visual way has attracted increasing attention because of the high resolution, low cost, high robustness, and ease of manufacture associated with doing so. Similar to the above electric-based tactile sensors, vision-based ones convert mechanical information to visual signals, which can then be tracked by an image sensor for further inference. Moreover, using an image sensor as the core electronic component in vision-based tactile sensors allows them to be compatible and seamlessly fused with any existing system that also uses cameras for visual input. Most importantly, it is also feasible to reconstruct the spatial direction of external forces, in addition to their magnitude.\n\n[10] Flexible PZT Thin Film Tactile Sensor for Biomedical Monitoring , pressure sensors, and tactile sensors. However, capacitive-based tactile sensors typically require high voltage operation, whereas piezoresistive-based tactile sensors encounter signal drift caused by temperature changes. Therefore, piezoelectric-based sensors were chosen to examine tactile sensing applications. Among various piezoelectric-based materials, lead zirconium titanate (PZT) thin-film is an excellent ferroelectric material for tactile sensor applications. PZT-based sensors have several advantages, such as high sensitivity, wide frequency bandwidth, and fast response. Thus, these sensors can be widely used for micro-electromechanical systems (MEMS) applications, such as in the areas of transducers, micromirrors,12, switches, gas sensors, pyroelectric sensors,16, energy harvesting devices, and tactile sensors. Tactile sensors fabricated using MEMS offer the advantages of small size, mature technologies, and low cost processing. PZT thin-films fabricated using the sol-gel method provides the advantages of easy processing, low annealing temperature, and excellent piezoelectric characteristics. Accurate Zr/Ti element composition can be controlled in the sol-gel deposition process; thus, a composite material with a high ferroelectric property (Zr/Ti with a ratio of 52:48) can be obtained. Various sensors, such as piezoelectric-based sensors, optical sensors, and laser Doppler sensors have been used for measurements of human body pulses at various artery regions. For piezoelectric-based tactile sensors, the mechanical energy can be transferred to electrical energy using an applied pressure, and the sensors have the advantages of high sensitivity, improved hysteresis, excellent repeatability, and high durability. In addition, flexible materials, such as aluminium nitride (AlN), lead-lanthanum-zirconate-titanate (PLZT), and polyvinylidene difluoride (PVDF) can be used for\n\n[11] Recent progress in flexible tactile sensor systems: from design to application tactile sensor, the differential electric displacement is given by dD = -d 31 T 1 -d 32 T 2 + d 33 T 3 , where d is the piezoelectric coefficients and T is the applied stress, with the subscripts 1 and 2 describing the contribution from shear stress and 3 from the normal stress. The piezoelectric tactile sensor has the advantages of high sensitivity, fast response time, and self-powered operation. As its operation does not have to involve the direct contact of the sensing object, it is widely used in the detection of acoustic vibration, sliding vibration, and other dynamic pressures. On the other hand, if an external force remains unchanged, the charge will be neutralized and cannot be measured statically. However, considering the unique energy collection characteristics of the piezoelectric materials, they have very important advantages in the development of fast dynamic response, low energy consumption, and selfpowered flexible tactile sensors. The piezoelectric materials commonly used in piezoelectric sensors include lead zirconate titanate (PZT) , ZnO , BaTiO 3 , LiTaO 3,60 , Group III-nitride (III-N) materials (AlN and GaN) , and intrinsic flexible polyvinylidene fluoride (PVDF) . Triboelectric mechanism The triboelectric effect is a common physical phenomenon in nature. When two substances with different electron binding energies come into contact with each other, one substance loses electrons to the other. This phenomenon is called the triboelectric effect . Triboelectric tactile sensors are built based on the triboelectric effect. By contacting and then separating two materials with\n\n[12] Piezoelectric force sensors for hexapod transportation platform distribution between the feet. It still remains unknown, whether or not the robot damages the surface or its legs. More robust and compliant locomotion was acquired by Buchli et al. (2009) using force sensors. Experiments were carried out on the irregular terrain and results showed that a robot with force sensors can surpass irregular terrain faster. Still, no results were given about the actual size of the force in each foot. Piezoelectric Sensors There are many different sensors that could be used as robot's force sensing: barometers, pressure sensors, tactile sensors, load cells or silicon-based sensors. In this work, we chose to upgrade hexapod robot's feet with piezoelectric sensors (Fig. 1). Unlike silicon-based sensors that are relatively small and brake in case of an overload (Beebe et al. 1998), piezoelectric sensors are of needed size and can withstand high pressure. Main advantages of these sensors are: -low cost; -high resolution (deflections can be micrometre size); -wide measuring range; -signal can be easily reproduced; -high temperature resistance; -insensitive to external electric and magnetic fields. Another reason we chose to use piezoelectric sensors is that they can be easily placed on robot's feet (Fig. 2). Not to break sensors or wires, we glued the hemisphere at the bottom and a small metal plate at the top. The whole robot with piezoelectric sensors is shown in Fig. 3. As we didn't have the characteristics of piezoelectric sensors, the experiment was made using an oscilloscope to obtain the voltage dependence on the pressure force U = f(F). We performed the\n\n"}
{"case_id": "ad5d8d8dcdab789fbc9231709db8c9cb", "answer_text": "\n\nTransferring robotic systems trained or designed in simulation to the real world, known as sim-to-real transfer, presents several significant challenges. These challenges stem primarily from the \"reality gap\"—the discrepancies between the simulated environment and the real world [13]. Understanding and addressing these challenges is crucial for the successful deployment of robotic systems trained in simulation into practical real-world applications. \n\nDynamics Mismatch [1]\n\nOne major challenge is the mismatch in dynamics between the simulated and real environments. Simulations often rely on simplified models for calculating physical interactions, such as forces and friction, which can differ significantly from real-world conditions [0][1]. For example, factors like actuator imperfections, motor backlash, and variable friction are difficult to precisely replicate in simulation [0]. This discrepancy can lead to poor performance or failure when the trained model is deployed in the real world [2].\n\nSensor discrepancies can significantly impact the performance of robotic control systems when transitioning from simulation to real-world environments. These discrepancies may include issues such as bias and noise in onboard sensors, as well as partial observability where real-world observations are limited to sensor readings, whereas in simulation, the full dynamical state of the system is available [0]. \n\nSimulated environments often provide perfect or near-perfect sensor readings, whereas real-world sensors are subject to noise, bias, and jitter [0][8]. The control system in a real robot must handle multiple noisy sensor inputs without overreacting to any single reading, a complexity that simulations do not usually capture [0]. Techniques like pixel-level domain adaptation have been employed to bridge the gap between synthetic and real camera images, improving the robustness of policies trained in simulation [8]. However, pixel-level domain adaptation often requires extensive labeled data for training, and the adapted images might still lack the realism necessary to fully bridge the gap, as they may fail to capture nuances like lighting variations and occlusions present in real-world settings. This can result in the adapted model performing suboptimally once deployed in real-world environments.\n\nPartial observability limits real world observations to sensors readings, whereas in simulation the full dynamical state of the system is available [0]. \n\nIn simulation, the full dynamic state of the system is often available, which is not the case in the real world. Real-world observations are limited to sensor readings, making it difficult to apply the same control strategies used in simulation directly [0]. This partial observability can hinder the system's ability to make informed decisions, compromising stability and performance [3].\n\nDomain Randomization is a technique used to bridge the gap between simulated and real-world environments by randomizing various aspects of the simulation during training to make the learned policies more robust to discrepancies between the simulated and real environments [1]. This method enables the policies to generalize to the dynamics of the real world without any training on the physical system [1]. By incorporating a wide range of conditions in simulation, domain randomization helps in transferring the learned policies to the real world successfully [9]. This approach has shown effectiveness in different robotic tasks, such as object pushing and humanoid robot motion, reducing the reality gap significantly [0]. \n\nDomain randomization is a technique used to address the reality gap by training models on a wide range of simulated scenarios with varying parameters [1]. This approach helps in developing policies that are robust to differences between the simulation and the real world. However, creating sufficiently varied yet realistic simulations can be challenging and computationally expensive [7]. For example, Tan et al. utilized domain randomization to train quadruped locomotion policies by varying parameters like mass, friction, and motor strengths, which helped in achieving robust transfer to real robots [10]. However, too wide a range of parameter variations can simulate infeasible scenarios, which may confuse the learning process [3]. One effective way to manage this is by starting with narrower distributions that are gradually widened based on the success of the policy in real-world trials [3].\n\nReal-Time Adaptation\n\nTraining robotic control policies in simulation environments offers numerous advantages, such as providing an abundant source of data and alleviating certain safety concerns during the training process. However, one significant challenge is transferring these control policies from simulation to the real world, a phenomenon often referred to as the \"reality gap\" [1]. The discrepancies between the simulated and real-world environments, including model inaccuracies and sensor noise, can impede the performance of the trained policies when deployed in real-world scenarios [8]. Addressing this gap is crucial for effective domain adaptation, where the goal is to ensure that representations and behaviors learned in the simulation are applicable to the real world [2]. Techniques such as dynamics randomization during training have been employed to develop policies that can adapt to varied dynamics, enhancing their robustness when transferred to real-world systems [1]. Consequently, these adaptive policies can perform reliably even when confronted with significant differences between the training and deployment environments, thereby increasing the feasibility of sim-to-real transfer [8]. \n\n\n\nReal-time adaptation of policies from simulation to the real world presents another layer of complexity. Some methods involve adapting simulation randomization using real-world data to better match the simulated environment to real-world conditions [3]. For instance, Clavera et al. demonstrated the effectiveness of this approach by training a robotic arm in simulation and updating the simulation parameters based on real-world feedback, thereby improving the robustness of the policy when deployed in the real world. This iterative process can be effective but requires careful calibration and validation against the real environment [12]. Additionally, the method outlined by Christiano et al. involves training an inverse-dynamics model using real-world data to refine the simulation, thus enhancing the accuracy and effectiveness of transferred policies [2].\n\nSimulations are attractive environments for training agents as they provide an abundant source of data and alleviate certain safety concerns during the training process. But the behaviours developed by agents in simulation are often specific to the characteristics of the simulator. Due to modeling error, strategies that are successful in simulation may not transfer to their real world counterparts[1]. In this paper, we demonstrate a simple method to bridge this “reality gap”. By randomizing the dynamics of the simulator during training, we are able to develop policies that are capable of adapting to very different dynamics, including ones that differ significantly from the dynamics on which the policies were trained. This adaptivity enables the policies to generalize to the dynamics of the real world without any training on the physical system[1]. Our approach is demonstrated on an object pushing task using a robotic arm. Despite being trained exclusively in simulation, our policies are able to maintain a similar level of performance when deployed on a real robot, reliably moving an object to a desired location from random initial configurations[1]. \n\nWe explore the impact of various design decisions and show that the resulting policies are robust to significant calibration error[1]. One of the key assumptions behind these methods is that the different domains share common characteristics such that representations and behaviours learned in one will prove useful for the other[2]. Learning invariant features has emerged as a promising approach of taking advantage of these commonalities, 12. Tzeng et al. and Gupta et al. explored using pairwise constraints to encourage networks to learn similar embeddings for samples from different domains that are labeled as being similar[2]. Daftry et al. applied a similar approach to transfer policies for controlling aerial vehicles to different environments and vehicle models[2]. In the context of RL, adversarial losses have been used to transfer policies between different simulated domains, by encouraging agents to adopt similar behaviours across the various environments[2]. Alternatively, progressive networks have also been used to transfer policies for a robotic arm from simulation to the real world[2]. By reusing features learned in simulation, their method was able to significantly reduce the amount of data needed from the physical system[2]. Christiano et al. transfered policies from simulation to a real robot by training an inverse-dynamics model from real world data[2]. While promising, these methods nonetheless still require data from the target domain during training[2].\n\nLearning continuous control in real world complex environments has seen a wide interest in the past few years and in particular focusing on learning policies in simulators and transferring them to the real world, as we still struggle with finding ways to acquire the necessary amount of experience and data in the real world directly[7]. While there have been recent attempts on learning by collecting large scale data directly on real robots , 2, 3, 4, such an approach still remains challenging as collecting real world data is prohibitively laborious and expensive[7]. Simulators offer several advantages, e.g. they can run faster than real-time and allow for acquiring large diversity of training data[7]. However, due to the imprecise simulation models and lack of high fidelity replication of real world scenes, policies learned in simulations often cannot be directly applied on real world systems, a phenomenon also known as the reality gap [7]. In this work, we focus on closing the reality gap by learning policies on distributions of simulated scenarios that are optimized for a better policy transfer[7].\n\nThe methods described above focus on extracting knowledge from simulation-trained agents in order to deploy them in real-life scenarios[4]. However, other approaches exist to the same end[4]. In recent years, simulators have been progressing towards more realistic scenarios and physics engines: Airsim , CARLA , RotorS , 16, and others [4]. With some of these simulators, part of the aim is to be able to deploy the robotic agents directly into the real world by providing training data and experiences with minimal mismatches between real and simulated settings[4]. Other research efforts have been directed towards increasing safety during training in real-settings[4]. Safety is one of the main challenges towards achieving online training of complex agents in the real-world, from robot arms to self-driving cars [4]. In this direction, recent works have shown promising results towards safe DRL that is able to ensure convergence even while reducing the exploration space[4]. In this survey, we do not cover specific simulators or techniques for direct learning in real-world settings, but instead focus on describing the main methods for transferring knowledge learned in simulation towards their deployment in real robotic platforms[4].\n\nUnfortunately, many of the capabilities demonstrated by simulated agents have often not been realized by their physical counterparts[13]. Many of the modern DeepRL algorithms, which have spurred recent breakthroughs, pose high sample complexities, therefore often precluding their direct application to physical systems[13]. In addition to sample complexity, deploying RL algorithms in the real world also raises a number of safety concerns both for the agent and its surroundings[13]. Since exploration is a key component of the learning process, an agent can at times perform actions that endanger itself or its environment[13]. Training agents in simulation is a promising approach that circumvents some of these obstacles[13]. However, transferring policies from simulation to the real world entails challenges in bridging the ”reality gap”, the mismatch between the simulated and real world environments[13]. Narrowing this gap has been a subject of intense interest in robotics, as it offers the potential of applying powerful algorithms that have so far been relegated to simulated domains[13]. \n\nExploring and learning in real-world settings can pose safety risks to both the robot and its surroundings [13]. Simulations provide a safer environment for training, but transferring those policies without ensuring they are safe for real-world deployment can lead to dangerous outcomes [13]. Several techniques aim to mitigate these risks, including:\n\n**Safe Reinforcement Learning:** Incorporates safety constraints into the learning process to prevent the agent from taking potentially dangerous actions [14].\n\n**Shielding:** A technique where a safety layer is added to the control policy to override any unsafe actions proposed by the learned policy [14].\n\n**Simulated Safety Training:** Uses simulation to train and validate the policy under a variety of potentially hazardous scenarios, ensuring that the learned policies are robust and safe before real-world deployment [13].\n\nComputational constraints have a significant impact on the effectiveness of sim-to-real transfer for robotic control policies. One major issue is the reality gap, where policies trained in simulation degrade in performance when applied to real-world systems due to inaccuracies in the simulation model [11]. Moreover, achieving high-fidelity simulations that accurately replicate real-world physics can be computationally expensive, which often leads to the use of less accurate simulators that may hinder effective sim-to-real transfer [11]. Additionally, transferring policies from simulation to reality is challenging due to discrepancies in sensor inputs and dynamics between simulated and real environments [8]. Various techniques, such as domain randomization and simulation randomization, have been proposed to bridge the reality gap and facilitate successful policy transfer [9]. However, these methods require careful calibration and extensive computational resources to adapt simulation parameters [1].\n\nHighly accurate simulations are computationally expensive, which can limit their practicality. A balance must be struck between the accuracy of the simulation and computational efficiency [11]. One approach is to use an initially less accurate but computationally inexpensive simulation to train baseline policies and then gradually increase the accuracy as needed while incorporating real-world data into the simulation model. This staged approach can help in maintaining computational efficiency without excessively compromising the realism needed to bridge the reality gap. Additionally, leveraging cloud computing and parallelization can distribute the computational load, allowing more complex simulations to run without excessive local resource consumption.\n\n### Real-World Data Collection and Fine-Tuning\n\nCollecting real-world data for fine-tuning and validating models is often laborious and expensive. While simulators offer the advantage of generating large and varied datasets quickly, real-world validation remains essential to ensure the robustness and reliability of the trained models [7]. Methods to address this include:\n\n**Progressive Refinement:** Begin with simulation-trained models and incrementally update them with real-world data to improve accuracy and robustness. This method can help in identifying the key areas where the simulation diverges from reality and make targeted adjustments [3]. \n\n**Active Learning:** Involves selectively sampling the most informative data points from the real world to maximize learning efficiency. This reduces the amount of real-world data needed by focusing efforts on the most critical areas for improvement [14]. \n\n**Transfer Learning:** Using pre-trained models from similar tasks or domains and fine-tuning them with real-world data specific to the new task. This approach leverages existing knowledge to reduce the amount of new data required [2] and speeds up the adaptation process [2]. \n\n### Ensemble Methods and Hybrid Models\n\nIn addition to the techniques mentioned, ensemble methods and hybrid models have shown promise in addressing the sim-to-real gap. Ensemble methods involve training multiple models and combining their outputs to improve robustness and accuracy when transferring to the real world. These methods can help mitigate the errors of individual models and provide more reliable performance. Hybrid models, on the other hand, combine data-driven approaches with traditional model-based methods to leverage the strengths of both. For instance, a hybrid approach might use a physics-based model to provide a rough estimate of the system's behavior, while a data-driven model refines the predictions based on real-world observations. This can enhance the model's ability to generalize from simulation to reality and reduce the dependency on large amounts of real-world data [9]. \n\nSummary: The success seen in simulation to the real world has remained a challenging problem [1]. Issues encountered when transferring from simulation to the real world may include the following: real world actuators exhibit a variety of imperfections including jitter, bias and motor backlash [1]. Additionally, on board sensors suffer from bias and noise [1]. The control system must respond to multiple noisy sensor inputs simultaneously while avoiding over-reacting to any single sensor reading [0]. Smoothing techniques applied to reduce the sensor spikes in general come at the cost of responsiveness to the sensor inputs which is vital to maintaining stability [0]. Partial observability limits real world observations to sensors readings, whereas in simulation the full dynamical state of the system is available [0]. Physics simulations make use of simplified models when calculating forces such as friction [8]. Such limitations make domain adaptation from a simulation environment to a real world environment difficult [7]. \n\nIn summary, several challenges arise when attempting to transfer robotic systems trained in simulation to the real world, including dynamics mismatches [1], sensor discrepancies [8], partial observability [0], domain randomization [7], real-time adaptation, safety concerns [13], computational constraints [14], and the need for real-world data collection and fine-tuning [12]. Additionally, ensemble methods and hybrid models offer promising avenues to further bridge the reality gap [2]. Addressing these challenges requires a combination of robust simulation techniques [3], real-world data integration [7], continuous adaptation [3], and innovative modeling approaches to ensure successful and safe deployment [13]. \n\n\nReferences:\n[0] Learning Bipedal Robot Locomotion from Human Movement success seen in simulation to the real world has remained a challenging problem. Issues encountered when transferring from simulation to the real world may include the following: • Real world actuators exhibit a variety of imperfections including jitter, bias and motor backlash. • On board sensors suffer from bias and noise. • The control system must respond to multiple noisy sensor inputs simultaneously while avoiding over-reacting to any single sensor reading. • Smoothing techniques applied to reduce the sensor spikes in general come at the cost of responsiveness to the sensor inputs which is vital to maintaining stability. • Partial observability limits real world observations to sensors readings, whereas in simulation the full dynamical state of the system is available. • Physics simulations make use of simplified models when calculating forces such as friction. Such limitations make domain adaptation from a simulation environment to a real world environment difficult. We propose the following techniques to overcome these challenges: • We implement a control scheme that treats the neural network output as joint position derivatives and then integrate the position derivatives into target joint positions. • We describe a progress based reward function that incorporates animation re-targeting into the training process. • We employ a domain randomization scheme that relies on simulated motor backlash, randomized friction, and randomized Young's modulus to enable the controller to transfer between simulation and the real world. We apply the controller trained in simulation to a real world humanoid robot and demonstrate a series of complex motions learned directly from human\n\n[1] Sim-to-Real Transfer of Robotic Control with Dynamics Randomization Simulations are attractive environments for training agents as they provide an abundant source of data and alleviate certain safety concerns during the training process. But the behaviours developed by agents in simulation are often specific to the characteristics of the simulator. Due to modeling error, strategies that are successful in simulation may not transfer to their real world counterparts. In this paper, we demonstrate a simple method to bridge this “reality gap”. By randomizing the dynamics of the simulator during training, we are able to develop policies that are capable of adapting to very different dynamics, including ones that differ significantly from the dynamics on which the policies were trained. This adaptivity enables the policies to generalize to the dynamics of the real world without any training on the physical system. Our approach is demonstrated on an object pushing task using a robotic arm. Despite being trained exclusively in simulation, our policies are able to maintain a similar level of performance when deployed on a real robot, reliably moving an object to a desired location from random initial configurations. We explore the impact of various design decisions and show that the resulting policies are robust to significant calibration error.\n\n[2] Sim-to-Real Transfer of Robotic Control with Dynamics Randomization The problem of transferring control policies from simulation to the real world can be viewed as an instance of domain adaptation, where a model trained in a source domain is transfered to a new target domain. One of the key assumptions behind these methods is that the different domains share common characteristics such that representations and behaviours learned in one will prove useful for the other. Learning invariant features has emerged as a promising approach of taking advantage of these commonalities, 12. Tzeng et al. and Gupta et al. explored using pairwise constraints to encourage networks to learn similar embeddings for samples from different domains that are labeled as being similar. Daftry et al. applied a similar approach to transfer policies for controlling aerial vehicles to different environments and vehicle models. In the context of RL, adversarial losses have been used to transfer policies between different simulated domains, by encouraging agents to adopt similar behaviours across the various environments. Alternatively, progressive networks have also been used to transfer policies for a robotic arm from simulation to the real world. By reusing features learned in simulation, their method was able to significantly reduce the amount of data needed from the physical system. Christiano et al. transfered policies from simulation to a real robot by training an inverse-dynamics model from real world data. While promising, these methods nonetheless still require data from the target domain during training.\n\n[3] Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience Closing the simulation to reality transfer loop is an important component for a robust transfer of robotic policies. In this work, we demonstrated that adapting simulation randomization using real world data can help in learning simulation parameter distributions that are particularly suited for a successful policy transfer without the need for exact replication of the real world environment. In contrast to trying to learn policies using very wide distributions of simulation parameters, which can simulate infeasible scenarios, we are able to start with distributions that can be efficiently learned with reinforcement learning, and modify them for a better transfer to the real world scenario. Our framework does not require full state of the real environment and reward functions are only needed in simulation. We showed that updating simulation distributions is possible using partial observations of the real world while the full state still can be used for the reward computation in simulation. We evaluated our approach on two real world robotic tasks and showed that policies can be transferred with only a few iterations of simulation updates using a small number of real robot trials.\n\n[4] Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey The methods described above focus on extracting knowledge from simulation-trained agents in order to deploy them in real-life scenarios. However, other approaches exist to the same end. In recent years, simulators have been progressing towards more realistic scenarios and physics engines: Airsim , CARLA , RotorS , 16, and others . With some of these simulators, part of the aim is to be able to deploy the robotic agents directly into the real world by providing training data and experiences with minimal mismatches between real and simulated settings. Other research efforts have been directed towards increasing safety during training in real-settings. Safety is one of the main challenges towards achieving online training of complex agents in the real-world, from robot arms to self-driving cars . In this direction, recent works have shown promising results towards safe DRL that is able to ensure convergence even while reducing the exploration space . In this survey, we do not cover specific simulators or techniques for direct learning in real-world settings, but instead focus on describing the main methods for transferring knowledge learned in simulation towards their deployment in real robotic platforms.\n\n[5] Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience Learning continuous control in real world complex environments has seen a wide interest in the past few years and in particular focusing on learning policies in simulators and transferring them to the real world, as we still struggle with finding ways to acquire the necessary amount of experience and data in the real world directly.While there have been recent attempts on learning by collecting large scale data directly on real robots , 2, 3, 4, such an approach still remains challenging as collecting real world data is prohibitively laborious and expensive. Simulators offer several advantages, e.g. they can run faster than real-time and allow for acquiring large diversity of training data. However, due to the imprecise simulation models and lack of high fidelity replication of real world scenes, policies learned in simulations often cannot be directly applied on real world systems, a phenomenon also known as the reality gap . In this work, we focus on closing the reality gap by learning policies on distributions of simulated scenarios that are optimized for a better policy transfer.\n\n[6] Learning Real-world Autonomous Navigation by Self-Supervised Environment Synthesis environments from real-world deployment experiences. B. Sim-to-real Transfer Limited by the safety and efficiency requirements in the real world, a learning-based navigation system is usually trained in simulation. However, policies trained in simulation can perform poorly in the real world due to the mismatch between the simulation and the real world. This phenomenon is commonly referred to as the sim-to-real gap. One major source of the sim-to-real gap is the discrepancies between the sensor input rendered in simulation and the real robot's sensors. For example, to bridge the gap between real-world and synthetic camera images of a robotic system, prior work has employed techniques such as pixellevel domain adaptation, which translates synthetic images to realistic ones at the pixel level,. These adapted pseudo-realistic images bridge the sim-to-real gap to some extent, so policies learned in simulation can be executed more successfully on real robots by adapting the images to be more realistic. Another source of the sim-to-real gap is caused by dynamics mismatch between simulation and the real world e.g., due to an imperfect physics engine. A common paradigm to reduce the dynamics mismatch is Grounded Simulation Learning (GSL), which either directly modifies (i.e., grounds) the simulator to better match the real world, or learns an action transformer that induces simulator transitions that more closely match the real world,. In contrast to the two sim-to-real gaps introduced above, this work addresses a gap caused by the environmental mismatch (e.g., differences in the configurations and shapes of obstacles, and start-goal locations). SES\n\n[7] Bi-Manual Manipulation and Attachment via Sim-to-Real Reinforcement Learning Unfortunately, policies trained in simulation rarely transfer to real-world systems due to many factors such as model discrepancies and noise in real-world state-estimation. This problem – often referred to as the Sim2Real problem – has lead to a significant body of work in recent literature, 32, 29. In this section, we outline how we have designed our simualted environments to enable successful transfer of RL policies from simulation to real-world robots.\n\n[8] Are We Making Real Progress in Simulated Environments? Measuring the Sim2Real Gap in Embodied Visual Navigation the logistical limitations of physical experimentation, transfer of agents trained in simulation to real platforms is a topic of much interest. There have been successful demonstrations of simulation-toreality transfer in several domains. The CAD2RL system of Sadeghi and Levine trained a collision avoidance policy entirely in simulation and deployed it on real aerial drones. Similarly, Muller et al. show that driving policies can be transferred from simulated cars to real remote-controlled cars by leveraging modularity and abstraction in the control policy. Tan et al. train quadruped locomotion policies in simulation by leveraging domain randomization and demonstrate robustness when deployed to real robots. Lastly, Hwangbo et al. train legged robotic systems in simulation and transfer the learned policies to reality. This research has been enabled in part by the increasing availability of cheaper and more reliable robot platforms. As robots become cheaper and increasingly useful, the need to train and evaluate performance on more and more tasks arises. This brings to the forefront the key question: can we establish a correlation between performance in simulation and deployment in reality? We focus on this question in the domain of visual indoor navigation. Habitat-PyRobot Bridge: Simple Sim2Real Deploying AI systems developed in simulation to physical robots presents significant financial, engineering, and logistical challenges -especially for non-robotics researchers. Approaching this directly requires researchers to maintain two parallel software stacks, one typically based on ROS for the physical robot and another for simulation. In addition to requiring significant duplication of effort, this model can also introduce\n\n[9] Crossing the Reality Gap: a Survey on Sim-to-Real Transferability of Robot Controllers in Reinforcement Learning as possible. In principle, simulators allow to train the controller with faster and safer procedures: once the policy has been learnt, it is transferred to the real system (sim-to-real transfer). However, sim-to-real transfer is only effective when the simulator is given a sufficiently accurate model of the real robot and the environment; unfortunately, the more accurate the simulation, the heavier the computational cost. A less accurate simulator is therefore often preferred, although it may result in a less effective sim-to-real transfer. The phenomenon in which a controller learnt on simulator degrades once applied on the real world is the so-called reality gap (RG). In the worst case, the RG leads to a failure of the policy when applied on the real world, which means a robot unable to achieve its goal. RL is not the only approach that can be affected by the RG. Any technique in which the controller design relies on a simulator of the real system can potentially exhibit a reality gap. Indeed, several works faced the RG problem in other frameworks, such as Evolutionary Computation,11,12,13,14,15,16,17 or Model Predictive Control,19,20,21. However, here we focus our attention only on those works facing the RG problem on robot controllers learnt with RL. Most of the many solutions proposed in the literature, are task-dependent and/or have been tested on a specific task only. The outcomes are that: (a) generalisation is not ensured, (b) and a comparison between different approaches is not feasible. Although a sketch of the current state of\n\n[10] Sim-to-Real Transfer of Robotic Control with Dynamics Randomization Pushing, a form of non-prehensile manipulation, is an effective strategy for positioning and orienting objects that are too large or heavy to be grasped. Though pushing has attracted much interest from the robotics community, 28, 29, it remains a challenging skill for robots to adopt. Part of the difficulty stems from accurately modeling the complex contact dynamics between surfaces. Characteristics such as friction can vary significantly across the surface of an object, and the resulting motions can be highly sensitive to the initial configuration of the contact surfaces. Models have been proposed to facilitate planning algorithms, 30, 28, but they tend to rely on simplifying assumptions that are often violated in practice. More recently, deep learning methods have been applied to train predictive models for pushing. While data-driven methods overcome some of the modeling challenges faced by previous frameworks, they require a large corpus of real world data during training. Such a dataset can be costly to collect, and may become prohibitive for more complex tasks. Clavera et al. demonstrated transferring pushing policies trained in simulation to a real PR2. Their approach took advantage of shaped reward functions and careful calibration to ensure that the behaviour of the simulation conforms to that of the physical system. In contrast, we will show that adaptive policies can be trained exclusively in simulation and using only sparse rewards. The resulting policies are able accommodate large calibration errors when deployed on a real robot and also generalize to variability in the dynamics of the physical system.\n\n[11] Sim-to-Real Transfer of Robotic Control with Dynamics Randomization Deep reinforcement learning (DeepRL) has been shown to be an effective framework for solving a rich repertoire of complex control problems. In simulated domains, agents have been developed to perform a diverse array of challenging tasks, 2, 3. Unfortunately, many of the capabilities demonstrated by simulated agents have often not been realized by their physical counterparts. Many of the modern DeepRL algorithms, which have spurred recent breakthroughs, pose high sample complexities, therefore often precluding their direct application to physical systems. In addition to sample complexity, deploying RL algorithms in the real world also raises a number of safety concerns both for the agent and its surroundings. Since exploration is a key component of the learning process, an agent can at times perform actions that endanger itself or its environment. Training agents in simulation is a promising approach that circumvents some of these obstacles. However, transferring policies from simulation to the real world entails challenges in bridging the ”reality gap”, the mismatch between the simulated and real world environments. Narrowing this gap has been a subject of intense interest in robotics, as it offers the potential of applying powerful algorithms that have so far been relegated to simulated domains.\n\n[12] Crossing the Reality Gap: a Survey on Sim-to-Real Transferability of Robot Controllers in Reinforcement Learning The growing demand for robots able to act autonomously in complex scenarios has widely accelerated the introduction of Reinforcement Learning (RL) in robots control applications. However, the trial and error intrinsic nature of RL may result in long training time on real robots and, moreover, it may lead to dangerous outcomes. While simulators are useful tools to accelerate RL training and to ensure safety, they often are provided only with an approximated model of robot dynamics and of its interaction with the surrounding environment, thus resulting in what is called the reality gap (RG): a mismatch of simulated and real control-law performances caused by the inaccurate representation of the real environment in simulation. The most undesirable result occurs when the controller learnt in simulation fails the task on the real robot, thus resulting in an unsuccessful sim-to-real transfer. The goal of the present survey is threefold: (1) to identify the main approaches to face the RG problem in the context of robot control with RL, (2) to point out their shortcomings, and (3) to outline new potential research areas.\n\n"}
{"case_id": "6453c86c72949978fb8db5405b48c923", "answer_text": "\n\nYes, there is substantial evidence that large language models (LLMs) can be effectively applied to robot planning tasks, spanning various aspects from task decomposition to motion planning and even multi-robot collaboration [1][2][8]. The following discussion integrates multiple pieces of evidence from recent research to elucidate the effectiveness of LLMs in this domain and offers a comparison of different frameworks to highlight their specific strengths and use cases.\n\nCommonsense Reasoning and Code Understanding [1]. \n\nOne of the primary benefits of using LLMs in robot planning tasks is their ability to leverage commonsense reasoning and code understanding to generate executable plans directly as programs. For instance, the ProgPrompt framework constructs prompts that include situational understanding of the world and robot capabilities, thereby enabling LLMs to generate situated and executable plans that generalize to new scenes, agents, and tasks, including real-robot deployments. ProgPrompt demonstrated state-of-the-art success rates in VirtualHome household tasks and successful deployment on a physical robot arm for tabletop tasks [1]. ProgPrompt excels in scenarios that require flexible adaptation to varying environments and robot capabilities, making it suitable for both virtual and physical tasks.\n\nFeasibility and Multimodal Feedback [3].\n\nResearch has shown that LLMs, such as GPT-3, ChatGPT, and CodeX, encode an extensive amount of common sense, making them suitable for robot task planning in various domains. For example, the SayCan framework takes advantage of LLMs by translating abstract service requests into feasible action sequences through the use of affordance functions, which assess action feasibility based on the current scene [2]. SayCan is particularly effective in household task domains where the feasibility of actions must be rigorously assessed to ensure successful execution. Similarly, Inner Monologue represents another significant advancement by integrating multimodal feedback from the environment, including visual and sensory inputs, to create more reliable and contextually aware task plans [3]. Inner Monologue is ideal for applications requiring real-time adaptation to environmental changes, leveraging its robust multimodal integration.\n\nLarge-Scale and Complex Environments [6]. \n\nFurthermore, LLMs have shown promise in complex and large-scale environments. SayPlan utilizes 3D scene graphs to ground task plans in multi-floor and multi-room environments. This involves hierarchical semantic search and iterative replanning, which refine initial plans based on feedback from a scene graph simulator, ensuring feasibility and avoiding planning failures. SayPlan was evaluated on environments spanning up to 3 floors and 36 rooms with 140 assets and objects, demonstrating its capability to ground large-scale, long-horizon task plans for a mobile manipulator robot [11]. SayPlan is most effective in large, structured environments where detailed spatial information is crucial for successful task execution.\n\nMulti-Robot Collaboration[8].\n\nLLMs are not only effective for single-robot task planning but also exhibit potential in multi-robot scenarios. The SMART-LLM framework converts high-level task instructions into multi-robot task plans through programmatic LLM prompts for task decomposition, coalition formation, and task allocation [8]. Experiments have shown that such frameworks can handle task instructions of varying complexities, generalizing seamlessly to new environments and robot types, thereby streamlining the transition from simulation to real-world applications [9]. Additionally, RoCo introduces a dialectic approach where multiple robots equipped with LLMs collaborate, generate sub-task plans, and refine their strategies using feedback from the environment. RoCo achieved high success rates across various tasks in the RoCoBench benchmark [12]. SMART-LLM and RoCo are particularly beneficial in scenarios requiring coordinated multi-robot operations, with SMART-LLM focusing on task allocation efficiency and RoCo emphasizing collaborative planning and real-time strategy refinement.\n\n\nAdaptive and Corrective Mechanisms [10].\n\n\nThe effectiveness of LLMs in robot planning is further enhanced through adaptive and corrective mechanisms. For instance, the KNOWNO framework aligns the uncertainty of LLM-based planners with real-world task environments using a combination of few-shot prompting and Bayesian inference, which allows robots to perform multi-step planning tasks more autonomously and efficiently [7]. Similarly, CoPAL employs a replanning strategy that adjusts for physically grounded, logical, and semantic errors, enhancing the executability and correctness of generated plans [10]. KNOWNO and CoPAL are essential in high-stakes environments where planning accuracy and adaptability to unforeseen conditions are paramount.\n\n### Challenges and Future Directions[^0^][^1^][^3^]\n\n[^0^]: Grounding Language with Visual Affordances over Unstructured Data Grounding Language with Visual Affordances over Unstructured Data Recent works have shown that Large Language Models (LLMs) can be applied to ground natural language to a wide variety of robot skills. However, in practice, learning multi-task, language-conditioned robotic skills typically requires large-scale data collection and frequent human intervention to reset the environment or help in correcting the current policies.\n[^1^]: ProgPrompt: Generating Situated Robot Task Plans using Large Language Models. \n[^3^]: Large Language Models for Robotics: Opportunities, Challenges, and Perspectives.\n\n\nHowever, it's important to also consider the disadvantages and challenges of using LLMs in robot planning tasks. One notable challenge is the significant computational resources required to train and deploy these models, which can be a barrier for widespread implementation. Additionally, while LLMs exhibit impressive capabilities, they are not infallible and can generate plans that are not feasible in all contexts due to a lack of specific domain knowledge or unforeseen environmental variables. This is evident in the necessity for frameworks like KNOWNO and CoPAL, which introduce corrective and uncertainty alignment mechanisms to address the limitations inherent in LLM-based planning [7, 10].\n\n ### Conclusion\n\nRecent advancements in large-scale language modeling have produced promising results in bridging their semantic knowledge of the world to robot instruction following and planning [0]. Planning with Large Language Models (LLMs) requires having a large set of diverse low-level behaviors that can be seamlessly combined together to intelligently act in the world [0]. \n\n In conclusion, multiple studies and frameworks demonstrate that LLMs can be effectively applied to robot planning tasks across various domains and complexities [1][3]. These models enhance task planning by utilizing commonsense reasoning, multimodal feedback, and adaptive replanning strategies, thus proving their utility in both single-robot and multi-robot scenarios [3][8]. The inclusion of specific quantitative results from the discussed studies further solidifies the evidence of their effectiveness. Nonetheless, the computational demands and the need for additional corrective mechanisms highlight areas for future improvement and optimization [0][10]. \n\n\nReferences:\n[0] Grounding Language with Visual Affordances over Unstructured Data Grounding Language with Visual Affordances over Unstructured Data Recent works have shown that Large Language Models (LLMs) can be applied to ground natural language to a wide variety of robot skills. However, in practice, learning multi-task, language-conditioned robotic skills typically requires large-scale data collection and frequent human intervention to reset the environment or help correcting the current policies. In this work, we propose a novel approach to efficiently learn general-purpose language-conditioned robot skills from unstructured, offline and reset-free data in the real world by exploiting a self-supervised visuo-lingual affordance model, which requires annotating as little as 1% of the total data with language. We evaluate our method in extensive experiments both in simulated and real-world robotic tasks, achieving state-of-the-art performance on the challenging CALVIN benchmark and learning over 25 distinct visuomotor manipulation tasks with a single policy in the real world. We find that when paired with LLMs to break down abstract natural language instructions into subgoals via few-shot prompting, our method is capable of completing long-horizon, multi-tier tasks in the real world, while requiring an order of magnitude less data than previous approaches. Code and videos are available at http://hulc2.cs.uni-freiburg.de I. INTRODUCTION Recent advances in large-scale language modeling have produced promising results in bridging their semantic knowledge of the world to robot instruction following and planning,,. In reality, planning with Large Language Models (LLMs) requires having a large set of diverse lowlevel behaviors that can be seamlessly combined together to intelligently act in the world. Learning such sensorimotor skills and grounding them\n\n[1] ProgPrompt: Generating Situated Robot Task Plans using Large Language Models We present an LLM prompting scheme for robot task planning that brings together the two strengths of LLMs: commonsense reasoning and code understanding.We construct prompts that include situated understanding of the world and robot capabilities, enabling LLMs to directly generate executable plans as programs.Our experiments show that ProgPrompt programming language features improve task performance across a range of metrics.Our method is intuitive and flexible, and generalizes widely to new scenes, agents and tasks, including a real-robot deployment.\n\n[2] Task and Motion Planning with Large Language Models for Object Rearrangement Many LLMs have been developed in recent years, such as BERT , GPT-3 , ChatGPT , CodeX , and OPT .These LLMs can encode a large amount of common sense  and have been applied to robot task planning , 29, 30, 31, 32, 33, 34, 35, 15, 36, 37.For instance, the work of Huang et. al. showed that LLMs can be used for task planning in household domains by iteratively augmenting prompts .SayCan is another approach that enabled robot planning with affordance functions to account for action feasibility, where the service requests are specified in natural language (e.g., “make breakfast”) .Compared with those methods, LLM-GROP optimizes both feasibility and efficiency while computing semantically valid geometric configurations.\n\n[3] Large Language Models for Robotics: Opportunities, Challenges, and Perspectives This interdisciplinary approach transcends the traditional boundaries of LLMs, which predominantly focused on textual comprehension and generation, ushering in an era where these models are adept at interpreting, correlating, and interacting with multiple data streams in unison. In this context, the LLM’s role evolves from mere language processing to a more integrative function, synthesizing and responding to complex data interplays. In the realm of Multimodal Tasks Planning with LLMs, recent advancements exemplified by projects like Inner Monologue and SayCan demonstrate the burgeoning complexity and sophistication in this field. Inner Monologue’s  methodology represents a significant leap in this domain, as it integrates multi-modal feedback sources from the environment. This integration enables the generation of more reliable and contextually aware task planning, harmonizing different sensory inputs to create a more cohesive understanding of the AI’s surroundings. Similarly, the SayCan’s  framework introduces a novel dimension to LLM applications. This system employs LLMs as a proxy for the model’s “hands and eyes,” generating the optimal long-horizon instructions and effectively scoring the affordance probability of the instruction on the current scene. This methodology not only enhances the AI’s ability to understand and interact with its immediate environment but also leverages the nuanced understanding of LLMs to plan and execute complex sequences of actions over extended periods.\n\n[4] Large Language Models for Robotics: Opportunities, Challenges, and Perspectives In the field of robot control, the natural language understanding capabilities of LLM can help robots make common-sense analyses. For example, LLM-GROP demonstrates how semantic information can be extracted from LLM and used as a way to make common-sense, semantically valid decisions about object placement as part of a task and motion planner that performs multistep tasks in complex environments in response to natural language commands . The research proposes a framework for placing language at the core of an intelligent body . By utilizing the prior knowledge contained in these models, better robotic agents can be designed that are able to solve challenging tasks directly in the real world. Through a series of experiments, it is demonstrated how the framework can be used to solve a variety of problems with greater efficiency and versatility by utilizing the knowledge and functionality of the underlying models. At the same time, the study introduces Linguistically Conditional Collision Function (LACO), a novel method to learn collision functions using only single-view image, language prompt, and robot configuration. LACO predicts collisions between robots and the environment, enabling flexible conditional path planning .\n\n[5] Large Language Models for Human-Robot Interaction: Opportunities and Risks In (Ren et al., 2023), the authors introduced a LLM based framework named KNOWNO for aligning the uncertainty of LLM-based planners, allowing them to know when they don’t know and ask for help when needed. With KNOWNO, robots can perform complex multi-step planning tasks with statistical guarantees on task completion while minimizing human help. KNOWNO uses a combination of few-shot prompting and Bayesian inference to align the uncertainty of LLM-based planners with the true uncertainty of the task environment. The framework is tested on a range of tasks, including mobile manipulation, navigation, and question-answering, and is shown to outperform modern baselines in terms of efficiency and autonomy. In short, LLM proved effective for planning in robots.\n\n[6] SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models In this work, we introduce SMART-LLM, an innovative framework designed for embodied multi-robot task planning. SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models (LLMs), harnesses the power of LLMs to convert high-level task instructions provided as input into a multi-robot task plan. It accomplishes this by executing a series of stages, including task decomposition, coalition formation, and task allocation, all guided by programmatic LLM prompts within the few-shot prompting paradigm. We create a benchmark dataset designed for validating the multi-robot task planning problem, encompassing four distinct categories of high-level instructions that vary in task complexity. Our evaluation experiments span both simulation and real-world scenarios, demonstrating that the proposed model can achieve promising results for generating multi-robot task plans. The experimental videos, code, and datasets from the work can be found at https://sites.google.com/view/smart-llm/.\n\n[7] SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models Our experiments validate that the proposed method can handle task instructions of varying complexities. Notably, our approach exhibits remarkable adaptability, allowing it to seamlessly generalize to new and unexplored environments, robot types, and task scenarios. This method streamlines the transition from simulations to real-world robot applications, enabling task plan samples from simulations to be used for generating task plans for real robot systems. In the future, we aim to enhance our work by implementing dynamic task allocation among robots and exploring multi-agent LLM frameworks for task planning.\n\n[8] CoPAL: Corrective Planning of Robot Actions with Large Language Models In the pursuit of fully autonomous robotic systems capable of taking over tasks traditionally performed by humans, the complexity of open-world environments poses a considerable challenge. Addressing this imperative, this study contributes to the field of Large Language Models (LLMs) applied to task and motion planning for robots. We propose a system architecture that orchestrates a seamless interplay between multiple cognitive levels, encompassing reasoning, planning, and motion generation. At its core lies a novel replanning strategy that handles physically grounded, logical, and semantic errors in the generated plans. We demonstrate the efficacy of the proposed feedback architecture, particularly its impact on executability, correctness, and time complexity via empirical evaluation in the context of a simulation and two intricate real-world scenarios: blocks world, barman and pizza preparation.\n\n[9] SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Task Planning Large language models (LLMs) have demonstrated impressive results in developing generalist planning agents for diverse tasks. However, grounding these plans in expansive, multi-floor, and multi-room environments presents a significant challenge for robotics. We introduce SayPlan, a scalable approach to LLM-based, large-scale task planning for robotics using 3D scene graph (3DSG) representations. To ensure the scalability of our approach, we: (1) exploit the hierarchical nature of 3DSGs to allow LLMs to conduct a 'semantic search' for task-relevant subgraphs from a smaller, collapsed representation of the full graph; (2) reduce the planning horizon for the LLM by integrating a classical path planner and (3) introduce an 'iterative replanning' pipeline that refines the initial plan using feedback from a scene graph simulator, correcting infeasible actions and avoiding planning failures. We evaluate our approach on two large-scale environments spanning up to 3 floors and 36 rooms with 140 assets and objects and show that our approach is capable of grounding large-scale, long-horizon task plans from abstract, and natural language instruction for a mobile manipulator robot to execute. We provide real robot video demonstrations on our project page https://sayplan.github.io.\n\n[10] RoCo: Dialectic Multi-Robot Collaboration with Large Language Models We propose a novel approach to multi-robot collaboration that harnesses the power of pre-trained large language models (LLMs) for both high-level communication and low-level path planning. Robots are equipped with LLMs to discuss and collectively reason task strategies. They generate sub-task plans and task space waypoint paths, which are used by a multi-arm motion planner to accelerate trajectory planning. We also provide feedback from the environment, such as collision checking, and prompt the LLM agents to improve their plan and waypoints in-context. For evaluation, we introduce RoCoBench, a 6-task benchmark covering a wide range of multi-robot collaboration scenarios, accompanied by a text-only dataset that evaluates LLMs’ agent representation and reasoning capability. We experimentally demonstrate the effectiveness of our approach — it achieves high success rates across all tasks in RoCoBench and adapts to variations in task semantics. Our dialog setup offers high interpretability and flexibility — in real world experiments, we show RoCo easily incorporates human-in-the-loop, where a user can communicate and collaborate with a robot agent to complete tasks together.\n\n"}
{"case_id": "7bb6a69f01d91674d5a925266e360b44", "answer_text": "\n\nEvaluating variable type inference for decompiled binaries is an essential domain within software reverse engineering, aimed at recovering semantic information lost during compilation and stripping processes [8]. Such efforts are critical for understanding and analyzing closed-source programs, improving decompiler outputs [2], and enhancing software security [8]. Several datasets have been employed for training and evaluating models in this context, each with unique characteristics and emphases [4]. \n\nDatasets with a Python Focus [3]. \n\n**ManyTypes4Py Dataset**: This large Python dataset includes partial type annotations for functions and variables and aims to serve as a benchmark for ML-based type inference models. It uses abstract syntax trees (ASTs) for static analysis, providing a comprehensive resource for training and evaluating ML models in type inference [3][9][14]. Metrics used for evaluation include the accuracy of type predictions and the model's ability to generalize to unseen code samples. The ManyTypes4Py dataset is utilized in experiments to test models like Type4Py, which benefits from the extensive annotations and AST analysis to improve type prediction accuracy. However, its limitation lies in its focus on Python, potentially limiting its applicability to other programming languages.\n\n**TYPEPY Dataset**: Featured in the evaluation of PYInfer, this dataset comprises 500 of the most common types derived from analyzing a large corpus of Python code. The dataset is rich in annotations and has been used to evaluate the effectiveness of type inference models, especially in light of the uneven distribution of real-world variable usage [7]. The evaluation typically involves metrics such as precision, recall, and the overall accuracy of predicted type annotations. A notable limitation is its focus on common types, which might not encompass the full range of types encountered in practice.\n\n\n### Datasets for General Binary Code Analysis\n\nCode analysis is a powerful way to eliminate vulnerabilities. Closed-source programs lack crucial information vital for code analysis because that information is stripped on compilation to achieve smaller executable size [0]. Restoration has always been a challenge for experts. Variable type information is fundamental in this process because it helps to provide a perspective on program semantic [1]. In this paper, we present an efficient approach for inferring types, and we overcome the challenge of scattered information provided by static analysis on stripped binaries. We discover that neighboring instructions are likely to operate the same type of variables, which are leveraged to enrich the features that we rely on. Therefore, we implement a system called CATI, which locates variables from stripped binaries and infers 19 types from variables. Experiments show that it infers variable type with 71.2% accuracy on unseen binaries. Meanwhile, it takes approximately 6 seconds to process a typical binary.\n\n\n**DIRT Dataset**: This dataset includes debug information that often changes the layout of the code as the decompiler adds structures and syntax for accessing them. The DIRT dataset is noteworthy for its focus on recovering types from C binaries [12]. TypeMiner is an example of work utilizing this dataset. TypeMiner employs simpler machine learning algorithms on a dataset consisting of 23,482 variables and 17 primitive types, aiming to recover types from stripped binaries. The limitation here is the relatively small size and the use of simpler machine learning algorithms, which may not capture the full complexity of types in more diverse datasets.\n\n**EKLAVYA Dataset**: Although not explicitly named, this dataset includes real open-source applications and synthetic programs. It has been employed to improve type information inferred by decompilers through supervised machine learning techniques [8]. EKLAVYA’s dataset facilitated the association of binary patterns with high-level constructs, significantly enhancing the accuracy of function return type predictions. Specific applications include training classification models that predict function return types with an F1-measure of 79.1%, compared to a 30% F1-measure from existing decompilers. The limitation here is the dataset's focus on specific compiler settings and the potential lack of generalizability to other compilers or languages.\n\n**CAPYBARA Dataset**: Introduced alongside the BinT5 model, this dataset supports the summarization of decompiled binaries. CAPYBARA includes both mined and synthetic data, designed to train and evaluate models like BinT5 in a novel automatic binary code summarization task [4][6]. The evaluation metrics often involve the quality of the generated summaries, assessed through measures like BLEU scores that compare the generated text to reference summaries. In practice, CAPYBARA is used to benchmark the BinT5 model's ability to generate coherent and accurate summaries of decompiled code, demonstrating the model's effectiveness and highlighting challenges in the summarization task. The limitation here is the complexity of aligning decompiled and source code accurately for effective training.\n\nDatasets for Variable Naming and Semantic Recovery [2].\n\n**DIRE Dataset**: Used in the development of the Decompiled Identifier Renaming Engine (DIRE), this dataset consists of 164,632 unique x86-64 binaries generated from C projects mined from GitHub. It serves to predict variable names identical to those in the original source code, reflecting its utility in improving code understandability and facilitating semantic recovery [2][13]. Evaluation metrics for this dataset include the accuracy of variable name predictions and the ability to maintain semantic consistency with the original source code. In DIRE, this dataset allows for training and testing naming models, which achieve a prediction accuracy of up to 74.3%, demonstrating the technique's efficiency in renaming variables in decompiled code. However, its limitation is the potential overfitting to specific patterns in the GitHub-mined dataset and the challenge of maintaining high accuracy across diverse coding practices.\n\nComparison of Datasets\n\n - **Size**: The size of datasets varies significantly. The ManyTypes4Py dataset includes more than 869K type annotations from 5,382 Python projects, making it one of the largest available [9]. On the other hand, the TYPEPY dataset, used in PYInfer, focuses on 500 common types, likely representing smaller but more targeted data [7]. \n\n - **Complexity**: DIRT and EKLAVYA datasets include debug information and are used for training simpler machine learning models, implying a moderate level of complexity [12]. In contrast, ManyTypes4Py and CAPYBARA datasets support more advanced models like transformers (BinT5), indicating higher complexity [4]. \n\nThe type of annotations provided also differs. ManyTypes4Py and TYPEPY offer extensive type annotations extracted from ASTs, aiding in detailed type inference tasks [3]. DIRT and EKLAVYA primarily focus on lower-level binary patterns and function return types [8]. CAPYBARA provides annotated summaries [4], while DIRE focuses on variable names, highlighting distinct semantic recovery purposes [2]. \n\nRecent advancements in machine learning have significantly improved the ability to infer variable types from binary code [0]. Techniques such as decompilation can help recover much of the information lost during compilation, including structure and type information [2]. However, challenges remain in reconstructing semantically meaningful variable names, which are important for code understandability [2]. To address these, probabilistic techniques have been developed to recover variable names by utilizing both lexical and structural information [2]. Furthermore, supervised machine learning approaches have been employed to enhance type information inferred by decompilers, achieving higher accuracy than traditional methods [5]. These advancements highlight the ongoing progress in code analysis and the potential for machine learning to further enhance reverse engineering practices [5]. \n\nRecent advancements in the field of variable type inference for decompiled binaries have significantly benefited from the development of these datasets [4]. There is a clear trend towards using more sophisticated machine learning models, particularly deep learning techniques, to handle the complexity of code decompilation and type inference [5]. \n\nOne notable advancement is the use of transformer-based models, such as BinT5, which leverage the CAPYBARA dataset to achieve impressive results in binary code summarization tasks [4]. These models demonstrate the potential of transfer learning and pre-trained language models in enhancing the quality and accuracy of decompiled code summaries [4].\n\nMoreover, the focus on creating extensive and diverse datasets, such as ManyTypes4Py, has enabled the training of more robust and generalizable models [9]. The inclusion of contextual semantic information and comprehensive annotations has proven to be crucial in improving the performance of type inference models across different programming languages and coding styles [7].\n\nAnother trend is the integration of probabilistic and graph-based models, as seen in the DIRE project, which combines lexical and structural information to improve the recovery of meaningful variable names [2]. This approach highlights the importance of multi-faceted data representations in tackling the challenges of decompilation. \n\nOverall, the field is moving towards leveraging larger, more diverse datasets and advanced machine learning techniques to enhance the accuracy and usability of decompiled code [5]. These advancements underline the importance of continuous dataset development and model innovation to address the evolving challenges in software reverse engineering [5].\n\n\nReferences:\n[0] CATI: Context-Assisted Type Inference from Stripped Binaries CATI: Context-Assisted Type Inference from Stripped Binaries Code analysis is a powerful way to eliminate vulnerabilities. Closed-source programs lack crucial information vital for code analysis because that information is stripped on compilation to achieve smaller executable size. Restoration has always been a challenge for experts. Variable type information is fundamental in this process because it helps to provide a perspective on program semantic. In this paper, we present an efficient approach for inferring types, and we overcome the challenge of scattered information provided by static analysis on stripped binaries. We discover that neighboring instructions are likely to operate the same type of variables, which are leveraged to enrich the features that we rely on. Therefore, we implement a system called CATI, which locates variables from stripped binaries and infers 19 types from variables. Experiments show that it infers variable type with 71.2% accuracy on unseen binaries. Meanwhile, it takes approximately 6 seconds to process a typical binary.\n\n[1] What Exactly Determines the Type? Inferring Types with Context What Exactly Determines the Type? Inferring Types with Context Closed-source programs lack crucial information vital for code analysis because that information is stripped on compilation to achieve smaller executable size. Variable type information is fundamental in this process. In this paper, we implement a system called CATI (Context-Assisted Type Inference), which locates variables from stripped binaries and infers 19 types from variables. Experiments show that it infers variable type with 71.2% accuracy on unseen binaries.\n\n[2] DIRE: A Neural Approach to Decompiled Identifier Naming The decompiler is one of the most common tools for examining binaries without corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Decompilers can reconstruct much of the information that is lost during the compilation process (e.g., structure and type information). Unfortunately, they do not reconstruct semantically meaningful variable names, which are known to increase code understandability. We propose the Decompiled Identifier Renaming Engine (DIRE), a novel probabilistic technique for variable name recovery that uses both lexical and structural information recovered by the decompiler. We also present a technique for generating corpora suitable for training and evaluating models of decompiled code renaming, which we use to create a corpus of 164,632 unique x86-64 binaries generated from C projects mined from GitHub. Our results show that on this corpus DIRE can predict variable names identical to the names in the original source code up to 74.3% of the time.\n\n[3] ManyTypes4Py: A Benchmark Python Dataset for Machine Learning-based Type Inference In this application, an ML model is expected to predict part of a word or token for a function or a variable. For DPLs, code completion is a challenging task as there is no type information available. To overcome this, ASTs are statically analyzed while providing type information. The ManyTypes4Py dataset can be used as a baseline for training a code completion model as it provides partial type annotations for functions and variables. Also, our AST analysis pipeline (LibSA4Py tool) can further be extended to infer types of nodes and variables for simple cases.\n\n[4] Extending Source Code Pre-Trained Language Models to Summarise Decompiled Binarie In this paper, we proposed a new automatic binary code summarisation task. With this new task, we also introduce CAPYBARA, a novel dataset to train and evaluate models on this task, with both mined as well as synthetic data. Paired with this dataset, we train BinT5, a Transformer-based code summarisation model to show the effectiveness of CAPYBARA. We used BinT5 to further explore the datasets, outlining the inherent difficulties in the data.\n\n[5] Improving type information inferred by decompilers with supervised machine learning Improving type information inferred by decompilers with supervised machine learning In software reverse engineering, decompilation is the process of recovering source code from binary files. Decompilers are used when it is necessary to understand or analyze software for which the source code is not available. Although existing decompilers commonly obtain source code with the same behavior as the binaries, that source code is usually hard to interpret and certainly differs from the original code written by the programmer. Massive codebases could be used to build supervised machine learning models aimed at improving existing decompilers. In this article, we build different classification models capable of inferring the high-level type returned by functions, with significantly higher accuracy than existing decompilers. We automatically instrument C source code to allow the association of binary patterns with their corresponding high-level constructs. A dataset is created with a collection of real open-source applications plus a huge number of synthetic programs. Our system is able to predict function return types with a 79.1% F1-measure, whereas the best decompiler obtains a 30% F1-measure. Moreover, we document the binary patterns used by our classifier to allow their addition in the implementation of existing decompilers. Introduction A decompiler is a tool that receives binary code as input and generates high-level code with the same semantics as the input. Although the decompiled source code can be recompiled to produce the original binary code, the high-level source code is not commonly the one originally written by the programmer. In fact, the source code is usually much less readable\n\n[6] Extending Source Code Pre-Trained Language Models to Summarise Decompiled Binarie To summarise, the main contributions of this paper are:•CAPYBARA111CAPYBARA: https://doi.org/10.5281/zenodo.7229809, a dataset of Combined Aligned decomPiled BinarY code And Related Annotations. A novel dataset of aligned, C, decompiled, stripped-decompiled and demi-stripped summary pairs222Decompiled code with strip-like obfuscation applied (Section III);•BinT5333BinT5: https://doi.org/10.5281/zenodo.7229913, a Binary summarisation CodeT5 model, a simple and straightforward adaptation of a source code trained code summarisation model to decompiled code using CAPYBARA (Section IV);•An empirical investigation on the impact of the properties of decompiled code and the properties of CAPYBARA (Sections V and  VI);The materials, including the processed and raw data, the trained model checkpoints and steps to replicate our experiments, are openly available in our replication package444Replication package: https://github.com/AISE-TUDelft/Capybara-BinT5.\n\n[7] PYInfer: Deep Learning Semantic Type Inference for Python Variables EVALUATION In this section, we evaluate PYInfer by answering the following research questions: • RQ 1: How effective is PYInfer at deriving the correct type annotations? • RQ 2: Does the number of classes considered for classification have a significant impact on PYInfer? • RQ 3: How does the threshold influence our PYInfer model? A. RQ 1: Model Effectiveness and Baseline Comparison 1) Dataset and Experiment Settings: In this RQ, we analyze our context model's performance with 500 most common types in the typePY dataset. Besides all built-in types in Python, we also consider a large amount of user-defined types. After analyzing the source code corpus, we find that duplicates exist in our dataset. The main reason is that some GitHub repositories reuse the same code from the others. We perform deduplication on our dataset, and derive 3,499,933 annotations for evaluation. We also inspect some of the type distribution, as shown in Table II. This uneven distribution reflects the difference in real-world variable usage on different types. Our whole data corpus is randomly split into training, validation, and testing data with the proportion of 60%:20%:20%. We run all experiments on a single machine with Intel i7-9700k CPU, 32GB RAM, and a single NVIDIA RTX 2070 Super GPU. 2) Implementing Details and Results: We train our PYInfer model with parameters in Table III, and report testing results. To analyze contextual code semantics, we generate the embeddings by analyzing contextual semantics in source code with a separator between each part in our margin settings. We obtain the vector\n\n[8] Type Learning for Binaries and Its Applications Type Learning for Binaries and Its Applications Binary type inference is a challenging problem due partly to the fact that during the compilation much type-related information has been lost. Most existing research work resorts to program analysis techniques, which can be either too heavyweight to be viable in practice or too conservative to be able to infer types with high accuracy. In this paper, we propose a new approach to learning types for binary code. Motivated by “duck typing,” our approach learn types for recovered variables from their features and properties (e.g., related representative instructions). We first use machine learning to train a classifier with basic types as its levels from binaries with debugging information. The classifier is then used to learn types for new and unseen binaries. While for composite types, such as <inline-formula><tex-math notation=\"LaTeX\">${pointer}$</tex-math></inline-formula> and <inline-formula><tex-math notation=\"LaTeX\">${struct}$</tex-math></inline-formula>, a points-to analysis is performed. Finally, several experiments are conducted to evaluate our approach. The results demonstrate that our approach is more precise, both in terms of correct types and compatible types, than the commercial tool Hex-Rays, the open source tool Snowman, and a recent tool EKLAVYA using machine learning. We also show that the type information our proposed system learns is capable of helping detect malware.\n\n[9] ManyTypes4Py: A Benchmark Python Dataset for Machine Learning-based Type Inference In this paper, we present ManyTypes4Py, a large Python dataset for machine learning (ML)-based type inference. The dataset contains a total of 5,382 Python projects with more than 869K type annotations. Duplicate source code files were removed to eliminate the negative effect of the duplication bias. To facilitate training and evaluation of ML models, the dataset was split into training, validation and test sets by files. To extract type information from abstract syntax trees (ASTs), a light-weight static analyzer pipeline is developed and accompanied with the dataset. Using this pipeline, the collected Python projects were analyzed and the results of the AST analysis were stored in JSON-formatted files. The ManyTypes4Py dataset is shared on zenodo and its tools are publicly available on GitHub.\n\n[10] Augmenting Decompiler Output with Learned Variable Names and Types languages like these is a slightly easier task: generating a parallel corpus is simple, since the types can simply be removed without changing the semantics. The DIRT dataset is fundamentally different: including debug information often changes the layout of the code as the decompiler adds structures and syntax for accessing them. To the best of our knowledge, the most directly-related work to DIRTY is TypeMiner. TypeMiner is a pioneering work, providing the proof-of-concept for recovering types from C binaries. However, they use much simpler machine learning algorithms and their dataset only consists of 23,482 variables and 17 primitive types. Escalada et al. has provided similar insights. They adopt simple classification algorithms to predict function return types in C, but they only consider from only 10 different (syntactic) types and their dataset is limited to 2,339 functions from real programs and 18,000 synthetic functions. Two other projects targeting the improvement of decompiler output using neural models are DIRE, which predicts variable names, DIRECT, which extends DIRE using transformer-based models, and Nero, which generates procedure names. Other approaches work directly on assembly,26,27, and learn code structure generation instead of aiming to recover developer-specified variable types or names. Similarly, DEBIN and CATI use machine learning to respectively predict debug information and types directly from stripped binaries without a decompiler. Discussion In this paper we presented DIRTY, a novel deep learningbased technique for predicting variable types and names in decompiled code. Still, DIRTY is limited in several ways that provide key opportunities\n\n[11] DIRE: A Neural Approach to Decompiled Identifier Naming With these insights we train and evaluate DIRE on a large dataset of C code mined from GitHub, showing that we can predict variable names identical to those chosen by the original developer up to 74.3% of the time.In short, we contribute:•Decompiled IdentifierRenaming Engine (DIRE), a technique for assigning meaningful names to decompiled variables that outperforms previous approaches.•A novel technique for generating corpora suitable for training both lexical and graph-based probabilistic models of variable names in decompiled code.•A dataset of 3,195,962 decompiled x86-64 functions and parse trees annotated with gold-standard variable names.11footnotemark: 1\n\n[12] ManyTypes4Py: A Benchmark Python Dataset for Machine Learning-based Type Inference We believe that there is a need for a large benchmark dataset that facilitates training ML-based type inference models, especially for Python. Unlike TypeScript’s compiler, the Python interpreter cannot infer the type of variables or function signatures at compile time. Motivated by this, we present the ManyTypes4Py dataset, a large dataset to train ML models for predicting type annotations in Python. Currently, we are working on the Type4Py model, which is trained on the earlier version of the ManyTypes4Py dataset. The experimental results show that the model trained on our dataset is overall more accurate when compared to the same model trained on a smaller dataset.\n\n"}
{"case_id": "3a19520deab02250c7384cfbc79fcbff", "answer_text": "\n\nPost-quantum cryptographic (PQC) algorithms such as Kyber and Dilithium are designed to be resilient against quantum computing attacks, setting them apart from traditional cryptographic methods like RSA and ECC [1]. However, this quantum resistance comes with performance and energy cost trade-offs [0]. \n\nPerformance Comparison: RSA and ECC, although widely used, are vulnerable to quantum attacks because they are based on mathematical problems that quantum computers can solve efficiently. Dilithium and Kyber, on the other hand, were specifically designed to resist such attacks and provide robust security in anticipation of quantum computing advances. [1] However, there are also performance trade-offs to consider. Regarding computational speed, RSA and ECC generally outperform PQC-based solutions such as Dilithium and Kyber. [1] \n\n\nKyber and Dilithium, like many PQC algorithms, generally exhibit slower computational performance compared to traditional cryptographic methods such as RSA and ECC. For instance, while RSA and ECC algorithms are optimized and widely used, they are vulnerable to quantum attacks [1]. Kyber and Dilithium, on the other hand, are designed to resist these attacks but may require more computational resources, resulting in slower performance, particularly in resource-constrained environments [1][2]. Specifically, Dilithium can be more computationally expensive for signature generation compared to RSA-4096 [0]. On more powerful devices such as a Raspberry Pi 4, Kyber's key encapsulation and decapsulation functions show comparable efficiency, requiring only 100-300 microseconds, although they may still lag behind traditional methods in some specific scenarios [0].\n\nEnergy consumption is a significant consideration in the deployment of post-quantum cryptography (PQC) schemes. Studies have shown that lattice-based cryptographic algorithms, such as Kyber and Dilithium, exhibit varying levels of energy efficiency. For example, evaluations of Dilithium and Falcon reveal that their power consumption is comparable to that of RSA-4096, a commonly used pre-quantum cryptographic algorithm [0]. Research also indicates that while lattice-based key encapsulation mechanisms (KEMs) such as Kyber may have favorable energy consumption profiles compared to traditional schemes like ECDH, this advantage is partly offset by the increased bandwidth requirements associated with PQC [6]. Additionally, the integration of hardware accelerators and instruction set extensions tailored for lattice-based PQC can significantly reduce energy consumption and enhance performance, making these schemes more viable for resource-constrained environments [12]. The careful selection and combination of post-quantum digital signature algorithms and KEMs, such as employing Dilithium for client-side certificates and Kyber for key exchanges, can optimize the trade-off between energy consumption, latency, and bandwidth in low-power IoT applications [4].\nThe energy consumption of PQC algorithms is another significant concern, especially for applications in low-power environments such as the Internet of Things (IoT). Studies have shown that lattice-based schemes like Kyber and Dilithium have energy costs that can be on par with or even exceed traditional cryptographic methods. For instance, on devices such as the Raspberry Pi 3 and 4, lattice-based KEMs like Kyber and NTRU have shown faster key exchanges than ECDH, but signature schemes like Dilithium still lag behind ECDSA in terms of energy efficiency [6]. In signature-based authentication scenarios, the power consumption of Dilithium and Falcon matches that of RSA-4096, demonstrating significant energy demands [0]. This suggests that while lattice-based PQC algorithms are feasible for certain applications, their higher computational and energy demands could be challenging for energy-constrained devices.\n\n**Key Size, Bandwidth, and Storage:**\nAnother critical factor is the key size associated with PQC algorithms. Compared to RSA and ECC, PQC algorithms typically have larger key sizes, which impacts storage and bandwidth requirements. Kyber, for instance, has a key size ranging from 800 bytes to 1.5 kilobytes, which is smaller than the 2-kilobyte key size of RSA but still larger than those of ECC [2][9]. Larger key sizes can increase the bandwidth requirements and affect the overall performance, especially in networked environments [6]. For comparison, ECC typically operates with much smaller key sizes. For example, ECC-256 has a key size of just 256 bits, providing efficient transmission and storage benefits [1]. These increased key sizes in PQC translate to higher transmission overhead and storage demands, which can be particularly burdensome in constrained environments like IoT devices.\n\nPractical Applications and Case Studies: Kyber (and lattice-based cryptography in general) can run efficiently even on constrained devices. For instance, Kyber768 can operate within just 13 kilobytes of RAM, although this process takes approximately 9.8 seconds with the aid of a cryptography co-processor. On more powerful devices like the Raspberry Pi 4, core functions of Kyber can be computed in 100-300 microseconds [0]. Lattice-based cryptography like Kyber and SABER have shown promising performance metrics, outperforming traditional schemes like RSA in certain conditions, especially when assisted by hardware acceleration [1]. Moreover, the integration of Kyber in practical applications is facilitated by its relatively small key sizes, which range from 800 bytes to 1.5 kilobytes, in contrast to the 2-kilobyte key size required for RSA [2]. Additionally, the power consumption of lattice-based signatures such as Dilithium and Falcon is comparable to RSA-4096, making them suitable for applications where energy efficiency is critical [1]. Recent advances also highlight that lattice-based schemes like Kyber can achieve impressive throughput performance, with optimizations allowing for thousands of operations per second on devices equipped with dedicated hardware like small GPUs [7]. \nPractical implementations of Kyber and Dilithium have already begun, highlighting their feasibility and potential challenges. For example, Cloudflare has integrated CRYSTALS-Kyber into its Reusable Cryptographic Library, and Amazon has introduced a hybrid Kyber mode, underscoring the industry's move towards adopting PQC [2]. Additionally, a post-quantum TLS protocol using Kyber-SPHINCS+ cipher suite demonstrated reasonable feasibility for secure communication, though it introduced significant latency and memory usage, especially due to the SPHINCS+ signatures [5]. Another case study involved running Kyber-512 on a Jetson Nano using GPU acceleration, where it achieved a throughput performance of 1,345 key encapsulations per second, illustrating the benefits of hardware acceleration for PQC [7].\n\n\n**Future Hardware Optimizations:** The efficient implementation of post-quantum cryptography (PQC) systems on various hardware platforms is crucial for their practical application. For instance, Kyber and Dilithium operations can achieve significant performance improvements through hardware acceleration [12]. Recent approaches utilize hardware/software co-design to integrate powerful accelerators into processors, enhancing the performance of lattice-based PQC algorithms while maintaining low silicon area overhead [12]. Moreover, advancements in GPU acceleration have demonstrated substantial performance boosts for cryptographic operations, such as key encapsulation and decapsulation, on embedded devices like the Jetson Nano [7]. The application of custom countermeasures against side-channel and fault injection attacks also contributes to the secure and efficient deployment of PQC systems on hardware [14]. These optimizations are critical as they address the latency and energy consumption challenges posed by PQC, especially in constrained environments like IoT devices [4].\n\nFuture advancements in hardware optimizations can mitigate some of the performance and energy costs associated with PQC algorithms. For instance, hardware acceleration using GPUs or FPGA implementations have shown promising results in improving the performance of PQC algorithms. A study demonstrated that GPU acceleration on devices like the Jetson Nano significantly enhances the throughput performance of Kyber-512, achieving 1,345 key encapsulations per second and 918 key decapsulations per second [7]. Similarly, integrating specialized hardware accelerators and optimizing the instruction set architecture (ISA) for specific lattice-based operations can also yield substantial performance gains. The RISQ-V architecture, which includes tightly coupled hardware accelerators for Kyber and Saber, has shown up to 9.6 times speedup for Kyber and significant reductions in energy consumption [12]. Additionally, the development of unified architectures like KaLi, which efficiently handle multiple PQC operations, represents a step towards optimizing latency and resource utilization for PQC implementations [10].\n\n**Systematic Evaluation of Future Hardware Optimizations:**\nTo accelerate linear performance bottlenecks, we developed a generic Number Theoretic Transform (NTT) multiplier, which, in contrast to previously published accelerators, is also efficient and suitable for schemes not based on NTT. For the critical non-linear operations, masked HW accelerators were developed, allowing a secure execution using RISC-V instruction set extensions [8]. With the proposed design, we achieved a cycle count of K:214k/E:298k/D:313k for Kyber and K:233k/E:312k/D:351k for Saber with NIST Level III parameter sets [8]. For the same parameter sets, the masking overhead for the first-order secure decapsulation operation including randomness generation is a factor of 4.48 for Kyber (D:1403k) and 2.60 for Saber (D:915k) [8]. Our results show that PKIs combining Dilithium-based client certificates and Falcon-based server and CA certificates together with KYBER-based key-exchanges achieve the best trade-off between bandwidth requirements, latency and energy consumption [4]. These results prove that the usage of post-quantum KEMs and DSAs is already feasible today in terms of energy demand and latency for low-power IoT devices with state-of-the-art hardware [4]. For smaller devices, Lee et al. show that the same technique is also feasible on a Jetson Nano, which is an embedded device with a Cortex-A53 processor and a small 128-core GPU [7]. It is shown that by utilizing GPU acceleration on the Jetson Nano, Kyber-512 can achieve a throughput performance of 1,345 key encapsulations per second and 918 key decapsulations per second [7].\n\nTo systematically evaluate the impact of future hardware optimizations, a multi-faceted approach can be adopted. Firstly, benchmarking diverse hardware platforms with and without hardware acceleration for PQC operations provides clear performance comparisons. This can be done by evaluating key metrics such as computation time, energy consumption, and thermal footprint across various devices under identical test conditions. Secondly, establishing standardized testbeds and utilizing public repositories of performance data can help track improvements over time. For example, the pqm4 library for ARM Cortex-M4-based microcontrollers and the liboqs library for Raspberry Pi 3 Model B Plus [14] provide valuable data for such analyses. Thirdly, leveraging simulation frameworks to predict performance improvements from theoretical hardware modifications can guide practical implementations. These combined strategies ensure a comprehensive understanding of how future hardware developments will influence the efficiency of PQC algorithms. \n\nIn summary, while post-quantum cryptographic algorithms such as Kyber and Dilithium provide robust security against quantum attacks [1], they currently exhibit higher performance and energy costs compared to traditional methods [0]. However, ongoing advancements in hardware optimization and specialized accelerators hold promise for bridging this performance gap and making PQC more feasible for a broader range of applications [12]. \n\n\nReferences:\n[0] Post-Quantum Cryptography for Internet of Things: A Survey on Performance and Optimization The efficiency of Kyber (and lattice-based cryptography in general) is also shown in, 36. In, Mayes demonstrated that the encryption arithmetic of Kyber768 can be run inside a MULTOS Trust Anchor with just 13 kilobytes of RAM, although the process requires approximately 9.8 seconds even with the aid of a cryptography co-processor. For more powerful devices, showed that a Raspberry Pi 4 requires 100-300 microseconds to compute main functions (key generation, encapsulation and decapsulation) of Kyber and SABER, yet lattice-based signatures (Dilithium and Falcon) are more expensive to compute in general. Another study,, investigated the power consumption of using Dilithium and Falcon for signature-based authentication. It was found that Dilithium and Falcon’s power consumption is in fact on par with RSA-4096, which is commonly used in the pre-quantum era.\n\n[1] Exploring Post Quantum Cryptography with Quantum Key Distribution for Sustainable Mobile Network Architecture Design Existing cryptographic solutions such as RSA and ECC can be compared with potential PQC-based solutions such as Dilithium and Kyber in terms of several key features. In terms of security levels, RSA and ECC, although widely used, are vulnerable to quantum attacks because they are based on mathematical problems that quantum computers can solve efficiently. Dilithium and Kyber, on the other hand, were specifically designed to resist such attacks and provide robust security in anticipation of quantum computing advances.However, there are also performance trade-offs to consider. Regarding computational speed, RSA and ECC generally outperform PQC-based solutions such as Dilithium and Kyber. The latter may require more computing resources and energy, leading to slower performance, especially in resource-constrained environments. The key size is another distinguishing feature. RSA and ECC often have smaller key sizes, which can be advantageous for efficiency and storage. In contrast, PQC-based solutions may require larger key sizes, which affects storage requirements and transmission overhead. RSA and ECC benefit from well-established interoperability with existing systems and protocols, contributing to their broad acceptance. However, PQC-based solutions such as Dilithium and Kyber may require further development and standardization efforts to achieve comparable interoperability with existing cryptographic infrastructures. Therefore, stakeholders must carefully consider the impact on performance, key sizes, and interoperability requirements when evaluating their suitability for specific applications.\n\n[2] Post Quantum Cryptography: Techniques, Challenges, Standardization, and Directions for Future Research the hardness of solving the module learning-witherrors (MLWE) problem. KYBER has been proposed as a public key algorithm that provides levels of security comparable to AES schemes. NIST uses AES-128,192,256 as standards for the security of level 1,3,5 respectively. Kyber flavors such as kyber-512,768,1024 can provide mildly comparable levels of security (approx. within 230 of AES-256). It is one of the most competitive proposals in place due to its performance in comparison with other proposed cryptographic schemes. The change in levels of security can be implemented by simply changing the order of the block matrices used in the algorithm. Due to the relatively faster performance of lattice systems, and smaller public key sizes, such systems make an attractive option for implementation. KYBER enforces this by displaying a key size of 800 bytes to 1.5 kilobytes, compared to the 2-kilobyte key required for present-day RSA. Due to its dependence on MLWE, many algebraic attacks applicable to the LWE problem must be defended against, due to public key compression, questions were raised on its effect on the cryptanalysis of the ciphertext, however, no effects have so far been found. In later versions, the compression was removed and parameters were shifted to symmetric primitives based on SHAKE256, rather than the previous SHA3-256 protocol. CRYSTALS-KYBER also shares its framework with CRYTSALS-DILITHIUM, a digital signature scheme,which makes it more attractive to implement as a complete suite. CRYSTALS-KYBER along with other post-quantum algorithms have been implemented by CloudFare in its Reusable Cryptographic Library. Amazon has introduced a hybrid KYBER mode\n\n[3] Secure IoT in the Era of Quantum Computers—Where Are the Bottlenecks? benefits of dedicated hardware accelerators are very limited for low-power IoT applications, especially for the popular lattice-based KEMs and the DSA Dilithium, for which accelerators have been proposed in many publications. The only exceptions to this are applications where Falcon is used for client-side signing or SIKE, as then latency and power requirements are significantly affected by computations. • Heterogeneous PKIs reduce latency and energy consumption: A combination of different post-quantum DSAs that exploits their individual benefits regarding bandwidth requirements and computational complexity yields the most energy-efficient implementation for low-power IoT environments. Our results show that PKIs combining Dilithium-based client certificates and Falcon-based server and CA certificates together with KYBER-based key-exchanges achieve the best trade-off between bandwidth requirements, latency and energy consumption. This is contrary to NIST's idea of standardizing only one procedure. • Classical and PQC-based PKIs optimize the trade-off between security and efficiency: In many applications, not all communication partners in the PKI need to utilize post-quantum-safe certificates. If the client deploys conventional ECDSA as the signing algorithm, the drawbacks of post-quantum-secure PKIs are greatly reduced while maintaining a reasonable attack barrier for the near future. These results prove that the usage of post-quantum KEMs and DSAs is already feasible today in terms of energy demand and latency for low-power IoT devices with state-of-theart hardware. Nevertheless, the overhead of PQC, especially DSAs, can be significantly reduced by decreasing the size of signatures and public keys rather than simplifying the computations, and we would like to motivate further research in this direction. Conflicts of Interest:\n\n[4] Post-Quantum Cryptography for Internet of Things: A Survey on Performance and Optimization Overall, recent works tend to be optimistic in regard to the performance of lattice-based PQC for IoT: despite significant performance overhead, PQC is still shown to be feasible in some reasonably constrained devices. Bürstinghaus-Steinbach et al. suggested that the NIST-selected, lattice-based algorithm Kyber runs faster than ECDH by at least one order of magnitude, when both algorithms are executed as primitives on four lightweight devices. However, the code-based SPHINCS+, which was selected by the NIST as an alternative to lattice, demonstrates slower execution than ECDSA. further discovered that a Post-Quantum TLS protocol using Kyber-SPHINCS+ cipher suite would introduce a significant burden in terms of latency and memory usage, mainly due to the cost of computing signatures with SPHINCS+.\n\n[5] Post-Quantum Cryptography for Internet of Things: A Survey on Performance and Optimization A key topic related to the efficiency of PQC in IoT is the Post-Quantum Transport Layer Security (PQTLS). As public-key cryptography is used for key exchange and signature verification in the pre-quantum TLS, their eventual upgrade to PQC should be expected. On a Cortex-M4 micro-controller unit (MCU), tested PQKEMs in place of ECDH in TLS 1.2 and measured the overhead, both in terms of latency and energy consumption. It was suggested that the overhead induced by PQC is feasible for IoT, especially with latticed-based KEMs which consumed less energy than ECDH. Furthermore, the authors noted that for lattice-based KEMs, a large portion of latency overhead was caused by larger bandwidth requirements, rather than the actual computation of PQC. As such the potential of hardware acceleration may be limited for PQTLS. A more recent work on this matter is, which used the OQS-OpenSSL from Open Quantum Safe to establish TLS 1.3 handshake over MQTT protocol. A key observation noted in this work is that lattice-based KEMs (Kyber, NTRU, NTRU Prime and SABER) achieved faster key exchange than EDCH on both RPi3 and RPi4, despite having to transmit larger packets. However, the same cannot be said for signature authentication as ECDSA outperformed all PQ signature schemes. A key difference between and is software implementation. integrated PQC ciphers within the WolfSSL library, which only supported TLS 1.2 at the time, while the OQS-OpenSSL library used by supports TLS 1.3 and was developed specifically for PQC ciphers. Considering the fact that TLS 1.3 is generally faster and superior to TLS 1.2, the information provided by might seem more relevant.\n\n[6] Post-Quantum Cryptography for Internet of Things: A Survey on Performance and Optimization For smaller devices, Lee et al. show that the same technique is also feasible on a Jetson Nano, which is an embedded device with a Cortex-A53 processor and a small 128-core GPU. It is shown that by utilizing GPU acceleration on the Jetson Nano, Kyber-512 can achieve a throughput performance of 1,345 key encapsulations per second and 918 key decapsulations per second. Although does not mention how this result compares to using CPU, it is possible to cross-check it against, which reports that on a Raspberry Pi 3 (which uses the same Cortex-A53 processor), the average execution time for Kyber-512 is 1.1 ms - or 909 operations for second - for both encapsulation and decapsulation. As the same paper reports that Kyber performs more than 9 times faster with GPU acceleration (RTX2060) than using CPU alone (Intel i9-9700F), it’s reasonable to suspect that for lightweight devices, accelerating PQC computation with small GPUs may not be as rewarding. However, more data is needed on this topic to obtain a definitive answer.\n\n[7] Masked Accelerators and Instruction Set Extensions for Post-Quantum Cryptography Side-channel attacks can break mathematically secure cryptographic systems leading to a major concern in applied cryptography. While the cryptanalysis and security evaluation of Post-Quantum Cryptography (PQC) have already received an increasing research effort, a cost analysis of efficient side-channel countermeasures is still lacking. In this work, we propose a masked HW/SW codesign of the NIST PQC finalists Kyber and Saber, suitable for their different characteristics. Among others, we present a novel masked ciphertext compression algorithm for non-power-of-two moduli. To accelerate linear performance bottlenecks, we developed a generic Number Theoretic Transform (NTT) multiplier, which, in contrast to previously published accelerators, is also efficient and suitable for schemes not based on NTT. For the critical non-linear operations, masked HW accelerators were developed, allowing a secure execution using RISC-V instruction set extensions. With the proposed design, we achieved a cycle count of K:214k/E:298k/D:313k for Kyber and K:233k/E:312k/D:351k for Saber with NIST Level III parameter sets. For the same parameter sets, the masking overhead for the first-order secure decapsulation operation including randomness generation is a factor of 4.48 for Kyber (D:1403k)and 2.60 for Saber (D:915k).\n\n[8] Post Quantum Cryptography: Techniques, Challenges, Standardization, and Directions for Future Research Due to the relatively faster performance of lattice systems, and smaller public key sizes, such systems make an attractive option for implementation. KYBER enforces this by displaying a key size of 800 bytes to 1.5 kilobytes, compared to the 2-kilobyte key required for present-day RSA.\n\n[9] KaLi: A Crystal for Post-Quantum Security Using Kyber and Dilithium Quantum computers pose a threat to the security of communications over the internet. This imminent risk has led to the standardization of cryptographic schemes for protection in a post-quantum scenario. We present a design methodology for future implementations of such algorithms. This is manifested using the NIST selected digital signature scheme CRYSTALS-Dilithium and key encapsulation scheme CRYSTALS-Kyber. A unified architecture, KaLi, is proposed that can perform key generation, encapsulation, decapsulation, signature generation, and signature verification for all the security levels of CRYSTALS-Dilithium, and CRYSTALS-Kyber. A unified yet flexible polynomial arithmetic unit is designed that can processes Kyber operations twice as fast as Dilithium operations. Efficient memory management is proposed to achieve optimal latency. KaLi is explicitly tailored for ASIC platforms using multiple clock domains. On ASIC 28nm/65nm technology, it occupies 0.263/1.107 mm2 and achieves a clock frequency of 2GHz/560MHz for the fast clock used for memory unit. On Xilinx Zynq Ultrascale+ZCU102 FPGA, the proposed architecture uses 23,277 LUTs, 9,758 DFFs, 4 DSPs, and 24 BRAMs, at 270 MHz clock frequency. KaLi performs better than the standalone implementations of either of the two schemes. This is the first work to provide a unified design in hardware for both schemes.\n\n[10] RISQ-V: Tightly Coupled RISC-V Accelerators for Post-Quantum Cryptography Empowering electronic devices to support Post-Quantum Cryptography (PQC) is a challenging task. PQC introduces new mathematical elements and operations which are usually not easy to implement on standard processors. Especially for low cost and resource constraint devices, hardware acceleration is usually required. In addition, as the standardization process of PQC is still ongoing, a focus on maintaining flexibility is mandatory. To cope with such requirements, hardware/software co-design techniques have been recently used for developing complex and highly customized PQC solutions. However, while most of the previous works have developed loosely coupled PQC accelerators, the design of tightly coupled accelerators and Instruction Set Architecture (ISA) extensions for PQC have been barely explored. To this end, we present RISQ-V, an enhanced RISC-V architecture that integrates a set of powerful tightly coupled accelerators to speed up lattice-based PQC. RISQ-V efficiently reuses processor resources and reduces the amount of memory accesses. This significantly increases the performance while keeping the silicon area overhead low. We present three contributions. First, we propose a set of powerful hardware accelerators deeply integrated into the RISC-V pipeline. Second, we extended the RISC-V ISA with 29 new instructions to efficiently perform operations for lattice-based cryptography. Third, we implemented our RISQ-V in ASIC technology and on FPGA. We evaluated the performance of NewHope, Kyber, and Saber on RISQ-V. Compared to the pure software implementation on RISC-V, our co-design implementations show a speedup factor of up to 11.4 for NewHope, 9.6 for Kyber, and 2.7 for Saber. For the ASIC implementation, the energy consumption was reduced by factors of up to 9.5 for NewHope, 7.7 for Kyber, and 2.1 for Saber. The cell count of the CPU was increased by a factor of 1.6 compared to the original RISC-V design, which can be considered as a moderate increase for the achieved performance gain.\n\n[11] Side-channel and Fault-injection attacks over Lattice-based Post-quantum Schemes (Kyber, Dilithium): Survey and New Results In this work, we present a systematic study of Side-Channel Attacks (SCA) and Fault Injection Attacks (FIA) on structured lattice-based schemes, with main focus on Kyber Key Encapsulation Mechanism (KEM) and Dilithium signature scheme, which are leading candidates in the NIST standardization process for Post-Quantum Cryptography (PQC). Through our study, we attempt to understand the underlying similarities and differences between the existing attacks while classifying them into different categories. Given the wide variety of reported attacks, simultaneous protection against all the attacks requires to implement customized protections/countermeasures for both Kyber and Dilithium. We therefore present a range of customized countermeasures, capable of providing defenses/mitigations against existing SCA/FIA, and incorporate several SCA and FIA countermeasures within a single design of Kyber and Dilithium. Among the several countermeasures discussed in this work, we present novel countermeasures that offer simultaneous protection against several SCA- and FIA-based chosen-ciphertext attacks for Kyber KEM. We implement the presented countermeasures within two well-known public software libraries for PQC: (1) pqm4 library for the ARM Cortex-M4-based microcontroller and (2) liboqs library for the Raspberry Pi 3 Model B Plus based on the ARM Cortex-A53 processor. Our performance evaluation reveals that the presented custom countermeasures incur reasonable performance overheads on both the evaluated embedded platforms. We therefore believe our work argues for usage of custom countermeasures within real-world implementations of lattice-based schemes, either in a standalone manner or as reinforcements to generic countermeasures such as masking.\n\n"}
{"case_id": "57f1dc7fb4c25e0cceb2ae6e8653b795", "answer_text": "\n\nEffectively measuring software engineering process improvements in real-world settings requires a comprehensive approach that incorporates multiple methodologies and frameworks. This multi-faceted strategy ensures that improvements are not only quantitatively assessed but also qualitatively understood, providing a holistic view of their impact [0].\n\nOne primary method to measure process improvements is through **surveys and feedback** from employees and customers, as these stakeholders are directly impacted by the changes. Surveys can capture qualitative data on how the changes have affected day-to-day operations, team dynamics, and overall satisfaction. Pettersson's approach integrates expert opinions to compare the improved process with the previous state, offering insights into the perceived effectiveness of the changes introduced [0]. For example, at Ericsson, employee feedback was used to assess process learning and commitment, providing detailed insights into how these changes were experienced on the ground [5]. However, a potential limitation of this method is that it may be subject to bias or unrepresentative sampling if not designed and administered properly.\n\nAnother critical methodology is **cost-benefit analysis**, which evaluates the financial impact of the improvements. This analysis considers not just the direct costs and savings but also indirect and intangible benefits like improved customer satisfaction and employee morale. Activities such as Activity Based Costing help to relate specific activities to their costs, aiding in a more precise estimation of the financial impact [1]. For instance, a software company implemented a cost-benefit analysis to evaluate training programs designed to improve employee motivation, resulting in tangible estimates of the benefits realized through enhanced productivity and morale. Nevertheless, accurately quantifying indirect benefits can be challenging and often requires consensus among stakeholders on the valuation methods used.\n\n**Statistical techniques** are also essential in measuring the effectiveness of process improvements. Techniques such as multiple regression, rank correlation, and chi-square tests of independence can provide quantifiable evidence of process improvements over time [2]. These methods help in identifying trends and correlations that might not be immediately apparent, thereby providing a more objective measure of effectiveness. However, it is crucial to use these methods with caution as relying on a single statistical technique may not fully capture the impact of the changes and could lead to incorrect conclusions [2]. For example, a longitudinal field study at a major software firm analyzed over 7,000 software defects and found significant reductions in high-severity defects with higher levels of process improvement, especially in complex systems [8]. A limitation of statistical techniques is that they require rigorous data collection protocols to ensure validity and reliability.\n\nThe **Goal–Question–Metric (GQM) approach** offers a structured way to measure improvements by linking organizational goals to specific questions and metrics [5]. This approach was successfully employed in a longitudinal study at Ericsson, where metrics were designed to measure process use, commitment, learning, and improvement. Such a framework ensures that the measurement activities are aligned with the strategic objectives of the organization and provide actionable insights into the effectiveness of the process improvements. However, defining appropriate goals and metrics can be complex and requires in-depth understanding of both the process and organizational context.\n\n**Longitudinal field studies** also provide valuable insights into the impact of process improvements. For example, a study analyzing 7,545 software defects found that higher levels of software process improvement significantly reduced the likelihood of high-severity defects, especially in large or complex systems [8]. This empirical evidence underscores the long-term benefits of sustained process improvement efforts and their impact on software quality. One challenge of longitudinal studies is that they require extended time frames and consistent data collection efforts, which can be resource-intensive.\n\n**Addressing confounding factors** is critical for the validity of any measurement strategy. Many studies fail to consider confounding factors that might influence the outcomes of process improvements [3]. To practically identify and control these factors, one approach is to integrate methods such as control groups, randomization, and longitudinal data collection. For instance, confounding factors might include changes in team composition, shifts in project scope, or external economic conditions. By isolating the variables through careful study design and employing statistical controls, researchers can better attribute observed changes to the process improvements themselves. For example, Ericsson used such methods to isolate the effects of specific process changes from other variables, providing a clearer picture of the impact of their process improvements [5]. A limitation here is that some confounding factors might be difficult to identify or control completely.\n\nAn emerging trend in measuring software engineering process improvements is the use of **automated measurement-based feedback systems**. These systems integrate measurement and process technologies to provide real-time feedback to developers and maintainers about their work. This automation can significantly enhance the accuracy and usefulness of measurements by offering continuous, on-line data about the tasks being performed. For example, a process-centered software engineering environment developed for measurement-based feedback offers detailed information about the tasks expected based on an integrated set of measurement and process views [7]. This kind of real-time feedback can help software professionals gain better control over their projects and improve their practices more dynamically. However, the implementation of automated systems requires initial investments in technology and the development of models suited to the specific workflow of the organization, which may pose a challenge for some companies.\n\nIn summary, effectively measuring software engineering process improvements in real-world settings involves a combination of qualitative and quantitative methods, including surveys [0], cost-benefit analysis [1], statistical techniques [2], the GQM approach [5], longitudinal field studies [8], consideration of confounding factors [3], and leveraging emerging trends like automated feedback systems [7]. This multi-layered approach provides a robust framework for evaluating the true impact of process improvements on both the organization and its stakeholders while considering the potential limitations and challenges of each methodology.\n\n\nReferences:\n[0] Evaluation and Measurement of Software Process Improvement—A Systematic Literature Review Survey: In the context of this work, a survey is definedas any method to collect, compare and evaluate quantitative or qualitativedata from human subjects. A survey can be conducted by interviewsor questionnaires, targeting employees affected by the process improvementinitiative or customers of the organization. Surveys can be an effectivemean to assess the changes introduced in an improvement effort sinceafter all, the development of software is a human-intensive task.The feedback provided by employees can therefore be used to improvethe understanding of the effects caused by the introduced changesand to steer future improvements. Gathering information from customers,on the other hand, can provide insight how the improvement affectsthe quality of products or services as perceived by their respectiveusers. This can be valuable to assess external quality characteristics,such as integrity, reliability, usability, correctness, efficiencyand interoperability , which otherwisewould be difficult to evaluate. The analysis of the improvement participants’feedback can be valuable if historical data for comparison is notavailable or if its quality / completeness limits the evaluabilityof the improvement. A systematic method to assess the effects causedby an improvement initiative is described by Pettersson .The approach can be useful if no or only limited historical data isavailable to construct a baseline which can serve as a reference pointfor the improvement evaluation. The post-evaluation is based on theexpert opinion of the directly involved personnel which compares theimproved process with the previous one. This lightweight process improvesthe visibility on the effects of the undertaken improvement initiativeand provides also information on how the change was experienced bythe involved roles. The method could be enhanced by integrating theconcept of \"contribution percentages\"as it was proposed by van Solingen .The idea is to let the experts assess how much the initiative actuallycontributed to the improvement, i. e. provide the possibilityto express that only a fraction of the change is attributable to theinitiative and other factors have also contributed to the enhancement.Such an approach could also support the identification of potentialconfounding factors (see Section 4.5).\n\n[1] Evaluation and Measurement of Software Process Improvement—A Systematic Literature Review Cost-Benefit Analysis: Evaluating an improvement initiativewith a cost-benefit measure is important since the allocated budgetfor the program must be justifiable in order not to risk its continuation , 1.Furthermore, it is necessary to avoid loss of money and to identifythe most efficient investment opportunities .When assessing cost, organizations should also consider other resourcesthan pure effort (which can be relatively easily measured), e. g. officespace, travel, computer infrastructure ,training, coaching, additional metrics, additional management activities,process maintenance . Activity Based Costinghelps to relate certain activities with the actual spent effort .Since cost and effort data can be collected in projects, they mustnot be estimated . On the other hand, thethereby obtained values are still an approximation and estimationsof both costs and benefits are inevitable .Since it is usually enough to know the ROI’s relative value (positive,balanced or negative), perfect accuracy is not required as long asthe involved stakeholders agree on the procedure how to assess it .Direct benefits and especially indirect and intangible benefits arebest assessed by multiple stakeholders ;some of the difficult to quantify benefits are: customer satisfaction,improved market share due to improved quality, reduced time-to-deliverand accuracy, feature-cost reduction, opportunity costs, reduced maintenancein follow-up projects, better reusability, employee satisfaction,increased resource availability . A usefultechnique to support the estimation is the so-called \"what-if-not\"analysis . Project managers couldbe asked to estimate how much effort was saved due to the implementedimprovement in follow-up projects. The saved effort would then beaccounted as a benefit. Another strategy would be to estimate the\"worth\" of a certain improvement, e. g. askingmanagers how many training days would they invest to increase employeemotivation and quantify the cost of such a training program .\n\n[2] Evaluation and Measurement of Software Process Improvement—A Systematic Literature Review As proposed by Henry et al. , severalstatistical techniques can be applied to evaluate the effectivenessof software process improvement in terms of increased estimation accuracy,product quality and customer satisfaction. The described methods aremultiple regression, rank correlation and chi-square tests of independencein two-way contingency tables, which, when applied repeatedly overtime can show the effectiveness of process improvements statistically .However, care must be taken when applying these techniques since asingle method alone may not show the true impact of the initiativeand wrong conclusions could be drawn .Furthermore Henry et al.  objectedthat in some cases the process improvement must be very effectivein order to show significant alterations in the statistical evaluationresults. Statistical methods are also used to assess process stabilitywhich is regarded as an important aspect of organizational capability .In order to evaluate stability, the authors propose trend, changeand shape metrics which can be used in the short- and long-term andare analyzed by visual inspection of the data summarized by descriptivestatistics (e. g. histograms and trend diagrams).\n\n[3] Evaluation and Measurement of Software Process Improvement—A Systematic Literature Review This paper presents a systematic literature review that investigateshow the impact of software process improvement initiatives (as definedin Section 3.4.3) is measured and evaluated.The aim is to identify and characterize the different approaches usedin realistic settings, i. e. to provide a comprehensive outlineand discussion of evaluation strategies and measurements used in thefield to assess improvement initiatives. The major findings of thisreview and their implications for research are:•Incomplete context descriptions: Seventy-five out of 148studies did not or only partially describe the context in which thestudy was carried out (see Section 3.5).In the area of process improvement it is however critical to describethe process change and its environment in order to provide resultswhich have the potential to be reused or to be transferred into differentsettings. Since a considerable body of knowledge on the impact ofimprovement initiatives is provided by industry reports (53, 36%),a precise and informative context description would be beneficialfor both practitioners and researchers.•Evaluation validity: In more than 50% of the studies inwhich improvement initiatives are evaluated, “Pre-Post Comparison”is used individually or in combination with another method (see Section 4.2).Considering that confounding factors are rarely discussed (19 outof 148 studies, see Section 4.5), theaccuracy of the evaluation results can be questioned. The severityof confounding is even increased by unsatisfactory context descriptions.A grounded judgment by the reader on the validity of the evaluationis prohibited by the absence of essential information.•Measurement validity: Kaner and Bond illustrated how important it is to define exactly the semantics ofa metric and the pitfalls that arise if it is not commonly agreedwhat the metric actually means, i. e. which attribute it actuallymeasures. This issue is related with farther reaching questions thanprocess improvement measurement and evaluation, and concerns fundamentalproblems of software measurement validity. Nevertheless, measurementdefinition inconsistencies, as shown in Section 4.3.2,inhibit the process of improvement itself since the comparison andcommunication of results is aggravated. The implication for researchis that it is difficult to identify and use the appropriate measuresfor improvement evaluation. A better support for defining, selectingand validating measures could enable a comparable and meaningful evaluationof SPI initiatives.•Measurement scope: The analysis on what is actually measuredduring or after an improvement initiative shows a focus on processand product quality (see Section 4.3). Fromthe software process improvement perspective this measurementgoal might be adequate and sufficient. It is however crucial to pushthe event horizon of improvement measurement beyond the level of projects(see Section 4.4) in order toconfirm the relatively short-dated measurements at the project orproduct level. Since the information needs for the different stakeholdersvary, appropriate improvement indicators need to be implemented. Atthe corporate level for example, business benefits realized by projectswhich encompass a wider scope than pilot improvement implementationsare of interest.Indicators for these long-term effects can be customer satisfaction,to assess quality improvement, and return on investment to evaluatethe economic benefits of improvement. The data presented in this review(see Section 4.3.2) suggeststhat these indicators tend to be less used in the evaluation of processimprovement as other, easier to collect, indicators. The implicationfor research is to integrate the success indicators into a facetedview on process improvement which captures its short- and long-termimpact.•Confounding factors: In a majority (129, 87%) of the reviewedstudies we could not identify a discussion on confounding factorsthat might affect the performance of SPI initiatives and thus theirevaluation. Since process improvement affects many aspects of a developmentproject, its results and effect on the organization, there are manypotential such confounding factors that threaten validity. Even thoughstudy design can often be used to limit the effects it is often notpractical to fully control the studied context. Thus future researchon SPI should always consider and discuss confounding factors. However,we note that no good conceptual model or framework for such a discussionis currently available.The results of this review encourage further research on the evaluationof process improvement, particularly on the conception of structuredguidelines which support practitioners in the endeavor of measuring,evaluating and communicating the impact of improvement initiatives.\n\n[4] 4 Measuring Process Innovations and Improvements 4 Measuring Process Innovations and Improvements A major challenge in process improvement is to understand process innovation and improvement efficiency and use. How do we know that process innovations and improvements give organizational benefits? We need a mechanism for measuring. In this paper, we report from a longitudinal action research study at the telecom company Ericsson where a measurement mechanism was designed and successfully used in practice to understand, learn, and measure process efficiency. In the concrete, the measurement mechanism was built through empirical testing combined with using a goal–question–metrics (GQM) approach. The resulting measurement mechanism consists of four correlated metrics that indicate process use, process commitment, process learning, and process improvement. The same measurement mechanism can also be used to obtain feedback and evaluation, thereby allowing the organi198 Part 3: Software Process Improvement zation to determine process efficiency and use, and to determine the success of the process improvements.\n\n[5] Measurement-based feedback in a process-centered software engineering environment Measurement-based feedback in a process-centered software engineering environment Software development organizations measure their real-world processes, products, and resources to achieve the goal of improving their practices. Accurate and useful measurement relies on explicit models of the real-world processes, products, and resources. These explicit models assist with planning measurement, interpreting data, and assisting developers with their work. However, little work has been done on the joint use of measurement and process technologies. We hypothesize that it is possible to integrate measurement and process technologies in a way that supports automation of measurement-based feedback. Automated support for measurement-based feedback means that software developers and maintainers are provided with on-line, detailed information about their work. This type of automated support is expected to help software professionals gain intellectual control over their software projects. The dissertation offers three major contributions. First, an integrated measurement and process modeling framework was constructed. This framework establishes the necessary foundation for integrating measurement and process technologies in a way that will permit automation. Second, a process-centered software engineering environment was developed to support measurement-based feedback. This system provides personnel with information about the tasks expected of them based on an integrated set of measurement and process views. Third, a set of assumptions and requirements about that system were examined in a controlled experiment. The experiment compared the use of different levels of automation to evaluate the acceptance and effectiveness of measurement-based feedback.\n\n[6] Does Software Process Improvement Reduce the Severity of Defects? A Longitudinal Field Study Does Software Process Improvement Reduce the Severity of Defects? A Longitudinal Field Study As firms increasingly rely on information systems to perform critical functions, the consequences of software defects can be catastrophic. Although the software engineering literature suggests that software process improvement can help to reduce software defects, the actual evidence is equivocal. For example, improved development processes may only remove the “easier” syntactical defects, while the more critical defects remain. Rigorous empirical analyses of these relationships have been very difficult to conduct due to the difficulties in collecting the appropriate data on real systems from industrial organizations. This field study analyzes a detailed data set consisting of 7,545 software defects that were collected on software projects completed at a major software firm. Our analyses reveal that higher levels of software process improvement significantly reduce the likelihood of high severity defects. In addition, we find that higher levels of process improvement are even more beneficial in reducing severe defects when the system developed is large or complex, but are less beneficial in development when requirements are ambiguous, unclear, or incomplete. Our findings reveal the benefits and limitations of software process improvement for the removal of severe defects and suggest where investments in improving development processes may have their greatest effects.\n\n"}
{"case_id": "6e7e3524d565599b3064ae05375956f7", "answer_text": "\n\nRecent techniques for text watermarking have demonstrated significant advancements in robustness and effectiveness, particularly against various forms of paraphrasing and editing attacks [2]. These techniques can be broadly classified into several categories: context-aware lexical substitution [7], adversarial watermark embedding [1], error correction-based methods [5], and semantic-based watermarking [13]. Each of these techniques has its strengths and vulnerabilities with respect to different types of transformations and attacks [7]. \n\n**1. Context-Aware Lexical Substitution** [7]. \nThe use of context-aware lexical substitution in text watermarking addresses the limitations of previous methods that neglected the context of target words. For instance, Yang et al. (2022) introduced a BERT-based infill model that generates contextually appropriate lexical substitutions, which are then used to embed watermarks [7]. Such methods ensure that the semantic integrity of the text is maintained while embedding the watermark. This approach is robust against simple synonym replacement and some forms of paraphrasing attacks. However, its effectiveness can be compromised when dealing with more sophisticated paraphrasing techniques that significantly alter sentence structure or semantic meaning without affecting individual words.\n\n**2. Adversarial Watermarking:** [1] \nThe Adversarial Watermarking Transformer (AWT) leverages adversarial training to hide data in text through word substitutions that are contextually appropriate and minimally intrusive [1]. AWT's robustness comes from its ability to learn watermark patterns without ground truth, making it resilient against various forms of attacks, including paraphrasing and editing. However, adversarial techniques may struggle with transformations that drastically change the text's structure or introduce significant semantic shifts, such as extensive rewriting by powerful language models like GPT-3 [1].\n\n**Error Correction-Based Methods:** This represents the first LLM multi-bit watermarking scheme that provides a provable robustness bound for every generated sentence. Here we use edit distance (Levenshtein distance) to measure the attacker’s ability to manipulate watermarked text. The edit distance between two strings is defined as the minimum number of operations (insertion, deletion, or substitution of a token) required to transform one string into the other.[5] \nError correction codes (ECC) have been employed to enhance the robustness of text watermarks. An example of this is the use of ECC to encode multi-bit watermarks, which ensures that minor edits to the text do not affect the embedded message [12]. This method is particularly robust against insertion, deletion, and substitution attacks, as even if some bits are altered, the original message can still be decoded correctly. Despite its robustness, this technique may falter under extensive editing or paraphrasing attacks that exceed the error correction threshold, leading to potential failures in recovering the original watermark.\n\n**4. Semantic-Based Watermarking:** [13]\nSemantic-based watermarking techniques, such as those using presuppositions, leverage natural language semantic structures to embed watermarks. These methods introduce subtle changes in information that are less noticeable and resilient against common transformations like passivization, topicalization, and extraposition [13]. The resilience of these methods to various syntactic transformations makes them highly robust. However, they may be vulnerable to sophisticated paraphrasing attacks that deeply understand and alter the semantics of the text.\n\n**Transformations and Attacks:** [7]\n\n**Robust Against:** [2]\n- **Paraphrasing Attacks:** Techniques like the use of error correction codes and context-aware lexical substitution show increased robustness against paraphrasing attacks from models such as GPT-3 or BART [2][8].\n- **Editing Attacks:** Error correction-based methods and semantic-based watermarking are robust against common editing attacks, including token insertion, deletion, and substitution [5][10].\n- **Rewrite Attacks:** Methods that do not rely on token order, such as those proposed by Zhao et al. (2023a), exhibit robustness against rewrite attacks that disrupt token sequences but fail to replace all tokens [0].\n\nNot Robust Against: [5]\n- **Advanced Paraphrasing:** While recent methods have shown improvements, extensive paraphrasing that significantly alters text structure and semantic content can still pose challenges. Advanced paraphrasing techniques using models with high diversity, such as DIPPER-2, can effectively remove watermarks [2].\n- **Semantic Shifts:** Transformations that introduce substantial semantic changes to the text, beyond simple lexical substitutions, can compromise many watermarking techniques. While techniques like semantic-based watermarking aim to address this, they are not foolproof against sophisticated semantic alterations [14].\n\nIn summary, recent text watermarking techniques exhibit varying degrees of robustness against different types of transformations and attacks. While methods like context-aware lexical substitution and error correction codes offer enhanced robustness against paraphrasing and editing attacks [7][12], challenges remain in addressing advanced paraphrasing and semantic shifts. Continuous research and development are necessary to further improve the resilience of watermarking methods in the face of evolving attack strategies. \n\n\nReferences:\n[0] A Survey of Text Watermarking in the Era of Large Language Models Firstly, watermark detection methods that do not rely on token order, like Zhao et al. (2023a)’s approach, demonstrate greater robustness since rewrite attacks can disrupt token sequences but are less effective at replacing all tokens. In contrast, algorithms like Kuditipudi et al. (2023)’s, where robustness hinges on token sequences, exhibit varying susceptibility levels to such attacks.Secondly, the watermark strength, or the extent of text modification, is crucial. Watermarking that necessitates substantial alterations from the original text tends to enhance resistance against rewrite attacks.Lastly, the length of the watermarked text is a significant factor. Empirical evidence from Kirchenbauer et al. (2023a) indicates that texts with over 600 tokens generally maintain robustness against rewrite attacks. It is also observed that manually erasing watermarks from texts of 400 to 800 tokens through rewriting poses a considerable challenge for humans.\n\n[1] Adversarial Watermarking Transformer: Towards Tracing Text Provenance with Data Hiding Recent advances in natural language generation have introduced powerful language models with high-quality output text. However, this raises concerns about the potential misuse of such models for malicious purposes. In this paper, we study natural language watermarking as a defense to help better mark and trace the provenance of text. We introduce the Adversarial Watermarking Transformer (AWT) with a jointly trained encoder-decoder and adversarial training that, given an input text and a binary message, generates an output text that is unobtrusively encoded with the given message. We further study different training and inference strategies to achieve minimal changes to the semantics and correctness of the input text.AWT is the first end-to-end model to hide data in text by automatically learning -without ground truth- word substitutions along with their locations in order to encode the message. We empirically show that our model is effective in largely preserving text utility and decoding the watermark while hiding its presence against adversaries. Additionally, we demonstrate that our method is robust against a range of attacks.\n\n[2] Provable Robust Watermarking for AI-Generated Text Paraphrasing attack.To demonstrate the superior robustness of our method, supported by our theorem, we devise experiments to compare its performance against Kirchenbauer et al. (2023). We employ different paraphrase attack techniques targeting the removal of the watermark. Firstly, we utilized two versions of the DIPPER model (Krishna et al., 2023), we denote them as “DIPPER-1” and “DIPPER-2”. DIPPER-2 has greater diversity than DIPPER-1. Additionally, we leverage the ChatGPT API, generating paraphrased text by providing prompts such as “Rewrite the following paragraph:”. Furthermore, we employ BART (Lewis et al., 2019) (bart-large-cnn, a large-sized model fine-tuned on the CNN Daily Mail dataset (Hermann et al., 2015)) for text summarization as another type of paraphrasing attack. The results of our experiments are shown in Figure 2 and Table 2.The results illustrate the substantial improvement in robustness achieved by our method compared to Kirchenbauer et al. (2023). Notably, our method achieves an accuracy rate of over 85% with a false positive rate of 10%.\n\n[3] Provably Robust Multi-bit Watermarking for AI-generated Text via Error Correction Code In this section, we demonstrate the robustness of our watermarking scheme against editing attempts (insertion, deletion, or substitution of a token). This represents the first LLM multi-bit watermarking scheme that provides a provable robustness bound for every generated sentence. Being able to know the robustness bound immediately after the sentence generation will make the watermark much more trustworthy in practice. Here we use edit distance (Levenshtein distance) (Navarro, 2001) to measure the attacker’s ability to manipulate watermarked text. The edit distance between two strings is defined as the minimum number of operations (insertion, deletion, or substitution of a token) required to transform one string into the other.\n\n[4] A Survey of Text Watermarking in the Era of Large Language Models The aforementioned watermarking methods relying on context-independent synonym substitution (WordNet & Word2Vec) often neglect the context of target words, potentially compromising sentence semantics and text quality. To address this, context-aware lexical substitution has been incorporated into text watermarking.Yang et al. (2022) introduced a BERT-based infill model for generating contextually appropriate lexical substitutions. The watermark detection algorithm parallels the generation process, identifying watermark-bearing words, generating substitutes, and applying inverse rules for message extraction.Yang et al. (2023) streamlined watermark detection by encoding each word with a random binary value and substituting bit-0 words with context-based synonyms representing bit-1. As non-watermarked text adheres to a Bernoulli distribution, altered during watermarking, statistical tests can effectively detect watermarks.Yoo et al. (2023b) enhanced robustness against removal attacks by fine-tuning a BERT-based infill model with keyword-preserving and syntactically invariant corruptions, achieving superior robustness compared to earlier methods.\n\n[5] Provably Robust Multi-bit Watermarking for AI-generated Text via Error Correction Code Paraphrasing Attack.  Paraphrasing attack is considered as a strong attack against watermarks in many previous works (Kirchenbauer et al., 2023a; Zhao et al., 2023; Yoo et al., 2023a). An attacker that has access to a public language model can use this model to rephrase the watermarked output of the generated model.Following (Yoo et al., 2023a), we use GPT-3.5 (text-davinci-003) (Ouyang et al., 2022) as the attacker’s paraphrasing model. We conducted experiments with T𝑇T of 200, 300, 400, and 500, and bit lengths of 4, 8, and 12, while other hyperparameters we use default values. The results are shown in Table 4. Our results indicate that with a fixed bit length, a larger T𝑇T leads to a higher match rate. That is because, with the increase of T𝑇T, the ratio of paraphrased text tokens which come from the watermarked text is quite stable. Then the ratio of green tokens is quite stable with the increase of T𝑇T. Similar with our findings in Figure 10, since the ratio of green tokens is stable, the watermark gets stronger with the increase of T𝑇T, and the match rate after paraphrasing increases.\n\n[6] Provably Robust Multi-bit Watermarking for AI-generated Text via Error Correction Code Homoglyph Attack. Following (Kirchenbauer et al., 2023a), we also explore the performance of our watermark against Homoglyph attack. In Homoglyph attack, an attacker modifies text by replacing characters with the same or a very similar-looking letter.For example, for the word ”hello” (one token), the attacker replaces the character ”l” with its similar-shaped Unicode characters, such as ”1”, it expands to three different tokens after attack.The results of our experiments are shown in Figure 9c, illustrating the substantial improvement in robustness achieved by our watermarking method. Consistent with the outcomes observed under discrete alternation attacks, our scheme sustains a high match rate, never dropping below 50%. Conversely, the match rate of Yoo et al. (Yoo et al., 2023a) falls below 50% and 30% at edit distances of 55 and 80, respectively.\n\n[7] Provably Robust Multi-bit Watermarking for AI-generated Text via Error Correction Code Here we give an example of how ECC enhances the robustness of multi-bit watermarking. Suppose we aim to embed 12 bits into a text with 200 tokens. There are two ways to do this. The first way is that we directly embed raw 12 bits. For the second way, we first use ECC scheme (23,12,3)23123(23,12,3) to encode those 12 bits into 23 bits, and then embed these 23 bits into the text. The ECC scheme (23,12,3)23123(23,12,3) has the property that any corruption of at most 3 bits within the 23-bit string does not change the decoded 12 bits. In the first case, the attacker only needs to be lucky enough to change one bit of the embedded message. In the second case, it is guaranteed by the properties of ECC that, any corruption to (23,12,3)23123(23,12,3) ECC encoded bit string less than 4 bits does not change the decode result of this encoded bit string. Therefore, in this case, the attacker needs to change the embedded message by at least 4 bits, this corresponds to 4/234234/23 of the message. Compared with the first case of changing 1/121121/12 of the message, this scheme enjoys about 2×\\times robustness improvement.\n\n[8] A method of text watermarking using presuppositions We propose a method for watermarking texts of arbitrary length using natural-language semantic structures. For the key of our approach we use the linguistic semantic phenomenon of presuppositions. Presupposition is the implicit information considered as well-known or which readers of the text are supposed to treat as well-known; this information is a semantic component of certain linguistic expressions (lexical items and syntactical constructions called presupposition triggers). The same sentence can be used with or without presupposition, or with a different presupposition trigger, provided that all the relations between subjects, objects and other discourse referents are preserved - such transformations will not change the meaning of the sentence. We define the distinct rules for presupposition identification for each trigger and regular transformation rules for using/non-using the presupposition in a given sentence (one bit per sentence in this case). Isolated sentences can carry the proposed watermarks. However, the longer is the text, the more efficient is the watermark. The proposed approach is resilient to main types of random transformations, like passivization, topicalization, extraposition, preposing, etc. The web of resolved presupposed information in the text will hold the watermark of the text (e.g. integrity watermark, or prove of ownership), introducing \"secret ordering\" into the text structure to make it resilient to \"data loss\" attacks and \"data altering\" attacks.\n\n[9] Provable Robust Watermarking for AI-Generated Text We study the problem of watermarking large language models (LLMs) generated text -- one of the most promising approaches for addressing the safety challenges of LLM usage. In this paper, we propose a rigorous theoretical framework to quantify the effectiveness and robustness of LLM watermarks. We propose a robust and high-quality watermark method, Unigram-Watermark, by extending an existing approach with a simplified fixed grouping strategy. We prove that our watermark method enjoys guaranteed generation quality, correctness in watermark detection, and is robust against text editing and paraphrasing. Experiments on three varying LLMs and two datasets verify that our Unigram-Watermark achieves superior detection accuracy and comparable generation quality in perplexity, thus promoting the responsible use of LLMs. Code is available at https://github.com/XuandongZhao/Unigram-Watermark.\n\n"}
{"case_id": "83360f179fbaf73e400858ee4e17eda2", "answer_text": "\n\nSimultaneous Localization and Mapping (SLAM) methods are evaluated using a variety of datasets that capture different environments, sensor configurations, and levels of complexity [0]. The choice of dataset significantly impacts the evaluation outcomes, emphasizing different aspects of SLAM performance [3]. Below, we discuss several commonly used datasets in the SLAM community. \n\nVisual SLAM Datasets [0][1][3][7][10]. \n\n **TUM RGB-D** [2]: \n   The TUM RGB-D dataset is widely used for evaluating visual SLAM systems, particularly those using RGB-D sensors. This dataset provides indoor scene sequences recorded using a Microsoft Kinect camera at 30Hz, with a resolution of 640x480 pixels [0][2][7]. It includes ground truth camera poses captured by an external motion capture system, making it suitable for evaluating both localization and mapping accuracy. The dataset is extensively used to benchmark systems like ORB-SLAM and LSD-SLAM, thanks to its diverse indoor environments and high-quality ground-truth data [2]. One unique feature is its focus on different types of motion, such as sitting and walking, which help in testing SLAM performance under various dynamic conditions [7].\n\n2. **KITTI**[3]:\n   The KITTI dataset is primarily used for evaluating visual and LiDAR-based SLAM methods. Collected using a car equipped with high-resolution stereo cameras and a 3D LiDAR, KITTI includes urban and highway driving scenarios. It offers synchronized data from GPS/IMU systems, which provides ground-truth poses for evaluating SLAM accuracy in outdoor environments [1][3]. This dataset is critical for developing and testing SLAM algorithms intended for autonomous driving applications due to its realistic driving scenarios and comprehensive sensor data [1].\n\n### Visual-Inertial SLAM Datasets[3]\n\n\n\n3. **EuRoC MAV** [0]. \n   The EuRoC MAV dataset is designed for evaluating visual-inertial SLAM approaches. It features synchronized and calibrated stereo camera images and IMU data collected from a Micro Aerial Vehicle (MAV) flying in both indoor and industrial environments. The dataset provides high-resolution images (752x480) and IMU measurements at 200Hz, along with accurate ground truth from a laser tracker system [1]. This dataset is particularly beneficial for testing algorithms that integrate visual and inertial data for robust localization in dynamic and challenging conditions [0][1]. An additional feature that sets EuRoC MAV apart is its use of MAVs, which introduces the added challenge of 3D flight dynamics [1].\n\n4. **TUM VI**: [1] \n   The TUM Visual-Inertial dataset (TUM VI) extends the capabilities of the TUM RGB-D dataset by including visual and IMU data. It captures a variety of indoor and outdoor scenes with long sequences, synchronized camera images (1024x1024 resolution), and IMU measurements [1]. This dataset allows the evaluation of visual-inertial SLAM systems across diverse scenarios, providing a more comprehensive benchmark for algorithms that utilize both types of data [1]. The dataset's variety in scene types and longer sequence lengths offer extensive challenges for evaluating SLAM algorithms in more realistic, continuous operation scenarios [1].\n\nMulti-Sensor and Specialized SLAM Datasets[0][1][3]. \n\n5. **OpenLORIS-Scene** [4]:\n   The OpenLORIS-Scene dataset focuses on lifelong SLAM and includes data collected in dynamic environments affected by day-night changes, moving people, and varying viewpoints. It integrates RGB-D, IMU, and odometry data, providing a comprehensive testbed for evaluating the robustness and accuracy of SLAM systems over extended periods and in real-world scenarios [0][4]. This dataset aims to push the boundaries of SLAM algorithms by introducing high-level scene understanding capabilities and testing them under continuous operation with changing conditions [4]. The dataset's focus on real-world deployment scenarios for service robots highlights its uniqueness and importance in SLAM research [4].\n\n6. **Hilti-Oxford** [5]. \n   The Hilti-Oxford dataset is designed to benchmark SLAM algorithms in industrial and construction environments. It includes sequences recorded in detailed and complex scenarios ranging from construction sites to neoclassical buildings. The dataset features a multi-modal sensor suite comprising LiDAR, five cameras, and an IMU. Ground truth poses with millimeter accuracy are provided using a novel method of extrinsic calibration and temporal synchronization [5][6]. This dataset's high accuracy and challenging environments make it suitable for testing SLAM algorithms for industrial applications where precision is paramount [5][6]. Additionally, the dataset's ability to simulate real-world construction and industrial conditions makes it an invaluable resource for robust SLAM evaluations [5].\n\nMost SLAM works evaluate their algorithms on one or more public datasets to justify their effectiveness in certain aspects [0]. The most well-used datasets include TUM RGB-D, EuRoC MAV, and KITTI [0]. A recent contribution is the TUM VI benchmark, where aligned visual and IMU data are provided [1]. One of the major distinctions of those datasets is their sensor types. While there is favor of RGB-D data source in recent SLAM algorithm research for dense scene reconstruction [7], there is a lack of dataset with both RGB-D and IMU data [1]. Our dataset provides aligned RGB-D-IMU data, along with odometry data which are widely used in the industry but often lack in public datasets [0]. \nTo aid in the quick reference and comparison of these datasets, a summary table is provided below [1]. \n\n| Dataset         | Sensor Types        | Environment Types           | Key Features                                            | Typical Usage                                                        |\nMost SLAM works evaluate their algorithms on one or more public datasets to justify their effectiveness in certain aspects [0]. The most well-used datasets include TUM RGB-D, EuRoC MAV and KITTI [3]. A recent contribution is the TUM VI benchmark, where aligned visual and IMU data are provided [1]. One of the major distinctions of those datasets is their sensor types. While there is favor of RGB-D data source in recent SLAM algorithm research for dense scene reconstruction, there is a lack of dataset with both RGB-D and IMU data [0]. Our dataset provides aligned RGB-D-IMU data, along with odometry data which are widely used in the industry but often lack in public datasets [0]. \n TUM RGB-D       | RGB-D               | Indoor                      | High ground-truth accuracy, 30Hz, various motions       | Benchmarking visual SLAM, ORB-SLAM, LSD-SLAM [7].\n| KITTI | Stereo Camera, LiDAR | Urban, Highway | Realistic driving scenarios, synchronized GPS/IMU | Autonomous driving, visual and LiDAR SLAM [3] |\nEuRoC MAV | Stereo Camera, IMU | Indoor, Industrial | 3D flight dynamics, high-res images, 200Hz IMU | Visual-inertial SLAM, MAV applications [0][1][3] \nTUM VI provides RGB-D and IMU data, making it suitable for evaluating visual-inertial SLAM algorithms [0]. The dataset includes indoor and outdoor scenes with long sequences and diverse environments, as well as containing 1024x1024 resolution images [1]. It serves as a comprehensive benchmark for visual-inertial SLAM [1]. \n| OpenLORIS-Scene | RGB-D, IMU, Odometry| Dynamic, Real-world | Lifelong SLAM, high-level scene understanding | Service robots, long-term SLAM performance [0] [4] |\n\nHilti-Oxford    | LiDAR, Cameras, IMU | Industrial, Construction     | Millimeter accuracy, challenging environments [5]          | Industrial SLAM, construction site navigation [5]                         |\n\n\nIn summary, the selection of SLAM evaluation datasets depends on the specific requirements of the SLAM system being tested, such as sensor types, environment, and the specific challenges the system needs to address [1]. Each dataset has its strengths, making it important for researchers to choose the contextually appropriate datasets to thoroughly benchmark and validate their SLAM algorithms [3]. \n\n\nReferences:\n[0] Are We Ready for Service Robots? The OpenLORIS-Scene Datasets for Lifelong SLAM Most SLAM works evaluate their algorithms on one or more public datasets to justify their effectiveness in certain aspects. The most well-used datasets include TUM RGB-D, EuRoC MAV and KITTI. A recent contribution is the TUM VI benchmark, where aligned visual and IMU data are provided. One of the major distinctions of those datasets is their sensor types. While there is favor of RGB-D data source in recent SLAM algorithm research for dense scene reconstruction, there is a lack of dataset with both RGB-D and IMU data. Our dataset provides aligned RGB-D-IMU data, along with odometry data which are widely used in the industry but often lack in public datasets.\n\n[1] Comparison of modern open-source Visual SLAM approaches and comparison. They should include ground truth information and have a redundant amount of sensors in order to test different SLAM approaches based on different sensors stacks. In our work we reviewed visual and visual-inertial SLAM approaches, and therefore one of the main dataset choice factors was camera images presence. Datasets were examined for the existence of loop closures and additional sensors (IMU, GPS, or wheel encoders). Here we do a short description of all the datasets that we considered. Our final choice for the experiments are five datasets: EuRoC MAV, TUM VI, KITI, Open Loris, and DRE dataset. Camera data is shown in Fig. 4. A commonly used option for evaluating visual and visual-inertial SLAM algorithms is the EuRoC MAV (Burri et al., 2016), but its image resolution and bit depth is not quite state-of-the-art anymore. Also this dataset is mostly suitable for Micro Air Vehicles and does not include odometry readings suitable for some SLAM approaches for mobile wheeled robots. SLAM article authors usually test and compare their algorithms on EuRoC dataset primarily but in many cases SLAM algorithms show good performance on this dataset and bad on other datasets. That will be shown later. TUM VI (Schubert et al., 2018) is other popular dataset for evaluating visual and visual-inertial SLAM algorithms. It includes more varied scenes including indoor and outdoor environments and longer sequences than EuRoC dataset. Dataset provides timesynchronized camera images with 1024x1024 resolution and IMU measurements. However, this dataset provides accurate pose ground truth from a motion capture system only at\n\n[2] Modelling Software Architecture for Visual Simultaneous Localization and Mapping from a Microsoft Kinect in two different environments at 30 Hz. The ground truth is defined by the time-synchronized 6 DoF poses provided by an external motion capture system at 100 Hz. The TUM RGB-D dataset has been used for evaluation by PL-SLAM, ORB-SLAM, LSD-SLAM, Semidense-VO, and PL-SVO, as shown in Table 3. The PTAM and ORB-SLAM pipelines deliver the best performances on this dataset out of the solely vision-based SLAM pipelines given in Table 3. consists of RGB-D sequences, ground truth camera poses, and 3D surface models for four different trajectories generated in a synthetic environment. This is the first dataset to provide a 3D surface ground truth for assessing reconstruction accuracy in SLAM systems. The ICL-NUIM dataset has been used for the evaluation of SVO, ORB-SLAM, DSO, and LSD-SLAM by Forster et al. and of PL-SVO by Gomez-Ojeda et al.. The New College dataset consists of 30 GB of data collected as 5 DoF odometry, omnidirectional, and stereo camera sequences captured by a robotic vehicle driving through college campus. The New College dataset has been used for evaluation of RSLAM and ORB-SLAM. Other notable datasets used for evaluation of the VSLAM systems include RobotCar dataset, TrakMark, SLAMBench2, and the dataset proposed by Martull et al.. Conclusions Numerous approaches to VSLAM including filter-based, feature-based, direct, semidirect, and semi-dense methods have been developed and tested in the past. Exploration of VSLAM has fueled research in several different areas of\n\n[3] Semantic Visual Simultaneous Localization and Mapping: A Survey on the ORB-SLAM2 framework and combine semantic objects with building sparse 3D semantic object maps in real-time. It must be noted that semantic maps are more widely used in intelligent scenarios than traditional visual maps. However, it needs to face the challenges of heavy calculation, recognition of different types of objects, and map storage. IV. SLAM DATASETS It is well known that most SLAM systems evaluate their algorithms on multiple public datasets to prove their effectiveness in some aspects due to the expensive equipment and the complexity of device operation. The most frequently used SLAM datasets include KITTI, TUM RGB-D, ICL-NUIM and EuRoC MAV. These datasets are collected from different environments, and suited for different vSLAM algorithms. Therefore, it is extremely important to find appropriate datasets to evaluate a vSLAM. Recently, Liu et al. collated datasets commonly used in SLAM works in the past decade and provided a comprehensive introduction and analysis of them, which will facilitate the SLAM community to find suitable datasets. However, this survey does not provide a detailed introduction and analysis of the datasets suitable for semantic vSLAM. To fill this gap, we organize datasets suitable for semantic vSLAM, from which we evaluate and compare. A. Datasets Classification The categorization of datasets is typically based on sensor differences or applicable scenarios to help them understand and utilize existing SLAM datasets. Depending on the sensor, SLAM-related datasets can be divided into LiDAR, vision, and vision-LiDAR fusion datasets. The advantage of vision sensors is that they are inexpensive and\n\n[4] Are We Ready for Service Robots? The OpenLORIS-Scene Datasets for Lifelong SLAM This paper introduces the OpenLORIS-Scene datasets and metrics for benchmarking lifelong SLAM algorithms. The datasets capture scene changes caused by day-night shifts and human activities, as well as viewpoint changes, moving people, poor illumination, and blur. We found these factors challenging enough to existing SLAM systems. New metrics are proposed to evaluate the localization robustness and accuracy separately. With the datasets and metrics, we hope to help identify shortcomings of SLAM algorithms and to encourage new designs with more robust localization capabilities, such as by introducing high-level scene understanding capabilities. The datasets can also be a testbed of the maturity for real-world deployment of future SLAM algorithms for service robots.\n\n[5] Hilti-Oxford Dataset: A Millimeter-Accurate Benchmark for Simultaneous Localization and Mapping Hilti-Oxford Dataset: A Millimetre-Accurate Benchmark for Simultaneous Localization and Mapping Simultaneous Localization and Mapping (SLAM) is being deployed in real-world applications, however many state-of-the-art solutions still struggle in many common scenarios. A key necessity in progressing SLAM research is the availability of high-quality datasets and fair and transparent benchmarking. To this end, we have created the Hilti-Oxford Dataset, to push state-of-the-art SLAM systems to their limits. The dataset has a variety of challenges ranging from sparse and regular construction sites to a 17th century neoclassical building with fine details and curved surfaces. To encourage multi-modal SLAM approaches, we designed a data collection platform featuring a lidar, five cameras, and an IMU (Inertial Measurement Unit). With the goal of benchmarking SLAM algorithms for tasks where accuracy and robustness are paramount, we implemented a novel ground truth collection method that enables our dataset to accurately measure SLAM pose errors with millimeter accuracy. To further ensure accuracy, the extrinsics of our platform were verified with a micrometer-accurate scanner, and temporal calibration was managed online using hardware time synchronization. The multi-modality and diversity of our dataset attracted a large field of academic and industrial researchers to enter the second edition of the Hilti SLAM challenge, which concluded in June 2022. The results of the challenge show that while the top three teams could achieve accuracy of 2cm or better for some sequences, the performance dropped off in more difficult sequences. I. INTRODUCTION SLAM research has made impressive progress, allowing the transition from lab demonstrations to real-world deployment. Open-source datasets play\n\n[6] Hilti-Oxford Dataset: A Millimeter-Accurate Benchmark for Simultaneous Localization and Mapping Simultaneous Localization and Mapping (SLAM) is being deployed in real-world applications, however many state-of-the-art solutions still struggle in many common scenarios. A key necessity in progressing SLAM research is the availability of high-quality datasets and fair and transparent benchmarking. To this end, we have created the Hilti-Oxford Dataset, to push state-of-the-art SLAM systems to their limits. The dataset has a variety of challenges ranging from sparse and regular construction sites to a 17th century neoclassical building with fine details and curved surfaces. To encourage multi-modal SLAM approaches, we designed a data collection platform featuring a lidar, five cameras, and an IMU (Inertial Measurement Unit). With the goal of benchmarking SLAM algorithms for tasks where accuracy and robustness are paramount, we implemented a novel ground truth collection method that enables our dataset to accurately measure SLAM pose errors with millimeter accuracy. To further ensure accuracy, the extrinsics of our platform were verified with a micrometer-accurate scanner, and temporal calibration was managed online using hardware time synchronization. The multi-modality and diversity of our dataset attracted a large field of academic and industrial researchers to enter the second edition of the Hilti SLAM challenge, which concluded in June 2022. The results of the challenge show that while the top three teams could achieve an accuracy of 2 cm or better for some sequences, the performance dropped off in more difficult sequences.\n\n[7] PLD-SLAM: A New RGB-D SLAM Method with Point and Line Features for Indoor Dynamic Scene and Discussion In this section, to verify our method, we have performed a series of experiments using the public datasets with ground-truth. For the quantitative evaluation of visual SLAM, the easy-to-use evaluation tool of python (https://vision.in.tum.de/data/datasets/rgbddataset/tools, accessed on 19 August 2020) is used to evaluate the SLAM systems and PLD-SLAM. The metric index of the absolute camera trajectory error is used to evaluate our SLAM method. The root-mean-square errors (RMSE) of the absolute translation error (ATE) is considered as the main evaluation criterion and applied to measure the difference between the ground-truth and estimated trajectory. RMSE is an important SLAM evaluation indicator, and the RMSE of the ATE reflects the accuracy and the performance of SLAM. For the well-known SLAM systems (e.g., ORB-SLAM2), the RMSE is usually utilized to evaluate the overall performance of SLAM. TUM Dataset The TUM dataset is the well-known RGB-D benchmark to evaluate the SLAM system. The datasets consist of various indoor scene sequences recorded using an RGB-D camera of Microsoft Kinect1. The image sequences are recorded in real indoor scenes at a frame rate of 30 Hz with a 640 × 480 size and provide the ground-truth trajectories captured by using a motion-capture system with higher accuracy. As a result, many wellknown SLAM schemes have adopted the TUM dataset as their benchmark dataset, such as ORB-SLAM2. In the fr3 sequences (e.g., sitting_xyz, sitting_half_sphere, sitting_rpy, sitting_static, walking_xyz, walking_half_sphere, walking_rpy, walking_static), the RGB-D camera is always facing the table. The camera movement trajectory and the person's movement mode are different\n\n[8] NICE-SLAM: Neural Implicit Scalable Encoding for SLAM Datasets. We consider 5 versatile datasets: Replica , ScanNet , TUM RGB-D dataset , Co-Fusion dataset , as well as a self-captured large apartment with multiple rooms.We follow the same pre-processing step for TUM RGB-D as in  .\n\n"}
{"case_id": "0dd4500b3ca1daf372dffb4378ee2c8f", "answer_text": "\n\nSeveral practically relevant properties and classes of \\( k \\)-node subgraphs in graphs that can be counted faster than cubic time have been identified in recent research to address the deficiencies of previous algorithms, particularly for sparse graphs [1]. These properties leverage various graph parameters, such as degeneracy and tree decomposition, to optimize subgraph counting algorithms [2]. \n\n **1. Degeneracy**[^1].\n\nDegeneracy is a measure of graph sparsity and indicates the smallest number \\( d \\) such that every subgraph of the graph has a vertex with degree at most \\( d \\). Utilizing degeneracy, pattern graphs can be counted more efficiently. For instance, if a graph \\( G \\) has degeneracy \\( d \\), and a pattern graph \\( H \\) has a low treewidth, the induced copies of \\( H \\) in \\( G \\) can be counted in time \\( f(d, k) \\cdot \\tilde{O}(n^{\\tau(H)}) \\), where \\( \\tau(H) \\) is called the DAG-treewidth of \\( H \\) [1]. This approach outperforms classical cubic-time algorithms for sufficiently sparse graphs by making the complexity dependent on the graph's degeneracy and the structure of the pattern graph, rather than solely on its size. For example, practical applications include network motif discovery in biological networks where the networks are typically sparse and have well-defined motifs of interest.\n\n**2. Tree Decompositions and Treewidth**[6]\nAnother important parameter that improves the efficiency of subgraph counting is treewidth. For graphs with bounded treewidth, subgraph counting can be significantly optimized. For example, a novel algorithm uses tree decompositions to count homomorphisms from \\( H \\) to \\( G \\) in time \\( f(d, k) \\cdot Õ(n^{s(H)}) \\), where \\( s(H) \\) is an associated measure called source-treewidth [2]. This method exploits the decompositional structure of the pattern graph to achieve better efficiency compared to traditional methods in certain sparsity conditions. This approach is particularly useful in computational chemistry for counting specific molecular substructures within larger molecular graphs.\n\n**3. Graphlet Counting** \n\n[11]\n\n\nGraphlets are small, connected, non-isomorphic subgraphs used to characterize the local structure of larger networks. Efficient algorithms for counting specific types of \\( k \\)-node graphlets (e.g., triangles, 4-node motifs) have been developed. For instance, optimized algorithms for counting 3- and 4-node graphlets can run in linear and near-linear time for graphs with bounded degeneracy, significantly faster than cubic time [11]. These algorithms leverage combinatorial arguments that relate smaller graphlet counts to larger ones, thus reducing the computation required for larger graphlets. These algorithms have been demonstrated to be practical and scalable, with applications across various domains including biology, where they help in understanding the complex relationships between proteins, and social network analysis, where graphlets are used to uncover hidden social structures.\n\n**4. Approximation and Sampling Methods** [5]. \nApproximation algorithms that use sampling techniques provide another avenue for faster subgraph counting. These methods trade off some accuracy for significant gains in computational speed. For example, a sampling-based algorithm achieves estimates for 4-vertex subgraphs with less than 1% error while being orders of magnitude faster than exact enumeration methods [5]. These methods often employ random walks or edge sampling strategies to approximate the count of subgraphs, such as the Path Sampling method, which is extremely fast even on very large graphs. Such techniques have proven effective for large-scale graphs where precise counting would be computationally prohibitive. For instance, in large-scale social networks and web graphs, these methods can quickly identify prevalent substructures, facilitating rapid analysis and visualization.\n\n**5. k-Core Decompositions** [7]\nThe k-core of a graph is its maximal subgraph in which each vertex has at least degree \\( k \\). The core number of a vertex is the highest \\( k \\) for which the vertex belongs to the k-core. Algorithms that utilize k-core decompositions can efficiently narrow down the graph to its densest parts, enabling faster counting of subgraphs within these dense regions [7]. This method optimizes counting by focusing on substructures where the density is highest, thus significantly reducing the search space. This technique is particularly useful in network robustness studies where identifying core structures can help understand critical nodes and potential vulnerabilities.\n\nThese properties and parameterizations enable the development of advanced, efficient subgraph counting algorithms tailored to specific graph structures and sparsity, significantly enhancing performance beyond the classic cubic time complexity [1]. \n\n\nReferences:\n[0] Faster Subgraph Counting in Sparse Graphs A fundamental graph problem asks to compute the number of induced copies of a $k$-node pattern graph $H$ in an $n$-node graph $G$. The fastest algorithm to date is still the 35-years-old algorithm by Ne\\v{s}et\\v{r}il and Poljak, with running time $f(k) \\cdot O(n^{\\omega\\lfloor\\frac{k}{3}\\rfloor + 2})$ where $\\omega \\le 2.373$ is the matrix multiplication exponent. In this work we show that, if one takes into account the degeneracy $d$ of $G$, then the picture becomes substantially richer and leads to faster algorithms when $G$ is sufficiently sparse. More precisely, after introducing a novel notion of graph width, the \\emph{DAG-treewidth}, we prove what follows. If $H$ has DAG-treewidth $\\tau(H)$ and $G$ has degeneracy $d$, then the induced copies of $H$ in $G$ can be counted in time $f(d,k) \\cdot \\tilde{O}(n^{\\tau(H)})$; and, under the Exponential Time Hypothesis, no algorithm can solve the problem in time $f(d,k) \\cdot n^{o(\\tau(H)/\\ln \\tau(H))}$ for all $H$. This result characterises the complexity of counting subgraphs in a $d$-degenerate graph. Developing bounds on $\\tau(H)$, then, we obtain natural generalisations of classic results and faster algorithms for sparse graphs. For example, when $d=O(\\operatorname{poly}\\log(n))$ we can count the induced copies of any $H$ in time $f(k) \\cdot\\tilde{O}(n^{\\lfloor \\frac{k}{4} \\rfloor + 2})$, beating the Ne\\v{s}et\\v{r}il-Poljak algorithm by essentially a cubic factor in $n$.\n\n[1] On exploiting degeneracy to count subgraphs On exploiting degeneracy to count subgraphs Motivated by practical applications, we study the complexity of counting the copies of a k-node pattern graph H in an n-node graph G, parameterized by the degeneracy d of G. We develop the source tree decomposition of H , a tree decomposition designed to exploit the degeneracy of G algorithmically to count homomorphisms, and an associated measure of source-treewidth s(H). Combining a simple dynamic programming with inclusion-exclusion arguments, for any given H we can count the homomorphisms from H to G in time f(d, k)·Õ(ns(H)). This allows us to obtain fast algorithms by bounding s(H) for interesting classes of patterns. For instance, we can count the induced or non-induced copies of any pattern faster than the decades-old stateof-the-art O(n ωk 3 ) algorithm of Nešetřil and Poljak as long as d < n or m < n, where m is the number of edges of G. We complement our upper bounds with almost-matching lower bounds based on the Exponential Time Hypothesis.\n\n[2] Path Sampling: A Fast and Provable Method for Estimating 4-Vertex Subgraph Counts Extremely fast.Our algorithm relies on a sampling based approach making it extremely fast even on very large graphs. Indeed, there are instances where a finely tuned enumeration code takes almost a day to compute counts of 4-vertex motifs whereas our algorithm only takes less than a minute to output accurate estimates. Refer to Fig. 2(a) for speedup over a well-tuned enumeration code.Our algorithm takes less a minute on an Orkut social network with 200 million edges, where the total count of each motif is over a billion (and most counts are over 10 billion).An input Flickr social network has more than 10 billion 4-cliques; we get estimate of thisnumber with less than 0.5%percent0.50.5\\% error within 30 seconds on a commodity machine. We do not preprocess any of the graphs,and simply read them as a list of edges.Excellent empirical accuracy.We empirically validate our algorithm on a largevariety of datasets, and it consistently gives extremely accurate answers. Refer to Fig. 2(b).We get <1%absentpercent1<1\\% relative error for all subgraph counts on all datasets, even those with more than 100M edges. (Exact counts were obtained by brute-force enumerations that took several days.)This is much more accurate than any existing method to count such motifs.We compare with existing sampling methods, and demonstrate that our algorithm is faster and more accurate thanthe state-of-the-art.Provable guarantees with error bars.Our algorithm has a provable guaranteeon accuracy and running time. Furthermore, we can quantify the accuracy/confidence on realinputs and runs of our algorithm. For a given number of samples, we can have a method toput an explicit error bar on our estimate, based on asymptotically tight versions of Chernoff’s bound.While these error bars are not as tight as the real errors in Fig. 2(b), we can stillmathematically prove that the errors are mostly within 5% and always within 10%.Trends in 4-vertex pattern counts:Given the rapid reporting of 4-vertexpattern counts, our algorithm can be used as a tool for motif analysis. We detect commontrends among a large variety of graphs. Not surprisingly, the 3-star is the most frequent4-vertex motif in all graphs we experimented upon. The least frequent is either the 4-cycleor the 4-clique. The chordal-4-cycle frequency is always more than that of the 4-cycle or 4-clique.Ugander et al  study what trends are merely implied by graphtheory, and what are actually features of real-world graphs. Such analyses requireaccurate estimates quickly, which our algorithm can provide. It is a promising direction to useour algorithm to provide more input to such studies.\n\n[3] Counting subgraphs via DAG tree decompositions $O(n^{\\frac{\\omega k}{3}+2})$ algorithm of Ne\\v{s}et\\v{r}il and Poljak for $d Introduction Given a host graph G on n nodes and a pattern graph H on k nodes, we want to count the number of induced subgraphs of G that are isomorphic to H. This problem is notoriously hard: it is generally believed that even just detecting a k-clique in an n-node graph requires time n Ω(k),8, and that counting cycles or matchings on k nodes requires time n Ω(k/ ln(k)). Correspondingly, the fastest known induced subgraph counting algorithm has running time O(n ωk 3 +O(1) ) where ω is the matrix multiplication exponent. One way of circumventing this barrier is to refine the bounds by adding parameters (besides n and k) that capture the structure of the input. For example, if H has vertex-cover number c then its non-induced copies in G can be counted in time f (k) · n c+O (1),35,5,11. Or, if H has treewidth t, then its homomorphisms into G can be counted in time f (k) · O(n t+1 ). Finding the right parameterization can be nontrivial, but gives back better algorithms and a deeper understanding of the problem. In this paper we focus on the case where G is sparse, which is often true in practice. The main measure of sparsity we adopt is degeneracy: the smallest integer d such that every subgraph of G has minimum degree bounded by d. We thus parameterize our bounds by n, k, and d, which is natural for\n\n[4] FastGAE: Fast, Scalable and Effective Graph Autoencoders with Stochastic Subgraph Decoding ij . • the core number of each node: f (i) = C(i). The k-core version of a graph is its largest subgraph for which every node has a degree higher or equal to k within this subgraph. The core number C(i) of a node i corresponds to the largest value of k for which i is in the k-core. Core decomposition has been widely used over the past years to quantify the significance of nodes and extract representative subgraphs (see Malliaros et al. (2019) for a review). They constitute a more global importance measure than the local node degree. Besides their popularity, we also selected these two metrics for computational efficiency. Indeed, contrary to numerous influence maximization or centrality-based measures (Newman, 2010), both can be evaluated in a linear O(m) running time (Batagelj & Zaversnik, 2003). As we empirically check in Section 4, this leads to fast and scalable computations of probability distributions, which is crucial for our FastGAE framework whose primary objective is scalability. Learning from Subgraphs Reconstructions After sampling, at each training iteration, we evaluate reconstruction losses only on the subgraph G (S) , which involves fewer operations w.r.t. standard decoders. We use the resulting approximate loss for gradients computations and weights updates via mini-batch gradient descent. We note that effective subset selection for faster learning has already provided promising results in the machine learning community (Tonnaer, 2017;Kaushal et al., 2018;Gonzalez & Miikkulainen, 2019) ; however, contrary to these works, we focus on an unsupervised graph-based problem, and we position ourselves outside the\n\n[5] Efficient Graphlet Counting for Large Networks From social science to biology, numerous applications often rely on graphlets for intuitive and meaningful characterization of networks at both the global macro-level as well as the local micro-level. While graphlets have witnessed a tremendous success and impact in a variety of domains, there has yet to be a fast and efficient approach for computing the frequencies of these subgraph patterns. However, existing methods are not scalable to large networks with millions of nodes and edges, which impedes the application of graphlets to new problems that require large-scale network analysis. To address these problems, we propose a fast, efficient, and parallel algorithm for counting graphlets of size k={3,4}-nodes that take only a fraction of the time to compute when compared with the current methods used. The proposed graphlet counting algorithms leverages a number of proven combinatorial arguments for different graphlets. For each edge, we count a few graphlets, and with these counts along with the combinatorial arguments, we obtain the exact counts of others in constant time. On a large collection of 300+ networks from a variety of domains, our graphlet counting strategies are on average 460x faster than current methods. This brings new opportunities to investigate the use of graphlets on much larger networks and newer applications as we show in the experiments. To the best of our knowledge, this paper provides the largest graphlet computations to date as well as the largest systematic investigation on over 300+ networks from a variety of domains.\n\n"}
{"case_id": "f7d0d99e1d158e6f89eee62b5aace89c", "answer_text": "\n\nSubgraph counting, the focus of the referenced paper, is a critical problem with numerous downstream applications across different fields [1]. These applications highlight the importance and utility of efficient algorithms for counting subgraphs with specific properties [0]. Here, we detail the most significant real-world applications of subgraph counting algorithms, supported by concrete examples [1], and also discuss the limitations, challenges, and computational complexity aspects of current subgraph counting algorithms [4].\n\n \n\n1. **Biological Network Analysis**. Subgraph counts are widely used to analyze real-world graphs, such as graphs representing telecommunication networks, biological structures and social interactions. Consequently, subgraph counts feature prominently in studies of biological and sociological networks ([1]).\n\n\n\nOne of the most notable applications of subgraph counting is in the analysis of biological networks. Biological networks, such as protein-protein interaction networks, metabolic networks, and gene regulatory networks, often exhibit specific subgraph patterns known as motifs, which are small recurring subgraphs that may represent fundamental biological processes. By counting these motifs, researchers can uncover insights into the structural and functional organization of biological systems [1].\n\nFor instance, in gene regulatory networks, the prevalence of certain motifs can indicate specific regulatory mechanisms [1]. Similarly, in protein-protein interaction networks, the identification of motifs can elucidate pathways and complexes critical for cellular functions [2]. These insights are foundational for understanding diseases and developing therapeutics [1][4]. Concrete implementations include the use of motif-based analysis in identifying cancer-related pathways and in drug discovery, where the structure-activity relationship of compounds is studied through subgraph patterns [4].\n\nSocial Network Analysis is an important tool in understanding the structure and dynamics of social networks. By analyzing the layout and interactions within these networks, researchers can identify key individuals or nodes, detect communities, and understand information flow. Techniques such as subgraph counting are used to analyze the frequency and patterns of small subgraphs within larger networks. These subgraphs, often referred to as motifs, can reveal essential structural features of the network [1]. Additionally, efficient algorithms for subgraph counting are crucial due to the potential size and complexity of real-world social networks [4].\n\nIn social network analysis, subgraph counting helps to identify and understand social structures and group dynamics. Social networks are composed of individuals (nodes) and their interactions (edges). By counting subgraphs such as cliques (complete subgraphs), researchers can identify tightly-knit communities or social circles within larger networks [3][8].\n\nFor example, the detection of triangles (3-cliques) in a social network can indicate the presence of strong, mutual relationships among three individuals, which is an indicator of social cohesion and trust within a network. These analyses are crucial for applications such as marketing, where understanding the spread of information and influence through social networks can inform strategies for viral marketing and information dissemination [3][8]. Real-world examples include the use of subgraph counting in platforms like Twitter and Facebook to recommend friends or content based on network structure and interaction patterns.\n\n  \n3. **Telecommunication Networks** Subgraph counts are widely used to analyze real-world graphs, such as graphs representing telecommunication networks, biological structures and social interactions [1].  \n\n\nIn telecommunication networks, subgraph counting is instrumental in optimizing network design and detecting vulnerabilities. These networks can be modeled as graphs where nodes represent communication devices and edges represent communication links [1]. By counting subgraphs corresponding to different connectivity patterns, network engineers can assess network reliability and performance [1]. \n\nFor instance, identifying cycles and other specific subgraphs can help in detecting redundant paths, which are essential for network resilience and fault tolerance. Additionally, the occurrence of particular subgraphs can point to potential bottlenecks or critical points that, if failed, could disrupt network communication. This application is crucial for designing robust and efficient telecommunication infrastructures [1][2]. Specific examples include the use of subgraph patterns in detecting and mitigating Distributed Denial of Service (DDoS) attacks and optimizing routing protocols.\n\nFraud detection and security in networked systems often leverage subgraph counting techniques to identify unusual or suspicious patterns. These methods involve the algorithmic task of computing the number of occurrences of specific subgraphs within a larger graph, which is crucial for analyzing complex networks such as telecommunications, biological structures, and social interactions [1]. The subgraph counts can reveal significant motifs that may serve as indicators of fraudulent activity or breaches in security protocols. Efficient algorithms and counting methods, such as those used for detecting graphlets, play a pivotal role in these applications, particularly when dealing with large-scale data [4].\n\nSubgraph counting algorithms are also applied in security and fraud detection. In financial networks or communication networks, certain subgraph patterns can indicate fraudulent activities or security breaches. For example, in a financial transaction network, the detection of specific subgraph patterns may reveal money laundering activities or other forms of financial fraud [1][3].\n\nSimilarly, in cybersecurity, network traffic analysis using subgraph counting can help in identifying suspicious communication patterns that may indicate the presence of malware or coordinated cyber-attacks. By understanding these patterns, security analysts can develop more effective monitoring and mitigation strategies to protect critical infrastructures [3][8]. For instance, anomaly detection in banking transactions and the identification of fraud rings are practical applications of these techniques.\n\nStatement: 5. **Chemical and Material Sciences** References: None of the provided evidence passages directly support or mention the specific area of \"Chemical and Material Sciences\" in the context provided. Therefore, no citations can be inserted accurately. \n\nIn chemical and material sciences, subgraph counting is used to analyze molecular structures [1]. Molecules can be represented as graphs where atoms are nodes and chemical bonds are edges. By counting subgraphs that correspond to specific chemical configurations, chemists can predict molecular properties and behaviors [4].\n\nFor instance, the presence of particular subgraph structures within a molecule can influence its reactivity, stability, and other chemical properties. This understanding is vital for designing new materials with desired characteristics, such as pharmaceuticals, polymers, and nanomaterials. Computational chemistry and material science extensively use these algorithms to screen and design novel compounds [2][14]. Real-world examples include the development of new drugs through the identification of active substructures and the design of advanced materials like graphene through pattern analysis of carbon atoms.\n\nComputational Complexity and Performance [14]. \n\nThe computational complexity of subgraph counting can be significant, especially for large graphs and larger subgraph sizes. The challenge lies in the combinatorial explosion of potential subgraphs to be counted. For instance, enumerating all k-vertex subgraphs can have a time complexity of \\(O(n^k)\\), where \\(n\\) is the number of vertices in the graph, making it infeasible for large \\(k\\) [5][6]. Consequently, there is a need for efficient algorithms that avoid exhaustive enumeration, such as sampling-based approaches, parallel computing, and compressed network representations like SCMD [2].\n\n\n\n### Limitations and Challenges\n\nCounting subgraphs is a fundamental problem in the analysis of complex networks and has significant theoretical and practical importance. In practice, subgraph counts are widely used to analyze real-world graphs, such as those representing telecommunication networks, biological structures, and social interactions [1]. For example, networks from various scientific domains like biochemistry, neurobiology, and ecology exhibit different small subgraph patterns, which can uncover fundamental building blocks of these networks [1].\n\nDespite its importance, subgraph counting faces substantial challenges. One major issue is the computational complexity, as enumerating all induced subgraphs of size \\( k \\) in a graph with \\( n \\) vertices may require a running time of \\( O(n^k) \\), making it infeasible for large graphs [5].\n\nWhile there are many algorithms developed to tackle subgraph counting, such as symmetry compression methods and heuristic graph isomorphism algorithms, these methods can still be computationally intensive, especially for large networks [2]. Moreover, designing efficient exact and approximate sampling algorithms remains a priority to manage the combinatorial explosion as the size of subgraphs increases [4].\n\n\n\nDespite their broad utility, current subgraph counting algorithms face several limitations and challenges [1]. \n\n1. **Scalability**: Many subgraph counting algorithms struggle with scalability, especially when dealing with large networks with millions or billions of nodes and edges. The computational complexity often grows exponentially with the size of the subgraph, making it infeasible for large-scale applications [6][12].\n\n2. **Accuracy**: Approximation methods are often employed to handle large datasets, but these can introduce errors and may not always guarantee accurate counts, which can be critical in sensitive applications like fraud detection and biological research [7].\n\n3. **Complex Network Structures**: Real-world networks often exhibit complex structures like overlapping communities, hierarchical organization, and varying levels of sparsity and density. These characteristics can complicate subgraph counting algorithms, which may need to be specialized or adapted to handle specific types of networks robustly [9][11].\n\n4. **Resource Intensity**: Subgraph counting can be resource-intensive, requiring significant computational power and memory, which limits its applicability in resource-constrained environments [13].\n\n\nConclusion [1].\n\n\nThe applications of subgraph counting algorithms span a broad spectrum of scientific and practical domains, from biology and social sciences to telecommunications and security [1]. Each of these applications leverages the ability to efficiently identify and count subgraphs with specific properties, providing valuable insights and enabling advancements in various fields [1]. However, the limitations and challenges of current algorithms highlight the need for continued research and development to make these methods more scalable, accurate, and efficient for real-world applications [1]. \n\n\nReferences:\n[0] From Graph Properties to Graph Parameters: Tight Bounds for Counting on Small Subgraphs A common formalism to unify different counting problem is the following [JM15b, JM15a, FR22, RSW23, DMW23, DRSW22. Let ΦΦ\\Phi be a graph property such as bipartite, clique, or planar; formally, ΦΦ\\Phi is a function that maps every graph to {0,1}01\\{0,1\\} and closed under isomorphism. Then given a graph G𝐺G and integer k𝑘k, the task is to count the number of k𝑘k-vertex induced subgraphs that satisfy property ΦΦ\\Phi, or more formally, to compute the value#IndSub​((Φ,k)→G)≔∑A⊆(V​(G)k)Φ​(G​[A).≔#IndSub((Φ,k)→G)subscript𝐴binomial𝑉𝐺𝑘Φ𝐺delimited-[𝐴\\text{\\#}{}\\mbox{$\\mathrm{IndSub}((\\Phi,k)\\to G)$}\\coloneqq\\sum_{A\\subseteq\\binom{V(G)}{k}}\\Phi(G[A).For a fixed graph property ΦΦ\\Phi, the problem #IndSub​(Φ)#IndSubΦ\\text{\\#}{}\\textsc{IndSub}(\\Phi) is definedas the problem that gets as input an integer k𝑘k and a graph G𝐺G and computes#IndSub​((Φ,k)→G)IndSub→Φ𝑘𝐺\\mathrm{IndSub}((\\Phi,k)\\to G).\n\n[1] Counting Subgraphs in Degenerate Graphs Subgraph counting refers to the algorithmic task of computing the number of copies (i.e., occurences) of a given graph H𝐻H in an input graph G𝐺G.Due to its fundamental nature, this problem has been studied extensively, both from a theoretical perspective and for practical applications.In practice, subgraph counts are widely used to analyze real-world graphs, such as graphs representing telecommunication networks, biological structures and social interactions. Consequently, subgraph counts feature prominently in studies of biological, 41, 42 and sociological, 27 networks, as well as in the network science literature in general, 30, 38, 44, 45, 46.For example, observed that networks coming from different areas of science (such as biochemistry, neurobiology, ecology, and engineering) have significantly different counts of small subgraphs.Such frequently occuring subgraphs are called motifs, and, quoting, “may uncover the basic building blocks of most networks”.Needless to say, some real-world graphs can be very large – having billions of vertices – thus making it all the more desirable to have fast subgraph counting algorithms.\n\n[2] A Survey on Subgraph Counting {NoHyper}Wanget al. (Wanget al., 2012) proposed a new methodcalled SCMD that counts subgraphs in compressed networks. SCMD applies a symmetrycompression method that finds sets of vertices that are in anhomeomorphism to cliques or empty subgraphs, which have the additionalproperty that any other vertex that connects to a vertex in the set isconnected to all other vertices in the set. These sets of verticesform a partition of the graph that is obtained using a methodpublished in (MacArthur et al., 2008), which is based on lookingat vertices in the same orbit.Thisis a versatile method that can usealgorithms like ESU or Kavosh to enumerate all subgraphs ofsizes from 1 to k𝑘k in the compressed network. Finally, SCMD “decompresses” the resultsby looking at all the different enumerated subgraphs and calculatingall the combinations that can form a decompressed subgraph. Forexample, for k=3𝑘3k=3, if a compressed 2-subgraph is found containingtwo vertices: one compressed vertex representing a clique of 5uncompressed vertices and a compressed vertex representing a singlevertex from the uncompressed graph, it results in (52)+(53)binomial52binomial53\\binom{5}{2}+\\binom{5}{3} triangles from the uncompressed graph, obtained bytaking two vertices from the clique vertex and one from the othervertex, which are all connected and thus form a triangle,(52)binomial52\\binom{5}{2}, plus taking three vertices from the clique vertex(53)binomial53\\binom{5}{3}. The authors argue that most complex networks exhibit high symmetriesand thus are improved by the application of this technique. Eventhough their work only includes undirected graphs, the authors affirmit is easy to extend the same concepts to directed networks. Xu et al.described another algorithm that enumerates subgraphs on compressed networks, called ENSA (Xu et al., 2014; Zhang and Xu, 2014). Their method is based on an heuristic graphisomorphism algorithm, and they also discuss an optimization based on identifyingvertices with unique degrees.\n\n[3] Counting Subgraphs in Degenerate Graphs Let H1,H2,…,Hksubscript𝐻1subscript𝐻2…subscript𝐻𝑘H_{1},H_{2},\\dots,H_{k} be an enumeration of all induced subgraphs of H𝐻H (including the empty one), up to isomorphism (that is, H1,…,Hksubscript𝐻1…subscript𝐻𝑘H_{1},\\dots,H_{k} are pairwise non-isomorphic). For each 1≤i≤k1𝑖𝑘1\\leq i\\leq k, setci:=∑U⊆V​(H):H​[U≅Hihom⁡(H​[V​(H)∖U,Kh).assignsubscript𝑐𝑖subscript:𝑈𝑉𝐻absent𝐻delimited-[𝑈subscript𝐻𝑖hom𝐻delimited-[𝑉𝐻𝑈subscript𝐾ℎc_{i}:=\\sum_{\\begin{subarray}{c}U\\subseteq V(H):\\\\H[U\\cong H_{i}\\end{subarray}}\\hom(H[V(H)\\setminus U,K_{h}).Note that c1,…,cksubscript𝑐1…subscript𝑐𝑘c_{1},\\dots,c_{k} depend only on H𝐻H.With this notation, we can rewrite (7) as follows:hom⁡(H,F+Kh)=∑i=1kci⋅hom⁡(Hi,F).hom𝐻𝐹subscript𝐾ℎsuperscriptsubscript𝑖1𝑘⋅subscript𝑐𝑖homsubscript𝐻𝑖𝐹\\hom(H,F+K_{h})=\\sum_{i=1}^{k}{c_{i}\\cdot\\hom(H_{i},F)}.(8)Note that for each 1≤i≤k1𝑖𝑘1\\leq i\\leq k we have ci>0subscript𝑐𝑖0c_{i}>0, since there is some U⊆V​(H)𝑈𝑉𝐻U\\subseteq V(H) for which H​[U≅Hi𝐻delimited-[𝑈subscript𝐻𝑖H[U\\cong H_{i} (by our choice of H1,…,Hksubscript𝐻1…subscript𝐻𝑘H_{1},\\dots,H_{k}), and for this U𝑈U it clearly holds that hom⁡(H​[V​(H)∖U,Kh)>0hom𝐻delimited-[𝑉𝐻𝑈subscript𝐾ℎ0\\hom(H[V(H)\\setminus U,K_{h})>0.\n\n[4] Counting five-node subgraphs Counting five-node subgraphs We propose exact count formulae for the 21 topologically distinct non-induced connected subgraphs on five nodes, in simple, unweighted and undirected graphs. We prove the main result using short and purely combinatorial arguments that can be adapted to derive count formulae for larger subgraphs. To illustrate, we give analytic results for some regular graphs, and present a short empirical application on real-world network data. We also discuss the well-known result that induced subgraph counts follow as linear combinations of non-induced counts. Introduction Networks are a fundamental tool for modelling the topological structure of complex systems. They are of considerable theoretical interest, and have practical applications in a wide variety of fields, including biology, statistical physics, and social science,9,13,20,28,32. An important problem is the enumeration of small connected induced or non-induced subgraphs (graphlets) on a network, a variant of the classical subgraph isomorphism problem in theoretical computer science. For example, subgraph counts are used for network classification, and to determine the statistical significance of small topological structures that arise frequently in real-world networks, where they may have specific functional roles,7,12,16,19,22,23,25,26,34. Analytical exact count formulae have been a subject of theoretical research since at least the early 1970s, although it can be computationally expensive to apply them, and this becomes exponentially hard as the number of nodes in the graphlet increases (combinatorial explosion). For this reason, there has been a great deal of work on the design of efficient exact and approximate sampling algorithms, with applications to increasingly massive datasets (the first\n\n[5] From Graph Properties to Graph Parameters: Tight Bounds for Counting on Small Subgraphs For example, if we wishto compute a function that depends somehow on patterns that have at most k𝑘k vertices in a graph G𝐺G withn𝑛n vertices, then one could do so by simply considering all induced subgraphs ofsize k𝑘k in G𝐺G.However, there are (nk)binomial𝑛𝑘\\binom{n}{k} many induced subgraphs of size k𝑘k, meaning that enumeratingall of them would require a running time of O​(nk)𝑂superscript𝑛𝑘O(n^{k}) which is unfeasible if thepattern size k𝑘k becomes too large.Instead, we require an algorithmwhose complexity does not explode with our pattern size k𝑘k.\n\n[6] Counting Subgraphs in Degenerate Graphs For vertices a,b∈V​(H)=V​(H→)𝑎𝑏𝑉𝐻𝑉→𝐻a,b\\in V(H)=V(\\vec{H}), denote by dist→​(a,b)→dist𝑎𝑏\\overrightarrow{\\text{dist}}(a,b) the length of a shortest directed path from a𝑎a to b𝑏b (in H→→𝐻\\vec{H}).Now, fix a good 2​k2𝑘2k-tuple M=(v0,…,vk−1,y0,…,yk−1)𝑀subscript𝑣0…subscript𝑣𝑘1subscript𝑦0…subscript𝑦𝑘1M=(v_{0},\\dots,v_{k-1},y_{0},\\dots,y_{k-1}) which minimizes the sum∑i=0k−1(dist→​(vi,yi)+dist→​(vi−1,yi)).superscriptsubscript𝑖0𝑘1→distsubscript𝑣𝑖subscript𝑦𝑖→distsubscript𝑣𝑖1subscript𝑦𝑖\\sum_{i=0}^{k-1}\\left(\\overrightarrow{\\text{dist}}(v_{i},y_{i})+\\overrightarrow{\\text{dist}}(v_{i-1},y_{i})\\right).(5)\n\n[7] A Survey on Subgraph Counting This family of algorithms relies on the idea of sampling path subgraphs to estimate the frequencies of the other subgraphs. Pathsubgraphs are composed by 2 exterior nodes and k−2𝑘2k-2 interior nodes (where k𝑘k is the size of the subgraph) arranged in a singleline; the interior nodes all have degree of 2, while the exterior nodes have degree of 1. Examples of these are the subgraphs G1subscript𝐺1G_{1}, G3subscript𝐺3G_{3}and G9subscript𝐺9G_{9} in Figure 6. The main idea for these algorithms, mainly for k≥4𝑘4k\\geq 4, is relating the number ofnon-induced occurrences of each subgraph of size k𝑘k in the other size k𝑘k subgraph. For example, when k=4𝑘4k=4, there are 4 non-inducedoccurrences of G3subscript𝐺3G_{3} in G5subscript𝐺5G_{5} or 12 non-induced occurrences of G3subscript𝐺3G_{3} in G8subscript𝐺8G_{8}. Table 5 shows this fullrelationship when k=4𝑘4k=4.\n\n[8] The parameterised complexity of counting connected subgraphs and graph motifs The number of multicoloured connected induced subgraphs in a graph G𝐺G can be computed by inclusion-exclusion from the numbers of connected induced subgraphs in the 2ksuperscript2𝑘2^{k} subgraphs of G𝐺G induced by different combinations of colour-classes. (Inclusion-exclusion methods have been used in a similar way in, for example,, 8.) Suppose the graph G𝐺G is coloured with colours [kdelimited-[𝑘[k, and for any C⊆[k𝐶delimited-[𝑘C\\subseteq[k let GCsubscript𝐺𝐶G_{C} be the subgraph of G𝐺G induced by the vertices with colours belonging to C𝐶C. Then, if Nk​(H)subscript𝑁𝑘𝐻N_{k}(H) denotes the number of connected induced k𝑘k-vertex subgraphs in H𝐻H, the number of colourful connected induced subgraphs in G𝐺G is exactly∑∅≠C⊆[k(−1)k−|C|​Nk​(GC).subscript𝐶delimited-[𝑘superscript1𝑘𝐶subscript𝑁𝑘subscript𝐺𝐶\\sum_{\\emptyset\\neq C\\subseteq[k}(-1)^{k-|C|}N_{k}(G_{C}).\n\n[9] The parameterised complexity of counting connected subgraphs and graph motifs For any symmetric property ΦΦ\\Phi, the output of p𝑝p-#Induced Subgraph With Property(Φ)Φ(\\Phi) is exactly k!𝑘k! times the output of p𝑝p-#Induced Unlabelled Subgraph With Property(Φ)Φ(\\Phi). The unlabelled version is less general than the labelled version, as this only allows us count induced subgraphs having some particular property rather than, for example, all (not necessarily induced) copies of some fixed graph H𝐻H. As an example, the labelled version can express problems such as p𝑝p-#Matching, whereas the former would only allow us to count k𝑘k-vertex induced subgraphs that contain a perfect matching (ignoring the fact that any one k𝑘k-vertex induced subgraph may contain many perfect matchings).\n\n[10] Counting Cohesive Subgraphs with Hereditary Properties A simple approach to count 𝖧𝖢𝖲𝗌𝖧𝖢𝖲𝗌\\mathsf{HCSs} in a graph is to list them. However, listing all 𝖧𝖢𝖲𝗌𝖧𝖢𝖲𝗌\\mathsf{HCSs} in a graph can be challenging due to potential overlap and a potentially large number of HCSs. To address this problem, we propose a general backtracking algorithm, called 𝖧𝖢𝖲𝖫𝗂𝗌𝗍𝖧𝖢𝖲𝖫𝗂𝗌𝗍\\mathsf{HCSList}, which can enumerates each 𝖧𝖢𝖲𝖧𝖢𝖲\\mathsf{HCS} in a graph exactly once. The key idea of 𝖧𝖢𝖲𝖫𝗂𝗌𝗍𝖧𝖢𝖲𝖫𝗂𝗌𝗍\\mathsf{HCSList} is described as follows. Since 𝖧𝖢𝖲𝖧𝖢𝖲\\mathsf{HCS} satisfies the hereditary property, we can enumerate an 𝖧𝖢𝖲𝖧𝖢𝖲\\mathsf{HCS} starting from a single vertex and maintaining a set R𝑅R that represents the current sub-𝖧𝖢𝖲𝖧𝖢𝖲\\mathsf{HCS}. We then iteratively grow R𝑅R to the desired size, and use a backtracking technique to list all 𝖧𝖢𝖲𝗌𝖧𝖢𝖲𝗌\\mathsf{HCSs}.\n\n[11] Counting Subgraphs in Degenerate Graphs [ProofFix numbersℓ1,2,ℓ1,3,ℓ2,3∈{⌊k/3⌋−1,⌈k/3⌉−1}subscriptℓ12subscriptℓ13subscriptℓ23𝑘31𝑘31\\ell_{1,2},\\ell_{1,3},\\ell_{2,3}\\in\\left\\{\\lfloor k/3\\rfloor-1,\\lceil k/3\\rceil-1\\right\\} such that ℓ1,2+ℓ1,3+ℓ2,3=k−3subscriptℓ12subscriptℓ13subscriptℓ23𝑘3\\ell_{1,2}+\\ell_{1,3}+\\ell_{2,3}=k-3.Fix a partition {4,…,k}=I1,2∪I1,3∪I2,34…𝑘subscript𝐼12subscript𝐼13subscript𝐼23\\{4,\\dots,k\\}=I_{1,2}\\cup I_{1,3}\\cup I_{2,3} whereIi,j|=ℓi,jsubscript𝐼𝑖𝑗subscriptℓ𝑖𝑗|I_{i,j}|=\\ell_{i,j} for each pair 1≤i<j≤31𝑖𝑗31\\leq i<j\\leq 3.We define G′superscript𝐺′G^{\\prime} and c′superscript𝑐′c^{\\prime} as follows. For each edge {x,y}∈E​(G)𝑥𝑦𝐸𝐺\\{x,y\\}\\in E(G), if c​(x)≠c​(y)𝑐𝑥𝑐𝑦c(x)\\neq c(y), say i=c​(x)𝑖𝑐𝑥i=c(x) and j=c​(y)𝑗𝑐𝑦j=c(y), 1≤i≠j≤31𝑖𝑗31\\leq i\\neq j\\leq 3, then replace the edge {x,y}𝑥𝑦\\{x,y\\} with a path of length ℓi,j+1subscriptℓ𝑖𝑗1\\ell_{i,j}+1 (between x𝑥x and y𝑦y), and color the ℓi,jsubscriptℓ𝑖𝑗\\ell_{i,j} internal vertices of this path with the ℓi,jsubscriptℓ𝑖𝑗\\ell_{i,j} colors in Ii,jsubscript𝐼𝑖𝑗I_{i,j}. If c​(x)=c​(y)𝑐𝑥𝑐𝑦c(x)=c(y) then simply remove the edge {x,y}𝑥𝑦\\{x,y\\}. To complete the definition of c′superscript𝑐′c^{\\prime}, we set c′​(v)=c​(v)∈superscript𝑐′𝑣𝑐𝑣delimited-[3c^{\\prime}(v)=c(v)\\in for every v∈V​(G)⊆V​(G′)𝑣𝑉𝐺𝑉superscript𝐺′v\\in V(G)\\subseteq V(G^{\\prime}).The resulting graph is G′superscript𝐺′G^{\\prime} and the resulting coloring is c′superscript𝑐′c^{\\prime}. Note that the vertices in V​(G)𝑉𝐺V(G) are colored with 1,2,31231,2,3, while the vertices in V​(G′)∖V​(G)𝑉superscript𝐺′𝑉𝐺V(G^{\\prime})\\setminus V(G) are colored with 4,…,k4…𝑘4,\\dots,k.We have|V​(G′)|≤|V​(G)|+(⌈k/3⌉−1)⋅|E​(G)|=O​(|V​(G)|+|E​(G)|)𝑉superscript𝐺′𝑉𝐺⋅𝑘31𝐸𝐺𝑂𝑉𝐺𝐸𝐺|V(G^{\\prime})|\\leq|V(G)|+(\\lceil k/3\\rceil-1)\\cdot|E(G)|=O(|V(G)|+|E(G)|) andE​(G′)|≤⌈k/3⌉⋅|E​(G)|=O​(|V​(G)|+|E​(G)|)𝐸superscript𝐺′⋅𝑘3𝐸𝐺𝑂𝑉𝐺𝐸𝐺|E(G^{\\prime})|\\leq\\lceil k/3\\rceil\\cdot|E(G)|=O(|V(G)|+|E(G)|). Moreover, if k≥6𝑘6k\\geq 6 then G′superscript𝐺′G^{\\prime} is 222-degenerate.Indeed, observe that the original vertices of G𝐺G form an independent set in G′superscript𝐺′G^{\\prime}, because each edge of G𝐺G is either removed or replaced by a path of length at least⌊k/3⌋≥2𝑘32\\lfloor k/3\\rfloor\\geq 2.Also, all vertices in V​(G′)∖V​(G)𝑉superscript𝐺′𝑉𝐺V(G^{\\prime})\\setminus V(G) have degree 222 in G′superscript𝐺′G^{\\prime}. Upon removing these vertices, we are left with an empty graph; hence G′superscript𝐺′G^{\\prime} is 222-degenerate, as claimed.\n\n[12] From Graph Properties to Graph Parameters: Tight Bounds for Counting on Small Subgraphs {restatable*}lemmalemincexcLet Φ:𝒢→ℚ:Φ→𝒢ℚ\\Phi\\colon\\mathcal{G}\\to\\mathbb{Q} denote a graph parameter and suppose that there is an algorithm𝔸𝔸\\mathbb{A} that computes for each graph H𝐻H and each H𝐻H-colored graph G𝐺G the value#​cp​-​IndSub​((Φ,H)→G)#cp-IndSub→Φ𝐻𝐺\\text{\\#}{}\\mathrm{cp}\\text{-}\\mathrm{IndSub}((\\Phi,H)\\to G) in time g​(|V​(H)|,|V​(G)|)𝑔𝑉𝐻𝑉𝐺g(|V(H)|,|V(G)|) for some computablefunction g𝑔g that is monotonically increasing.Further, for a graph C∈𝒢s𝐶subscript𝒢𝑠C\\in\\mathcal{G}_{s} and graphs H2,H3,…,Hssubscript𝐻2subscript𝐻3…subscript𝐻𝑠H_{2},H_{3},\\dots,H_{s}, writeΦ~​(G)=Φ​(C​⟨G,H2​…,Hs⟩)~Φ𝐺Φ𝐶𝐺subscript𝐻2…subscript𝐻𝑠\\tilde{\\Phi}(G)=\\Phi(C\\big{\\langle}G,H_{2}\\dots,H_{s}\\big{\\rangle})for the graph parameter that inserts G𝐺G into C​⟨⋅,H2,…,Hs⟩𝐶⋅subscript𝐻2…subscript𝐻𝑠C\\big{\\langle}\\cdot,H_{2},\\dots,H_{s}\\big{\\rangle} and evaluates the result on ΦΦ\\Phi.\n\n[13] Counting Subgraphs in Somewhere Dense Graphs First, there is a unique function a^^𝑎\\hat{a} from subsets of E​[Hr𝐸delimited-[superscript𝐻𝑟E[H^{r} to rationals such that, for every surjectively Hrsuperscript𝐻𝑟H^{r}-coloured graph (G~,c~)~𝐺~𝑐(\\tilde{G},\\tilde{c}), the number of vertex-colourful k𝑘k-independent sets in (G~,c~)~𝐺~𝑐(\\tilde{G},\\tilde{c}) equals∑Aa^​(A)⋅#​𝖧𝗈𝗆​((Hr​[A,𝗂𝖽Hr)→(G~,c~)),subscript𝐴⋅^𝑎𝐴#𝖧𝗈𝗆→superscript𝐻𝑟delimited-[𝐴subscript𝗂𝖽superscript𝐻𝑟~𝐺~𝑐\\sum_{A}\\hat{a}(A)\\cdot\\#\\mathsf{Hom}((H^{r}[A,\\mathsf{id}_{H^{r}})\\to(\\tilde{G},\\tilde{c}))\\,,(10)where the sum is over all subsets of E​[Hr𝐸delimited-[superscript𝐻𝑟E[H^{r}. Additionally,a^​(E​(Hr))=±χ^≠0,^𝑎𝐸superscript𝐻𝑟plus-or-minus^𝜒0\\hat{a}(E(H^{r}))=\\pm\\hat{\\chi}\\neq 0\\,,(11)where χ^^𝜒\\hat{\\chi} is the so-called alternating enumerator for the graph property of being an independent set — we omit the definition since the only property needed for χ^^𝜒\\hat{\\chi} is it being easily computable and non-zero (see ).\n\n"}
{"case_id": "c6740fc5bd8daa6a8628ae279f07806a", "answer_text": "\n\nTheories of distributed cognition have significantly influenced the design of features in social computing systems by providing a framework that emphasizes the importance of understanding cognitive processes as distributed across individuals, tools, and environments [1]. This perspective has led to the development of systems that support collective intelligence, enhance coordination, and facilitate shared understanding among users [4].\n\nOne of the central tenets of distributed cognition is the idea that cognitive processes are not confined to an individual's mind but are distributed across people and artifacts in the environment. This concept has guided the design of social computing systems to support collaborative work and knowledge sharing. For example, the design of the Wikipedia collaborative platform exemplifies how distributed cognition can inform the creation of tools that enable collective knowledge construction. The cognitive organization that occurs within social interactions on Wikipedia serves as a catalyst for intelligence to emerge on a group level, facilitating the collaborative editing and refinement of articles [4]. A process model developed as part of the analysis shows how collaborative knowledge construction in Wikipedia is achieved, providing empirical support for this approach. Specifically, Wikipedia employs mechanisms such as history tracking, discussion pages, and revision control, which collectively support distributed cognitive processes by enabling users to trace changes, engage in dialogue, and collaboratively refine content.\n\nAdditionally, distributed cognition emphasizes the importance of external representations and tools in supporting cognitive processes. This has informed the design of information visualization (InfoVis) systems, where visual tools are used to externalize knowledge, organize work, and create new demands for problem-solving [7]. By providing users with visual aids to interact with and interpret complex data, these systems extend cognitive processes beyond the individual and into the environment, facilitating better decision-making and problem-solving. A real-life scenario described in the research highlights the impact of InfoVis systems designed using the distributed cognition framework, demonstrating their effectiveness in enhancing cognitive processes. For example, InfoVis systems often incorporate interactive dashboards and data manipulation tools that allow users to explore and contextualize large datasets, thereby supporting distributed cognition by making data insights more accessible and actionable. These systems address specific tasks such as trend analysis, data correlation, and pattern recognition, which are critical for informed decision-making in domains like healthcare, finance, and education.\n\nThe integration of distributed cognition with distributed leadership theories has also led to the development of tools that support online creative collaboration. For instance, Pipeline, a collaboration tool designed for Newgrounds, redistributes leadership roles among team members to reduce the burden on individual leaders. This tool was developed based on the understanding that effective collaboration requires both technological support and the distribution of cognitive and leadership tasks across the group [10]. Pipeline incorporates features such as task delegation, progress tracking, and communication facilitation to ensure that leadership responsibilities are shared evenly. A case study of a six-week, 30-artist collaboration called Holiday Flood showed how Pipeline supported redistributed leadership, providing empirical evidence of its effectiveness. In these systems, leadership and cognitive responsibilities are designed to be fluid, adapting to the needs of the project and team dynamics, which ensures more balanced participation and reduces bottlenecks associated with conventional leadership models. Specifically, these tools address the challenges of project coordination, task prioritization, and resource allocation in creative projects, enhancing productivity and project completion rates.\n\nFurthermore, the principles of distributed cognition have been applied to the design of knowledge management systems in organizations. By acknowledging the unique properties and processes of distributed human-computer information systems, researchers have developed prototypes that enhance organizational memory and knowledge management. These systems are designed to capture and support the dynamic interactions between people and their environment, facilitating effective communication and coordination [14]. For example, such systems often include features like shared repositories, collaboration tools, and automated knowledge capture mechanisms. An empirical study combining insights from multiple case studies further validates the benefits of these systems in achieving effective design outcomes. These systems address tasks such as information retrieval, knowledge dissemination, and collaborative problem-solving within organizations, thus improving efficiency and innovation.\n\nIn the context of education, distributed cognition has informed the design of collaborative learning environments. For example, research on collaborative computer science learning has shown how distributed cognition can be used to analyze and improve interaction and learning within groups of students. By leveraging the knowledge of individuals and the affordances of various tools, these learning environments support the collective accomplishment of complex tasks [2]. Qualitative case study analysis of middle school students programming computer music provides concrete evidence of the benefits of applying distributed cognition theories to educational settings. The study highlights methods such as the use of shared programming interfaces, collaborative problem-solving tasks, and iterative project-based learning to optimize the learning experience through distributed cognitive processes. These designs address educational tasks such as concept understanding, skill development, and collaborative project execution, thereby enriching the overall learning process.\n\nAnother noteworthy example is the design of social media platforms. Social media features such as tagging, sharing, commenting, and liking are influenced by distributed cognition theories [4]. These features enable users to collectively process and disseminate information efficiently. For instance, on platforms like Twitter and Facebook, users leverage hashtags to categorize content and facilitate the discovery of information by others [4]. These tagging and categorization systems allow cognitive processes to be distributed among users, thereby enhancing the collective processing and retrieval of information [4]. Social media systems address tasks such as information dissemination, community engagement, and trend analysis, facilitating more effective communication and interaction among users [4].\n\nHowever, the application of distributed cognition theories to social computing systems also faces several challenges and limitations. One significant challenge is ensuring the seamless integration of human and technological components within a distributed cognitive system. This involves addressing issues related to usability, interoperability, and the cognitive load imposed on users by complex interfaces and functionalities [14]. Additionally, while distributed cognition frameworks encourage collaborative work and knowledge sharing, they may inadvertently lead to issues of cognitive overload where users are overwhelmed by the amount of information and interactions required to participate effectively [2]. This necessitates careful design considerations to balance the distribution of cognitive tasks and prevent user fatigue [2]. Moreover, the reliance on external representations and tools can introduce vulnerabilities, such as system failures or security breaches, which can disrupt distributed cognitive processes and affect overall system performance [11].\n\nOverall, the theories of distributed cognition have provided a robust framework for designing social computing systems that facilitate collaboration, enhance cognitive processes, and support the collective intelligence of groups [0]. By focusing on the distribution of cognitive processes across individuals and artifacts, these systems enable users to interact more effectively, share knowledge, and achieve coordinated outcomes while also considering the potential limitations and challenges to ensure optimal design and functionality [1]. The impact of these theories is evident in various domains, from collaborative work environments to education and social media, demonstrating their versatility and relevance in addressing complex cognitive and social tasks [2].\n\n\nReferences:\n[0] Distributed Cognition: A Perspective from Social Choice Theory Distributed Cognition: A Perspective from Social Choice Theory Distributed cognition refers to processes which are (i) cognitive and (ii) distributed across multiple agents or devices rather than performed by a single agent. Distributed cognition has attracted interest in several fields ranging from sociology and law to computer science and the philosophy of science. In this paper, I discuss distributed cognition from a social-choice-theoretic perspective. Drawing on models of judgment aggregation, I address two questions. First, how can we model a group of individuals as a distributed cognitive system? Second, can a group acting as a distributed cognitive system be ‘rational’ and ‘track the truth’ in the outputs it produces? I argue that a group’s performance as a distributed cognitive system depends on its ‘aggregation procedure’ – its mechanism for aggregating the group members’ inputs into collective outputs – and I investigate the properties of an aggregation procedure that matter.\n\n[1] Designing Information Technology to Support Distributed Cognition Designing Information Technology to Support Distributed Cognition Cognition in organizations is a distributed phenomenon, in which individual members of an organization reflect upon their experience, make plans, or take action. Organizational learning or organizational cognition are familiar terms, but it is only the individual persons in an organization who create interpretations and test understandings, as they think and learn in their organizational setting. Coordinated outcomes emerge in organizations when individuals think and act in ways that take others in the organization and their interdependencies into account.We argue that much of the effort to design information technology to support cognition in organizations has not addressed its distributed quality. Such systems have tended to focus either on the individual as an isolated decision maker, or on the group as a producer of a decision or policy statement in common. In distributed cognition, by contrast, the group is a set of autonomous agents who act independently yet recognize that they have interdependencies. To guide the design of information technology, we propose that distributed cognition be viewed as a hermeneutic process of inquiry, emphasizing the importance of individual interpretation and group dialogue.Hermeneutics provides a theory of the interpretive process through which an individual gives meaning to organizational experience. Inquiry systems provide a theory of how a community of inquirers build and test knowledge representations through dialogue. Together, hermeneutics and inquiry systems are used to propose a set of design principles to guide the development of information technology that supports distributed cognition. The design principles we describe in the paper are\n\n[2] Using Distributed Cognition Theory to Analyze Collaborative Computer Science Learning Using Distributed Cognition Theory to Analyze Collaborative Computer Science Learning Research on students' learning in computing typically investigates how to enable individuals to develop concepts and skills, yet many forms of computing education, from peer instruction to robotics competitions, involve group work in which understanding may not be entirely locatable within individuals' minds. We need theories and methods that allow us to understand learning in cognitive systems: culturally and historically situated groups of students, teachers, and tools. Accordingly, we draw on Hutchins' Distributed Cognition theory to present a qualitative case study analysis of interaction and learning within a small group of middle school students programming computer music. Our analysis shows how a system of students, teachers, and tools, working in a music classroom, is able to accomplish conceptually demanding computer music programming. We show how the system does this by 1) collectively drawing on individuals' knowledge, 2) using the physical and virtual affordances of different tools to organize work, externalize knowledge, and create new demands for problem solving, and 3) reconfiguring relationships between individuals and tools over time as the focus of problem solving changes. We discuss the implications of this perspective for research on teaching, learning and assessment in computing.\n\n[3] Group Intelligence: A Distributed Cognition Perspective The question of whether intelligence can be attributed to groups or not has been raised in many scientific disciplines. In the field of computer-supported collaborative learning, this question has been examined to understand how computer-mediated environments can augment human cognition and learning on a group level. The era of social computing which represents the emergence of Web 2.0 collaborative technologies and social media has stimulated a wide discussion about collective intelligence and the global brain. This paper reviews the theory of distributed cognition in the light of these concepts in an attempt to analyze and understand the emergence process of intelligence that takes place in the context of computer-mediated collaborative and social media environments. It concludes by showing that the cognitive organization, which occurs within social interactions serves as a catalyst for intelligence to emerge on a group level. Also a process model has been developed to show the process of collaborative knowledge construction in Wikipedia that characterizes such cognitive organization.\n\n[4] Distributed Cognition as a Theoretical Framework for Information Visualization Distributed Cognition as a Theoretical Framework for Information Visualization Even though information visualization (InfoVis) research has matured in recent years, it is generally acknowledged that the field still lacks supporting, encompassing theories. In this paper, we argue that the distributed cognition framework can be used to substantiate the theoretical foundation of InfoVis. We highlight fundamental assumptions and theoretical constructs of the distributed cognition approach, based on the cognitive science literature and a real life scenario. We then discuss how the distributed cognition framework can have an impact on the research directions and methodologies we take as InfoVis researchers. Our contributions are as follows. First, we highlight the view that cognition is more an emergent property of interaction than a property of the human mind. Second, we argue that a reductionist approach to study the abstract properties of isolated human minds may not be useful in informing InfoVis design. Finally we propose to make cognition an explicit research agenda, and discuss the implications on how we perform evaluation and theory building.\n\n[5] Redistributing leadership in online creative collaboration In this paper, we integrate theories of distributed leadership and distributed cognition to account for the roles of people and technology in online leadership. When leadership is distributed effectively, the result can be success stories like Wikipedia and Linux. However, finding a successful distribution is challenging. In the online community Newgrounds, hundreds of collaborative animation projects called \"collabs\" are started each year, but less than 20% are completed. We suggest that many collabs fail because leaders are overburdened and lack adequate technological support. We introduce Pipeline, a collaboration tool designed to support and transform leadership, with the goal of easing the burden on leaders of online creative projects. Through a case study of a six-week, 30-artist collaboration called Holiday Flood, we show how Pipeline supported redistributed leadership. We conclude with implications for theory and the design of social computing systems.\n\n[6] Distributed Cognition: Understanding Complex Sociotechnical Informatics. Distributed cognition theory posits that our cognitive tasks are so tightly coupled to the environment that cognition extends into the environment, beyond the skin and the skull. It uses cognitive concepts to describe information processing across external representations, social networks and across different periods of time. Distributed cognition lends itself to exploring how people interact with technology in the workplace, issues to do with communication and coordination, how people's thinking extends into the environment and sociotechnical system architecture and performance more broadly. We provide an overview of early work that established distributed cognition theory, describe more recent work that facilitates its application, and outline how this theory has been used in health informatics. We present two use cases to show how distributed cognition can be used at the formative and summative stages of a project life cycle. In both cases, key determinants that influence performance of the sociotechnical system and/or the technology are identified. We argue that distributed cognition theory can have descriptive, rhetorical, inferential and application power. For evidence-based health informatics it can lead to design changes and hypotheses that can be tested.\n\n[7] Designing a Knowledge Management System for Distributed Activities: A Human Centered Approach Designing a Knowledge Management System for Distributed Activities: A Human Centered Approach In this study we use the principles of distributed cognition and the methodology of human-centered distributed information design to analyze a complex distributed human-computer system, identify its problems, and generate design requirements and implementation specifications of a replacement prototype for effective organizational memory and knowledge management. We argue that a distributed human-computer information system has unique properties, structures and processes that are best described in the language of distributed cognition. Distributed cognition provides researchers a richer theoretical understanding of human-computer interactions and enables re-searchers to capture the phenomenon that emerges in social interactions as well as the interactions between people and structures in their environment.\n\n"}
{"case_id": "34b00939c1190990c1c9be590cb07476", "answer_text": "\n\nThe discussion of AI as a design material in the literature reflects a multi-faceted view that touches on the potential, challenges, and evolving methodologies in incorporating AI into the design process [1][2]. This perspective spans various domains, including human-computer interaction (HCI), user experience (UX) design, and creative practices, with different researchers tackling the conceptual and practical aspects of integrating AI into design workflows [0][7]. \n\nConceptual Framing and Metaphors [1][2][14].\n\nOne prominent theme is the conceptual framing of AI technologies as design materials that designers can explore and experiment with rather than merely tools for achieving pre-defined objectives [3][7]. This metaphorical approach helps demystify AI, making its capabilities and limitations more accessible to designers. For example, metaphors such as \"ML-as-monster\" have been used to facilitate early-stage design exploration and identify areas of concern that warrant further inquiry [14]. Such metaphor-driven Research-through-Design (RtD) projects highlight the interplay between AI and human creativity, fostering a deeper understanding of AI's materiality and design implications [3].\n\nPractical Applications in Design Workflows [7][8][9].\n\nConcrete examples from the literature provide deeper insights. For instance, the use of text-to-image AI tools like 3DALL-E in 3D design workflows [9] showed that designers could construct text and image prompts that inspire and guide their modeling tasks. Participants in a study using 3DALL-E expressed that the tool helped prevent design fixation and fostered innovative design considerations by providing fresh perspectives. Another example is from the study by Wintersberger et al., which explored the use of explainable AI (XAI) in augmented reality (AR) by showing traffic-related information while driving, thereby improving user trust [13].\n\n\nChallenges in Using AI as a Design Material[^2].\n[^2]: Yildirim et al., 2022; Subramonyam et al., 2022; Yang, 2018.\n\n\nAnother critical discussion revolves around the challenges associated with using AI as a design material. Designers often find themselves detached from the AI fabrication process, which typically involves data scientists and engineers who may not be well-versed in user-centered design principles [2]. This gap can lead to issues such as the black-box nature of AI models, which surprise designers with their stochastic behaviors and errors, making it difficult to calibrate AI accurately when generating design solutions. Consequently, designers may struggle to account for unintended ethical issues that arise from the AI's operational dynamics [2].\n\n### Strategies and Tools for Mitigating Challenges\n\nReferences: [11]\n\nTo tackle these challenges, the literature suggests several strategies and tools. Human-AI guidelines have been developed to provide cognitive scaffolding and educational materials for designers working with AI [11]. These guidelines are essential in offering structured approaches to design, ensuring that designers can effectively engage with AI capabilities and constraints. Additionally, tools that integrate UI prototyping with AI model exploration have been proposed, enabling designers to bridge the gap between their design practices and the technical underpinnings of AI. For instance, Subramonyam et al. introduced data probes as design tools enabled by user data to identify desirable AI characteristics and user experiences [11]. These tools allow for hands-on experimentation and iterative design practices, fostering a more symbiotic relationship between AI and design.\n\nMoreover, integrated platforms like Google’s Teachable Machine and Microsoft's Lobe, which facilitate the creation of custom machine learning models through intuitive interfaces, exemplify tools that democratize AI for designers. These platforms lower the technical barrier, enabling designers to train and deploy AI models without extensive technical expertise [11]. Such tools not only empower designers but also promote a more collaborative environment where AI and design workflows can converge seamlessly.\n\n### Ethical Considerations [5]\n\nMoreover, the ethical concerns associated with AI in design are receiving increasing attention. Researchers have emphasized the need for ethical guidelines and frameworks to ensure that AI-enhanced designs do not inadvertently cause harm or reinforce biases [6]. For instance, integrating ethical considerations into the design process, such as evaluating potential biases in AI data and models, is seen as crucial for responsible AI usage. There are also calls for transparency and explainability in AI systems to allow designers and end-users to understand AI decisions better and ensure that these systems operate fairly and justly [12]. Ethical frameworks such as \"Fairness, Accountability, and Transparency in Machine Learning\" (FAT/ML) provide comprehensive strategies for incorporating ethical considerations into AI design processes.\n\nAI in Creative Processes[7].\n\nThe application of AI in creative processes is extensively discussed within the literature. The ability of AI to expand creative possibilities is a significant focus. Creative professionals often use AI to generate novel examples and meet their inspiration needs, thereby expanding the range of possible design solutions [7]. This is particularly beneficial for overcoming design fixation—a phenomenon where designers' adherence to pre-existing ideas constrains design outcomes [8]. By providing fresh perspectives and breaking away from existing conventions, AI can considerably enhance creative freedom.\n\nFor example, in a four-week-long design exploration using generative text-to-image models, AI tools were employed to ideate and visualize robotic artifacts [8]. This process helped designers uncover existing assumptions and stereotypes about robots, leading to more novel and contextually situated designs. The iterative and exploratory nature of AI allows for rapid prototyping and variation, which can be particularly advantageous in the early stages of design when numerous possibilities are still being considered.\n\nMoreover, AI's role in enhancing various stages of the UX design process is well-documented. AI can assist in understanding the context of use, uncovering user requirements, designing solutions, and evaluating outcomes [0]. This comprehensive support can significantly improve the efficiency and accuracy of the design process while fostering greater innovation and creativity. Intelligent functionalities, such as AI systems that infer user intent and provide contextual recommendations, can further enrich the design process [12].\n\n\n### Current Trends and Future Directions\n\nAI is rapidly being integrated into UX design to enhance processes and address new challenges. AI can be leveraged in the UX design process, assisting in understanding the context of use, uncovering user requirements, aiding solution design, evaluating design, and assisting in the development of solutions [0]. This integration transforms UX design by introducing novel digital tools that can create more innovative and creative solutions [0]. Additionally, AI technologies are increasingly becoming tangible design materials within the HCI community, contributing to developing conceptual vocabularies and metaphors that aid in design processes [1]. Current research also frames AI as a design material unique to UX and HCI practitioners, highlighting the detachment challenges between designers and AI material fabrication, which can limit the effective application of AI in design [2].\n\n\nRecent trends highlight an increased emphasis on collaborative platforms and tools that integrate AI directly into design workflows, allowing for more seamless and intuitive interactions. Researchers are keen on exploring how AI can be made more transparent and interpretable to designers, thus mitigating the black-box effect and enabling more informed design decisions [1]. Future directions point towards the development of hybrid systems where AI and human designers co-create, leveraging the strengths of both to achieve superior design outcomes. There is also a growing interest in AI ethics, with researchers advocating for the incorporation of ethical guidelines at every stage of the design process to ensure responsible AI usage [7].\n\nComparative Advantage of AI Tools[7]\n\nA key aspect not sufficiently addressed in the original response is the comparison of different AI tools in terms of their effectiveness and usability for designers. For example, while tools like 3DALL-E are praised for their ability to prevent design fixation and foster creativity by generating diverse visual prompts [9], other tools may excel in providing real-time contextual recommendations [12] or aiding in ethical design considerations [2]. Understanding these comparative advantages allows designers to select appropriate tools that align with their specific design needs and workflows. \n\nAI has the potential to drastically change the user experience (UX) design process by introducing novel digital tools that improve efficiency and accuracy, while creating more innovative and creative solutions [0]. For instance, AI can be leveraged in various key areas of UX design, including understanding the context of use, uncovering user requirements, aiding solution design, and evaluating design [0]. Researchers have also framed AI as a design material to help expose unique difficulties for designers working with AI [2]. Examples include using AI to generate quick outputs as a source of inspiration in the creative process [7], and incorporating AI in 3D design workflows to prevent design fixation and inspire new ideas [9]. The role of AI is critical for AR devices to provide intelligent services by understanding users’ current states and their environment [12]. For inclusive AI curriculum materials, developing a student's AI identity is important, which includes understanding AI, exploring its ethical and societal impacts, and building community and leadership qualities [5]. \n\nIn summary, the literature discusses AI as a design material from both conceptual and practical perspectives [1]. By incorporating concrete examples and case studies, it highlights the potential of AI to enhance creativity and innovation in design while also addressing the challenges and proposing solutions to bridge the gap between designers and the technical intricacies of AI [2]. This holistic view underscores the transformative impact of AI on design practices and the ongoing efforts to integrate AI seamlessly and effectively into the design process [7]. \n\n\nReferences:\n[0] Artificial intelligence (AI) for user experience (UX) design: a systematic literature review and future research agenda PurposeThe aim of this article is to map the use of AI in the user experience (UX) design process. Disrupting the UX process by introducing novel digital tools such as artificial intelligence (AI) has the potential to improve efficiency and accuracy, while creating more innovative and creative solutions. Thus, understanding how AI can be leveraged for UX has important research and practical implications.Design/methodology/approachThis article builds on a systematic literature review approach and aims to understand how AI is used in UX design today, as well as uncover some prominent themes for future research. Through a process of selection and filtering, 46 research articles are analysed, with findings synthesized based on a user-centred design and development process.FindingsThe authors’ analysis shows how AI is leveraged in the UX design process at different key areas. Namely, these include understanding the context of use, uncovering user requirements, aiding solution design, and evaluating design, and for assisting development of solutions. The authors also highlight the ways in which AI is changing the UX design process through illustrative examples.Originality/valueWhile there is increased interest in the use of AI in organizations, there is still limited work on how AI can be introduced into processes that depend heavily on human creativity and input. Thus, the authors show the ways in which AI can enhance such activities and assume tasks that have been typically performed by humans.\n\n[1] The Entoptic Field Camera as Metaphor-Driven Research-through-Design with AI Technologies The address of AI technologies in terms of design in the HCI community has centered primarily on how AI can be made tangible as a design material for designers (see e.g. (Yang et al., 2020) on a series of studies). At the same time, there is a growing body of research that centers on the materiality—i.e. the arrangement of components and the materialization of their functional interoperation in specific outputs—of AI technologies to develop conceptual vocabularies, and particularly, metaphors. The use of metaphors by designers is a staple that goes back to canonical texts (e.g. (Schön, 1979)). As has been pointed out by numerous authors in HCI and beyond (Agre, 1997a, b; Blackwell, 2006; Murray-Rust et al., 2022), the term AI in itself is already metaphorical at root in that it assigns, symbolically, capacities to technical devices that are usually reserved for living beings; and especially, human beings.\n\n[2] How Do UX Practitioners Communicate AI as a Design Material? Artifacts, Conceptions, and Propositions Doordan’s examples from his framework were centered on tangible products, but prior work in the design and HCI community has argued for the importance of applying the framework to understand the design and use of rapidly advancing digital technology (Hondros, 2015; Robles and Wiberg, 2010). Specifically, researchers have framed AI as a design material to expose unique difficulties for designers working with AI (Dove et al., 2017; Yang, 2018; Luciani et al., 2018; Benjamin et al., 2021; Holmquist, 2017; Yildirim et al., 2022). Through the lens of Doordan’s material framework, we observe that the detachment of designers from the fabrication of the material can trigger a cascade of subsequent challenges. The material—in the form of AI models, infrastructure, and data—are often prepared by AI teams (data scientists, AI engineers, etc.) who are far removed from user-centered design principles that ground designers’ work (Yildirim et al., 2022; Subramonyam et al., 2022; Yang, 2018). As a result, designers work with AI without sufficient understanding of its capabilities and limitations—e.g., a blackbox (Zhou et al., 2020). Designers are then easily surprised by AI’s stochastic behaviours and errors, limiting their abilities to properly calibrate the material when generating design solutions (Beaudouin-Lafon and Mackay, 2009; Yang et al., 2020; Benjamin et al., 2021) and account for unintended ethical issues (Holmquist, 2017; Lee et al., 2020). Lastly, it is challenging for designers to collect appreciation—that is, holistically evaluate AI-enabled designs with users. The material itself may constantly evolve new data and user inputs, and new AI capabilities may be unlocked sporadically (Yang, 2018; Yang et al., 2020).\n\n[3] The Entoptic Field Camera as Metaphor-Driven Research-through-Design with AI Technologies In this paper, we engage this question by way of a metaphor-driven Research-through-Design (RtD) project. In doing so, we treat AI technologies as a design material for exploration and experimentation, rather than as means or ends in themselves—an approach which the HCI community has long pursued regarding computing and emerging information technologies (see e.g.  (Lim et al., 2013; Sundström et al., 2011)). Indeed, RtD (and design research) with AI technologies is a highly active research field within the HCI community (Benjamin et al., 2021; Biggs and Desjardins, 2020; Wakkary et al., 2017; Lindley et al., 2020a) and regarding the use of metaphors especially (Dove and Fayard, 2020; Murray-Rust et al., 2022). In our approach, we engage AI image technologies by way of a principal design metaphor: entoptic phenomena. Entoptic (Greek: ‘within vision’) phenomena are perceptions that do not directly correspond to material reality, but rather stem from the physiological interplay of eye and brain (cf. (noa, 2010; Helmholtz, 1867; Lewis-Williams and Dowson, 1988)). A common example for an entoptic phenomenon are ‘floaters.’ These are small dark shapes, spots, or web-like structures that appear in perception. They are caused by material inside the eye that physically interferes with the light as it passes through the lens and vitreous fluid on its route to hitting the retina, subsequently being rendered as particular perceptual shapes by the brain.\n\n[4] Design Considerations for Inclusive AI Curriculum Materials Design Considerations for Inclusive AI Curriculum Materials This lightning talk describes the curriculum design approach developed and used to create Artificial Intelligence (AI) curriculum units for a wide range of high school and college students. We describe our design considerations, beginning with developing a student's AI Identity. We have identified four key pillars to focus on when building curricular materials to help develop students? AI identity: AI understanding, using an ethics and societal impact lens, community building, and leadership development. To carry out these four pillars, we use six pedagogical building blocks to design quality curriculum materials. These building blocks include: utilizing active learning and universal design for learning principles, developing students' data literacy and AI impact analysis skills, and exposing students to diverse job descriptions, applications, and role models within the field of AI. These building blocks ensure that, though our curricular materials are developed for different audiences with differing levels of experience with AI or coding, we are able to standardize a high level of pedagogical quality across all materials. These pillars and building blocks have been developed through literature review and discussion with AI/CS researchers and professionals working in AI. In this talk, we will show how these curricula were implemented in three different settings: (1) a pilot collegiate-level workshop, (2) an intensive multi-week high school summer program, and (3) a free online platform openly available to high school teachers and students.\n\n[5] XAIR: A Framework of Explainable AI in Augmented Reality Paradigm.If explanations are presented using audio, the design space is relatively limited (e.g., volume, speed). We refer readers to existing literature on audio design (e.g., (Frauenberger et al., 2007; Kern and Schmidt, 2009)).The design space of the visual paradigm for explanations, however, is much larger.First, from a formatting perspective, explanation content can be presented in a textual format (e.g., narrative, dialogue) (Lakkaraju et al., 2016; Myers et al., 2006), graphical format (e.g., icons, images, heatmaps) (Zeiler and Fergus, 2014; Simonyan et al., 2013), or a combination of both.Second, from a pattern perspective, an explanation can be displayed either in an implicit way (i.e., embedded in the environment, such as a boundary highlight of an object) or explicit way (i.e., distinguished from the environment, such as a pop-up dialogue window) (Lindlbauer et al., 2019; Tatzgern et al., 2016; DiVerdi et al., 2004).The pattern is closely related to the adaptiveness of the AR interface (Dai et al., 2017; Wang et al., 2019). With 3D sensing capabilities, the location of an explanation can be body-based (mostly explicit), object-based (implicit or explicit), or world-based (implicit or explicit)  (Lu and Xu, 2022; Bonanni et al., 2005; LaViola Jr et al., 2017; Xu et al., 2018). Prior AR research has explored adaptive interface locations (Luo et al., 2022; Müller and Dauenhauer, 2016), e.g., interfaces should be adaptive based on the semantic understanding of the ongoing interaction (Cheng et al., 2021; Qian et al., 2022; Rzayev et al., 2020) and ergonomic metrics (Evangelista Belo et al., 2021).\n\n[6] Designing Participatory AI: Creative Professionals’ Worries and Expectations about Generative AI In fields that require creativity, it is perhaps not surprising that respondents highlighted using quickly generated output as a source of inspiration in their creative process. Creative professionals often rely on readily available examples of design for inspiration (Herring et al., 2009), and the availability of AI to generate innumerable novel examples was seen as a powerful opportunity for ‘opening up new solution spaces,’ e.g.: “It will allow me to iterate through a much bigger possibility space” (P12); and “it will make some work a lot easier/more efficient as you can try out different ideas in a very short amount of time” (P6). In this role, AI is imagined to augment what we call the divergent parts of the creative process by offering examples and opening up novel and larger solution spaces (Dorst and Cross, 2001).\n\n[7] Creative AI for HRI Design Explorations Design fixation, a phenomenon describing designers' adherence to pre-existing ideas or concepts that constrain design outcomes, is particularly prevalent in human-robot interaction (HRI), for example, due to collectively held and stabilised imaginations of what a robot should look like or behave. In this paper, we explore the contribution of creative AI tools to overcome design fixation and enhance creative processes in HRI design. In a four weeks long design exploration, we used generative text-to-image models to ideate and visualise robotic artefacts and robot sociotechnical imaginaries. We exchanged results along with reflections through a digital postcard format. We demonstrate the usefulness of our approach to imagining novel robot concepts, surfacing existing assumptions and robot stereotypes, and situating robotic artefacts in context. We discuss the contribution to designerly HRI practices and conclude with lessons learnt for using creative AI tools as an emerging design practice in HRI research and beyond.\n\n[8] 3DALL-E: Integrating Text-to-Image AI in 3D Design Workflows Text-to-image AI are capable of generating novel images for inspiration, but their applications for 3D design workflows and how designers can build 3D models using AI-provided inspiration have not yet been explored. To investigate this, we integrated DALL-E, GPT-3, and CLIP within a CAD software in 3DALL-E, a plugin that generates 2D image inspiration for 3D design. 3DALL-E allows users to construct text and image prompts based on what they are modeling. In a study with 13 designers, we found that designers saw great potential in 3DALL-E within their workflows and could use text-to-image AI to produce reference images, prevent design fixation, and inspire design considerations. We elaborate on prompting patterns observed across 3D modeling tasks and provide measures of prompt complexity observed across participants. From our findings, we discuss how 3DALL-E can merge with existing generative design workflows and propose prompt bibliographies as a form of human-AI design history.\n\n[9] How Do UX Practitioners Communicate AI as a Design Material? Artifacts, Conceptions, and Propositions To tackle these challenges, researchers and practitioners have established human-AI guidelines to offer both cognitive scaffolding and educational materials when designers work with AI (Google, 2022; Microsoft, 2022; Apple, 2022). Researchers have also proposed tools that combine UI prototyping with AI model exploration (Subramonyam et al., 2021a), process models (Zhou et al., 2020; Subramonyam et al., 2021b) and boundary representations (Yildirim et al., 2022; Subramonyam et al., 2022) for human-centered AI teams, and metaphors for generative probing of models (Dove and Fayard, 2020). In particular, Subramonyam et al. (Subramonyam et al., 2022) found that UX and AI practitioners use low-level representations which they call leaky abstractions (e.g., low-fidelity prototypes and raw data) to communicate across domain boundaries. They also introduce data probes (Subramonyam et al., 2021b) as design probes enabled by user data to identify desirable AI characteristics and user experiences.\n\n[10] XAIR: A Framework of Explainable AI in Augmented Reality The role of AI will be critical for AR devices if they are to provide intelligent services.The integration of sensors enables AR systems to understand users’ current states (Huang et al., 2018; Stappen et al., 2020; Schröder and Ritter, 2017) and their environment (Liu et al., 2020; Miech et al., 2020) to provide a variety of intelligent functionalities.For example, AR could infer user intent (Admoni and Srinivasa, 2016) and provide contextual recommendations for daily activities (e.g., recipe recommendations when a user opens the fridge during lunch) (Adomavicius and Tuzhilin, 2011; Lam et al., 2021; Lee et al., 2021).The rich interaction between the outcomes of AI and end-users requires effectively designed XAI that can support users in a variety of contexts, such as when users are confused or surprised while encountering an unexpected AI outcome, or when they want to make sure that an AI outcome is reliable and trustworthy (Mohseni et al., 2021; Amershi et al., 2019).Recent work has started to explore the application of XAI in AR (Ahmed et al., 2022).For instance, Wintersberger et al. found that showing traffic-related information in AR while driving can provide much needed explanation to users and improve user trust (Wintersberger et al., 2018).Danry et al. explored the use of an explainable AI assistant integrated within wearable glasses to enhance human rationality (Danry et al., 2020).Zimmermann et al. found that introducing XAI during an AR-based shopping process could improve user experiences (Zimmermann et al., 2022).However, these studies proposed their own case-by-case XAI designs. In this research, we aggregated the major factors identified in the literature and studied the when/what/how questions systematically.\n\n[11] XAIR: A Framework of Explainable AI in Augmented Reality In this paper, we propose XAIR, a framework to guide XAI design in AR.Based on a literature review of multiple domains, we identified the problem space using three main questions, i.e., when to explain, what to explain, and how to explain.We combined the results from a large-scale survey with over 500 end-users (Study 1) and iterative workshops with 12 experts (Study 2) to develop XAIR and a set of eight design guidelines.Using our framework, we walked through example XAI designs in two everyday AR scenarios.To evaluate XAIR’s utility, we conducted a study with 10 designers (Study 3).The study revealed that designers found XAIR to be a helpful, comprehensive reference that could inspire new design thoughts and provide a backup of designer intuitions.Moreover, to demonstrate the effectiveness of XAIR, we instantiated two design examples in a real-time AR system and conducted another user study with 12 end-users (Study 4). The results indicated excellent usability of the AR system.XAIR can thus help future designers and researchers achieve effective XAI designs in AR and help them explore new design opportunities.\n\n[12] The Entoptic Field Camera as Metaphor-Driven Research-through-Design with AI Technologies While the applicability of particular metaphors (e.g., learning, sentience) has been debated since the early days of AI research (McDermott, 1976), the stubborn opacity of contemporary AI technologies at runtime (Burrell, 2016; Dourish, 2016) also means that it requires in-depth practical and conceptual engagement to gain a foothold on them. Therefore, it is no surprise that design research work with AI technologies has centered on metaphors that make AI technologies approachable to design practice by suggesting symbolic translations. To date, analogical metaphors in the form of ‘X-as-Y’ are the most common. For instance, Murray-Rust and colleagues propose“alternative metaphors for designers working with AI metaphors” (Murray-Rust et al., 2022) such as corporations, geofoam, or fossils. Each of these carries with them an implicit symbolic register of terms, shapes and movements; thereby holding potential design approaches that operate below surface-level concerns such as opacity or technical accuracy. RtD’s capacity for practical experiments is a natural fit for probing emerging technologies for new metaphors (see e.g. (Ricketts and Lockton, 2019)). As an example, Fayard and Dove’s work on the metaphor of ML-as-monster (Dove and Fayard, 2020) has shown the generative potential of this approach: through metaphor-driven making of ‘ML-monsters,’ the participants in their co-design workshop discerned “territories of concern at an early stage of design and [pointed to where exploratory inquiry may be most needed.” In this regard, RtD is already working on opening up AI technologies for extensive and inquisitive designerly approaches.\n\n"}
{"case_id": "95a152ddae491933ef75c797f3584b08", "answer_text": "\n\nParticipatory design (PD) and co-design share many similarities, primarily revolving around the inclusion of stakeholders in the design process [0]. However, key differences exist in their conceptualizations, roles of participants, and specific outcomes which set them apart [1].\n\n\n### Conceptual Differences:\n\nThe concept of participatory design or Co-Design is not new in architecture, particularly in collaboration with clients. Sanders and Stappers (2008) explained the difference between Co-Design and participatory design as follows: Co-Design is a specific instance of co-creation [0]. In particular, it is the process through which collective creativity is applied over the entire process of design. In general, co-creation (and, thus, Co-Design) is strategic in terms of environmental, social and economic sustainability of the design of the system. On the other hand, participation is the involvement of stakeholders in a discussion. As both of these approaches involve the client, there is a very permeable boundary between the two [1].\n\n\nParticipatory design emerged from the political and social activism of the 1970s in Scandinavia. It emphasizes involving non-expert stakeholders (e.g., end-users and community members) to democratize the design process and challenge existing power structures [4]. PD is committed to creating equitable design environments where all voices are heard and valued, focusing on the social contexts in which designs will be deployed. The goal is to balance power dynamics between stakeholders and designers by providing a hybrid space for meaningful contributions [4].\n\nConversely, co-design is a specific instance of co-creation that extends beyond mere participation to involve collective creativity across the entire design process [0][1]. Co-design emerged from design fields and emphasizes active collaboration between designers and non-designers. This collaboration typically involves various phases from problem identification to the development and final production of design solutions [0][4]. While co-design values stakeholder input, it may not always focus explicitly on addressing power imbalances, unlike PD.\n\nRoles and Involvement: Co-design is a specific instance of co-creation, defined as \"collective creativity as it is applied across the whole span of a design process\" [0]. This approach involves actively engaging stakeholders, not merely as providers of contextual information but also as co-designers who collaboratively develop solutions [2]. The notion of co-design extends beyond traditional user-centered design by emphasizing the active involvement of end-users throughout the entire process, from ideation to final production [8]. This paradigm shift towards participatory design seeks to empower stakeholders, enabling them to contribute meaningfully to the design process and resulting in more contextually relevant and accepted outcomes [10].\n\nIn participatory design, users and other stakeholders act as co-researchers and play a critical role in ideation and decision-making. Their involvement ensures that the designs meet their needs and address their concerns, leading to more appropriate and user-centered solutions [2][4]. By incorporating diverse perspectives, PD aims to achieve mutual learning, where all participants gain insights and knowledge from each other [6]. This mutual learning aspect is less emphasized in co-design.\n\nCo-design views stakeholders as integral collaborators rather than just participants. Stakeholders are actively involved throughout all stages of the design process, contributing not just their needs but also actively shaping the design outcomes. The process is characterized by simultaneous mutual collaboration, where ideas are built, negotiated, and accomplished through a collective agreement [5]. This approach often leads to the generation of low-threshold design artefacts like sketches or low-fidelity prototypes, making the process more tangible and iterative [2].\n\n### Process Orientation:\n\nParticipatory design and co-design both share the advantages of integrating individuals' skills and experiences through collaborative exchanges (Sanders and Stappers, 2008) [0]. Co-design is defined by any form of co-creation, between designers and non-designers, following a design process [1]. Co-design can actively involve end-users throughout the design process from problem definition, through ideas generation, development, testing, and final production [8]. The co-design process recognizes that the knowledge that stakeholders bring is both explicit and tacit, necessitating their active involvement for mutual contextual appreciation [11]. This approach is highlighted for its potential in generating culturally appropriate solutions in contexts such as health literacy enhancement [13]. \n\nThe co-design process is highly structured and procedural. It often involves well-defined workshops and methodologies such as brainstorming, storytelling, and developing personas and scenarios [9]. This structure ensures that the collaborative efforts are focused and productive, resulting in designs that stakeholders value and support [3]. The explicit steps in co-design methodologies make the process more accessible and replicable for practitioners [3].\n\nParticipatory design, while also methodical, places a stronger emphasis on the qualitative aspects of collaboration, such as fostering trust and building networks among participants [0][1]. This is achieved through ongoing dialogues, shared experiences, and collective problem-solving activities that emphasize relational dynamics between stakeholders. The participatory approach extends beyond the creation of physical artefacts to include developing social frameworks and policies that impact stakeholders [4].\n\n### Outcome Orientation:\n\nOutcome orientation is critical in co-design processes as it ensures that the design efforts are aligned with the stakeholders' goals and desired outcomes. This approach prioritizes achieving tangible and meaningful results that address users' needs and aspirations, thereby enhancing the overall effectiveness and relevance of the design solution [0][1]. In particular, co-design promotes strong networks, shared understanding, and trust among stakeholders, which are essential elements for successful outcomes [0]. Moreover, the participatory nature of co-design fosters collective creativity and mutual learning, contributing to the achievement of sustainable and impactful design solutions [1][6]. Co-design also emphasizes the importance of involving diverse stakeholders throughout the design process, ensuring their active participation and contribution to the final outcome [10]. This inclusive approach not only leads to better design solutions but also empowers participants by valuing their input and perspectives [8]. \n\nCo-design often focuses on the end product, striving to ensure that the resulting design artefacts are user-centered and effective [8]. In contrast, participatory design equally values the process and the social impact of design activities. PD aims to create sustainable social changes and empower participants by involving them meaningfully throughout the design and implementation phases [13][4].\n\nSummary Table of Differences:\nCo-Design is a specific instance of co-creation [0]. In particular, it is the process through which collective creativity is applied over the entire process of design [0]. Participatory design involves the involvement of stakeholders in a discussion and has a permeable boundary with Co-Design [0].\n\n| Aspect               | Participatory Design (PD)                                  | Co-design [0] [1]\n\nThe concept of participatory design or Co-Design is not new in architecture, particularly in collaboration with clients [0]. Sanders and Stappers (2008) explained the difference between Co-Design and participatory design as follows: Co-Design is a specific instance of co-creation [0]. In particular, it is the process through which collective creativity is applied over the entire process of design [0]. In general, co-creation (and, thus, Co-Design) is strategic in terms of environmental, social and economic sustainability of the design of the system [0]. On the other hand, participation is the involvement of stakeholders in a discussion [0]. \n\nInvolving all the stakeholders throughout the development process, sharing information and practices, and building trust between people, are some of the key elements identified for a successful Co-Design process [1]. \n\nThe outcome of the participatory design workshops could take the form of some low-threshold design artefacts (like sketches, drawings, or collages), as well as mockups [2]. \n| **Conceptual Roots** | Political and social activism [1] | Design fields [0] \n\n\nStatement: | **Focus** | Democratize design, balance power dynamics | Collective creativity across all design stages \\[4\\]\n\n\n \n| **Roles**            | Stakeholders as co-researchers [14]                            | Stakeholders as integral collaborators [0] |\n\nGovernance in co-design offers mutual learning and applies the concept of social contexts to structured processes and tangible outcomes. Mutual learning is a key aspect of participatory design. By introducing multiple stakeholders, co-design processes not only bring together different knowledges but also create opportunities for collective articulation and mutual understanding [6]. \n | **Outcome Focus**    | Social impact and sustainability [0]                         | User-centered design artefacts [2]                     | \n\nIn summary, while both participatory design and co-design emphasize stakeholder involvement and collaborative creativity, they differ in their origins, conceptual frameworks, roles of participants, process structures, and focus on outcomes. Participatory design is rooted in social activism and power dynamics, aiming to democratize the design process [4]. Co-design, originating from design fields, emphasizes procedural collaboration across all design stages, leading to tangible and user-centered design outcomes [0]. \n\n\nReferences:\n[0] Hall of the Future: a Systemic Research Project for Public Interiors and Spaces using Co-Design Tools. the city, to transform it into a living room for citizens. In order to achieve this result, the Politecnico di Milano suggested to the Chamber of Commerce to apply a Co-Design approach to the project. The concept of participatory design or Co-Design is not new in architecture, particularly in collaboration with clients. Sanders and Stappers (2008) explained the difference between Co-Design and participatory design as follows: Co-Design is a specific instance of co-creation. In particular, it is the process through which collective creativity is applied over the entire process of design. In general, co-creation (and, thus, Co-Design) is strategic in terms of environmental, social and economic sustainability of the design of the system. On the other hand, participation is the involvement of stakeholders in a discussion. As both of these approaches involve the client, there is a very permeable boundary between the two. For example, Szebeco and Tan (2010) Designers within the public sector promote use of Co-Design. Designers and facilitators working within public sector spaces and services are motivated to engage with Co-Design approaches and develop practices together. They believe that Co-Design reaches its full potential through strong networks, sharing and raising awareness of design in all its forms and provisions, and enhancing the various facets in applied disciplines. Involving all the stakeholders throughout the development process, sharing information and practices, and building trust between people, are some of the key elements identified for a successful Co-Design process (Selloni, 2017). For the Hall of the Future project a team from the Politecnico di Milano and\n\n[1] Systems Thinking for Bottom-Up Social Innovation: An Examination of Social Practices is the role of collaboration, which has become an important focus of design in recent years. Sanders and Stappers (2008) address the ambiguity of participatory methods by defining the terminology associated with collaborative work. Participatory thinking and collective creativity allow multiple stakeholders to develop peripheral awareness of neighbouring domains of practice and realise shared objectives and goals. The differences between the roles of co-creation, co-design and facilitation are distinguished by how the designer or domain expert is positioned within the collaborative working group. According to Sanders and Stappers (2008), co-creation involves any form of collective creativity between experts and non-experts. Co-design is defined by any form of co-creation, between designers and non-designers, following a design process. Designers, researchers and experts can also assume the position of facilitators, leading and guiding participants through creative processes. This produces a hierarchy of collaboration, where participants may enter a participatory working group as a co-creator and eventually engage in the process of codesign. Furthermore, the role of facilitator is increasingly important to develop and foster future collaborative teams to create more impact for sustainable systems. 3.1.Participatory Approach to Social Design Social innovation requires the alignment of activities, processes and policies across micromeso-macro-levels. Figure 1 illustrates the hierarchy of collaboration, positioning the modes of participation to the three levels of action. Figure 1. Levels of Social Design The micro-level, concerning the practices of community, inclusivity and narratives, engages participants through the model of co-creation. Participants are involved in collective creativity, but the methods and processes may not necessarily follow those arising\n\n[2] Investigating socially assistive systems from system design and evaluation: a systematic review highlighted a participatory design (or codesign) approach. The major difference between the two concerns the role of the end-users. In a user-centered approach, end-users are involved as the providers of the contextual information, which helps the designers to better understand the design challenge and requirements. Whereas in a participatory design approach, in addition to providing contextual information, end-users also actively share the role of designers to formulate the design solution together. In user-centered development, various techniques have been used to understand the needs, attitudes, or experiences of the end users, including interviews (e.g.,,75), ethnographic studies (e.g.,), focus group (e.g.,,80), trial and customization (e.g.,), feedback sessions (e.g.,), etc.. Whereas in a participatory design approach, besides the above mentioned techniques to gather user-centered insights, the end-users are also invited to participatory, or co-design workshops, to generate design concepts or solutions together with a group of stakeholders (e.g., see,91,92). The outcome of the participatory design workshops could take the form of some low-threshold design artefacts (like sketches, drawings, or collages), as well as mockups (e.g., see), or low-fidelity prototypes (e.g., see). End-user involvement in different stages of design The reported user and stakeholder involvements also took place in different stages along the design process, from rather early moments of ideation all the way to the later stages which overlapped with design validation, For example, utilized a focus group in an early stage to identify difficulties of deaf people in using assistive technologies. Whereas, addressing a similar context, involved deaf\n\n[3] Co-creating Virtual Reality Interventions for Alcohol Prevention: Living Lab vs. Co-design Addressing the need for collaborative involvement in health intervention design requires application of processes that researchers and practitioners can apply confidently to actively involve end-users and wider stakeholder groups. Co-creation enables participation by focusing on empowering a range of stakeholders with opportunities to influence the final intervention design. While collaboration with users and stakeholders during intervention design processes are considered vital, clear articulation of procedures and considerations for various co-creation methodologies warrants further research attention. This paper is based on two case studies conducted in Australia and Denmark where researchers co-created virtual reality interventions in an alcohol prevention context. This paper explored and reflected on two co-creation methods–co-design and the Living Lab—and showcased the different processes and procedures of each approach. The study demonstrates that both approaches have merit, yet highlights tensions in distinguishing between the application of each of the respective steps undertaken in each of the processes. While a lot of similarities exist between approaches, differences are evident. Overall, it can be said that the Living Lab is broader in scope and processes applied within the Living Labs approach are more abstract. The co-design process that we applied in the first case study is described more granularly delivering a clear a step-by-step guide that practitioners can implement to co-design solutions that end-users value and that stakeholders support. An agenda to guide future research is outlined challenging researchers to identify the most effective co-creation approach.\n\n[4] The Participatory Turn in AI Design: Theoretical Foundations and the Current State of Practice •User-centered design (UCD): involving end users during need-finding and evaluation of designs.The double-diamond UCD process has become the most common design process in industry practice (Council, 2005). Technology designers engage end users in identifying user needs and assessing design ideas (Mao et al., 2005; Olson and Kellogg, 2014).•Service design: involving direct and indirect stakeholders impacted by a technology service in its design. Using methods such as service blueprints and value stream mapping, service designers consider how their design decisions impact stakeholders (Forlizzi, 2018; Forlizzi and Zimmerman, 2013; Forlizzi, 2018; Saad-Sulonen et al., 2020; Holmlid, 2012).•Participatory design (PD): incorporating diverse voices in design in order to challenge power structures. In contrast with UCD and service design, PD is designed to enable non-expert stakeholders to provide direct input on technology design (Gregory, 2003; Simonsen and Robertson, 2012).PD provides guidance for practitioners to create a hybrid space (i.e., neither in the stakeholders or the designers’ domain) that allows diverse non-expert stakeholders to meaningfully contribute, while uncovering and challenging the power dynamics between stakeholders and designers, as well as within groups of stakeholders(Le Dantec and Fox, 2015; Harrington et al., 2019; Muller and Kuhn, 1993).•Co-design: creative cooperation between stakeholders and technology designers across the whole span of a design process (Sanders and Stappers, 2008; Steen, 2013; Kleinsmann and Valkenburg, 2008). While closely related to PD, co-design typically lacks its explicitly political component (Steen, 2013). Further, co-design often focuses more explicitly on designing specific products or artifacts (Kleinsmann and Valkenburg, 2008), while PD may focus more on the larger sociotechnical system in which particular artifacts will be embedded (Simonsen and Robertson, 2012).•Value-sensitive design (VSD): accounting for the values of direct and indirect stakeholders. VSD is similar to PD, but focuses on identifying and incorporating the values of direct and indirect stakeholders in technology design (Friedman, 1996; Friedman et al., 2002).•Participatory action research (PAR): engaging stakeholders as co-inquirers to co-construct research plans and interventions (Elliot, 1991; Unertl et al., 2016; Wallerstein and Duran, 2006; Hayes, 2014; Rasmussen, 2004). Originating in the social sciences (and adopted in HCI (Hayes, 2014; Harrington et al., 2019)), PAR involves stakeholders collaborating with researchers to develop research questions and study designs, conduct research with researchers, and interpret results (Unertl et al., 2016; Wallerstein and Duran, 2006; Hayes, 2014; Rasmussen, 2004; Elliot, 1991), focusing on research rather than design (as in PD) (Gleerup et al., 2019).•Social choice theory (SCT) and mechanism design: quantitative aggregation of stakeholder preferences.SCT focuses on identifying people’s preferences for (e.g.) public policies and mathematically aggregating those in a preference-ranking model (e.g., Arrow, 2012; Sen, 1977, 1986; Kahng et al., 2019; Saha et al., 2020). Prior work in the field of mechanism design has used SCT to develop a framework for including stakeholders in algorithmic decision-making (Abebe and Goldner, 2018; Finocchiaro et al., 2021; Hitzig, 2020; Viljoen et al., 2021).•Participatory democracy and civic participation: involving citizens and stakeholders in civic decision-making. Although sharing a focus on policy outcomes with SCT, participatory democracy has a broader focus on methods for democratic decision-making, political action, and public engagement (e.g., Arnstein, 1969; Fung, 2006; Polletta, 2012; Lippmann, 1993; Dewey, 1954; Macpherson, 1973; Polletta, 2012).•Deliberation theory: qualitatively weighing and discussing competing perspectives and policies.Deliberation theory emerged in response to mechanistic approaches to aggregating stakeholder preferences (e.g., social choice theory) (Öberg, 2016; Fishkin and Luskin, 2005; Owen and Smith, 2015). It emphasizes the importance of bringing together small groups of people to discuss and qualitatively weigh competing arguments for policies (Fishkin and Luskin, 2005).\n\n[5] Co-Designing in Social VR. Process awareness and suitable representations to empower user participation co-design is an important component in the process compared to cooperative design consisting in an asynchronous process (Kvan 2000). Here, cooperative design refers to when a group of individuals accomplish different tasks separately in order to achieve a common goal (Achten 2002). Participatory design and co-design both share the advantages of integrating individuals' skills and experience(s), as revealed through the unfolding exchanges (Schuler and Namioka 1993). Moles (1986) calls them repertoire, that is to say, a constitutive knowledge of each one, which joins Schön's own notion of repertoire (1985). Mattelmäki et al. (2011) raise the question of power attribution to all participants in a fair way (democratic) in collaborative design processes. We define co-design as a particular collaborative design process. It requires engaging the group in simultaneous mutual collaboration and actively involving all participants. The generation of concepts then takes place together through a spontaneous exchange where ideas are built, negotiated and accomplished by a common agreement. To build on these concepts, our proposed co-design approach using Social VR (see section 6) has two main ingredients: a process made aware to the participants through the structure of verbal exchanges, and a particular representational ecosystem that allows layperson to fully understand design proposals and fully engage in the co-design process. Process awareness: informed through verbal exchanges Verbal communication constitutes, above sketches, the prime design tool allowing designers to externalize intentions and launch ideation (Jonson 2005). In a collaborative setting, designers communicate their ideas verbally, but also through gestures and representations (graphical and physical). The conversation around collaborative\n\n[6] The breakdown of the municipality as caring platform: lessons for co-design and co-learning in the age of platform capitalism element of the process and/or outcome is at stake. More advanced levels of participation stimulate shared commitment around the issue and process at stake, often leading to co-ownership (Light et al. 2013;Seravalli 2014). Figure 2 links codesign to mutual learning. Mutual learning is discussed as a rationale (Simonsen and Robertson 2012) and a methodological stand (Bratteteig et al. 2012) in participatory design, meaning that participants' learning is seen as something emerging from and through co-design processes. By introducing multiple stakeholders, co-design processes not only bring together different knowledges, but also create opportunities for collective articulation and mutual understanding. We distinguish mutual learning, a welcome product alongside a design activity, from the notion of 'co-learning', which we understand as a collaborative effort explicitly aimed at creating the conditions for learning together and in which collaborative making, when present, is instrumental to that learning (e.g. Boys 2017, in DiSalvo et al. 2017). We understand co-learning as an explicit and structured collaborative reflective process (see Figure 3). Co-learning, then, is related to co-research approaches, such as participatory action research (e.g. Senge and Scharmer 2006, on communities). However, while design research (Rodgers and Yee 2014) and action research (Reason and Bradbury 2006) stress coming into a situation to change it, co- learning is rather about making collaborative learning opportunities that attend to a specific issue and/or situation through the co-design of and participation in informative encounters and reflective engagement opportunities. Literature focusing on citizens' participation discusses collaborative learning as a key characteristic of higher levels of participation (Pretty 1995;Collins and\n\n[7] Enhancing health literacy through co-design: development of culturally appropriate materials on genetic risk and customary consanguineous marriage knowledge, aims to draw on diverse perspectives and integrates the creativity of designers and people not formally trained in design (Sanders and Stappers, 2008). The form and extent of participation may vary within approaches that are labelled as co-design. However, co-design is distinguished from 'consultation' by actively involving end-users throughout the design process from problem definition, through ideas generation, development, testing and final production (Sanders and Simons, 2009). The arguments for co-design in health and social care are both pragmaticuser participation results in more appropriate designsand moralpeople have a right to be involved in design processes that have implications for their lives. This type of approach can be particularly important when addressing populations who are poorly served or where understanding and trust between 'professionals' and citizens is lacking. Co-design can help to ensure that interventionsincluding those aimed at enhancing health literacyreflect the diverse needs, values and wishes of different population groups. For example, Burke et al. (2004) co-designed health education materials with Vietnamese-American community members in Seattle on the topic of Hepatitis B prevention, screening and treatment. Their study highlighted the utility of 'coalitions' that bring together local people, community service organisations and health-care providers. These coalitions ensured that education materials integrated important elements, such as beliefs about traditional medicine and added legitimacy to the initiative. Our team was made up of a multi-lingual nurse researcher, a public health researcher, two designers with expertise in participatory design, an anthropologist, a product designer and a multilingual research assistant. We actively involved representatives from community organisations, individuals identified\n\n[8] A review of research with co-design methods in health education NGOs, carers, district nurses, community pharmacists and social care workers (e.g. Vella et al., 2015) and only 2 papers with the government itself (e.g. Yokota et al., 2018). Our review suggests that stakeholder involvement in the co-design process takes two forms: the first relates to stakeholders being the 'subjects' of the research, so they are involved in shaping and delivering the research itself; and the second relates to service design and delivery where stakeholders who use the services are involved in changing and/or designing the (new) service. RQ1: How are co-design methods used to support stakeholders' involvement in different stages? The analysis demonstrates that the most popular methodology was experience-based co-design. The most commonly used co-design method is the workshop, which often includes design methods such as brainstorming and reflective activities. In health educational contexts, workshops methods tend to be based on the development of artefacts (e.g. cards) and use of visualisation techniques (e.g. films), narration techniques (e.g. storytelling or storyboarding), as well as other user-centred design approaches, such as the development of narratives, personas, and scenarios. Interviews, focus groups and observations were also widely used. Social science methods tend to be applied at an early stage in the co-design process (e.g. while framing the problem), while the design methods are more commonly applied in stages 2 and 3 (i.e. design criteria, ideas information and plan prototype). Both social methods and design methods appear in the trial experiment output (final stage). Public health workers are included most frequently in health co-design processes. The community (i.e. the\n\n[9] What Do I Need to Design for Co-Design? Supporting Co-design as a Designerly Practice goals or needs of the user, to a participatory culture that seeks to enable and empower diverse stakeholder participation in design work. Sanders and Stappers define co-creation and co-design as a part of this shift towards participatory approaches, which defines new roles and opportunities for users to take on. In a broad sense, co-creation is defined as \"any act of collective creativity, i.e. creativity that is shared by two or more people\" and co-design is defined as \"collective creativity as it is applied across the whole span of a design process\", p. 6. There is often confusion between these terms, but these concepts can be differentiated based on who is involved in the act, the goals of involvement, and when they are involved in the design process. Co-creation has its origins in the fields of business studies and marketing, whereas co-design draws more from design-focused fields following participatory design or co-operative design traditions. These two terms are often intertwined, but building on the definitions, we consider co-design as an instance of co-creation, and standardize on the term co-design throughout this paper as our point of focus. In the following sub-sections, we will provide a broad outline of knowledge that has been generated in relation to co-design practices and experiences, using the left side of Figure 1 to indicate knowledge-building that has been focused on theory-building or generation of principles to guide future co-design engagement (Section 2.1.1) and knowledge-building that has been focused on specific examples of co-design environments (Section 2.1.2). Guidelines\n\n[10] ‘Collective making’ as knowledge mobilisation: the contribution of participatory design in the co-creation of knowledge in healthcare the term participatory design here deliberately to link to the rich literature from this branch of design, but recognise, and intentionally use co-design in the rest of the paper as it has more traction in health. The two terms, we feel for a predominantly health audience, can be used interchangeably. This positions design practice in the co-creation arena, and aligns with the UK's National Institute for Health Research (NIHR) policy about Patient and Public Involvement (PPI) practices and empowerment and emancipation. Tsoukas and Vladimirou define knowledge as \"… the individual ability to draw distinctions within a collective domain of action, based on an appreciation of context or theory or both…\" (p979). As such, co-design approaches align to critical theory and critical realism paradigms in the KMb landscape where methods are developed as a contribution to action and emancipation with a particular emphasis on working with end users in solution focussed processes. Co-design has an emphasis on process, where facilitation brings different participants together to elicit and share first-hand experiences and first-hand knowledge perspectives. Co-design has therefore an ethos of empowerment and real engagement, placing such practices on the higher rungs of the ladder of participation described by Arnstein. The co-design process recognises that the knowledge that stakeholders bring, is both explicit and tacit. Tsoukas and Vladimirous describe this as 'contextual appreciation'. Surfacing this knowledge and recognising its value are two key reasons for first-hand participation; it is often only participants who have mutual contextual appreciation, albeit from different perspectives, that\n\n[11] Co-creation, co-design, co-production for public health - a perspective on definition and distinctions. Collaboration between community members, researchers, and policy makers drives efforts to solve complex health problems such as obesity, alcohol misuse, and type 2 diabetes. Community participation is essential to ensure the optimal design, implementation and evaluation of resulting initiatives. The terms 'co-creation', 'co-design' and 'co-production' have been used interchangeably to describe the development of initiatives involving multiple stakeholders. While commonalities exist across these concepts, they have essential distinctions for public health, particularly related to the role of stakeholders and the extent and timing of their engagement. We summarise these similarities and differences drawing from the cross-disciplinary literature, including public administration and governance, service management, design, marketing and public health. Co-creation is an overarching guiding principle encompassing co-design and co-production. A clear definition of these terms clarifies aspects of participatory action research for community-based public health initiatives.\n\n[12] Use of participatory action research approach to develop a self-management resource for persons living with dementia 2018), and Belgium (Hendriks et al., 2013). Participatory processes have helped inform guidelines and frameworks and identify innovative approaches and strategies for supporting people with dementia as co-researchers, co-designers (Dupuis et al., 2012a;Hendriks et al., 2013;Phillipson & Hammond, 2018;Wang et al., 2019) and as co-analysts (Clarke et al., 2018), including people living with dementia in later phases of dementia (Smith & Phillipson, 2020). Although some participatory approaches have included people living with dementia in a specific phase of the process (i.e., predesign or evaluative phases), more recent studies have been engaging people living with dementia in co-design in all phases of the process (Wang et al., 2019). This research demonstrates that meaningful inclusion of people living with dementia in research design is not only possible but also beneficial to both people living with dementia and the design of the research (Wang et al., 2019). There were five common elements of PAR that made it an appealing methodology for our process. People experiencing specific phenomena or working in particular settings are included as co-researchers throughout the process. This means actively engaging all in meaningful ways in collaborative decision-making; collecting and analyzing data; reflecting, interrogating, and interpreting the information; determining the best ways to share information and in constructing the story to be told; and acting on the information (Dupuis & Whyte, 2017, p. 106;Kemmis et al., 2014) Understanding what is of practical significance to the people and settings involved and drawing on local knowledge and methods is of utmost importance (Reason & Bradbury, 2008). PAR integrates knowing\n\n"}
{"case_id": "6296a33a2bcc31f1b8f9a765e63aaf3b", "answer_text": "\n\nObservational studies offer unique insights that interviews alone may not uncover, especially in system-level research where understanding actual behaviors, processes, and interactions within their natural context is critical [1]. Systems papers might prefer observational studies over interviews because they provide direct, real-time data on how systems operate and interact, capturing nuances and details that structured interviews might overlook [2]. Here are specific examples of systems papers that utilize observational studies for formative research: \n\n**Ecology and Environmental Studies** [0]\n\n**Intermittent River Ecology**: Although more than half the world's river networks comprise channels that periodically cease to flow and dry, river ecology was largely developed from and for perennial systems. Ecological knowledge of IRs is rapidly increasing, so there is a need to synthesise this knowledge and deepen ecological understanding. In this Special Issue, we bring together 13 papers spanning observational case studies, field and laboratory experiments and reviews to guide research and management in this productive field of freshwater science. We summarise new developments in IR ecology, identify research gaps and needs, and address how the study of IRs as highly dynamic ecosystems informs ecological understanding more broadly. This series of articles reveals that contemporary IR ecology is a multifaceted and maturing field of research at the interface between aquatic and terrestrial ecology [0]. \n   The study of intermittent river ecology emphasizes the need for a comprehensive understanding of ecological dynamics in rivers that periodically cease to flow and dry. This research leverages observational case studies along with field and laboratory experiments to provide insights into these highly dynamic ecosystems [0]. By observing these river systems directly, researchers gain critical information on flow continuity and connectivity that would not be possible through interviews alone. Methods used include direct field observations and documenting ecological behaviors and interactions over time to capture the full scope of periodic changes. The impact of these observations lies in revealing the intricate dynamics of river ecosystems that are not evident from secondhand accounts, thereby informing robust ecological management practices.\n\nHealth and Medical Research [14].\n\n2. **Global Health Research**: [1]\n   An article highlights the importance of incorporating observation into formative research, arguing that focus groups, interviews, and knowledge, attitude, and practice (KAP) surveys often fail to capture critical insights obtainable through direct observation [1]. Several case studies demonstrate how observational methods have yielded valuable information that shaped intervention designs more effectively than interviews could. Specifically, structured and unstructured observations were employed, allowing researchers to gather nuanced data on behaviors and contexts in various health settings. The critical contribution of these observations is in identifying real-world operational issues and behaviors that standardized methods might overlook, thus leading to more effective and culturally appropriate health interventions.\n\n3. **Remote Patient Monitoring** [13]:\n\n\n   An observational study on remote patient monitoring devices aimed at early detection of in-hospital patient deterioration involved the direct observation of patient interactions with wearable monitors. This provided essential data for assessing the device's efficacy and potential for timely intervention [13]. Methods included continuous monitoring and recording of patient data, using the wearable devices to capture real-time health metrics that were later analyzed for early warning signs. The critical impact here is in validating the real-world functionality and effectiveness of health monitoring devices, providing readiness for practical implementation that would not be sufficiently informed by interview data alone.\n\n**Business Operations Research**[2][3][12][14]\n\n4. **Business Operations** [2]:\n   An empirical study of family businesses demonstrated the utility of observations in contextualizing narratives collected from interviews. Observations provided the detailed operational processes necessary to accurately interpret interview data [2]. This approach underscores the importance of combining multiple data collection methods to achieve a holistic understanding of the study subject. Methods included taking detailed field notes during company visits and systematically observing key business operations to complement interview findings. The significant impact here is in providing a grounded understanding of business operations that interviews alone could not validate, leading to more accurate and actionable insights into business practices.\n\n**Collaboration and Educational Studies**\n\nStudies based on collaboration and education often incorporate a variety of qualitative and quantitative research methods to gather and analyze data, including interviews, surveys, and observations [1]. Observational methods, in particular, provide critical insights that can be difficult to obtain through other methods, such as interviews or surveys, and can offer firsthand encounters with phenomena of interest [5]. \n\n\n\nHybrid Collaboration and Meetings [3].\n   A study focused on hybrid collaboration and meetings research found that many systems papers collected multiple types of data, including observations. These observational data were integral for understanding the dynamics of hybrid settings and were merged with interview and survey data to provide a comprehensive analysis [3]. Techniques involved recording and analyzing interactions during meetings and hybrid collaborations to identify patterns and behaviors that would inform the development of effective collaboration tools. The importance of this methodology is in capturing the subtleties of virtual and in-person interactions, thereby informing better hybrid work models that interviews might not fully elucidate.\n\n6. **Educational Evaluation** [1]. \n   An educational study assessing the usability of a structured online peer evaluation system utilized observations to gather data on student behaviors. This method allowed for a more accurate assessment of the system's effectiveness by capturing real-time interactions and responses that interviews and surveys could not [12]. Observations included monitoring student interactions on discussion boards and peer evaluation activities, providing firsthand data on user engagement and system usability. These detailed observations contribute significantly by uncovering user interactions and technical issues, ensuring the system's robustness and user-friendliness in practical settings.\n\nPublic Health Research[11]. \n\n7. **Public Health Research**: [11]\n   Observational methods are used alongside structured approaches like cohort studies and case-control studies to provide a broader understanding of health phenomena. These observations help illuminate the context and operational challenges that purely interview-based methods might miss [14]. Techniques included observing patient and provider behaviors in clinical settings, documenting interactions and treatment processes to better understand the implementation and impact of public health interventions. These observations are crucial in identifying gaps and inefficiencies in health service delivery, leading to more effective public health strategies.\n\nIn summary, these examples illustrate how observational studies are instrumental in formative research within various systems. They provide critical insights that validate and contextualize findings from other data sources [1], leading to more robust and actionable research outcomes. Observational studies enrich our understanding by capturing the real-time dynamics and contextual factors that structured interviews may overlook [8], thus significantly contributing to the field's advancement. \n\n\nReferences:\n[0] Challenges, developments and perspectives in intermittent river ecology Although more than half the world's river networks comprise channels that periodically cease to flow and dry [intermittent rivers (IRs), river ecology was largely developed from and for perennial systems. Ecological knowledge of IRs is rapidly increasing, so there is a need to synthesise this knowledge and deepen ecological understanding. In this Special Issue, we bring together 13 papers spanning observational case studies, field and laboratory experiments and reviews to guide research and management in this productive field of freshwater science. We summarise new developments in IR ecology, identify research gaps and needs, and address how the study of IRs as highly dynamic ecosystems informs ecological understanding more broadly. This series of articles reveals that contemporary IR ecology is a multifaceted and maturing field of research at the interface between aquatic and terrestrial ecology. This research contributes to fresh water and general ecology by testing concepts across a range of topics, including disturbance ecology, metacommunity ecology and coupled aquatic-terrestrial ecosystems. Drying affects flow continuity through time and flow connectivity across longitudinal, lateral and vertical dimensions of space, which aligns well with the recent emphasis of mainstream ecology on meta-system perspectives. Although most articles here focus on the wet phase, there is growing interest in dry phases, and in the terrestrial vegetation and invertebrate assemblages living in and along IR channels. We encourage interdisciplinary studies on IRs to further blur the demarcation between aquatic and terrestrial ecosystems and develop more integrated perspectives. As a result of climate change and human modification of landscapes and waterways, flooding and drought are expected to become more extreme and widespread. Shifts in streamflow regimes from perennial to intermittent may exacerbate the duration and frequency of dry phases in IRs with serious implications for river ecosystems and the quality and diversity of services they provide.\n\n[1] Observe Before You Leap: Why Observation Provides Critical Insights for Formative Research and Intervention Design That You'll Never Get From Focus Groups, Interviews, or KAP Surveys of intervention components. Formative research allows community participation in the design of research and program protocols, which leads to greater community acceptance. So formative research is about much more than just behavior change interventions. Now, what about methods? If you want to do formative research, how should you go about it? Formative research can incorporate many methods, both qualitative and quantitative. Focus groups tend to be the most common, perhaps because they are most familiar. Interviews and knowledge, attitude, and practice (KAP) surveys are also popular. However, as you've probably gathered by now, I'm going to argue that those methods are often insufficient. If you're doing formative research, you should also consider observation. Researchers seem more hesitant about observation than other methods, perhaps because they don't know how to do it, consider it too laborintensive or costly, feel uncomfortable with the idea of watching other people, or worry about reactivity-the phenomenon where those being observed change their behavior due to the observer's presence. But observation can generate insights you won't get using any other method. And those insights can often prove critical. In this article, I present 4 case studies on different global health topics, from corralling domestic poultry to measuring the competency of skilled birth attendants (SBAs). These examples illustrate some of the scenarios in which observation-both structured and unstructuredcan be useful, and they highlight the types of insights it can provide. In each case study, observation yielded critical information that would have been difficult or impossible to obtain any other way. For\n\n[2] How to observe business operations: An empirical study of family business to earlier periods. We found several studies that once again only mention case observations without explaining how they conducted their observations: We took field notes, documenting our company visits and observations. The research output of this stage is not presented here but was extremely useful for the research team to contextualize the participants' narratives of the succession process collected in the interviews, p. 583., p. 173. The authors started by independently examining the data in interviews, observation notes, and documents. Then, a coding process was carried out by reading and rereading transcripts, notes, and documents and then using codes for sentences or paragraphs in order to organize data We also found that observational methods are often used as an incidental data source and that some papers describe the interview process in far more detail than the observational process entails: We integrated data collected from a wide range of sources: interviews, historical records, financial data, sustainability data, and direct observations to triangulate (to adopt different angles in observing the same phenomenon). We chose to use the semistructured interview because of its high degree of flexibility and because it offers the opportunity to address themes that can come to light during the semistructured interviews. Data collection was conducted from March 2015 to April 2015. The interview transcriptions, field notes, and documentary analysis were coded into key themes, p. 977. Some studies briefly account for the frequency of observations but do not explain how they conducted their observations, omitting their observation subject, location, number, or other\n\n[3] What was Hybrid? A Systematic Review of Hybrid Collaboration and Meetings Research study explicitly (i.e., consciously or on purpose, but still without necessarily naming it 'hybrid') whereas 28 of the papers included the hybrid setting implicitly (e.g., it was only a natural by-product of the settings) in their studies. One of the papers had both explicit and implicit cases of HCM and seven of the papers are marked as 'not applicable\" for reasons of either the paper does not involve empirical research or we are not knowledgeable about the further details of their choice of setting. Regarding the data collection, we can claim that many of the papers collected multiple types of data and merged and analyzed those data together. Only six of the papers did not include any empirical work. Out of 62 papers, 21 have collected individual-based interview data, whereas two of the papers have collected group interview data. Interviews are followed by questionnaires (20), video recordings (16), surveys (7), system logging/log files (7), observation (5), literature review (3), field notes (3), diaries/journals (3), group discussion/focus groups (2), performance (2), reflections (1), gaze recordings (1) and a rich case study data (1) consisting of six different cases and scenarios of challenges of distributed teams. While questionnaires and/or surveys were used in a large number of papers to gather data, in sum, only three papers are based on fully quantitative findings,49,128. TAXONOMY OF HCM Based on the systematic literature review, we introduce a taxonomy for the description and categorization of HCM. The summarized findings of all 62 articles used to develop this taxonomy are shown at\n\n[4] The 3 Cs of Content, Context, and Concepts: A Practical Approach to Recording Unstructured Field Observations to capture any data that do not fit into a priori categories. In such a setting, the research question serves as a guide, not a mandate, and it leaves room to address unexpected occurrences. Although the 3 Cs template provides an excellent entrée for the use of field observations, there are other observational alternatives. Structured observations may be better indicated for inquiry exploring systematically the nature and metrics of phenomena, with integration across multiple observers for the purpose of statistical analysis. Another option is video recording, which yields multiple data sources, such as verbal, paraverbal, and nonverbal responses, and requires specific levels of skills and techniques for analysis. 50 The 3 Cs approach to unstructured field observations can be used when observation is the primary research method 46 or in tandem with another research method, such as qualitative interviews. 51 In the Qatar recruitment study, the observations were conducted more with the intent of being supplemental, but ultimately served as the primary source for a specific publication. 46 The template, which can be submitted for institutional review board approval, provides a straightforward mechanism for recording events and behaviors in almost any project involving human participants. As mixed methods gain increasing popularity in health services research, 52 unstructured field observations can play an important role in contextualizing other types of qualitative and quantitative data, resulting in more rigorous research designs and data collection.\n\n[5] The Qualitative Report The Qualitative Report A Comparative Review of Qualitative Research: A Guide to Design A Comparative Review of Qualitative Research: A Guide to Design and Implementation and Implementation is to ask good questions and asking good questions takes practice. Finally, the author suggests six basic interview questions experience and behavior questions, opinion, and values questions, feeling questions, knowledge questions, sensory questions, background/demographic questions, and skilled interviewers can do much to bring about positive interaction. Chapter six present being a careful observer and shows distinguishing from interviews in two ways. First, observations take place in the setting where the phenomenon of interest naturally occurs rather than a location designated for the purpose of interviewing; second, observational data represent a firsthand encounter with the phenomenon of interest rather than a secondhand account of the world obtained in an interview. In addition, observations are also conducted to triangulate emerging findings; that is, they are used in conjunction with interviewing and document analysis to substantiate the findings and to provide some knowledge of the context or to provide specific incidents, behaviors, and so on that can be used as reference points for subsequent interviews. The other point they described that what to observe is partly a function of how structured the observer wants to be. We observe the physical setting, the participants, activities and interactions, conversation, subtle factors, your own behavior. And also, the researcher can assume one of several stances while collecting information as an observer, furthermore the author identifies types of observation as complete participant, participant as observer, observer as participant, and complete observer. Finally, what is written down or mechanically recorded from a period of observation becomes the raw data from which a study's findings\n\n[6] A Technique of Modeling in Public Health Research and Development create or develop forms a. Study of relevant documents and research b. Study from real context i. Situation, Problem, Need Example: Studying primary data Survey about situations (agent, host, environment), problems, and needs their own work (health promotion, prevention, treatment, and rehabilitation of Communicable Diseases; CDs and Shared values), PESTLE (Policy, Economic, Social, Technology, Law, and Environment) from samples of population with 3 methodologies as follows: 1) Cohort study/ Prospective studies are the most trustworthy observational study, but like any observational study, they are subject to confounding, 2) Case control/Retrospective studies are often much more feasible, but potentially subject to recall bias and unrepresentative sampling, and 3) Cross sectional studies provide a quick snapshot of an association, but need to be interpreted with care. Example: Studying secondary data Do the same with primary data from documents of office and/or other offices by using SWOT analysis then analyze the organization in the following two sections, 1) Using PESTLE for analyzing Opportunity and Threat; OT in the part of external organization, 2) Using 7S for analyzing Strength and Weakness; SW in the part of internal organization. After that use SIPOC Model for analyzing own works details. Write process or flowchart (one of five elements of SIPOC Model) their work then use the team to brainstorm by using the multi-voting method to find the team's settlement. Problems will take place in 3 parts of SIPOC Model are SIP in part of OC are just monitor. When the problems have been analyzed then set priority the most important\n\n[7] Evaluating a Structured Online Peer Evaluation System Among Graduate-Level Communication Capstone Students Through Action Research was small, the observations provided an opportunity to gather data on actual student behaviors instead of relying solely on students' self-reported feelings and perceptions (Schmuck, 1997). One-on-One Interviews In alignment with UNCM IRB requirements, qualitative one-on-one interviews were conducted following the term conclusion. The purpose of the 20-25-minute semi-structured interviews was to question participants about their experiences with the structured peer evaluation system and the peer review tool kit (see Appendix C). The interviews yielded direct quotes from participants and offered insight into their opinions and experiences (Patton, 2014). Researcher's Handwritten Interview Notations During each of the one-on-one interviews, the researcher recorded handwritten notes of impressions and interesting aspects as they surfaced (see Appendix C). Interviews were approached through in-depth inquiry to ensure that the research topic was fully discussed and documented in support of potential changes to current systems (Patton, 2014). Student Post Artifacts Student post artifacts were created within the discussion board forum of D2L Brightspace by way of student interaction during the term. As study participants provided original and response posts during the peer review activities, conversational threads developed. These student post artifacts remained within the Capstone course environment during and after the study term and were later collected for CoI assessment (Garrison & Arbaugh, 2007). Data Analysis The data analysis process for this study embodied methodological techniques to analyze the data and to ensure that the information provided alignment to the study's research questions (Mertler, 2017). Quantitative Data Analysis The use of preterm and post-term questionnaires provided the opportunity to measure and\n\n[8] Assessing the Usability of a Novel Wearable Remote Patient Monitoring Device for the Early Detection of In-Hospital Patient Deterioration: Observational Study Background Patients admitted to general wards are inherently at risk of deterioration. Thus, tools that can provide early detection of deterioration may be lifesaving. Frequent remote patient monitoring (RPM) has the potential to allow such early detection, leading to a timely intervention by health care providers. Objective This study aimed to assess the potential of a novel wearable RPM device to provide timely alerts in patients at high risk for deterioration. Methods This prospective observational study was conducted in two general wards of a large tertiary medical center. Patients determined to be at high risk to deteriorate upon admission and assigned to a telemetry bed were included. On top of the standard monitoring equipment, a wearable monitor was attached to each patient, and monitoring was conducted in parallel. The data gathered by the wearable monitors were analyzed retrospectively, with the medical staff being blinded to them in real time. Several early warning scores of the risk for deterioration were used, all calculated from frequent data collected by the wearable RPM device: these included (1) the National Early Warning Score (NEWS), (2) Airway, Breathing, Circulation, Neurology, and Other (ABCNO) score, and (3) deterioration criteria defined by the clinical team as a “wish list” score. In all three systems, the risk scores were calculated every 5 minutes using the data frequently collected by the wearable RPM device. Data generated by the early warning scores were compared with those obtained from the clinical records of actual deterioration among these patients. Results In total, 410 patients were recruited and 217 were included in the final analysis. The median age was 71 (IQR 62-78) years and 130 (59.9%) of them were male. Actual clinical deterioration occurred in 24 patients. The NEWS indicated high alert in 16 of these 24 (67%) patients, preceding actual clinical deterioration by 29 hours on average. The ABCNO score indicated high alert in 18 (75%) of these patients, preceding actual clinical deterioration by 38 hours on average. Early warning based on wish list scoring criteria was observed for all 24 patients 40 hours on average before clinical deterioration was detected by the medical staff. Importantly, early warning based on the wish list scoring criteria was also observed among all other patients who did not deteriorate. Conclusions Frequent remote patient monitoring has the potential for early detection of a high risk to deteriorate among hospitalized patients, using both grouped signal-based scores and algorithm-based prediction. In this study, we show the ability to formulate scores for early warning by using RPM. Nevertheless, early warning scores compiled on the basis of these data failed to deliver reasonable specificity. Further efforts should be directed at improving the specificity and sensitivity of such tools. Trial Registration ClinicalTrials.gov NCT04220359; https://clinicaltrials.gov/ct2/show/NCT04220359\n\n[9] Analytical Methods for a Learning Health System: 2. Design of Observational Studies electronic health data (EHD) data can be used to improve performance in learning health systems, (2) describe how to frame research questions to use EHD most effectively, and (3) determine the basic elements of study design and analytical methods that can help to ensure rigorous results in this setting. • Paper 1, \"Framing the Research Question,\" 2 focuses on clarifying the research question, including whether assessment of a causal relationship is necessary; why the randomized clinical trial (RCT) is regarded as the gold standard for assessing causal relationships, and how these conditions can be addressed in observational studies. • Paper 2, this paper, addresses how study design approaches, including choosing appropriate data sources, methods for design and analysis of natural and quasi-experiments, and the use of logic models, can be used to reduce threats to validity in assessing whether interventions improve outcomes of interest. • Paper 3, \"Analysis of observational studies,\" 3 describe how analytical methods for individuallevel electronic health data EHD, including regression approaches, interrupted time series (ITS) analyses, instrumental variables, and propensity score methods, can be used to better assess whether interventions improve outcomes of interest. • Paper 4, \"Delivery system science,\" 4 addresses translation and spread of innovations, where a different set of questions comes into play: How and why does the intervention work? How can a model be amended or transported to work in new settings? In these settings, causal inference is not the main issue, so a range of quantitative, qualitative, and mixed research designs are needed. Generating Evidence & Methods\n\n"}
{"case_id": "e61be146ef53f1a5483c668fc4f6390c", "answer_text": "\n\nInteractive Machine Teaching (IMT) provides a framework that can be effectively applied to curating social media feeds through several distinct principles. These principles revolve around leveraging human expertise and iterative feedback to train machine learning (ML) models more effectively, even in cases where the human input is not from ML experts [1]. \n\nPrinciple 1: Emphasizing Human Agency and Expertise [1].\nOne of the core principles of IMT is that it places significant agency in the hands of human teachers who guide the learning process. This is in contrast to traditional machine learning frameworks that primarily rely on pre-labeled datasets and algorithmic training [1]. In the context of social media feeds, this means that users can actively participate in shaping their own feed content by providing feedback and guiding the algorithms. This approach makes the process more transparent and empowers users to have a direct impact on what the model learns, which is particularly vital in environments where user engagement and satisfaction are paramount [1][2][3].\n\nPrinciple 2: Interactive and Iterative Feedback Loops [0]. \nIMT frameworks typically include a \"teaching loop\" that consists of stages like curriculum design, explanation of concepts, and performance evaluation [1]. This feedback can be both structured—like tagging and labeling specific types of content—and unstructured—like providing natural language feedback or show-and-tell gestures [2]. The iterative nature of IMT means that users can continuously refine the model's performance by repeatedly providing feedback and observing changes, leading to a more accurate curation of the social media feed over time [3].\n\nFor instance, recent studies have demonstrated the effectiveness of this iterative feedback loop. Zhou and Yatani (2022) showed how integrating both structured and unstructured feedback allowed users to refine social media algorithms more accurately based on their nuanced preferences [2].\n\nPrinciple 3: Balancing Structured and Unstructured Input [2]. \nTo capture the nuanced evaluations users make about social media content, IMT systems need to balance structured (e.g., tags, labels) and unstructured input (e.g., freeform text, gestures) [2]. This dual approach ensures that the diverse ways in which users assess content can be comprehensively captured and utilized by the model. Specific methods for achieving this balance include using form-like interfaces for tagging, coupled with natural language fields for more nuanced feedback, and allowing for unstructured object show-and-tell gestures that are processed by the system. For instance, Zhou et al. (2022) emphasize the importance of integrating both structured and unstructured feedback to accurately capture user preferences and behaviors, which is crucial for an effective social media feed curation system [2].\n\nPrinciple 4: Real-Time Adaptability and Corrective Feedback [8].\nIMT enables real-time adaptability by allowing users to provide on-the-spot corrections to the system’s outputs. For instance, features like object annotation guided by user gestures can be immediately corrected if users notice errors [8]. Applied to social media, this means that users could flag inappropriate content or adjust content recommendations in real-time as they interact with their feed, thereby continuously aligning the feed with their preferences [8].\n\nIn practical applications, this was evidenced by the \"LookHere\" system, which allowed users to highlight objects in real-time and correct misclassifications on the spot, demonstrating how real-time corrective feedback can enhance system performance [8]. For example, a user could interact with their social media feed, flagging certain posts as inappropriate or not of interest, and see those changes reflected immediately, preventing similar content from appearing in the future. This immediacy in feedback and adaptation can significantly enhance user satisfaction and engagement, ensuring the feed remains relevant and enjoyable.\n\nPrinciple 5: Leveraging Semi-Supervised Learning [14].\nIMT also takes advantage of semi-supervised learning methods to propagate labels from annotated data to more extensive datasets, thereby reducing the need for massive amounts of pre-labeled data. For example, graph-based semi-supervised learning can use user-provided attributes or distance metric learning to inform the model about similar content [6][14]. This is particularly useful for social media feeds as it allows the algorithm to generalize user preferences from a limited set of labeled interactions, making the system more scalable and efficient [6][14].\n\nPractical implementations, such as those discussed in the study by Ramos et al. (2020), showcased how semi-supervised learning can extend the impact of limited user input across broader datasets, improving the learning process without extensive data labeling [6][14].\n\nStatement: Principle 6: Context-Sensitive Learning and Display. \nAn IMT system can leverage the context in which user interactions occur to enhance learning. For example, in augmented reality (AR) settings, IMT has been used to interpret user demonstrations to create tangible interactions that are context-sensitive [10]. Similarly, social media feed curations could use the context around user interactions—like time of day, recent activity, or trending topics—to dynamically adjust content recommendations [10].\n\n### Potential Limitations and Challenges [1]\n\n\nWhile IMT shows great promise for enhancing social media feed curation, several challenges need to be addressed [1]. \n\n1. **Scalability**: As IMT relies heavily on user input, scaling the approach to millions of users might require significant computational resources and advanced backend systems [0]. \n   \n2. **Quality of User Feedback**: The effectiveness of IMT is contingent on the quality and consistency of user feedback. Variability in user input can lead to inconsistencies in the model's learning process [2].\n\n3. **Privacy Concerns**: Allowing users to provide real-time feedback and corrections could raise privacy concerns, especially if sensitive personal information is involved [8].\n\n4. **Bias and Fairness**: Users' feedback can introduce or perpetuate biases in the model, which can lead to unfair content curation practices. Ensuring that the system remains fair and unbiased is crucial [0]. \n\nInteractive machine teaching (IMT) is an interaction framework by which subject matter experts—who are often not experts in machine learning—draw upon their personal expertise to train machine learning (ML) models that can operate effectively within their domain (Ramos et al., 2020; Ng et al., 2020; Jörke et al., 2023; Feng and Mcdonald, 2023; Zhou and Yatani, 2022; Wall et al., 2019) [1]. This way, users feel more agency while maintaining a firmer grasp of what the model learns, making models more transparent and debuggable (Ramos et al., 2020; Kim and kyung Lim, 2021) [1]. \nBy incorporating these principles—emphasizing human agency [0][1], utilizing interactive feedback loops [0][1], balancing structured and unstructured input [2], enabling real-time adaptability [5], leveraging semi-supervised learning [14], and employing context-sensitive learning [6]—IMT can significantly enhance the experience of curating social media feeds. These principles ensure that users have more control, the system is continuously improving based on iterative feedback [1], and the curation process is flexible and responsive to the diverse ways users interact with and evaluate social media content [2]. \n\nApplying these theoretical principles in practice has shown promising results, as evidenced by recent studies and systems like those by Zhou et al. (2022) [2], Ramos et al. (2020) [1], and the \"LookHere\" system [8], which have successfully integrated IMT into real-world applications, thereby validating its effectiveness in improving user experience and system performance.\n\nComparisons with Other Methods\nWhen comparing IMT to other popular methods used for social media feed curation, it becomes evident that IMT offers a unique advantage by deeply integrating human expertise into the learning process [1]. Traditional methods such as collaborative filtering, content-based filtering, and hybrid approaches rely primarily on algorithmic predictions based on user behavior data or content similarities, often with limited active user input [0]. \n\nFor example, collaborative filtering uses user-item interactions to predict user preferences but does not typically incorporate real-time corrective feedback from users [8]. Content-based filtering relies on the attributes of items and user profiles, but again lacks the iterative and interactive element that IMT brings [1]. Hybrid methods that combine both approaches might offer improved accuracy but still do not fully leverage the depth of human input that IMT systems utilize [0]. \n\nIMT stands out by allowing users to directly shape the feed through structured and unstructured feedback [2], enhancing the system's responsiveness and personalization capabilities. This makes IMT particularly well-suited for dynamic environments like social media, where user preferences can change rapidly, and real-time adaptability is crucial [0].\n\n\nReferences:\n[0] Mapping the Design Space of Teachable Social Media Feed Experiences IMT shares similar goals with the paradigm of reinforcement learningand, more specifically, reinforcement learning with human feedback (RLHF) (Ouyang et al., 2022).Both IMT and RLHF strive to interactively and iteratively embed specialized human knowledge into a machine-learned system. In RLHF, an algorithmic agent strives to find a policy (a map of the agent’s states to actions) that maximizes some long-run measure of reinforcement as defined by a reward function in a dynamic environment (Sutton and Barto, 2018; Kaelbling et al., 1996; Christiano et al., 2017), after which human feedback is be used by the agent to fine-tune the model to act in accordance with the user’s intentions (Ouyang et al., 2022; Stiennon et al., 2020). The key difference between IMT and RLHF, however,is that in IMT, the human teacher has agency—defining the curriculum, explaining concepts, and evaluating performance—whereas in RL(HF), the human is merely an oracle that the agent queries to guide its decisions (Christiano et al., 2017).While we see potential in both IMT and RLHF to better align a feed curation agent with user values, we seek to center user agency throughout the feed curation experience. As such, we focus on IMT and defer exploration of RLHF approaches to future work.\n\n[1] Mapping the Design Space of Teachable Social Media Feed Experiences Interactive machine teaching (IMT) is an interaction framework by which subject matter experts—who are often not experts in machine learning—draw upon their personal expertise to train machine learning (ML) models that can operate effectively within their domain (Ramos et al., 2020; Ng et al., 2020; Jörke et al., 2023; Feng and Mcdonald, 2023; Zhou and Yatani, 2022; Wall et al., 2019). While conventional ML is primarily concerned with developing algorithms that automatically learn conceptual representations from training data, IMT argues that learnable representations should directly come from human knowledge (Wall et al., 2019; Ng et al., 2020). This way, users feel more agency while maintaining a firmer grasp of what the model learns, making models more transparent and debuggable (Ramos et al., 2020; Kim and kyung Lim, 2021). IMT consists of three main stages that form a “teaching loop” (Ramos et al., 2020):\n\n[2] Mapping the Design Space of Teachable Social Media Feed Experiences D4: Seek structured and unstructured feedback.IMT workflows have traditionally been scaffolded with affordances that elicit structured feedback (e.g., data highlighting and labelling). Our findings in Section 4.2.4 and the range of both positive and negative feature-characteristic pairs in the taxonomy suggest the need for teachable algorithms that accept both structured and unstructured feedback in order to capture users’ nuanced evaluation of social media posts, which corroborates existing research in IMT. For example, Jörke et al. (Jörke et al., 2023) provided a form-like interface for specifying key teaching concepts in which some fields allow the selection of existing data tags while others allow the user to write freeform natural language. Likewise, Zhou et al. (Zhou and Yatani, 2022) noted the importance of relying on users’ unstructured object show-and-tell gestures in addition to a structured UI.This combination of structured and unstructured feedback is desirable in real-world applications as users may make nuanced judgements that cannot be captured with structured feedback alone, but also require some guidance to express concepts in a format parsable by an algorithmic learner. Parsingunstructured input, however, is now significantly less challenging due to technological advancements in large language models. We take this into consideration when balancing structured and unstructured feedback.\n\n[3] Mapping the Design Space of Teachable Social Media Feed Experiences Given this, we draw upon literature in interactive machine teaching (IMT) (Ramos et al., 2020; Ng et al., 2020; Wall et al., 2019) to chart out avenues for enabling teachable feed experiences111By “experiences,” we refer to the combination of in-feed interface affordances and underlying algorithmic behavior orchestrated by, or enabled using, those affordances. on social media. IMT proposes an interaction framework by which a user (the human teacher) without expertise in machine learning can train a model (the algorithmic learner) to accomplish desired tasks with limited amounts of pre-labelled data. Applications of IMT in social media settings, however, has been limited. To extend IMT’s framework to social media, we conducted a think-aloud study (N=24𝑁24N=24) with users from four feed-based social media platforms with diverse cultures and affordances—Instagram, Mastodon, TikTok, and Twitter222As of July 23, 2023, Twitter has been rebranded to X. Since the platform was still known as Twitter during the study, we will refer to it as so throughout the paper.—to answer the following question:\n\n[4] Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching Existing approaches to creating functional AR prototypes often rely on simple textual or visual programming.For example, many marker-based AR prototyping tools use block-based or node-based programming, such as ARCadia (Kellyet al., 2018), iaTAR (Leeet al., 2005), ComposAR (Seichteret al., 2008), RealityEditor (Heunet al., 2013), andStoryMakeAR (Glenn et al., 2020).Alternatively, trigger-action authoring, which is often used with simplified visual programming, allows users to create interactive behaviors by binding a trigger event with a corresponding action, as seen in ProGesAR (Ye and Fu, 2022), Situated Game-Level Editing (Nget al., 2018),MRCAT (Whitlock et al., 2020), and Aero (Inc., 2022a).In either case, most of these tools require the user to explicitly specify the desired trigger and action, which can be difficult to work with real-world interactions due to complexity and ambiguity.In contrast, our tool, while leveraging trigger-action authoring, allows the creation of interactive behaviors through physical user demonstration, similar to Rapido (Leiva et al., 2021), CAPturAR (Wanget al., 2020), and GesturAR (Wanget al., 2021b).Compared to these tools, however, our tool can support more flexible and open-ended demonstrations by leveraging interactive machine teaching.For example, while Rapido (Leiva et al., 2021) focuses on screen-based interactions, our tool allows the user to demonstrate tangible and physical interactions.This approach not only allows gesture (Wanget al., 2021b) or body-based interaction (Wanget al., 2020) but also supports a range of user-defined tangible, gestural, and context-driven interactions, such as object deformation, environment detection, and face recognition.While there may be a trade-off in tracking accuracy, our approach can significantly reduce the need for multiple different tools (Ashtari et al., 2020; Krauß et al., 2022) and fill the gap in the fragmented AR prototyping landscape (Nebeling andSpeicher, 2018).\n\n[5] Becoming the expert - interactive multi-class machine teaching If we are given a feature representation for our teaching set, one common approach for computing the similarity wi​jsubscript𝑤𝑖𝑗w_{ij} between two images uses an RBF kernelwi​j=exp(−γ∥𝐱i−𝐱j∥)22,w_{ij}=\\exp(-\\gamma{}\\lVert\\mathbf{x}_{i}-\\mathbf{x}_{j}\\rVert{}_{2}^{2}),(4)where γ𝛾\\gamma{} is a length scale parameter that controls how much neighboring images influence each other.Using matrix notation of , we define an N×C𝑁𝐶N\\times C matrix F=P^l​(y|𝐱,𝒟t)𝐹subscript^𝑃𝑙conditional𝑦𝐱subscript𝒟𝑡F=\\hat{P}_{l}(y\\,|\\,\\mathbf{x},\\mathcal{D}_{t}), where each element fi​c=P^l​(yi=c|𝐱i)subscript𝑓𝑖𝑐subscript^𝑃𝑙subscript𝑦𝑖conditional𝑐subscript𝐱𝑖f_{ic}=\\hat{P}_{l}(y_{i}=c\\,|\\,\\mathbf{x}_{i}).We can propagate information from the labels provided by the student for the teaching set, encoded as a𝒟t|×Csubscript𝒟𝑡𝐶|\\mathcal{D}_{t}|\\times C matrix Ftsubscript𝐹𝑡F_{t}, to the unlabeled images Fusubscript𝐹𝑢F_{u},Fu=(Su​u−Wu​u)−1​Wu​t​Ft,subscript𝐹𝑢superscriptsubscript𝑆𝑢𝑢subscript𝑊𝑢𝑢1subscript𝑊𝑢𝑡subscript𝐹𝑡F_{u}=(S_{uu}-W_{uu})^{-1}W_{ut}F_{t},(5)where S𝑆S is a diagonal matrix with entries si​i=∑jwi​jsubscript𝑠𝑖𝑖subscript𝑗subscript𝑤𝑖𝑗s_{ii}=\\sum_{j}w_{ij}.All entries in Ftsubscript𝐹𝑡F_{t} are 00, except where the human learner has estimated (correctly or incorrectly) the class label c𝑐c for teaching image 𝐱isubscript𝐱𝑖\\mathbf{x}_{i}, which we set to fi​c=1subscript𝑓𝑖𝑐1f_{ic}=1.Wu​usubscript𝑊𝑢𝑢W_{uu} is the similarity matrix for the unobserved images, a subset of the full matrix W𝑊W.As in , we can efficiently evaluate Equation (3) using standard matrix operations for datasets featuring 200020002000 images in under one second using unoptimized Python code.\n\n[6] Gesture-aware Interactive Machine Teaching with In-situ Object Annotations In addition, as mentioned in Section 8.2.3, future systems should study how to enable users to actively correct object highlights when they observe prediction errors.Although the segmentation annotation in LookHere allows users to choose appropriate frames for teaching, the role of users in this Human-AI collaboration is relatively passive.When users observe the failure case of object highlights, they should be given an opportunity to actively correct the error (Smith-Renner et al., 2020), instead of passively avoiding those data.Allowing such in-situ correction initiated by users can further empower the ability of IMT systems to “leverage human capabilities and human knowledge beyond labels” (Ramos et al., 2020), achieving better human-AI collaboration.\n\n[7] Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching This paper presents Teachable Reality, an augmented reality (AR) prototyping tool to create interactive tangible AR applications that can use arbitrary everyday objects as user inputs.When creating functional tangible AR prototypes, current tools often need to rely on markers-based inputs or pre-defined interactions (gesture, location, body posture, electronic devices, etc), which limits the flexibility, customizability, and generalizability of possible interactions.In contrast, Teachable Reality incorporates interactive machine teaching to immersive AR authoring, which captures the user’s demonstrated action as in-situ tangible input, by leveraging on-demand and user-defined compute vision classification.The user can use these classified inputs to create interactive AR applications based on an action-trigger programming model.We showcase various applications examples, which include tangible and deformable interfaces, context-aware assistants, augmented and situated displays and body-driven experiences.The results of our user study confirm the flexibility of our approach to quickly and easily create various tangible AR prototypes.\n\n[8] Becoming the expert - interactive multi-class machine teaching In this work we approximate the student’s conditional distribution given the teaching set, P^l​(y|𝐱,𝒟t)subscript^𝑃𝑙conditional𝑦𝐱subscript𝒟𝑡\\hat{P}_{l}(y\\,|\\mathbf{x},\\mathcal{D}_{t}), using graph based semi-supervised learning , 44.Using the Gaussian Random Field (GRF) semi-supervised method of , we can propagate the student’s estimate of the class labels for the current teaching set, 𝒟tsubscript𝒟𝑡\\mathcal{D}_{t}, to the unobserved images 𝒟usubscript𝒟𝑢\\mathcal{D}_{u} by defining a similarity matrix W∈ℝN×N𝑊superscriptℝ𝑁𝑁W\\in\\mathbb{R}^{N\\times{}N}.The benefit of using a graph based approach is that we do not need to work directly in feature space, and can instead use the similarity, wi​jsubscript𝑤𝑖𝑗w_{ij}, between image pairs.This gives us the flexibility of allowing similarity to be defined using feature vectors extracted from the images, human provided attributes, or using distance metric learning .\n\n"}
{"case_id": "e307068003936acfd8f82207c67e25bb", "answer_text": "\n\nQuadruped robots typically use 3 degrees of freedom (DoF) for each leg to achieve a balance between maneuverability, stability, and adaptability across various terrains. Each of these three DoFs is generally assigned to crucial joint movements: one for hip abduction/adduction (side swing), one for hip extension/flexion (front swing), and one for knee extension/flexion [0][1]. This configuration allows for a more comprehensive range of motion, mimicking the dynamic characteristics of biological organisms, which helps in performing complex locomotion tasks and adapting to unexpected obstacles.\n\nAdvantages of 3 DoF Legs [1]. \n\nThe primary advantage of having 3 DoF per leg is the improved maneuverability and stability it offers. For instance, the three DoF leg design allows for both hip and knee joints to articulate independently, providing greater flexibility and adaptability to various terrains [3]. This helps in maintaining balance and stability, which is particularly important for dynamic movements such as running or navigating uneven surfaces. Additionally, the added degree of freedom makes it easier for the robot to switch gaits and move in different directions efficiently [9]. This level of control is crucial for applications requiring intricate and precise movements, such as search and rescue missions or exploratory tasks in unstructured environments [0][2].\n\nHaving three DoF also enhances the robot’s ability to handle more complex tasks, providing better load distribution and improved walking stability. For instance, parallel-legged mechanisms that utilize three DoFs can distribute payload more efficiently across multiple actuators, thereby reducing the actuation torque and enhancing the robot's stability during locomotion [8]. This advantage is particularly crucial in applications requiring high mobility and stability under variable loads, such as carrying sensors or other equipment [1][5].\n\nDisadvantages of 2 DoF Legs: Reduced-DOF leg designs aim to utilize two or fewer active DOFs on each leg, which can significantly reduce the overall weight and control complexity of the robotic system. However, robots with such leg designs typically walk mainly over flat terrains and can only perform curved leg trajectories, generating body oscillations. More DOFs in leg morphology improve maneuverability and adaptation to challenging terrains. Additionally, two-DOF legs may not be flexible enough to turn, which can lead to overall instability of the robot [4][6].\n\nOn the other hand, using only 2 DoF per leg introduces significant limitations. A two DoF leg typically involves simpler movements confined to planar motions, which severely restricts the robot's ability to adapt to uneven terrains and complex tasks [1][4]. This configuration usually includes one DoF for hip extension/flexion and one for knee extension/flexion, but lacks the crucial abduction/adduction movement of the hip. This limitation results in a less flexible and less adaptive robot, making it difficult to navigate rough or inclined terrains effectively [1][4].\n\nFurthermore, two DoF legs tend to make the control and maneuvering of the robot more challenging, particularly in scenarios requiring dynamic stability and precise movements [10]. A robot with two DoF legs may struggle to maintain a constant body height during locomotion, leading to increased instability and the potential for tipping over, especially when the robot makes turns or encounters obstacles [14]. Although reducing the number of actuators can lead to lower energy consumption, it can degrade the performance as the robot might not be able to execute more complex maneuvers required in diverse operational scenarios [4][10].\n\n### Summary\n\nQuadruped robots, such as Spot, MIT mini-Cheetah, and others, use an elbow-up configuration for their limbs, consisting of a two-segmented structure for both front and hind legs, each with three Degrees of Freedom (DoF) [0]. These degrees include one for adduction-abduction and two for hip and knee flexion/extension, enabling better movement and stability over various terrains [0].\n\nIn summary, while two DoF leg designs can simplify control and reduce energy consumption, they fall short in providing the necessary flexibility and stability required for effective locomotion across varied terrains. The three DoF design offers a balanced solution, ensuring improved adaptability, maneuverability, and stability, which are crucial for the reliable performance of quadruped robots in complex and dynamic environments [6].\n\nIncorporating graphical representations or figures could further enhance the understanding of how each DoF contributes to the movement and flexibility of the robot [3].\n\n\nReferences:\n[0] ManyQuadrupeds: Learning a Single Locomotion Policy for Diverse Quadruped Robots Elbow-Up configuration for all limbs (3-DoF):This configuration, used in robots like Spot, MIT mini-Cheetah, A1, Go1, B1, Laikago, Aliengo, and Spot-micro, consists of a 2-segmented elbow-up for both front and hind legs. Each leg has three Degrees of Freedom (DoF): one for adduction-abduction and two for hip and knee flexion/extension. We solve the inverse kinematics analytically with constraints to an elbow-up configuration.\n\n[1] THREE DEGREE OF FREEDOM LEG DESIGN FOR QUADRUPED ROBOTS AND FRACTIONAL ORDER PID (PIλDμ) BASED CONTROL THREE DEGREE OF FREEDOM LEG DESIGN FOR QUADRUPED ROBOTS AND FRACTIONAL ORDER PID (PIλDμ) BASED CONTROL Quadruped robots are legged mobile robots that increase their popularity in robotic and control areas due to their complex dynamic structure with high mobility in different terrain conditions compared to wheeled systems. In this study; A 3-DoF linear leg model and its control are provided in order to enable quick and effectively simulate about on such subjects that walking planning, foot trajectory design and body stability control of the robot. A realistic physical model with parameters such as the dimensions, masses, inertia of limbs and the stiffness and damping values of joints is designed and simulated on Matlab/Simulink/Simscape environment. By taking into account the angular position ranges of the joints required for the robot to perform a standard step trajectory during the walk, the linear StateSpace model of the system (torque input-angular position output) is obtained using the linearization tools over the physical model. The unit step responses of the physical model are compared with the obtained linear model responses under constant torque input and it is understood to give similar results with small error values. Using the linear model, the angular position control of the system is achieved with PIλDμ controllers designed by selecting various parameters of fraction orders as comparatively the classical PID controller. Simulation results are presented and investigated. INTRODUCTION In the field of robotics, there are a growing interest in quadruped (four-legged) robots that mimic the dynamic characteristics of biological organisms. They can walk with agile\n\n[2] A New Metamorphic Parallel Leg Mechanism with Reconfigurable Moving Platform unstructured environment. Analysis of Degree of Freedom for Metamorphic Parallel Leg Mechanism To realize discontinuous contacts with three-dimensional terrains for a walking robot, a walking leg mechanism should have at least three spatial degrees of freedom, which moreover should include at least one prismatic degree of freedom in order to adapt to the terrain well and decrease foot-ground impact. A mammalian leg generally contains two rotational degrees of freedom around the hip joint and one translational degree of freedom. e leg with such 2R1T (where R denotes a rotational degree of freedom and T denotes a translational degree of freedom) degrees of freedom is easy to control and can walk quickly but is not flexible enough to turn; it therefore may lead to the overall instability of the robot. If the leg has one more rotational degree of freedom (the leg now has 3R1T degrees of freedom), the small side swing generated by the robot body during turning can be adjusted, thus improving walking flexibility and stability of the robot. Aiming at the requirements of degrees of freedom for walking robots in complex and variable environments, this paper proposes a new concept of metamorphic parallel leg mechanism with a reconfigurable moving platform (as the reconfigurable foot of the robot). e metamorphic parallel leg mechanism has two kinds of degrees of freedom and corresponding configuration states: when the robot walks on flat terrain, each leg adopts the configuration state of 2R1T, as shown in Figure 1; when it walks on complex terrain (such as\n\n[3] Design and Development of an Autonomous Quadruped Robot the quadruped. To understand the biological inspiration behind the quadruped robot, consider the locomotion and limb arrangement of a dog. A dog has mainly two bones in their hind legs, Femur and Tibla which helps in the movement of legs.Here, each leg of a dog gives 3 DOF and contains mainly 2 parts ignoring the feet as in doesn't help in the degrees of movement. This 3 DOF movement of the legs help it traverse in all terrains in all direction with proper gait patterns and also provides us with a better agility and maneuverability compared to other quadruped designs.This led us to developing a similar design for the quadruped with 4 legs with 2 main segments providing 3 degrees of freedom. The structure of leg design in dogs by the nature was rather complex and hard to achieve with perfection, which directed us to developing a similar, simpler structure with the capability and structural nature to follow the maneuverability requirement of the robot. Considering the leg of a dog in robotic terms a structure in which all 4 legs are symmetrical is developed in nature where, the upper hip joint provides the robot with the ability to turn in the lateral plain, helping in direction change. Whereas the lower hip and knee joints helps in locomotion of the robots according to gait patterns. The basic design of this mammal inspired quadruped robot consists of 4 limbs to hold the torso and assist in its movement. Each leg is a rigid 2 link mechanism with each\n\n[4] Reuleaux Triangle-Based Two Degrees of Freedom Bipedal Robot are other types of leg configuration, such as those using parallel mechanisms.) Therefore, a multi-legged robot usually requires 6 actuators, with being the number of leg pairs. If non-SPOC feet are implemented, even more actuators are required to control the additional DOFs from the feet, which increases the robot complexity and the overall energy consumption. To address the challenges of multiple DOFs leg mechanism, researchers have proposed the reduced-DOF leg designs which aim to utilize two or fewer active DOFs on each leg. This way, with fewer actuators, the overall weight and control complexity of the robotic system could be significantly reduced. For instance, Torige et al. developed a centipede-like robot that consists of six segments. Each segment utilizes four motors to control two legs. Hoffman et al. further promoted this concept by utilizing two passive revolute joints to couple the motion between two legs. This way, the two legs use only two linear actuators to extend the body, to lift the legs, and to drive the robot forward simultaneously. The RHex hexapod robot is another typical reduced-DOF leg design which utilizes six continuously rotating C-shaped legs to drive the robot and uses a differential drive to achieve steering. Yoenda et al. designed a quadruped robot with three active DOFs. This robot consists of a front and a rear section that can rotate about the orientation of the robot. The roll of each section, coupled with rotation of the U-shaped front and rear legs, allows the robot to move\n\n[5] Single-Leg Structural Design and Foot Trajectory Planning for a Novel Bioinspired Quadruped Robot us, the leg mechanism designed in this paper adopts an electric drive. To improve the stability and movement ability of the quadruped robot, the leg of the quadruped robot designed in this paper adopts a knee and elbow face-to-face structure. e quadruped robot model is shown in Figure 1 and includes the machine body, waist structure, leg structure, and electrical components. e leg mechanism includes three degrees of freedom (DOF): hip joint side swing, hip joint front swing, and knee joint front swing. Common leg structural models of quadruped robots include zoom structures and linkage structures. e zoom structure leg is highly efficient, and the control strategy is relatively simple, but it is often bulky and does not have bionic characteristics; therefore, it is inconvenient to install and is not conducive to robot miniaturization. For example, the ASV robot developed by the Ohio State University adopts the zoom structure; its ratio of self-weight to bearing weight is only 0.08, and the performance of the whole machine is poor. Linkage structure legs can be categorized as series types, parallel types, and parallel-series types. A series leg is simple in structure, easy to control, and has a large working space, but its load carrying capacity is relatively small, and installing the drive unit or transmission element at the joint increases the inertia of the leg, such as the ANYmal robot developed by ANYbotics. Inspired by a cat's leg structure, Park et al. designed a single-DOF parallel leg mechanism composed of 9 connecting rods and\n\n[6] Insect-Inspired Robots: Bridging Biological and Artificial Systems over locomotion studies. With two DOFs per leg, a simplified hexapod robot can be built. This choice is a good compromise between energetic cost and mobility. However, this type of robot walks mainly over flat terrains and can only perform curved leg trajectories, generating body oscillations. Previously, insect-based hexapod robots were built with three DOFs per leg (Table 1, in which the trochanter segment joint is merged with the femur and each joint only comprises one DOF, see Figure 2). Reflecting the standard insect leg model, this type of leg permits roaming in a slightly rough or slopped terrain in addition to a flat terrain walk. More DOFs in leg morphology improve maneuverability and adaptation to challenging terrains,60. Additional actuators help to adjust robot orientation according to the slope in order to increase stability. Experiments have shown that 4 or 5 joints per leg enable robots to cope with high gradient slopes in any orientation (e.g., up to 50 ○ slopes, see,61, or up to 43 ○ slopes, see). Unfortunately, these improvements increase the level of complexity of control commands and the robot's price and weight (Table 2), they also concomitantly, reduce autonomy due to the high power consumption of numerous actuators. To sum up, from the large number of robots based on three-DOF legs, this appears to be the right compromise to walk on a flat terrain. Despite the three DOFs per leg trend, from a biological point of view, an insect possesses more than three joints with one\n\n[7] The analysis of new 3 DOF parallel delta type leg for a quadruped robot to maximize actuation torque reduction The analysis of new 3 DOF parallel delta type leg for a quadruped robot to maximize actuation torque reduction The legged type robot generally requires high torques for stable walking and running. However, the robot's electric motor driven actuators usually have a low payload to weight ratio. To overcome this problem and use actuators' power more efficiently, we apply a parallel mechanism to a robot's leg design. A parallel mechanism can distribute a payload over actuators and reduce the actuation torque, and thus enhance the robot's walking stability. The process of optimal design of 3 DOF parallel delta type leg is 1) definition of workspace and payload based on tasks, such as walking and climbing a stair, 2) derivation of inverse kinematics and equation of motion, 3) optimization of a robot leg's mechanism, and finally 4) feasibility of the 3 DOF Parallel Delta type Leg (3PDL) for a quadruped robot compared with the 3 DOF Serial type Leg (3SL). In this paper, it is shown that the optimal 3PDL has a strong tendency of greatly reducing the maximum actuation torque and enhancing the manipulability in the certain workspace.\n\n[8] Learning Active Spine Behaviors for Dynamic and Efficient Locomotion in Quadruped Robots The robot is designed as three modules: two body modules and one spine module. The body modules are connected via spine, as shown in Fig. 2a.The overall size and form factor of the robot is similar to Stoch.Each body module is composed of two legs.A single leg contains three degrees of freedom.Each of them corresponds to the flexor and extension movement of hip, knee and abduction movements. However, the simulation model uses only hip and knee motion while keeping the abduction locked in position.Each leg comprises of a five bar linkage mechanism, where two of the links remain actuated. This enables the leg to follow a given trajectory in a plane.The central spine is designed as a serial 222 DOF mechanism. Each of the spine joint is actuated.Overall, the robot simulation model consists of 101010 actuated degrees of freedom, including four legs and the spine.The key specifications of the simulation model are summarized in Table I.\n\n[9] Reuleaux Triangle-Based Two Degrees of Freedom Bipedal Robot forward. Furthermore, single-DOF crank driven mechanisms,, and two-DOF legs that generate an approximately straightline support phase foot trajectories,23 have been proposed for the construction of legged robots for simplified control and design complexity. To build an easy-to-use and light legged platform for tail research purposes, the authors also proposed a reduced-DOF leg mechanism named Robotic Modular Leg (RML),25, which has two DOFs per leg and can be implemented in a quadruped or biped robot in a modular manner. The \"modular\" in this context refers to the mechanical modules inside a robot, which is different from the more generalized \"modular\" concept that many independent modular robots constitute a larger robotic system. The RML leg mechanism applied decoupled actuation to simplify control and used a double parallelogram mechanism to keep the foot parallel to the robot chassis. However, this design still requires two actuators, which is too heavy and too complicated (in terms of foot position planning) for the tail research. Therefore, looking for a single DOF leg mechanism that is able to stably walk and steer becomes a practical requirement. However, by reviewing the literature, most existing single DOF leg mechanisms (such as the RHex robot that was frequently used in robotic tail research) are not able to maintain a constant body height, which induces instabilities during locomotion and thus is less attractive for our purpose. Therefore, motivated by looking for a single DOF leg mechanism that can stably walk and steer, together with three additional requirements based on Kaneko,\n\n[10] Reuleaux Triangle-Based Two Degrees of Freedom Bipedal Robot Reuleaux Triangle—Based Two Degrees of Freedom Bipedal Robot This paper presents the design, modeling, analysis, and experimental results of a novel bipedal robotic system that utilizes two interconnected single degree-of-freedom (DOF) leg mechanisms to produce stable forward locomotion and steering. The single DOF leg is actuated via a Reuleaux triangle cam-follower mechanism to produce a constant body height foot trajectory. Kinematic analysis and dimension selection of the Reuleaux triangle mechanism is conducted first to generate the desired step height and step length. Leg sequencing is then designed to allow the robot to maintain a constant body height and forward walking velocity. Dynamic simulations and experiments are conducted to evaluate the walking and steering performance. The results show that the robot is able to control its body orientation, maintain a constant body height, and achieve quasi-static locomotion stability. Introduction To mimic the most commonly observed locomotion form in nature, legged robots have been a popular topic since the early days of robotic research. Leg designs with more active degrees of freedom (DOF) help to enhance the mobility and dexterity of legged robots, such as the ANYmal quadruped robot, the MIT Cheetah series quadruped robots,4, the Adaptive Suspension Vehicle, the ATRIAS robot, and the HyQ quadruped robot. These robots utilize multiple DOFs per leg to position their (primarily) single-point-of-contact (SPOC) feet. The typical configuration of this type of legs uses three DOFs per leg,9, namely, one hip abduction/adduction DOF, one hip extension/flexion DOF, and one knee extension/flexion DOF (note that there\n\n"}
{"case_id": "880129a65489610c5c805e174e256072", "answer_text": "\n\nA suitable theoretical lens for qualitative research concerning Robotic Process Automation (RPA) implementations can be derived from multiple domains, such as knowledge management, organizational change, and technology adoption theories [2]. Each of these domains offers distinct perspectives that can be particularly insightful for understanding the multifaceted impacts of RPA on business processes and organizational structures [7].\n\nKnowledge Management and Collaboration\n\nWith increasing demand for digitization and automation, enterprises are seeking to leverage the emerging technology of robotic process automation (RPA) to enhance business process handling [0]. Specifically, AI algorithms are fundamentally changing the nature of work, challenging humans in knowledge work [2]. Accordingly, literature on the quantitative effects of RPA is scarce [14].\n\n\n\n1. **Knowledge Embodiment**: One theoretical lens that is particularly relevant is the theory of knowledge embodiment, which provides a framework for understanding how humans and machines collaborate to perform tasks. This theory encompasses cognitive stages such as cognitive reasoning, collaborating, scaffolding, and extending, which help conceptualize the process of knowledge work transformation when AI technologies like RPA are implemented [2]. By adopting this lens, researchers can investigate how RPA affects knowledge work, how employees adapt to new automated workflows, and how the cognitive tasks are distributed between humans and machines.\n\nStakeholder and Public Sector Perspectives [9].\n\n2. **Stakeholder Management and Public Value Creation**: Another useful theoretical framework comes from the e-government domain, focusing on stakeholder management and public value creation [9]. This lens is particularly beneficial in public sector RPA implementations, as it emphasizes the importance of stakeholder engagement and the creation of public value. By applying this framework, researchers can explore how different stakeholders perceive the value generated by RPA, how it impacts public services, and how it aligns with broader organizational goals.\n\n### Process Optimization and Metrics\n\nProcess optimization and metrics are crucial in the successful implementation of Robotic Process Automation (RPA) initiatives. Understanding the specific processes that are most suitable for automation can significantly enhance efficiency and productivity [4]. Several studies emphasize the importance of utilizing systematic evaluation models to identify RPA-suitable business processes [0][1]. These models often incorporate a range of criteria, from process flow and logic to performance indicators like the \"automation rate\" [4]. Additionally, empirical research supports the notion that productivity and efficiency perspectives should guide the initial implementation of RPA solutions, focusing on tasks that require the most time and resources to complete [4]. Organizations are increasingly adopting RPA not just to replace manual labor, but to achieve higher performance and better quality outcomes through optimized and automated workflows [5][10]. However, the quantitative effects of RPA can vary, and it is important to assess both improvements and potential deteriorations in business processes [14].\n\n\n\n3. **Process Mining and Performance Metrics**: Process mining techniques also provide a valuable theoretical perspective for RPA research. This approach helps identify the most suitable processes for automation by analyzing event logs and discovering common patterns in business processes [4]. The performance metrics derived from process mining, such as the \"automation rate\" and task completion times, can serve as concrete criteria for evaluating the effectiveness of RPA implementations. This lens supports a data-driven approach to selecting and optimizing processes for automation and measuring the impact on productivity and efficiency.\n\n### Technology Adoption [0][4][5]\n\n\n\n4. **Technology Adoption and Diffusion Theories**: Theories like the Technology Acceptance Model (TAM) and Diffusion of Innovations (DOI) can also be applied to RPA research to understand how organizations adopt and diffuse new automation technologies. These theories provide insights into the factors influencing the acceptance and successful implementation of RPA, such as perceived ease of use, perceived usefulness, organizational readiness, and the roles of champions and change agents within the organization [1].\n\nIndustry-Specific Analysis\n\n5. **Value Chain Analysis**: Applying a value chain analysis framework can also be helpful in understanding the potential benefits and impacts of RPA within specific industry sectors, such as healthcare [5]. This analysis examines the specific functions and processes within an organization that can be enhanced through RPA, identifying areas for cost savings, productivity gains, and quality improvements. It allows researchers to map out the value-creating activities and how RPA can optimize these activities.\n\nComparative Analysis\n\nTo determine the most suitable theoretical lens for a specific RPA research project, it is essential to compare their relative strengths and limitations [14].\n\n- **Knowledge Embodiment**: Best for examining the interplay between human and machine cognitive tasks and the transformation of knowledge work [2]. However, it might be less effective for understanding specific organizational or stakeholder perspectives.\n\n  \n- **Stakeholder Management and Public Value Creation**: Particularly useful in public sector contexts. It excels at assessing stakeholder perspectives and public value but may not address detailed process efficiencies as effectively as other models [9]. \n\n- **Process Mining and Performance Metrics**: Ideal for data-driven analysis of process efficiencies and identifying automation candidates [4]. However, it may not provide deep insights into human factors or stakeholder engagement [2]. \n\n\n- **Technology Adoption and Diffusion Theories**: Useful for understanding the overall adoption process of RPA within organizations and factors influencing acceptance [8]. It might not offer detailed process-level insights or stakeholder-specific impacts.\n\n\n- **Value Chain Analysis**: Highly effective for industry-specific evaluations and identifying value-creating activities [5]. It may not address the broader impacts on knowledge work or stakeholder perspectives as thoroughly [14].\n\n\n\n\n### Conclusion\n\nRobotic Process Automation (RPA) is the automation of rule-based routine processes to increase process efficiency and to reduce process costs [14]. In practice, however, RPA is often applied without knowledge of the concrete effects its introduction will have on the automated process and the involved stakeholders [14]. Accordingly, literature on the quantitative effects of RPA is scarce [14]. The objective of this paper is to provide empirical insights into improvements and deteriorations of business processes achieved in twelve RPA projects in the automotive industry. The results indicate that the positive benefits promised in literature are not always achieved in practice [14]. In particular, shorter case duration and better quality are not confirmed by the empirical data gathered in the considered RPA projects [14]. These quantitative insights constitute a valuable contribution to the currently rather qualitative literature on RPA [14].\n\n\n\nIn conclusion, selecting the appropriate theoretical lens for qualitative research on RPA implementations depends on the specific research questions and the context of the study. For instance, if the focus is on understanding human-machine interaction and cognitive task distribution, the theory of knowledge embodiment would be most suitable [2]. In contrast, if the focus is on public sector implementations and stakeholder engagement, the stakeholder management and public value creation framework would be more appropriate [9]. \n\nA comprehensive understanding can often be achieved by using a combination of these theories, as it allows for a multifaceted exploration of the impacts of RPA on organizational processes, stakeholder engagement, and overall performance [7]. Researchers should thus carefully consider their specific objectives and the context of their study to select the most appropriate theoretical lenses [7]. \n\n\nReferences:\n[0] Robotic Process Automation: Developing a Multi-Criteria Evaluation Model for the Selection of Automatable Business Processes With increasing demand for digitization and automation, enterprises are seeking to leverage the emerging technology of robotic process automation (RPA) to enhance business process handling. However, confronted with a multitude of processes, decision-makers face the challenge of selecting and prioritizing business processes that are best suited for RPA. Hence, the objective of this research is the development of a process evaluation model to identify RPA-suitable business processes. Therefore, results from a systematic literature research and six qualitative expert interviews are combined. In conclusion, a threestep evaluation model with 14 selection criteria is derived. This model is then evaluated using two case studies. Our research contributes to theory by summarizing the state of research on RPA as depicted in 25 relevant publications. Furthermore, practitioners can apply our results to make better decisions in the selection of suitable business processes and thus, profit the most from the technology.\n\n[1] Robotic Process Automation: Developing a Multi-Criteria Evaluation Model for the Selection of Automatable Business Processes Robotic Process Automation: Developing a Multi-Criteria Evaluation Model for the Selection of Automatable Business Processes With increasing demand for digitization and automation, enterprises are seeking to leverage the emerging technology of robotic process automation (RPA) to enhance business process handling. However, confronted with a multitude of processes, decision-makers face the challenge of selecting and prioritizing business processes that are best suited for RPA. Hence, the objective of this research is the development of a process evaluation model to identify RPA-suitable business processes. Therefore, results from a systematic literature research and six qualitative expert interviews are combined. In conclusion, a threestep evaluation model with 14 selection criteria is derived. This model is then evaluated using two case studies. Our research contributes to theory by summarizing the state of research on RPA as depicted in 25 relevant publications. Furthermore, practitioners can apply our results to make better decisions in the selection of suitable business processes and thus, profit the most from the technology.\n\n[2] Knowledge Embodiment of Human and Machine Interactions: robotic-Process-Automation at the Finland Government Artificial intelligence (AI) technologies are fundamentally changing the nature of work. Specifically, AI algorithms are challenging the humans in knowledge work. Therefore, while enjoying the benefits of AI, many organisations face difficulties managing knowledge work performed together by humans and machines. To date, there is a lack of empirical research on how organisations manage the transformation of knowledge work when adopting AI technologies. With the aim of delving into this emerging phenomenon, we conducted an in-depth case study at the Finnish government shared services centre during their implementation of an AI-centric robotic process automation (RPA) technology. To theorise how humans and machines work together, we adopted a theory of knowledge embodiment. Based on our initial qualitative interpretive analysis of the case data, we identified four cognitive stages: cognitive reasoning, cognitive collaborating, cognitive scaffolding, and cognitive extending. Moving forward, we deepen our understanding of how these stages form a process of knowledge embodiment. Our research aims to contribute to theory by conceptualising the knowledge embodying process in the future of work, meantime extending the theory of knowledge embodiment. We contribute to practice by providing implications on how humans and machines perform knowledge work together in organisations where AI technologies are used.\n\n[3] Robotic process automation using process mining - A systematic literature review and how robots need to adapt to an alternating business environment. The \"automation rate\" can be added as a performance indicator to quantify the RPA initiative. Halaška and Šperka (2020) focused on process flow and logic criteria for implementing an RPA solution. In particular the recommendations made for RPA implementation are based on common patterns that are discovered using the process flow and the login criteria for a certain process. Their approach also focuses on discovering business processes using process mining techniques based on the event logs collected from information systems. This helps find common patterns in a process that are suitable for RPA. They focused on measuring the productivity and efficiency of tasks without RPA automation that are affected by the resource allocations and the specific type of human resources assigned for each task. They measured the time it takes to complete a task in the common patterns based on the quantity of resources allocated for such a task. They concluded that from productivity and efficiency perspectives, it is recommended to start implementing RPA solutions for tasks that take the most amount of time to be completed. The analysts should be considering automating first the common patterns that maximize productivity and efficiency. Geyer-Klingeberg et al. (2018) also highlight best practices for a successful RPA implementation where process mining can accelerate and improve this approach, taking into consideration that not all processes are suitable for RPA: 1) Selecting the appropriate process use case that can be automated. 2) Standardizing the processes before automation. 3) Prioritizing the\n\n[4] Robotic Process Automation - Creating Value by Digitalizing Work in the Private Healthcare? Organizations are applying digitalization to the constantly increasing amounts of different organizational processes. The healthcare sector is also changing and actively seeking better ways to enhance performance, especially in the private healthcare sector. Automation of workflow processes, e.g., Robotic Process Automation (RPA), in organizations has been emerging as a solution to this demand, 4. To meet this clear demand, automation of workflow processes in organizations has been a rising trend during the past few years. We analyze the value creating functions of the RPA potential in the private healthcare industry sector, using modified Walter et al.'s function-oriented value analysis as our theoretical lens for identifying the potential of RPA.\n\n[5] The use of theory in qualitative research: Challenges, development of a framework and exemplar. Whilst theoretical grounding is considered important for sound research methodology, consensus on the application of theory in qualitative research remains elusive. Novice researchers may experience challenges in applying theory in qualitative research and these may contribute to the under-use, over-reliance or inappropriate application of theory. Practical guidance on how theory can inform and guide the conduct of qualitative research is needed. The purpose of this paper was to propose a framework for the application of theory in qualitative research and provide an exemplar. The Theoretical Application Framework for Qualitative Studies (TAF-QS) was developed from the synthesis of existing literature and the authors' own experience of the application of theory. The TAF-QS encourages researchers to articulate which theoretical framework or conceptional framework they are drawing on and how this will be applied by reflecting on the purpose and the context of the study. The TAF-QS provides support for researchers to explore how theory can be applied and how to achieve this in qualitative research. The use of theory in qualitative research.\n\n[6] Robotic Process Automation: A Scientific and Industrial Systematic Mapping Study The automation of robotic processes has been experiencing an increasing trend of interest in recent times. However, most of literature describes only theoretical foundations on RPA or industrial results after implementing RPA in specific scenarios, especially in finance and outsourcing. This paper presents a systematic mapping study with the aim of analyzing the current state-of-the-art of RPA and identifying existing gaps in both, scientific and industrial literature. Firstly, this study presents an in-depth analysis of the 54 primary studies which formally describe the current state of the art of RPA. These primary studies were selected as a result of the conducting phase of the systematic review. Secondly, considering the RPA study performed by Forrester, this paper reviews 14 of the main commercial tools of RPA, based on a classification framework defined by 48 functionalities and evaluating the coverage of each of them. The result of the study concludes that there are certain phases of the RPA lifecycle that are already solved in the market. However, the Analysis phase is not covered in most tools. The lack of automation in such a phase is mainly reflected by the absence of technological solutions to look for the best candidate processes of an organization to be automated. Finally, some future directions and challenges are presented.\n\n[7] Exploring the Use of Robotic Process Automation in Local Government Exploring the Use of Robotic Process Automation in Local Government This paper presents an ongoing research project designed to map the current use of Robotic Process Automation (RPA) in local government. The project runs over three years (20202022). The empirical foundation consists of multiple qualitative and interpretative case studies, conducted in close cooperation with practitioners working in a set of Swedish municipalities. The theoretical foundation rests on previous work on stakeholder management and public value creation in the e-government domain. Through this project we ultimately strive to develop an analytical tool that can be used by researchers and practitioners to decide if, and to what degree, a specific case handling process can (and should) be automated. In this paper, the background and overall design of the project is presented, together with a discussion on our preliminary and expected findings.\n\n[8] Novel Approach for Robotic Process Automation with Increasing Productivity and Improving Product Quality using Machine Learning Novel Approach for Robotic Process Automation with Increasing Productivity and Improving Product Quality using Machine Learning Robotic Process Automation (RPA) is one of the smartest technology evolutions in recent years. It is, a software installed on a system. RPA can be implemented in a well-defined environment with defined procedures and clarity with reference to decision making. RPA’s limitation is that it cannot be automated if it involves decision making supported by knowledgebased application. Highly invasive and intertwined supply chains are now confronted by producers, which reduce manufacturing life cycles and raise product sophistication. You therefore sense the need, at all stages of value formation, to change and adjust more rapidly. The theory of self-optimization is a positive method to coping with uncertainty and unexpected delays within supply chains, devices and processes. It would also boost manufacturing industries' stability and productivity. This paper explores the idea of development processes that are self-optimized. Following a quick historical analysis and understanding the particular needs, specifications and self-optimizing criteria of the various stages of value generation from supply chain planning and management to manufacture and assembly. Examples at both stages are used to demonstrate the self-optimization principle and to explain its simplicity and efficiency ability.. We proposed Novel approach for Robotic Process Automation with increasing productivity and improving product quality using machine learning\n\n[9] Towards Quantifying the Effects of Robotic Process Automation Towards Quantifying the Effects of Robotic Process Automation Robotic Process Automation (RPA) is the automation of rule-based routine processes to increase process efficiency and to reduce process costs. In practice, however, RPA is often applied without knowledge of the concrete effects its introduction will have on the automated process and the involved stakeholders. Accordingly, literature on the quantitative effects of RPA is scarce. The objective of this paper is to provide empirical insights into improvements and deteriorations of business processes achieved in twelve RPA projects in the automotive industry. The results indicate that the positive benefits promised in literature are not always achieved in practice. In particular, shorter case duration and better quality are not confirmed by the empirical data gathered in the considered RPA projects. These quantitative insights constitute a valuable contribution to the currently rather qualitative literature on RPA.\n\n"}
{"case_id": "050725a9a16e6694b491f735c63ace96", "answer_text": "\n\nWhen fabricating industrial robotic grippers, several key considerations come into play for material selection. These considerations are crucial as they directly influence the gripper's performance, versatility, and application suitability [0]. \n\nMaterial Considerations [0]\n\n1. **Load Capacity and Strength**: One of the primary considerations is the material's load capacity, which directly impacts the gripper's ability to handle heavy objects. Rigid materials, such as metals and hard plastics, inherently offer high load capacities and strength, making them suitable for lifting and manipulating heavy and robust items [4]. On the other hand, soft grippers made from materials like silicone elastomers and thermoplastic polyurethanes (TPUs) are generally less capable of providing the same degree of load-bearing strength due to their flexibility and compliance [6].\n\n2. **Flexibility and Compliance**: The flexibility of a material is another critical aspect. Soft materials provide a high degree of flexibility and compliance, which allows the gripper to conform to various shapes and sizes of objects. This is particularly beneficial when handling delicate, irregularly-shaped, or fragile items without causing damage [1][10]. Conversely, rigid materials offer limited flexibility, which can restrict the gripper’s ability to adapt to different object geometries [5].\n\n3. **Safety and Human Interaction**: In environments where human-robot interaction occurs, the safety provided by soft materials is a significant advantage. Soft robotic grippers are less likely to cause injury upon contact with humans due to their compliant nature [14]. Rigid grippers, while strong, pose a higher risk of injury and thus might require additional safety mechanisms [10].\n\n4. **Environmental Compatibility**: Material selection must also consider the operational environment, including factors like humidity, temperature, and dust. Soft materials, particularly those used in pneumatic or fluidic elastomer actuators, can exhibit variable performance under different environmental conditions [14]. Conversely, rigid materials tend to have more stable performances across varied environments but might be less adaptable to sudden changes [14].\n\n5. **Integration of Sensing Elements**: The ability to embed sensors within the gripper material is another important consideration. Soft materials can incorporate stretchable sensors to provide tactile feedback, enhancing the gripper's interaction with objects [3][12]. Such integration is more challenging in rigid materials, although advancements are being made in embedding sensors within these harder structures [3].\n\nAdvantages and Disadvantages of Soft vs. Rigid Gripper Materials.\n\nSoft grippers have experienced a growing interest due to their considerable flexibility that allows them to grasp a variety of objects, in contrast to hard grippers, which are designed for a specific item [14]. One of their most remarkable characteristics is the ability to manipulate soft objects without damaging them [14]. This, together with their wide range of applications and the use of novel materials and technologies, renders them a very robust device [14]. However, compared to rigid robotic grippers, the load capacity of soft robotic grippers is relatively low [10]. Controlling the stiffness of soft grippers is a method that can be implemented to overcome these drawbacks [10]. Soft grippers based on variable stiffness have some of the advantages of both rigid and soft robotic grippers, including flexibility and compliance in the low-stiffness state, as well as higher structural rigidity and load acceptance in the high-stiffness state [10]. A stiffness-controllable structure is usually integrated into grippers as a module. In the low-stiffness state, adaptive grasping is easier to implement, while in the high-stiffness state, the gripper can effectively generate sufficient holding force [10]. \n\nOn the other hand, traditional rigid grippers are based on stiff materials, enabling end effectors with high load capacity and precision for industrial applications [4]. Recent advances in soft robotics have led to a proliferation of novel gripper designs with increased conformability to accommodate objects of varying shape, size, material, and surface properties, allowing for grippers that can safely manipulate a wide range of objects [4]. While compliant materials offer noted advantages for robotic grasping, their ability to deform limits their load capacity [4]. Therefore, stiffness selection is critical in gripper design, and the use of materials with tunable stiffness can be exploited for new functionality [4]. Combining both rigid and soft elements into the robot's fingers can create a gripper that leverages the benefits of both designs [2]. This allows for a more versatile and capable gripper, able to handle a wider array of objects than a purely rigid or purely soft gripper could manage alone [2]. \n\nSoft Gripper Materials: Advances in soft robotics, materials science, and stretchable electronics have enabled rapid progress in soft grippers [0]. Here, a critical overview of soft robotic grippers is presented, covering different material sets, physical principles, and device architectures [1]. Soft gripping can be categorized into three technologies, enabling grasping by: a) actuation, b) controlled stiffness, and c) controlled adhesion [2]. A comprehensive review of each type is presented. Compared to rigid grippers, end-effectors fabricated from flexible and soft components can often grasp or manipulate a larger variety of objects [0]. Such grippers are an example of morphological computation, where control complexity is greatly reduced by material softness and mechanical compliance [1]. Advanced materials and soft components, in particular silicone elastomers, shape memory materials, and active polymers and gels, are increasingly investigated for the design of lighter, simpler, and more universal grippers, using the inherent functionality of the materials [0]. Embedding stretchable distributed sensors in or on soft grippers greatly enhances the ways in which the grippers interact with objects [3]. Challenges for soft grippers include miniaturization, robustness, speed, integration of sensing, and control [0]. Improved materials, processing methods, and sensing play an important role in future research [4]. \n\n\n- **Advantages**: Compared to rigid grippers, end-effectors fabricated from flexible and soft components can often grasp or manipulate a larger variety of objects. Such grippers are an example of morphological computation, where control complexity is greatly reduced by material softness and mechanical compliance [0]. Advanced materials and soft components, in particular silicone elastomers, shape memory materials, and active polymers and gels, are increasingly investigated for the design of lighter, simpler, and more universal grippers, using the inherent functionality of the materials [1]. Embedding stretchable distributed sensors in or on soft grippers greatly enhances the ways in which the grippers interact with objects [3].\n\n\n  - **Flexibility and Conformity**: Soft materials can easily conform to various object shapes, which is useful for handling irregular and delicate items without causing damage [1][10].\n  - **Safety**: Due to their compliance, soft grippers are safer for human interaction and reduce the risk of injury [14].\n  - **Sensor Integration**: Soft materials can integrate stretchable and flexible sensors, providing valuable tactile feedback during operations [3][12].\n\n\n- **Disadvantages**: Compared to rigid robotic grippers, the load capacity of soft robotic grippers is relatively low [10]. This limitation is due to their inherent compliance and flexibility, which makes it difficult for them to exert large forces [2].\n\n  - **Limited Load Capacity**: Soft materials generally have lower load capacities compared to rigid materials, which limits their use in applications requiring high strength [4][10].\n  - **Environmental Sensitivity**: The performance of soft materials can be more susceptible to environmental conditions, such as humidity and temperature changes [14].\n\n\n\nRigid Gripper Materials: Traditionally, robotic grippers are based on stiff materials, enabling end effectors with high load capacity and precision for industrial applications [4].\n\n\n- **Advantages**: Soft robotic grippers exhibit several advantages over their rigid counterparts. Notably, their flexibility and compliance enable them to grasp objects with varying shapes and sizes without causing damage [0]. They are particularly beneficial in applications requiring the handling of delicate items, as the inherent softness and mechanical compliance reduce control complexity [1]. Moreover, advancements in materials science have led to the development of lighter, simpler, and more adaptable grippers using materials such as silicone elastomers and shape memory materials [1]. Embedding stretchable sensors in soft grippers also enhances their interaction with objects [1].\n  - **High Load Capacity**: Rigid materials provide the necessary strength for handling heavy objects, making them suitable for industrial applications requiring high load-bearing capabilities [4][6].\n  - **Stability**: Rigid materials typically exhibit stable performance across diverse environmental conditions, ensuring reliability [14].\n  \n- **Disadvantages**:\n[0][1][4][10][13][14]\n\n  - **Lack of Flexibility**: Rigid materials cannot easily adapt to different shapes and sizes of objects, potentially causing damage to delicate or irregularly-shaped items [5][10].\n  - **Safety Concerns**: Rigid grippers pose a higher risk of injury in human-robot interactions and require additional safety measures [10].\n\n\nHybrid approaches that combine rigid and soft elements in robotic grippers have been developed to address the limitations of both purely rigid and purely soft designs [2]. These hybrid designs leverage the strengths of both rigid and soft components, enabling manipulation of a wider range of objects, from small and irregular to large and heavy [13]. For example, recent designs integrate soft adhesives with rigid structures, allowing for diverse grasping capabilities [9]. Combining these elements can enhance the gripper’s versatility and load capacity, as soft materials offer flexibility and adaptability while rigid elements provide structural support and the ability to exert greater forces [5].\n\n\nTo address the limitations of both types of materials, hybrid grippers combining rigid and soft elements are being developed. These grippers aim to leverage the strengths of both materials, thus enhancing versatility and performance. For example, the integration of soft, adhesive sheets with rigid grippers allows for adaptable, safe, and strong gripping capabilities, expanding the range of manageable objects [5][13].\n\nConclusion\n\nAdvances in soft robotics, materials science, and stretchable electronics have enabled rapid progress in soft grippers [0]. Here, a critical overview of soft robotic grippers is presented, covering different material sets, physical principles, and device architectures [0]. Soft gripping can be categorized into three technologies, enabling grasping by: a) actuation, b) controlled stiffness, and c) controlled adhesion [0]. Compared to rigid grippers, end-effectors fabricated from flexible and soft components can often grasp or manipulate a larger variety of objects [0]. Such grippers are an example of morphological computation, where control complexity is greatly reduced by material softness and mechanical compliance [0]. Advanced materials and soft components, in particular silicone elastomers, shape memory materials, and active polymers and gels, are increasingly investigated for the design of lighter, simpler, and more universal grippers, using the inherent functionality of the materials [0]. Embedding stretchable distributed sensors in or on soft grippers greatly enhances the ways in which the grippers interact with objects [0]. Challenges for soft grippers include miniaturization, robustness, speed, integration of sensing, and control [0]. Improved materials, processing methods, and sensing play an important role in future research [0]. \n\nThe choice between soft and rigid materials for robotic grippers depends on the specific application requirements, including load capacity, flexibility, safety, environmental factors, and the integration of sensing capabilities [0]. Hybrid designs present a promising direction, combining the benefits of both material types to achieve superior performance and adaptability in industrial robotic applications [2]. \n\n\nReferences:\n[0] Soft Robotic Grippers. Advances in soft robotics, materials science, and stretchable electronics have enabled rapid progress in soft grippers. Here, a critical overview of soft robotic grippers is presented, covering different material sets, physical principles, and device architectures. Soft gripping can be categorized into three technologies, enabling grasping by: a) actuation, b) controlled stiffness, and c) controlled adhesion. A comprehensive review of each type is presented. Compared to rigid grippers, end-effectors fabricated from flexible and soft components can often grasp or manipulate a larger variety of objects. Such grippers are an example of morphological computation, where control complexity is greatly reduced by material softness and mechanical compliance. Advanced materials and soft components, in particular silicone elastomers, shape memory materials, and active polymers and gels, are increasingly investigated for the design of lighter, simpler, and more universal grippers, using the inherent functionality of the materials. Embedding stretchable distributed sensors in or on soft grippers greatly enhances the ways in which the grippers interact with objects. Challenges for soft grippers include miniaturization, robustness, speed, integration of sensing, and control. Improved materials, processing methods, and sensing play an important role in future research.\n\n[1] Soft Robotic Grippers Soft Robotic Grippers Advances in soft robotics, materials science, and stretchable electronics have enabled rapid progress in soft grippers. Here, a critical overview of soft robotic grippers is presented, covering different material sets, physical principles, and device architectures. Soft gripping can be categorized into three technologies, enabling grasping by: a) actuation, b) controlled stiffness, and c) controlled adhesion. A comprehensive review of each type is presented. Compared to rigid grippers, end‐effectors fabricated from flexible and soft components can often grasp or manipulate a larger variety of objects. Such grippers are an example of morphological computation, where control complexity is greatly reduced by material softness and mechanical compliance. Advanced materials and soft components, in particular silicone elastomers, shape memory materials, and active polymers and gels, are increasingly investigated for the design of lighter, simpler, and more universal grippers, using the inherent functionality of the materials. Embedding stretchable distributed sensors in or on soft grippers greatly enhances the ways in which the grippers interact with objects. Challenges for soft grippers include miniaturization, robustness, speed, integration of sensing, and control. Improved materials, processing methods, and sensing play an important role in future research.\n\n[2] RISO: Combining Rigid Grippers with Soft Switchable Adhesives to address the shortcomings of their rigid counterparts. There are a variety of soft gripper designs,: most relevant are approaches that leverage adhesives. This includes gecko-inspired adhesives-, electrostatic adhesives,, dry adhesives, and thermal adhesives. Across each of these approaches the adhesive is combined with soft materials to create a overall compliant gripper; put another way, the robot never uses rigid components to interact with target objects. Although soft designs improve the robot's ability to grip small and irregular objects, the lack of rigid structure fundamentally limits the gripper's capacity to exert large forces. Rigid-Soft Grippers. Our hypothesis is that the combination of rigid and soft elements opens the door to diverse object manipulation. Recent works have moved towards rigid and soft grippers by integrating both elements into the robot's fingers-. We note that these existing rigid and soft gripper designs do not leverage adhesives. For example, Park et al. intersperse rigid blocks throughout soft materials, while Guo et al. vary a finger's stiffness by locking its rigid backbone in place. Gafer et al. similarly connect rigid links with soft joints to make fingers that scoop-up items. Hussain et al. and Ham et al. use a tendondriven mechanism to control the stiffness of the fingers, while Wu et al. integrate a jointed endoskeleton structure with soft robotic fingers to provide a higher gripping force. Overall, these state-of-the-art designs result in grasps that are both rigid and soft: the object is\n\n[3] Fabrication of a Soft Robotic Gripper With Integrated Strain Sensing Elements Using Multi-Material Additive Manufacturing With the purpose of making soft robotic structures with embedded sensors, additive manufacturing techniques like fused deposition modeling (FDM) are popular. Thermoplastic polyurethane (TPU) filaments, with and without conductive fillers, are now commercially available. However, conventional FDM still has some limitations because of the marginal compatibility with soft materials. Material selection criteria for the available material options for FDM have not been established. In this study, an open-source soft robotic gripper design has been used to evaluate the FDM printing of TPU structures with integrated strain sensing elements in order to provide some guidelines for the material selection when an elastomer and a soft piezoresistive sensor are combined. Such soft grippers, with integrated strain sensing elements, were successfully printed using a multi-material FDM 3D printer. Characterization of the integrated piezoresistive sensor function, using dynamic tensile testing, revealed that the sensors exhibited good linearity up to 30% strain, which was sufficient for the deformation range of the selected gripper structure. Grippers produced using four different TPU materials were used to investigate the effect of the Shore hardness of the TPU on the piezoresistive sensor properties. The results indicated that the in situ printed strain sensing elements on the soft gripper were able to detect the deformation of the structure when the tentacles of the gripper were open or closed. The sensor signal could differentiate between the picking of small or big objects and when an obstacle prevented the tentacles from opening. Interestingly, the sensors embedded in the tentacles exhibited good reproducibility and linearity, and the sensitivity of the sensor response changed with the Shore hardness of the gripper. Correlation between TPU Shore hardness, used for the gripper body and sensitivity of the integrated in situ strain sensing elements, showed that material selection affects the sensor signal significantly.\n\n[4] The Role of Stiffness in Versatile Robotic Grasping The Role of Stiffness in Versatile Robotic Grasping Traditionally, robotic grippers are based on stiff materials, enabling end effectors with high load capacity and precision for industrial applications. Recent advances in soft robotics have led to a proliferation of novel gripper designs with increased conformability to accommodate objects of varying shape, size, material, and surface properties, allowing for grippers that can safely manipulate a wide range of objects. While compliant materials offer noted advantages for robotic grasping, their ability to deform limits their load capacity. Therefore, stiffness selection is critical in gripper design, and the use of materials with tunable stiffness can be exploited for new functionality. Here, we present a mechanics-based investigation of the design of versatile grippers that can accommodate both soft and stiff grasping modalities. We examine the ability to form contact and how different types of gripping forces, including frictional, normal, and adhesive interactions, can be leveraged and controlled. We use analytical models based on elastic beam theory and contact mechanics to quantify the relationship between gripper deflection, contact area, contact pressure, and load capacity. We then use these models to define quantitative conditions for successful grasping as a function of the geometry of the object and the stiffness and geometry of the gripper. Finally, we conclude with an experimental case study and a discussion of how stiffness can be selected and modulated to realize successful grasping for different classes of objects.\n\n[5] Combining and Decoupling Rigid and Soft Grippers to Enhance Robotic Manipulation We therefore seek to expand the range of graspable objects by combining rigid and compliant components within the robot’s gripper.Recent works have started to develop grippers that integrate both elements chen2023soft ; hernandez2023current ; nasab2017soft ; park2018hybrid ; li2022stiffness ; peng2024enhanced ; gafer2020quad ; keely2024kiri ; guo2020self .However, within existing designs the rigid and soft capabilities are inherently coupled; e.g., robotic fingers that alternate between rigid links and soft joints.By contrast, our core insight is that we can decouple and couple the rigid and soft mechanisms by combining traditional rigid end-effectors with soft adhesive sheets.This combination and decoupling has the potential to fundamentally increase the range of robotic grasps: from purely rigid (e.g., manipulating a bottle of syrup) to purely soft (e.g., picking up a pile of candy) to a combination of rigid and soft (e.g., holding beads of multiple sizes).\n\n[6] Model-based Design and Simulation of a Soft Robotic Gripper for Fabric Material Handling are many other novel approaches as well, including dielectric elastomer actuators, ionic polymer-metal composites, shape memory materials, etc. Aside from actuating passive structures, soft and adjustable gripping can also be achieved by varying the stiffness of the gripping structure. These approaches do not fit the purpose of this research and will not be discussed further. Methodology There are three important characteristics of fabric materials to consider for handling operations: their light weight, high deformability, and complex geometry. Compared to other objects manipulated in industry, common fabric materials such as carbon fiber fabric or glass fiber fabric have much lighter density, making them easier to carry. Hence, the demand for the gripping force is necessary but not very high. Conversely, their smooth surfaces may require a higher gripping force to prevent slippage during transfer activities, which will lead to fiber damage. Fabric materials are highly deformable, causing draping and shifting during the pick and place operations. Deformability also causes many types of fabrics vulnerable to hard contact, making grasping more challenging. The geometrical complexity is related to various mold shapes used in industry. During the application, the gripper might need to pick or place fabric plies with respect to curved or cornered surfaces. Deformation of the fabrics also increases the geometrical complexity. Aside from these three characteristics, efficiency and motion speed are also required as for any other pick and place operations. Although traditional grippers might be able to perform fabric picking operations under limited scenarios as past literature suggests, they can hardly remain feasible to deal\n\n[7] Combining and Decoupling Rigid and Soft Grippers to Enhance Robotic Manipulation To create RISOs, one or more of these soft adhesive sheets are mounted onto the surfaces of rigid robotic grippers. Because the soft adhesives only change shape on their surface and not throughout their volume, they can be readily incorporated onto end-effectors without loss of functionality.Similarly, the rigid grippers can actuate normally as they are not encumbered by soft adhesive sheets.This mechanical combination and decoupling provides the fundamental advantage of our design: we can have purely rigid, purely soft, or combined rigid/soft grasps depending on where the soft adhesives are mounted and how the overall RISO is controlled.\n\n[8] Bio‐Inspired Soft Grippers Based on Impactive Gripping amounts of force so that heavy objects can be picked up. Soft robotic grippers consist of a continuous or discontinuous soft structure, which ensures both flexibility and compliance. The flexibility and compliance of soft robotic grippers are helpful for improving their grasping efficiency and adaptability to objects with different shapes. Soft robotic grippers also exhibit the advantage of security when interacting with humans. However, compared to rigid robotic grippers, the load capacity of soft robotic grippers is relatively low. Controlling the stiffness of soft grippers is a method that can be implemented to overcome these drawbacks. Soft grippers based on variable stiffness have some of the advantages of both rigid and soft robotic grippers, including flexibility and compliance in the low-stiffness state, as well as higher structural rigidity and load acceptance in the high-stiffness state. A stiffness-controllable structure is usually integrated into grippers as a module. In the low-stiffness state, adaptive grasping is more comfortable to implement, while in the high-stiffness state, the gripper can effectively generate sufficient holding force. The current stiffening strategies include the use of the jamming effect, electrorheology, magnetorheology, low-melting materials, and SMPs, as shown in Table 3. Electrorheological (ER) and magnetorheological (MR) fluids have a millisecond-level response time. However, they usually require high currents or high voltages to activate. Low-melting materials and SMPs have a broad range of modulus variations, and their stiffness can be changed by heating or cooling. The jamming effect is the most common method by which to change stiffness, as the stiffness is controlled by negative pressure,\n\n[9] A soft gripper with programmable effective length, tactile and curvature sensory feedback grippers. For example, dielectric elastomers are exploited in the design of soft grippers such as the DEMES gripper and the wrapping gripper. Low melting point alloys and shape memory polymers (SMPs),8 enable variable stiffness of the grippers via the phase transition effect. Magnetic-actuated,10, thermally activated, solution stimulated,13, and light-responsive materials endow the grippers with untethered and microscale features. In addition, the selfhealing material enables soft grippers with recovering ability while encountering damage, cuts, and perforations caused by sharp objects. Other micro grippers based on piezoelectric stack actuators or voice coil actuators are also developed. Soft grippers based on fluidic elastomer actuators (FEAs) have the properties of low-cost, environmental robust and easy fabrication,20. These properties make the FEA based grippers suitable for uncertain, dynamic task environments, including safe human-robot interactions. They have several promising applications including pick-place tasks, wearable devices, and underwater biological sampling,24. Soft sensors including the stretchable optical waveguides, the magnet hall sensor, the fabric capacitive sensor, and liquid metal sensors,29 are developed, which can be integrated into FEA grippers to provide feedback during grasping. Recently, the soft gripper with somatosensory feedback further enriches the gripper's functionality in terms of innervating the gripper with multiple conductive features. Besides soft sensors, variable stiffness mechanisms are deployed to improve the load capacity,32, and multiple degrees of freedom paradigms are explored to enrich the multimodal kinematics. Despite these research advances, we imagine that implementing a gripper\n\n[10] RISO: Combining Rigid Grippers with Soft Switchable Adhesives candy). complete grasps by leveraging adhesion or conforming to the object's shape. However, these grippers also come with a trade-off. On the one hand, they enable robot arms to pick up small, fragile, and irregular objects that may not be possible with rigid grippers; on the other hand, soft grippers are often unable to grasp large, heavy items or objects with rough surfaces that are suitable for rigid grippers. Put another way, rigid and soft grippers have separate grasping domains. When a robot arm uses grippers that are either purely rigid or purely soft, it fundamentally limits the types of objects that robot can manipulate. Instead of applying a single gripper type, our insight is that: Mounting soft controllable adhesives on standard rigid grippers enables a diverse range of robotic grasping. In this paper we introduce a novel RIgid-SOft (RISO) gripper that integrates elements of both rigid and soft paradigms (see Figure 1). RISO takes advantage of soft materials that can rapidly and repeatedly switch between low and high adhesion. We attach these switchable adhesives to the bottom of a standard parallel gripper: when the robot moves to pick up an object, it can either (a) pinch the object using the rigid gripper, or (b) stick to the object using the soft adhesive. As a first step towards characterizing this unified gripping formulation, we here explore how humans and robot arms control the RISO gripper to pick up and release everyday objects with diverse sizes, weights, and shapes. Overall, we make the following contributions:\n\n[11] Comparison of Different Technologies for Soft Robotics Grippers. Soft grippers have experienced a growing interest due to their considerable flexibility that allows them to grasp a variety of objects, in contrast to hard grippers, which are designed for a specific item. One of their most remarkable characteristics is the ability to manipulate soft objects without damaging them. This, together with their wide range of applications and the use of novels materials and technologies, renders them a very robust device. In this paper, we present a comparison of different technologies for soft robotics grippers. We fabricated and tested four grippers. Two use pneumatic actuation (the gripper with chambered fingers and the jamming gripper), while the other two employ electromechanical actuation (the tendon driver gripper and the gripper with passive structure). For the experiments, a group of twelve objects with different mechanical and geometrical properties have been selected. Furthermore, we analyzed the effect of the environmental conditions on the grippers, by testing each object in three different environments: normal, humid, and dusty. The aim of this comparative study is to show the different performances of different grippers tested under the same conditions. Our findings indicate that we can highlight that the mechanical gripper with a passive structure shows greater robustness.\n\n"}
{"case_id": "eef3ee38231d9fa5ffabbec75d1c5b50", "answer_text": "\n\nBenchmarking and evaluating obstacle avoidance strategies in mobile robots involve diverse approaches and criteria to ensure comprehensive and applicable assessments. A good benchmark should be versatile, covering both static and dynamic environments [5], and include multiple evaluation metrics that capture different aspects of performance [4].\n\nExisting Benchmarking Suites and Testbeds[5]. \n\n**DynaBARN** presents a simulation testbed specifically designed to evaluate navigation systems in environments with dynamic obstacles of varying motion profiles [1]. It systematically generates obstacles with different speeds, trajectory smoothness, and acceleration/deceleration rates to test robot adaptability across a range of scenarios, thereby offering a robust platform for benchmarking dynamic obstacle avoidance. This suite also includes a demonstration collection pipeline to benchmark autonomous navigation against human performance, enhancing its utility for learning from demonstration approaches.\n\n**Arena-Bench** focuses on highly dynamic environments, providing tools to generate scenarios and evaluate algorithms on a variety of robotic platforms [5]. It supports training and evaluation tasks, plotting results across different navigational metrics. Arena-Bench highlights the importance of deploying DRL (Deep Reinforcement Learning) in realistic scenarios, bridging the gap between simulation and real-world deployment.\n\n**Robobench**, introduced by Weisz et al., presents a platform suitable for an array of tasks, such as navigation, manipulation, and observation. It offers a uniform ground for benchmarking in the field of robotics using a simulation engine and planning framework that can handle different types of robots [2].\n\n**Bench-mr**, presented by Heiden et al., provides the utility to test, evaluate, and compare different motion-planning techniques in complex environment scenarios, focusing on static environments but offering significant insights into path planning and navigation [2].\n\nBenchmarking Studies and Approaches\n\nThe comparison of **Particle Swarm Optimization (PSO)** and **Q-learning** for multi-robot obstacle avoidance demonstrates that PSO generally achieves the highest fitness, while Q-learning performs significantly better with continuous states than discrete states [0]. This study emphasizes the importance of state space representation in learning-based navigation strategies and the benefits of distributed algorithms like PSO in multi-robot settings.\n\n**Vision-based Deep Reinforcement Learning Approaches**: Vision-based mobile robotics obstacle avoidance employing Deep Q-Networks (DQN) has shown promising results in simulated environments like those using the gym-gazebo toolkit. This setup evaluates recent DRL algorithms in different simulated environments, providing a standard framework to compare performance across conditions like varying field of view and sensor readings [8].\n\nStandard Metrics and Evaluation Criteria[3]. \n\nSeveral metrics have been proposed to quantify navigation difficulty and benchmark algorithm performance. For example, **TEB (Timed Elastic Band)** is noted for producing smoother and faster-reacting paths compared to other planners like DWB and E-band, which were tested on mobile robots in simulations and real-world experiments [3]. This study suggests using metrics such as path smoothness, reaction time to dynamic obstacles, and goal-reaching efficiency to evaluate local planners effectively.\n\n\n\n#### Expanding on Standard Metrics:\nCurrent benchmarks for dynamic obstacle avoidance do not provide a way to alter how obstacles move and instead use only a single method to uniquely determine the movement of obstacles, e.g., constant velocity, the social force model, or Optimal Reciprocal Collision Avoidance (ORCA). Using a single method in this way restricts the variety of scenarios in which the robot navigation system is trained and/or evaluated, thus limiting its robustness to dynamic obstacles of different speeds, trajectory smoothness, acceleration/deceleration, etc., which we call motion profiles [1].\n\nIn order to improve navigation benchmarking efforts, different standards are proposed. The authors introduce the standard of worlds with different types of obstacles. They also propose adding variations in how the dynamic obstacles are set up (from simpler to more complicated environments) and perform experiments in real and simulation environments [3].\n\nExisting benchmarks for robot navigation algorithms mostly focus on static environments, but few exist that cover both dynamic and static ones [5].\n\n\n- **Path Smoothness**: Quantified by calculating the sum of the differences between consecutive position measurements as the robot travels along a path. The smoother the path, the lower the cumulative difference [3].\n- **Reaction Time to Dynamic Obstacles**: Measured by timing how quickly a planner can react to a sudden obstacle appearance. For instance, TEB was found to react faster than DWB, which sometimes pauses to recalculate its route [3].\n- **Goal-Reaching Efficiency**: The efficiency with which a robot reaches its target can be measured by the time taken and the distance traveled, with shorter and quicker paths being preferable [3].\n - **Collision Rates**: The frequency of collisions or near-misses with obstacles can provide direct insight into the safety and reliability of the obstacle avoidance strategy [1]. \n\n**Human Performance Benchmarking**: Real-world data acquisition methods, such as those used in DynaBARN to collect navigation trials performed by human operators, allow for the comparison of autonomous system performance against human benchmarks. This provides a practical perspective on the efficacy and reliability of navigation algorithms [1].\n\n**Robobench** introduces a concept of standard environments and evaluation metrics, which emphasize the importance of uniform benchmarking across different task categories like navigation, manipulation, and observation [2]. This approach ensures that comparisons are made fairly, considering the different types of tasks and robot platforms.\n\n### Challenges and Recommendations\n\n\n\nDespite the advances in benchmarking strategies, several challenges remain. Many approaches are often tested in specifically designed environments, making general comparisons difficult [5]. For instance, **Dynamic Obstacle Avoidance Benchmarking** suggests incorporating variations in how dynamic obstacles are set up, from simpler to more complicated environments, to enhance robustness evaluations [4]. This emphasizes the need for standardized environments and metrics to facilitate broader and fairer comparisons.\n\n Common Limitations of Current Benchmarking Approaches [5]. \n\nOne common limitation is the **lack of generalization** in many benchmarks, which are designed to test specific scenarios that may not cover the full spectrum of real-world conditions [5]. This can lead to a narrow evaluation that misses critical edge cases encountered in diverse environments [1]. \n\nAnother limitation is the **over-reliance on simulated environments** without sufficient validation in real-world settings [6]. While simulations provide a controlled and repeatable testing ground, they often fail to capture the complexities and unpredictabilities of the physical world, such as sensor noise, unexpected obstacles, and dynamic changes in the environment [6].\n\nAdditionally, scalability issues arise in benchmarks that require significant computational resources or extensive time to evaluate numerous algorithms across varied conditions. This can impede the practical applicability of such benchmarking frameworks, particularly for real-time applications [5]. \n\nConclusion\n\nThe design of high-performing robotic controllers constitutes an example of expensive optimization in uncertain environments due to the often large parameter space and noisy performance metrics [0]. There are several evaluative techniques that can be employed for on-line controller design. Adequate benchmarks help in the choice of the right algorithm in terms of final performance and evaluation time [2]. Existing benchmarks for robot navigation algorithms mostly focus on static environments, but few exist that cover both dynamic and static ones [5]. Nonetheless, path planning is a vital aspect of autonomous navigation, not only in static but also in dynamic environments [2]. In the multi-robot case, both Q-learning approaches require a similar amount of time as in the single robot case, but the time required by PSO can be significantly reduced due to the distributed nature of the algorithm [0]. We also show that in the single robot case, PSO and Q-learning with discrete states require a similar amount of total learning time to converge, while the time required with Q-learning with continuous states is significantly larger [0]. We provide results of four classical and learning-based navigation systems in DynaBARN, which can serve as baselines for future studies [1]. In order to improve navigation benchmarking efforts, different standards are proposed [3]. \n\nGood benchmarking and evaluation strategies for mobile robot obstacle avoidance should include [1].\n\n1. **Diverse Test Environments:** Covering static and dynamic obstacles with systematically generated scenarios [1][5]. Suites like DynaBARN and Arena-Bench illustrate the importance of dynamically varying obstacle profiles and high-fidelity simulations.\n2. **Multiple Evaluation Metrics:** Including path smoothness, reaction time, goal-reaching efficiency, adaptability to varying obstacles, and collision rates [3][4]. For example, TEB's faster reaction time and path smoothness provide significant insights into its practical applicability.\n3. **Simulation and Real-World Testing:** Bridging the gap between simulation and deployment in real scenarios [5][6]. This dual evaluation helps ensure that performance in controlled environments translates to real-world efficacy.\n4. **Comparative Analysis:** Utilizing benchmarks like DynaBARN, Arena-Bench, and Robobench for comprehensive assessments against existing algorithms [1][2][5]. These platforms emphasize the essential task of comparing new approaches against established baselines under standardized conditions.\n\nBy addressing these facets and acknowledging common limitations, researchers can better evaluate and compare the efficacy of different obstacle avoidance strategies [6], facilitating advancements in autonomous navigation technologies [3]. \n\n\nReferences:\n[0] A comparison of PSO and Reinforcement Learning for multi-robot obstacle avoidance The design of high-performing robotic controllers constitutes an example of expensive optimization in uncertain environments due to the often large parameter space and noisy performance metrics. There are several evaluative techniques that can be employed for on-line controller design. Adequate benchmarks help in the choice of the right algorithm in terms of final performance and evaluation time. In this paper, we use multi-robot obstacle avoidance as a benchmark to compare two different evaluative learning techniques: Particle Swarm Optimization and Q-learning. For Q-learning, we implement two different approaches: one with discrete states and discrete actions, and another one with discrete actions but a continuous state space. We show that continuous PSO has the highest fitness overall, and Q-learning with continuous states performs significantly better than Q-learning with discrete states. We also show that in the single robot case, PSO and Q-learning with discrete states require a similar amount of total learning time to converge, while the time required with Q-learning with continuous states is significantly larger. In the multi-robot case, both Q-learning approaches require a similar amount of time as in the single robot case, but the time required by PSO can be significantly reduced due to the distributed nature of the algorithm.\n\n[1] DynaBARN: Benchmarking Metric Ground Navigation in Dynamic Environments Safely avoiding dynamic obstacles while moving toward a goal is a fundamental capability of autonomous mobile robots. Current benchmarks for dynamic obstacle avoidance do not provide a way to alter how obstacles move and instead use only a single method to uniquely determine the movement of obstacles, e.g., constant velocity, the social force model, or Optimal Reciprocal Collision Avoidance (ORCA). Using a single method in this way restricts the variety of scenarios in which the robot navigation system is trained and/or evaluated, thus limiting its robustness to dynamic obstacles of different speeds, trajectory smoothness, acceleration/deceleration, etc., which we call motion profiles. In this paper, we present a simulation testbed, DynaBARN, to evaluate a robot navigation system's ability to navigate in environments with obstacles with different motion profiles, which are systematically generated by a set of difficulty metrics. Additionally, we provide a demonstration collection pipeline that records robot navigation trials controlled by human users to compare with autonomous navigation performance and to develop navigation systems using learning from demonstration. Finally, we provide results of four classical and learning-based navigation systems in DynaBARN, which can serve as baselines for future studies. We release DynaBARN open source as a standardized benchmark for future autonomous navigation research in environments with different dynamic obstacles. The code and environments are released at https://github.com/aninair1905/DynaBARN.\n\n[2] Arena-Bench: A Benchmarking Suite for Obstacle Avoidance Approaches in Highly Dynamic Environments presented in, where model-based, as well as learning-based methods, namely TEB, DWA, MPC, CADRL, and their DRL agent were evaluated and compared based on different performance metrics. A similar benchmark suite named Bench-mr was presented in a paper by Heiden et al.. It provides the utility to test, evaluate, and compare different motion-planning techniques in complex environment scenarios. Although the focus was placed on wheeled mobile robots, all notions of dynamic obstacles were disregarded. Thus, only static situations were considered. Other benchmarks that focus on static environments are presented in the works of Althoff et al. or Moll et al.. Nonetheless, path planning is a vital aspect of autonomous navigation, not only in static but also in dynamic environments. Rano et al. proposed a methodology used to evaluate the performance of different approaches, given that specific scenarios can vary from each other. Consequently, this methodology attempts to quantify several aspects of an actual environment, such as the amount of space occupied by obstacles and the space available for the robot to maneuver between different static obstructions. Based on these metrics, scenarios were classified and evaluated legitimately compared to previous methodologies. Robobench, introduced by Weisz et al. is another venture of a similar notion, trying to create uniform grounds for benchmarking in the field of robotics. They present a platform suitable for an array of tasks, such as navigation, manipulation, and observation. Different types of robots can be evaluated using an appropriate simulation engine and planning framework.\n\n[3] Benchmarking local motion planners for navigation of mobile manipulators and, motion planning algorithms such as DWB, TEB, and E-band are tested using small mobile robots in both simulation and real experiments. The general consensus is that DWB performs the fastest, E-band is the most accurate local planner, and TEB plans motions that lead the robot to the goal slightly faster than the other two algorithms. The authors also report that TEB would react the fastest to a dynamic obstacle, while DWB sometimes stops to calculate its subsequent route. In and, similar results are reported with a heavy differential-driven robot carrying a Baxter manipulator. These results indicate that in general, TEB performs better than the baseline methods at obstacle avoidance in simulation and real experiments. In order to improve navigation benchmarking efforts, different standards are proposed,,. In, the authors introduce the standard of worlds with different types of obstacles. They also propose adding variations in how the dynamic obstacles are set up (from simpler to more complicated environments) and perform experiments in real and simulation environments. There are also studies, focusing more on crafting standard metrics that can meaningfully benchmark the navigation algorithms using standard worlds/environments. Metrics used in evaluations are key to analyze the performance of the algorithms. In, the authors have found that TEB generates much smoother paths when using a metric they introduced. This metric constitutes the sum of the difference between two position measurements when the mobile base is traveling along a path. However, this method may fail to discern how smooth the generated\n\n[4] Benchmarking Metric Ground Navigation decades-. Sophisticated navigation systems have been developed using classical control methods-, path and motion planning,,, or, more recently, machine learning techniques-. However, despite the plethora of works in mobile robot navigation, there is no generally accepted metric by which to compare different approaches against one another, even for navigation in a simple metric world, where only geometric obstacles are considered. Although in relatively open space, navigation performance of different systems may not vary significantly, the lack of an accepted metric becomes particularly relevant in environments that are more difficult to navigate. Those environments include, for example, unstructured or confined spaces for search and rescue and highly constrained spaces where agile maneuvers are required for robots with nonholonomic motion constraints. Newly developed navigation systems are only tested and compared 1 to existing ones in a limited number of ad hoc environments with unquantified difficulties. To address the lack of a standardized method to test and compare mobile robot navigation systems, this work provides a benchmark dataset of 300 simulated test environments, which can also be easily instantiated in the physical world. We design a set of metrics to quantify navigation difficulty of these simulated environments for ground mobile robots to move in obstacle-occupied spaces without collision. We then unify this set of metrics via a learned function approximator to introduce a novel measure of an environment's difficulty level for metric ground navigation. Through an extensive amount of 3000 simulated trials using two widely used planners, Dynamic\n\n[5] Arena-Bench: A Benchmarking Suite for Obstacle Avoidance Approaches in Highly Dynamic Environments A common barrier is that most of the research work evaluated their approaches on specifically designed simulation environments or test setups, making a general comparison against existing approaches difficult. Furthermore, deployment and integration of DRL into real robotic platforms is still an open frontier due to safety reasons Manuscript received: February, 24, 2022; Revised May, 14, 2022; Accepted June, 12, 2022. This letter was recommended for publication by Editor Tamim Asfour upon evaluation of the Associate Editor and Reviewers' comments. 1 The authors are with the Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany d.kaestner@tu-berlin.de Digital Object Identifier (DOI): see top of this page. Fig. 1. Arena-bench is a benchmark suite that enables to train and evaluate navigation approaches in realistic dynamic environments. It provides tools to develop navigation approaches, design and generate scenarios, and evaluation tasks on a variety of robotic platforms. The results can be plotted on a variety of different navigational metrics.,. Thus, a benchmark to properly assess those approaches in realistic scenarios and against existing algorithms is not only an essential step towards the deployment of DRL into real robots but also assists in the development and validation of learning-based approaches on mobile robots. Existing benchmarks for robot navigation algorithms mostly focus on static environments, but few exist that cover both dynamic and static ones. Moreover, existing benchmarks for navigation in dynamic environments often contain and compare only a small number of planners,,. On that\n\n[6] Reactive Obstacle-Avoidance Systems for Wheeled Mobile Robots based on Artificial Intelligence we would be probably sacrificing accuracy in the robot movements, as opposed to what would happen with the improved accuracy of fuzzy logic. Hence, the decision on which one to choose will ultimately depend on the end-user application. Discussion We identified strengths and challenges of reactive Obstacle-Avoidance systems based on approaches of an ANN-BP architecture and FIS rules. We show the comparisons in Tables 4-6. We highlight in these tables differences regarding the environmental input data collected by the robots in the real world, the environmental operating conditions, and a comparison of the conditions considered for obstacle avoidance. Therefore, in the analysis, we consider aspects such as: sensor response, field of vision, distance measurement, type of objects identified, type of surface, real experimental environment, effects or noise over the sensors, dynamics of the real environments, limitations of the mobile robots that affect the reliability, and efficiency of approaches to work well in real-world deployments. In addition, we present a summary of the strengths and challenges of the approaches considered. The strengths define the main contributions of each approach to avoid obstacles. The challenges describe the problems that must be solved to measure how successful or efficient these approaches would be when implemented in the real world where real conditions exist. An important remark is that we were able to identify that, to the best of our knowledge, most of the approaches published elsewhere were implemented only in a simulated environment, such as,16. Conclusions Wheeled mobile robots have allowed human beings to perform activities in challengingaccess\n\n[7] Vision-Based Mobile Robotics Obstacle Avoidance With Deep Reinforcement Learning We used the gym-gazebo toolkit  to evaluate recent DRL algorithms in different simulation environments. This toolkit is an extension of OpenAI gym  for robotics using the Robot Operating System (ROS)  and Gazebo  that allows for an easy benchmark of different algorithms with the same virtual conditions. For the conducted experiments in this work, two different modified simulation environments from the original project are used; namely Circuit2-v0 and Maze-v0 (Figure 4). As a simulated robotics platform, a TurtleBot2 with a Kobuki base is used. This model comes with a simulated camera with 808080°degree\\mathrm{\\SIUnitSymbolDegree} FOV, 480×480480480480\\times 480 resolution, and clipping at 0.050.050.05 and 8.08.08.0 distances. The laser range sensor information is provided by a simple sensor model, which is mounted on top of the robot base. The sensor has a 270270270°degree\\mathrm{\\SIUnitSymbolDegree} FOV and outputs sparse range readings (100100100 points). In all our simulation results, each plot shows a 95 %times95percent95\\text{\\,}\\mathrm{\\char 37\\relax} confidence interval of the mean across 333 seeds.\n\n"}
{"case_id": "38dea1ab1b53857ef1e9ffad2aab6691", "answer_text": "\n\nBiologically-inspired algorithms have made significant contributions to robot path planning, leveraging principles and mechanisms observed in nature to solve complex navigation problems [1]. These algorithms can be broadly categorized based on their inspiration sources, such as neural networks, evolutionary processes, and swarm intelligence [1]. Here, we will discuss several prominent biologically-inspired robot path planning algorithms, highlighting their methods and limitations. \n\n\n### 1. Biologically-Inspired Neural Networks (BINNs) [0]\n\n\nBiologically-inspired neural networks are popular for path planning due to their ability to model the dynamic nature of environments. For instance, a hybrid model combining Self-Organizing Maps (SOMs) and BINNs has been proposed for task assignment and path planning of autonomous underwater vehicles (AUVs) in 3D environments. The SOM algorithm assigns tasks to AUVs, while the BINN updates weights to plan collision-free paths and effective navigation [0]. Another study employed a BINN to address real-time path planning for mobile robots in non-stationary environments, ensuring collision-free trajectories [11]. However, these approaches can suffer from inefficiencies in computational costs and the potential for suboptimal paths due to local minima [3].\n\n\n### 2. Swarm Intelligence[^1]\n\n[^1]: [1]\n\n\nInspired by the collective behavior of social insects and animals, swarm intelligence algorithms like Ant Colony Optimization (ACO) and Particle Swarm Optimization (PSO) have proven effective in path planning. These methods are advantageous due to their decentralized control and robustness. For example, the combination of genetic algorithms and ACO has been employed for UAV path planning, optimizing flight paths based on environmental conditions and sensor data [13]. The integration of PSO in path planning algorithms also shows promise, particularly in handling dynamic environments and optimizing path lengths effectively [12].\n\n\n\n### 3. Evolutionary Algorithms [1]\n\n\n\nEvolutionary algorithms, such as genetic algorithms, leverage the principles of natural selection to evolve optimal solutions over generations. These algorithms are widely used in robotic path planning for their ability to explore large search spaces and handle complex constraints. A noteworthy application includes the joint use of genetic algorithms and ACO for UAV path planning, which optimizes paths based on multiple objectives such as energy efficiency, time, and environmental risk [13]. Despite their versatility, these algorithms can be computationally expensive and require careful tuning of parameters to achieve optimal performance [7].\n\nBehavior-Based Controllers [14]. \n\nBehavior-based controllers draw inspiration from biological systems like the spinal fields of frogs and rats. These controllers generate behaviors based on activation levels, providing a robust and simple approach to navigating dynamic environments. A study demonstrated the efficacy of such controllers in mobile robots, showing that they can adapt to various scenarios effectively [14]. However, the simplicity of this approach may limit its ability to handle highly complex or unpredictable environments without additional enhancements [14].\n\n### 5. Fuzzy Logic Systems\n\nFuzzy logic systems have become a significant area of study in the field of bio-inspired algorithms, particularly for mobile robot navigation in unknown environments. Novel approaches have been proposed that integrate fuzzy logic with neural networks to enhance path planning and obstacle avoidance capabilities. For example, a hybrid fuzzy logic neural network model has been developed, which combines the strengths of both techniques to enable efficient navigation and corner-turning capabilities for mobile robots [6]. The fusion of fuzzy logic and neural networks allows for improved real-time decision-making and adaptability in dynamic and uncertain environments. Simulation studies have validated the effectiveness of these hybrid models, demonstrating their ability to navigate robots to targets and handle complex navigation tasks with high reliability.\n\n\n\nIncorporating fuzzy logic into biologically-inspired models offers a means to handle uncertainty and imprecision in path planning. A hybrid algorithm combining fuzzy logic and neural networks was developed for mobile robots, utilizing sensor inputs to navigate obstacles and reach targets [6]. This method improves the adaptability and robustness of path planning algorithms, although it may require extensive computational resources for complex environments [6].\n\nTopological navigation models have been deployed in mobile robots, and there is a novel bio-inspired topological navigation model that consists of topological map construction, active path planning, and path execution, specifically designed for aerial mobile robots [5]. \n\nInspired by animals' spatial representations, topological navigation models utilize topological maps and active path planning to achieve reliable navigation. These models have been applied to aerial mobile robots, where visual landmark recognition and compass orientation guide the robot through challenging environments [5]. The reward diffusion algorithms in these models help construct paths with reliable visual cues, mimicking animal-like navigation behavior [5].\n\nThe algorithm embeds the biologically inspired neural network (BINN) into the SOM neural networks. The task assignment and path planning aim to arrange a team of AUVs to visit all appointed target locations, while assuring obstacle avoidance without speed jump. The SOM neuron network is developed to assign a team of AUVs to achieve multiple target locations in underwater environments [0]. Then, in order to avoid obstacles and speed jump for each AUV that visits the corresponding target location, the BINN is utilized to update weights of the winner of SOM, and achieve AUVs path planning and effective navigation [0]. The effectiveness of the proposed hybrid model is validated by simulation studies [0]. \n\nHere is a summary table comparing the key attributes of the discussed algorithms [1][7]. \n\n| Algorithm Type                | Application Environments        | Computational Efficiency | Robustness                  | Notable Strengths                           | Limitations                            |[1]\n\nAn integrated biologically inspired self-organizing map (SOM) algorithm is proposed for task assignment and path planning of an autonomous underwater vehicle (AUV) system in 3-D underwater environments with obstacle avoidance [0]. The algorithm embeds the biologically inspired neural network (BINN) into the SOM neural networks. The task assignment and path planning aim to arrange a team of AUVs to visit all appointed target locations, while assuring obstacle avoidance without speed jump [0]. The SOM neuron network is developed to assign a team of AUVs to achieve multiple target locations in underwater environments [0]. Then, in order to avoid obstacles and speed jump for each AUV that visits the corresponding target location, the BINN is utilized to update weights of the winner of SOM, and achieve AUVs path planning and effective navigation [0]. The effectiveness of the proposed hybrid model is validated by simulation studies [0]. \nBiologically-Inspired Neural Networks (BINNs) Dynamic, real-time [0] Moderate High Effective for collision avoidance and real-time planning Prone to local minima and computationally expensive [8].\nSwarm Intelligence | Dynamic, decentralized [1] | High [1] | High [1] | Optimizes path lengths effectively in dynamic settings [3] | May require tuning for specific applications [7] |\n\nEvolutionary Algorithms | Complex, constraint optimization | Low to Moderate | Moderate to High | Explores large search spaces, versatile | Computationally expensive, parameter tuning needed [1]. \nBehavior-Based Controllers | Dynamic, simple to moderate [14] | High | High | Simple, robust for various scenarios [14] | Limited handling of highly complex environments [14]|\n\n| Fuzzy Logic Systems           | Uncertain, imprecise environments | Moderate                  | High                        | Handles uncertainty well, adaptive [6]        | Computationally intensive for complex setups [6] |\n\n | Topological Navigation Models | Visual, spatial representation  | Moderate                  | High                        | Mimics reliable animal-like navigation [5] | Dependent on visual landmark accuracy  |\n\n\nIn conclusion, biologically-inspired algorithms provide diverse and powerful approaches for robot path planning [1]. They offer solutions that range from neural network-based real-time collision avoidance [9] to evolutionary optimization of complex paths [7]. Each method has its strengths and limitations, and the choice of algorithm often depends on the specific requirements of the application, such as the nature of the environment, computational resources, and the desired optimization criteria [1]. \n\n\nReferences:\n[0] Biologically Inspired Self-Organizing Map Applied to Task Assignment and Path Planning of an AUV System An integrated biologically inspired self-organizing map (SOM) algorithm is proposed for task assignment and path planning of an autonomous underwater vehicle (AUV) system in 3-D underwater environments with obstacle avoidance. The algorithm embeds the biologically inspired neural network (BINN) into the SOM neural networks. The task assignment and path planning aim to arrange a team of AUVs to visit all appointed target locations, while assuring obstacle avoidance without speed jump. The SOM neuron network is developed to assign a team of AUVs to achieve multiple target locations in underwater environments. Then, in order to avoid obstacles and speed jump for each AUV that visits the corresponding target location, the BINN is utilized to update weights of the winner of SOM, and achieve AUVs path planning and effective navigation. The effectiveness of the proposed hybrid model is validated by simulation studies.\n\n[1] A Survey on Robot Path Planning using Bio-inspired Algorithms A Survey on Robot Path Planning using Bio-inspired Algorithms The path planning problem has been an essential topic in autonomous robotics. There are many new path planning methods that have been developed, and those using bioinspired algorithms attract extensive attention. This paper analyzes several bio-inspired algorithms and surveys recent developments of robot path planning. Swarm intelligence, evolutionary algorithms, and neurodynamics are three primary robot path planning methods that are focused on in this survey. In addition, the pros and cons of these three methods are summarized and analyzed.\n\n[2] AUV Global Security Path Planning Based on a Potential Field Bio-Inspired Neural Network in Underwater Environment Lee developed an anytime algorithm using PSO to solve the intelligent robot path-planning problem. Another commonly used path-planning algorithm is the bio-inspired neural network in the field. Cao et al. made an in-depth study of AUV path planning. They devised a combination of a velocity synthesis algorithm with a biologically inspired neurodynamics model for an underwater environment affected by ocean currents. It is expected to provide shorter paths than other algorithms in an underwater environment with ocean currents and obstacles. Yang et al. introduced a bio-inspired neural network algorithm to lead the robot to search for unknown environments. This method is capable of planning more reasonable and shorter collision-free complete coverage searching paths in an unknown environment. Although the bio-inspired neural network algorithm has obtained some good results for a mobile robot, its workspace is different from the practical underwater environment. Although most AI methods have a powerful ability of global optimization, the efficiencies of the different methods are uneven for the same problem. For example, the bio-inspired neural network approach of AUVs is an original approach that can be applied to generate a real-time collision-free trajectory under a dynamic uncertain environment. The planned path may not prevent an AUV from approaching obstacles \"too close\". In this case, the AUV may collide with an obstacle in the actual underwater environment. Furthermore, the generated path may not be in accord with the restriction that the path length should be short to the greatest extent when the approach is applied in point-to-point path planning\n\n[3] A Bio-Inspired Goal-Directed Visual Navigation Model for Aerial Mobile Robots A Bio-Inspired Goal-Directed Visual Navigation Model for Aerial Mobile Robots Reliably navigating to a distant goal remains a major challenge in robotics. In contrast, animals such as rats and pigeons can perform goal-directed navigation with great reliability. Evidence from neural science and ethology suggests that various species represent the spatial space as a topological template, with which they can actively evaluate future navigation uncertainty and plan reliable/safe paths to distant goals. While topological navigation models have been deployed in mobile robots, relatively little inspiration has drawn upon biology in terms of topological mapping and active path planning. In this paper, we propose a novel bio-inspired topological navigation model, which consists of topological map construction, active path planning and path execution, for aerial mobile robots with visual landmark recognition and compass orientation capability. To mimic the topological spatial representation, the model firstly builds the topological nodes based on the reliability of visual landmarks, and constructs the edges based on the compass accuracy. Then a reward diffusion algorithm akin to animals’ path evaluation process is developed. The diffusion process takes the topological structure and landmark reliability into consideration, which helps the agent to construct the path with visually reliable nodes. In the path execution process, the agent combines orientation guidance and landmark recognition to estimate its position. To evaluate the performance of the proposed navigation model, a systematic series of experiments were conducted in a range of challenging and varied real-world visual environments. The results show that the proposed model generates animal-like navigation behaviours, which avoids travelling across\n\n[4] A fuzzy logic based bio-inspired system for mobile robot navigation A fuzzy logic based bio-inspired system for mobile robot navigation This paper presents a new path planning method for mobile robots in unknown environments. The structure of the proposed algorithm is a hybrid fuzzy logic neural networks, and hence it benefits from the potentials of these two techniques. For modeling the mobile robot, the proposed system adopts the Braitenberg's automata models that were developed for agents. Wheels of the robot are represented by a bio-inspired neuron of a neural network, where each wheel receives different sensor inputs indicating different signals from either excitatory or inhibitory synapses. Training of the neural network weighting is automatically achieved through the fuzzy system that is developed to adjust the weighting between each synapse and neuron of the network. To assess the performance of the developed algorithm, simulation results are presented. It was shown that the proposed method can successfully navigate the robot to the target, and turn the robot at corners for given desired angles. The methodology proposed herein improves the Braitenberg navigation scheme and offers insights into using biologically inspired systems for path planning.\n\n[5] Bio-Inspired Optimization-Based Path Planning Algorithms in Unmanned Aerial Vehicles: A Survey Advancements in electronics and software have enabled the rapid development of unmanned aerial vehicles (UAVs) and UAV-assisted applications. Although the mobility of UAVs allows for flexible deployment of networks, it introduces challenges regarding throughput, delay, cost, and energy. Therefore, path planning is an important aspect of UAV communications. Bio-inspired algorithms rely on the inspiration and principles of the biological evolution of nature to achieve robust survival techniques. However, the issues have many nonlinear constraints, which pose a number of problems such as time restrictions and high dimensionality. Recent trends tend to employ bio-inspired optimization algorithms, which are a potential method for handling difficult optimization problems, to address the issues associated with standard optimization algorithms. Focusing on these points, we investigate various bio-inspired algorithms for UAV path planning over the past decade. To the best of our knowledge, no survey on existing bio-inspired algorithms for UAV path planning has been reported in the literature. In this study, we investigate the prevailing bio-inspired algorithms extensively from the perspective of key features, working principles, advantages, and limitations. Subsequently, path planning algorithms are compared with each other in terms of their major features, characteristics, and performance factors. Furthermore, the challenges and future research trends in UAV path planning are summarized and discussed.\n\n[6] An effective vector-driven biologically-motivated neural network algorithm to real-time autonomous robot navigation An effective vector-driven biologically-motivated neural network algorithm to real-time autonomous robot navigation A novel biologically-motivated neural networks approach associated with developed vector-driven autonomous robot navigation is proposed in this paper. The biologically-motivated neural networks (BNN) algorithm is employed to guide an autonomous robot to reach goal with obstacle avoidance motivated by Grossberg's model for a biological neural system. As the robot plans its trajectory toward the goal, unreasonable path will be inevitably planned. A vector-based guidance paradigm is developed for guidance of the robot locally so as to plan more reasonable trajectories. In addition, square cell map representations are proposed for realtime autonomous robot navigation. The BNN based scheme demonstrates that the algorithms avoid the issue of local minima in path planning. In this paper, both simulation and comparison studies of an autonomous robot navigation demonstrate that the proposed model is capable of planning more reasonable and shorter collision-free paths in non-stationary and unstructured environments compared with other approaches.\n\n[7] Real-time Collision-free Path Planning of Robot Manipulators using Neural Network Approaches In this paper, a novel neural network approach to real-time collision-free path planning of robot manipulators in a nonstationary environment is proposed, which is based on a biologically inspired neural network model for dynamic trajectory generation of a point mobile robot. The state space of the proposed neural network is the joint space of the robot manipulators, where the dynamics of each neuron is characterized by a shunting equation or an additive equation. The real-time robot path is planned through the varying neural activity landscape that represents the dynamic environment. The proposed model for robot path planning with safety consideration is capable of planning a real-time “comfortable” path without suffering from the “too close” nor “too far” problems. The model algorithm is computationally efficient. The computational complexity is linearly dependent on the neural network size. The effectiveness and efficiency are demonstrated through simulation studies.\n\n[8] A Complete Coverage Path Planning Method for Mobile Robot in Uncertain Environments In this paper, a novel complete coverage path planning method based on the biologically inspired neural networks, rolling path planning and heuristic searching approach is presented for mobile robot motion planning with obstacles avoidance. The biologically inspired neural network is used to model the environment and calculate the environment information, while the rolling planning technique and the heuristic searching algorithm are utilized for the path planning. Simulation studies show that the proposed method is very effective for the dynamic uncertain environments\n\n[9] Path planning and trajectroy tracking of a mobile robot using bio-inspired optimization algorithms and PID control Path planning and trajectory tacking are the fundamental task in mobile robotic science, and they enable the robot to navigate autonomously. In this work, the path planning task is carried out using three bio-inspired optimization algorithms, including PSO, ABC and FA. The duty of the algorithms is to determine a collision-free path through fixed obstacles in the working environment. The maximum speed of the robot is applied to the optimization problem as a constraint. In order to evaluate the performance of the algorithms, four workspaces with different obstacle layout are simulated in MATLAB, and the quality of path planning task is analyzed statistically and numerically, considering four different criteria, including, convergency quality, convergency time, path length and success rate. In the next step, a control model is designed to track the path curve determined by the path planning algorithms. A PID-based control structure is simulated in MATLAB Simulink and the controller was able to track the pre-determined traj ectories with proper approximation. The controller is applied on a dynamic model of a two-wheeled mobile robot offered by. In order to validate the control inputs it is necessary to apply them on a real platform. The experimental study is implemented on a two-wheeled mobile robot which is designed and built based on the authors' previous paper in various enverioment and obstacles. The result shows control inputs were applied to the real robot and the robot was able to imitate the applied path curve, and find its way toward the target point without colliding obstacles in real and simulation task.\n\n[10] Optimal UAV Path Planning: Sensing Data Acquisition Over IoT Sensor Networks Using Multi-Objective Bio-Inspired Algorithms The use of unmanned aerial vehicles (UAVs) has been considered to be an efficient platform for monitoring critical infrastructures spanning over geographical areas. UAVs have also demonstrated exceptional feasibility when collecting data due to the wide wireless sensor networks in which they operate. Based on environmental information such as prohibited airspace, geo-locational conditions, flight risk, and sensor deployment statistics, we developed an optimal flight path planning mechanism by using multi-objective bio-inspired algorithms. In this paper, we first acquire data sensing points from the entire sensor field, in which UAV communicates with sensors to obtain sensor data, then we determine the best flight path between neighboring acquisition points. Using the proposed joint genetic algorithm and ant colony optimization from possible UAV flight paths, an optimal one is selected in accordance with sensing, energy, time, and risk utilities. The simulation results show that our method can obtain dynamic environmental adaptivity and high utility in various practical situations.\n\n[11] Bioinspired Intelligent Algorithm and Its Applications for Mobile Robot Control: A Survey intelligence and autonomy of mobile robots. The goal of path planning is to find an optimal or suboptimal path from the starting position to the target position,65. Various methods have been used to deal with path planning problems, such as the potential field methods and the fuzzy control methods. These methods have achieved certain success. However, there are still some problems needed to be further studied, including path planning in unknown and dynamic environments and the local minimum problem of most optimization algorithms. Recently, some BIAs have been proposed in solving the problems in path planning of mobile robots. The detailed information of typical BIAs in robot path planning is described in the sequel. Siddique and Amavasai proposed a behavior based controller inspired by the concept of spinal fields found in frogs and rats. The core idea of this bioinspired behavior-based controller is that the robot controller consists of a collection of spinal fields which are responsible for generating behaviors depending on their activation levels. The collection of spinal fields is denoted by 2 , 3 , 4 ). Finally, four experiments are carried out with Khepera robot, and the results show that their developed bioinspired behavior-based controller is simple but robust. Yang and Meng used a bioinspired neural network to realize the dynamic collision-free trajectory generation for mobile robot in a nonstationary environment. This bioinspired neural network is based on a shunting model, which is obtained from a computational membrane model for a patch of membrane in a biological neural system (proposed\n\n"}
{"case_id": "d4e24c4f015d55b91350ce6267a7af5e", "answer_text": "\n\nDexterous, in-hand robotic manipulation represents a significant advancement in robotics, with practical applications spanning various domains. The most crucial applications include advanced industrial manufacturing [14], medical and surgical robotics, service robotics [13], exploratory robotics in hazardous or inaccessible environments [3], research and development, and consumer electronics. \n\nDexterous robotic manipulation of unknown objects can open the way to novel tasks and applications of robots in semi-structured and unstructured settings, from advanced industrial manufacturing to exploration of harsh environments [3]. \nOne of the primary applications of dexterous in-hand manipulation is in industrial manufacturing, where precision and adaptability are paramount. Traditional industrial robots with parallel jaw grippers are often limited in their ability to handle complex tasks that involve intricate manipulations. Multi-fingered robotic hands with dexterous manipulation capabilities can perform tasks such as assembling small parts, manipulating delicate components, and handling fragile objects without causing damage [3]. This capability is essential for sectors like electronics manufacturing, where components are small and sensitive, requiring precision that human hands typically provide. The introduction of dexterous manipulation in such settings can significantly improve efficiency and reduce production errors.\n\nMedical and Surgical Robotics\nAnother critical application area is in medical and surgical robotics. The precision and fine motor control required in surgical procedures can be facilitated by dexterous robotic hands. These systems can assist surgeons in performing complex operations with higher precision and less invasiveness than traditional manual methods. For example, dexterous robots can be used in minimally invasive surgeries where they can navigate through small incisions and manipulate tissues or organs with high accuracy, reducing patient recovery times and improving surgical outcomes [5]. Additionally, robots equipped with dexterous manipulation can aid in telemedicine, allowing specialists to perform procedures remotely with the help of robotic hands controlled via haptic feedback.\n\nService robotics has recently seen advancements through the integration of machine learning techniques in adaptive robot hands, combining analytical models and constrained optimization schemes to simplify dexterous in-hand manipulation [0]. These methods help in executing fingertip-based, within-hand manipulation tasks with minimal sensory information and control effort and have been demonstrated experimentally on various adaptive hands. Additionally, the use of deep reinforcement learning (RL) has also achieved significant progress, particularly in learning and transferring in-hand manipulation skills from simulation to real-world settings, thus enabling agile and robust manipulation using low-cost hardware and vision-based pipelines [1].\nService robots that operate in human environments, such as homes, offices, and hospitals, benefit greatly from dexterous in-hand manipulation. These robots can perform a variety of tasks, ranging from household chores to assisting the elderly and disabled with their daily activities. For instance, a service robot with dexterous hands can prepare meals, handle different objects, and assist in personal care tasks such as dressing or grooming. The ability to manipulate a wide range of household items with precision enables these robots to act as effective aides, improving the quality of life for individuals who require assistance [10].\n\nExploratory and Hazardous Environment Robotics [3]. \nIn hazardous or inaccessible environments, such as deep-sea exploration, space missions, or nuclear facilities, dexterous robotic hands can perform tasks that are dangerous or impossible for humans. These robots can manipulate tools, conduct repairs, and gather samples in environments that are otherwise hostile to human life. For example, in underwater exploration, dexterous manipulation allows robots to handle delicate coral samples or repair underwater structures without causing environmental damage. Similarly, in space exploration, robotic hands can perform maintenance on satellites or space stations, reducing the need for risky human extravehicular activities [7].\n\n### Research and Development\n\nResearch and development in the field of dexterous robotic manipulation focus on leveraging advanced methodologies such as deep reinforcement learning, robust control policies, and vision-based state estimation to enhance the capabilities of robotic hands [2]. The hybrid approach that combines analytical modeling, constrained optimization, and machine learning techniques simplifies the execution of intricate in-hand manipulation tasks [0]. By training in-hand manipulation skills in simulation, it is possible to achieve high levels of dexterity when deploying these skills on physical robots [6]. Additionally, learning-based dexterous manipulation systems have been successfully transferred from simulation to reality, demonstrating robust performance in various robotic settings [1]. As a result, these advancements open up new possibilities for the application of robots in semi-structured and unstructured settings, from industrial environments to everyday household tasks [3]. \nMoreover, the field of research and development itself benefits significantly from advancements in dexterous manipulation. Developing improved manipulation techniques requires extensive experimentation and fine-tuning, often facilitated by versatile robotic platforms. These robots serve as testbeds for new algorithms, sensors, and control strategies, pushing the boundaries of what is possible in robotics [1]. The knowledge gained from such research can then be transferred to practical applications in various domains, further driving innovation.\n\nConsumer Electronics and Robotics[3]\nIn the consumer electronics industry, dexterous robotic hands can be used in the assembly and testing of intricate devices such as smartphones and wearables. These robots can manipulate small components with high precision, ensuring that each device meets quality standards and reducing the likelihood of defects [0]. Additionally, consumer robots with dexterous manipulation abilities can perform tasks such as fixing electronic devices or assembling furniture, making them valuable household helpers [14].\n\nOverall, the versatility of dexterous, in-hand robotic manipulation allows it to be applied across a wide range of practical scenarios, significantly enhancing the capabilities of robots to perform complex, precise, and adaptive tasks in both structured and unstructured environments [3]. As the technology continues to evolve, the potential applications and benefits of dexterous robotic hands will likely expand even further [5].\n\n\nReferences:\n[0] Combining Analytical Modeling and Learning to Simplify Dexterous Manipulation With Adaptive Robot Hands Combining Analytical Modeling and Learning to Simplify Dexterous Manipulation With Adaptive Robot Hands In this paper, we focus on the formulation of a hybrid methodology that combines analytical models, constrained optimization schemes, and machine learning techniques to simplify the execution of dexterous, in-hand manipulation tasks with adaptive robot hands. More precisely, the constrained optimization scheme is used to describe the kinematics of adaptive hands during the grasping and manipulation processes, unsupervised learning (clustering) is used to group together similar manipulation strategies, dimensionality reduction is used to either extract a set of representative motion primitives (for the identified groups of manipulation strategies) or to solve the manipulation problem in a low-d space and finally an automated experimental setup is used for unsupervised, automated collection of large data sets. We also assess the capabilities of the derived manipulation models and primitives for both model and everyday life objects, and we analyze the resulting manipulation ranges of motion (e.g., object perturbations achieved during the dexterous, in-hand manipulation). We show that the proposed methods facilitate the execution of fingertip-based, within-hand manipulation tasks while requiring minimal sensory information and control effort, and we demonstrate this experimentally on a range of adaptive hands. Finally, we introduce DexRep, an online repository for dexterous manipulation models that facilitate the execution of complex tasks with adaptive robot hands. Note to Practitioners—Robot grasping and dexterous, in-hand manipulations are typically executed with fully actuated robot hands that rely on analytical methods, computation of the hand object system Jacobians, and extensive numerical simulations for deriving optimal strategies. However,\n\n[1] DeXtreme: Transfer of Agile In-hand Manipulation from Simulation to Reality •We demonstrate a system for learning-based dexterous in-hand manipulation that uses low-cost hardware (one order of magnitude less expensive than), uses a purely vision-based pipeline, sets more diverse pose targets, uses orders-of-magnitude cheaper compute, and offers further insights into this problem with detailed ablations.•We develop a highly robust pose estimator trained entirely in simulation which works through heavy occlusions and in a variety of robotic settings e.g. https://www.youtube.com/watch?v=-MTsm0Uh_5o.•While not directly comparable to due to different hardware, our purely vision-based state estimation results not only outperform their best vision-based results, but also fare comparably to their marker-based results.•We will also release both our vision and RL pipelines for reproducibility. We seek to provide a much broader segment of the research community with access to a novel state-of-the-art in-hand manipulation system in hopes of catalyzing further studies and advances.\n\n[2] Learning dexterous in-hand manipulation In this work, we demonstrate methods to train control policies that perform in-hand manipulation and deploy them on a physical robot.The resulting policy exhibits unprecedented levels of dexterity and naturally discovers grasp types found in humans, such as the tripod, prismatic, and tip pinch grasps,and displays contact-rich, dynamic behaviours such as finger gaiting, multi-finger coordination, the controlled use of gravity, and coordinated application of translational and torsional forces to the object.Our policy can also use vision to sense an object’s pose — an important aspect for robots that should ultimately work outside of a controlled lab setting.\n\n[3] Learning by Demonstration and Robust Control of Dexterous In-Hand Robotic Manipulation Skills Dexterous robotic manipulation of unknown objects can open the way to novel tasks and applications of robots in semi-structured and unstructured settings, from advanced industrial manufacturing to exploration of harsh environments. However, it is challenging for at least three reasons: the desired motion of the object might be too complex to be described analytically, precise models of the manipulated objects are not available, the controller should simultaneously ensure both a robust grasp and an effective in-hand motion. To solve these issues we propose to learn in-hand robotic manipulation tasks from human demonstrations, using Dynamical Movement Primitives (DMPs), and to reproduce them with a robust compliant controller based on the Virtual Springs Framework (VSF), that employs real-time feedback of the contact forces measured on the robot fingertips. With this solution, the generalization capabilities of DMPs can be transferred successfully to the dexterous in-hand manipulation problem: we demonstrate this by presenting real-world experiments of in-hand translation and rotation of unknown objects.\n\n[4] Deep Dynamics Models for Learning Dexterous Manipulation Dexterous manipulation with multi-fingered hands represents a grand challenge in robotics: the versatility of the human hand is as yet unrivaled by the capabilities of robotic systems, and bridging this gap will enable more general and capable robots. Although some real-world tasks can be accomplished with simple parallel jaw grippers, there are countless tasks in which dexterity in the form of redundant degrees of freedom is critical. In fact, dexterous manipulation is defined  as being object-centric, with the goal of controlling object movement through precise control of forces and motions – something that is not possible without the ability to simultaneously impact the object from multiple directions. Through added controllability and stability, multi-fingered hands enable useful fine motor skills that are necessary for deliberate interaction with objects. For example, using only two fingers to attempt common tasks such as opening the lid of a jar, hitting a nail with a hammer, or writing on paper with a pencil would quickly encounter the challenges of slippage, complex contact forces, and underactuation. Success in such settings requires a sufficiently dexterous hand, as well as an intelligent policy that can endow such a hand with the appropriate control strategy.\n\n[5] Learning dexterous in-hand manipulation In this work, we demonstrate thatin-hand manipulation skills learned with RL in a simulator can achieve an unprecedentedlevel of dexterity on a physical five-fingered hand.This is possible due to extensive randomizations of the simulator,large-scale distributed training infrastructure, policies with memory, anda choice of sensing modalities which can be modelled in the simulator.Our results demonstrate that,contrary to a common belief,contemporary deep RL algorithmscan be applied to solving complex real-world robotics problems whichare beyond the reach of existing non-learning-based approaches.\n\n[6] The JamHand: Dexterous Manipulation with Minimal Actuation. From using chopsticks to grab items off a plate, to snapping together two LEGO bricks in one hand, common manipulation tasks are easy for humans. However, grasping and dexterous manipulation still rank among the principal grand challenges in robotics. A key challenge is the complex interaction between hand biomechanics and motor control, leading to humanoid hands that remain too complex and costly for use in daily tasks. Here, we bypass this challenge by offering an alternative approach based on multi-finger material phase transition effects. By limiting our focus to dexterous manipulation, we are able to design a robotic hand that can achieve six fundamental dexterous manipulations as well as precision and power grasps, all with only two actuators. We further demonstrate our system on a range of real-world grasping and manipulation challenges. Besides practical application, these results suggest that leveraging the phase transition of granular materials is a viable technique for reducing the hand complexity required for performing daily tasks.\n\n[7] Dexterous Manipulation with Deep Reinforcement Learning: Efficient, General, and Low-Cost In this work, we study real-world model-free reinforcement learning as a means to learn complex dexterous hand manipulation behaviors. We show that model-free deep RL algorithms can provide a practical, efficient, and general method for learning with high dimensional multi-fingered hands. Model-free RL algorithms can easily be applied to a variety of low-cost hands, and solve challenging tasks that are hard to simulate accurately. This kind of generality and minimal manual engineering may be a key ingredient in endowing robots with the kinds of large skill repertoires they need to be useful in open-world environments such as homes, offices, and hospitals. We also show that the sample complexity of model-free RL can be substantially reduced with suboptimal kinesthetic demonstrations, while also improving the resulting motion quality.\n\n[8] A hand-centric classification of human and robot dexterous manipulation. This work contributes to the development of a common framework for the discussion and analysis of dexterous manipulation across the human and robotic domains. An overview of previous work is first provided along with an analysis of the tradeoffs between arm and hand dexterity. A hand-centric and motion-centric manipulation classification is then presented and applied in four different ways. It is first discussed how the taxonomy can be used to identify a manipulation strategy. Then, applications for robot hand analysis and engineering design are explained. Finally, the classification is applied to three activities of daily living (ADLs) to distinguish the patterns of dexterous manipulation involved in each task. The same analysis method could be used to predict problem ADLs for various impairments or to produce a representative benchmark set of ADL tasks. Overall, the classification scheme proposed creates a descriptive framework that can be used to effectively describe hand movements during manipulation in a variety of contexts and might be combined with existing object centric or other taxonomies to provide a complete description of a specific manipulation task.\n\n[9] Dextrous Tactile In-Hand Manipulation Using a Modular Reinforcement Learning Architecture Many important application domains, like industrial manufacturing or housework, still has still to be done by humans.One important reason for this is the need for dextrous fine manipulation.In particular, the task of in-hand manipulation, i.e., reorienting an object inside a hand, is often needed but challenging due to the intricate multi-contact dynamics, the coordination of the many degrees of freedom (DOF) of a multi-fingered hand, and the estimation of the state of the manipulated object.In this paper, we extend our previous work  on learning purely tactile in-hand manipulation with a torque-controlled hand from being able to rotate a cube around a single axis, to reorienting the cube to any of the 24 possible orientations in a π/2𝜋2\\pi/2-raster (see Fig. 1).We solve the task in a realistic setting by holding the hand upside down, hence, demanding permanent force closure, and without external sensors (like cameras) but using only the fingers’ integrated position and torque sensors.The task is learned from scratch using a modular deep reinforcement learning architecture where learning the manipulation strategy is separated from learning an estimator for the cube state. Fig. 2 and Fig. 3 give an overview of the robotic and task setup as well as the control architecture.\n\n"}
{"case_id": "e7bef53d393712d4b1010a62ee647fb2", "answer_text": "\n\nCoverage motion planning algorithms for floor-cleaning robots are diverse in their methodologies and effectiveness. These algorithms can be broadly categorized into those utilizing decomposition methods [1], search-based methods [2], heuristics-based approaches [3], and hybrid methods [4]. Each category offers unique benefits and trade-offs concerning efficiency, completeness, and adaptability to dynamic environments [0].\n\n1. **Decomposition-Based Methods**: [1], [12] \n   - **Cellular Decomposition**: This method involves dividing the cleaning area into smaller cells that can be individually managed. The complete coverage path planning algorithm proposed by [1] employs cellular decomposition to split the workspace into cells and efficiently covers each cell based on the distance between their centroids. This approach ensures minimal turning paths and reduces energy consumption and working time, demonstrating its effectiveness through simulation and experimental results. One specific advantage of cellular decomposition is its straightforward implementation and systematic coverage, though it may struggle in highly dynamic or unpredictable environments.\n   - **Boustrophedon Cellular Decomposition**: A specific type of cellular decomposition, the boustrophedon method divides the area into slices that the robot covers in a back-and-forth pattern. When combined with A* search for re-navigation, this method can ensure complete coverage even in the presence of obstacles, as demonstrated by [4] and [12]. This approach leverages backtracking points to navigate through unvisited spaces efficiently. Its primary advantage is the methodical sweeping motion which minimizes overlap and missed spots, but it may be less efficient if the environment contains numerous static obstacles.\n\n2. **Search-Based Methods**: \n\nSearch-based methods encompass various techniques aimed at achieving efficient coverage path planning and obstacle avoidance for mobile robots. For example, A* search combined with boustrophedon motions ensures complete coverage of a workspace by guiding the robot to the nearest backtracking point after detecting obstacles [4]. Another innovative approach involves using a modified A* algorithm to enhance the computational efficiency and path optimization for reconfigurable robots, demonstrating significant improvements in terms of area covered and distance traveled [3]. Additionally, the Complete Coverage D* (CCD*) algorithm has been developed to account for robot dimensions and dynamically adapt to changes in the environment, providing a safe and efficient path for floor-cleaning robots [2].\n\n   - **D* Search Algorithm**: The Complete Coverage D* (CCD*) algorithm is an extension of the traditional D* algorithm, adapted for complete coverage by considering the robot dimensions and emphasizing path safety and reduced path length [2]. This method is particularly effective in dynamic environments where the robot can adapt its path in response to changes in the environment. The CCD* algorithm ensures that the robot's movements are safe and efficient, even as new obstacles appear, making it highly suitable for large, complex spaces such as warehouses or industrial areas. However, CCD* may require significant computational resources to continually re-optimize paths in real-time.\n   - **A* Algorithm**: Modifications to the A* algorithm can significantly enhance path planning efficiency. For instance, the Modified A* Algorithm proposed by [3] improves computational effort, reconfiguration time, and path efficiency, resulting in better performance in simulations with varying environmental complexities. Real-world implementations of such algorithms are seen in commercial robotic vacuums that need to navigate around furniture and other household obstacles. The advantage here is the balance between optimality and computational efficiency, though it may still be computationally intensive for highly dynamic environments.\n\n3. **Heuristics-Based Methods**: [8]\n   - **Evolutionary Algorithms**: Utilizing genetic algorithms (GA) and ant colony optimization (ACO), these methods solve the complete coverage path planning problem by modeling it as a traveling salesman problem. Works such as [9] and [11] demonstrate that evolutionary algorithms can effectively optimize the robot’s navigation sequence, balancing energy consumption between locomotion and shape-shifting in reconfigurable robots. These algorithms are particularly advantageous in environments where the layout is highly variable and requires continual adaptation. One drawback is the potential complexity and computational overhead associated with evolutionary computations.\n   - **Divide and Conquer Strategy**: This approach optimizes path planning through geometric problem-solving and heuristic optimization, as illustrated by [8]. The strategy provides shorter coverage paths compared to traditional contour planners and shows promise for real-world applications, particularly in multi-room environments. This method is particularly useful in real-life applications requiring the cleaning of large and intricate floor layouts, such as office buildings or schools. The advantage lies in its ability to handle multiple rooms efficiently, but it may need further refinement to address more complex environments with higher obstacle density.\n\n4. **Hybrid Approaches**: Coverage path planning (CPP) techniques are essential in floor cleaning robotic systems. While numerous approaches demonstrate the benefits of conventional CPP techniques, they are mostly limited to fixed morphological platforms. A novel motion planning technique for a Tetris-inspired reconfigurable floor cleaning robot named \"hTetro\" that can reconfigure its morphology to any of the seven one-sided Tetris pieces has shown significantly higher performance in terms of area recovered compared to traditional methods [5]. Similarly, a new complete coverage algorithm, the Complete Coverage D* (CCD*) algorithm, was developed based on the D* search of a two-dimensional occupancy grid map of the environment, emphasizing safety of motion and reductions in path length and search time [2]. \n\n   - **Tetriamond and Polyomino Tiling**: Novel approaches for reconfigurable robots involve using tiling theory to optimize area coverage. For example, the motion planning technique for Tetris-inspired robots like “hTetro” adapts polyomino tiling to maximize the area covered [5][13]. This method achieves significantly higher performance metrics compared to traditional methods due to its adaptability to different shapes and spaces. Its major advantage is the ability to cover areas efficiently by reconfiguring the robot's shape, though this can add mechanical complexity and increase the energy requirements for shape-shifting.\n   - **Behavior-Based Algorithms**: Sensory-driven, behavior-based algorithms, like those used in the Gator robot, combine distinct behaviors to achieve area coverage. These behaviors include obstacle avoidance, wall following, and seeking open areas. Such an approach, despite its simpler implementation, can achieve an efficiency close to 85% in practical scenarios [14]. This method is often used in commercial robotic vacuums for household use, balancing simplicity and efficiency. The primary advantage is the low computational requirement and robustness in varied environments, though it may not always achieve complete coverage as systematically as more structured methods.\n\n**Current Limitations and Ongoing Challenges**[13]:\nDespite the significant advancements in coverage motion planning algorithms, several challenges persist. One of the primary limitations is the high computational demand required for some search-based and heuristics-based methods, such as CCD* [2] and evolutionary algorithms [9], which may limit their real-time applicability in highly dynamic environments. \n\nMoreover, while cellular decomposition and boustrophedon methods perform well in structured and static environments, they may become less efficient or even fail in highly dynamic or unpredictable scenarios where the environment changes frequently [4].\n\nHybrid approaches, although promising, often introduce additional mechanical complexity, energy requirements, and potential reliability issues, particularly for reconfigurable robots [3]. Their adaptability comes at the cost of increased system complexity, which can lead to higher maintenance needs and possibly reduced long-term reliability [5]. \n\nIn summary, effective coverage motion planning algorithms for floor-cleaning robots include cellular decomposition methods for structured coverage [1], search-based methods like D* and A* algorithms for adaptive path planning [2][4], and heuristics-based approaches for optimization in complex environments [10]. Hybrid methods that combine tiling theories and behavior-based strategies offer innovative solutions for specific challenges in coverage planning [13][14]. Each approach's effectiveness is context-dependent, highlighting the need to choose the right algorithm based on specific operational requirements and environmental conditions. \n\n\nReferences:\n[0] Complete coverage path planning and guidance for cleaning robots Complete coverage path planning and guidance for cleaning robots This paper describes a complete coverage path planning and guidance methodology for a mobile robot, having the automatic floor cleaning of large industrial areas as a target application. The proposed algorithms rely on the a priori knowledge of a 2D map of the environment and cope with unexpected obstacles not represented on the map. A template based approach is used to control the path execution, thus incorporating, in a natural way, the kinematic and the geometric model of the mobile robot on the path planning procedure. The novelty of the proposed approach is the capability of the path planner to deal with a priori mapped or unexpected obstacles in the middle of the working space. If unmapped obstacles permanently block the planned trajectory, the path tracking control avoids these obstacles. The paper presents experimental results with a LABMATE mobile robot, confirming the feasibility of the total coverage path and the robustness of the path tracking behaviour based control.\n\n[1] Complete coverage path planning for multi-robots based on Complete coverage path planning for multi-robots based on Complete coverage path planning is major problem for autonomous mobile robot, which concerns both efficiency and completeness of coverage. In this paper, a complete coverage path planning algorithm is proposed for two indoor floor-caring robots, such as cleaning or inspecting industrial and public floor areas, to have the minimal turning path based on the shape and size of the cell covering the whole working area. The proposed algorithm divides the whole cleaning area into cells by cellular decomposition method, and then provides efficient covering order over the cells based on distance among centroids of cells. It also provides more optimal coverage path and reduces the rate of energy consumption and working time. Both simulation and experimental results demonstrate the effectiveness of the algorithm. As compared performance indices between experiment and simulation, the total number of turns and the working time show the practical efficiency and robustness of the proposed algorithm.\n\n[2] Complete Coverage D* Algorithm for Path Planning of a Floor-Cleaning Mobile Robot Complete Coverage D* Algorithm for Path Planning of a Floor-Cleaning Mobile Robot Abstract Inspired by the path transform (PT) algorithm of Zelinsky et al. the novel algorithm of complete coverage called complete coverage D* (CCD*) algorithm is developed, based on the D* search of the two-dimensional occupancy grid map of the environment. Unlike the original PT algorithm the CCD* algorithm takes the robot's dimension into account, with emphasis on safety of motion and reductions of path length and search time. Additionally, the proposed CCD* algorithm has ability to produce new complete coverage path as the environment changes. The algorithms were tested on a Pioneer 3DX mobile robot equipped with a laser range finder.\n\n[3] Effective Motion planning and Navigation for Reconfigurable Mobile Robots using Modified A* Algorithm Effective Motion planning and Navigation for Reconfigurable Mobile Robots using Modified A* Algorithm Robotic systems that can transform their structure while in operation are known as reconfigurable robots. The additional configuration flexibility makes them ideal for a variety of industrial applications involving motion planning, such as sanitizing, space vehicle maintenance, and surveillance. Every floor cleaning robot needs to have a coverage route planning strategy. Navigation and its planning in the case of a moving cleaning robot, using a grid map is the subject of this paper. An essential assumption has been made here, that the given autonomous vehicle is fully operational and has a consistent reactive navigation system. As a result, such topics are not covered in this article. The study introduces an enhancement to the conventional A* algorithm. These changes are particularly concerned with computational effort, reconfiguration time, and path efficiency. These were then tested in a simulation-based circumstance with varying degrees of environmental complexity, which demonstrated the utility of the proposed algorithm. The results show that the suggested technique showcases a good performance in terms of the area covered and the distance traveled.\n\n[4] An online complete coverage algorithm for cleaning robots based on boustrophedon motions and A* search An online complete coverage algorithm for cleaning robots based on boustrophedon motions and A* search This paper presents an online complete coverage algorithm for cleaning robots based on boustrophedon motions combined with A* search algorithm. In our approach, while performing a boustrophedon motion to cover an unvisited area in the workspace, the robot detects and stores backtracking points. To execute the next boustrophedon motion, A* search is employed as a backtracking mechanism which guides the robot to the nearest backtracking point. Experimental results prove that the proposed algorithm ensures the complete coverage of the workspace in the finite moving steps of the robot. Furthermore, our proposed approach is efficient in terms of the covered path length and the number of boustrophedon motions.\n\n[5] Motion planner for a Tetris-inspired reconfigurable floor cleaning robot Coverage path planning technique is an essential ingredient in every floor cleaning robotic systems. Even though numerous approaches demonstrate the benefits of conventional coverage motion planning techniques, they are mostly limited to fixed morphological platforms. In this article, we put forward a novel motion planning technique for a Tetris-inspired reconfigurable floor cleaning robot named “hTetro” that can reconfigure its morphology to any of the seven one-sided Tetris pieces. The proposed motion planning technique adapts polyomino tiling theory to tile a defined space, generates reference coordinates, and produces a navigation path to traverse on the generated tile-set with an objective of maximizing the area coverage. We have summarized all these aspects and concluded with experiments in a simulated environment that benchmarks the proposed technique with conventional approaches. The results show that the proposed motion planning technique achieves significantly higher performance in terms of area recovered than the traditional methods.\n\n[6] A Divide and Conquer Strategy for Sweeping Coverage Path Planning The room coverage is solved by a geometrical approach, while defining the shortest tour is solved as an optimisation problem using a heuristic algorithm. The results show that the complete coverage paths obtained with the DnCS-RCPP are shorter than those obtained with the Contour planner, providing an excellent alternative to efficiently compute complete coverage paths for sweeping robots when the target area is composed of several rooms. The overall processing time required to obtain suitable paths provides promising results to be considered for implementation on real robotics platforms. In future works, the generalisation of the method for rooms with more than two possible doors, as well as larger areas such as malls or sports facilities, will be addressed, taking into consideration the kinematic constraints imposed by the mechanical configuration of the autonomous cleaning robot, such as commercial scrubber dryer robots that require a specific turning radius to avoid leaving wet marks or spills on the floor.\n\n[7] Evolutionary Algorithm-Based Complete Coverage Path Planning for Tetriamond Tiling Robots Tiling robots with fixed morphology face major challenges in terms of covering the cleaning area and generating the optimal trajectory during navigation. Developing a self-reconfigurable autonomous robot is a probable solution to these issues, as it adapts various forms and accesses narrow spaces during navigation. The total navigation energy includes the energy expenditure during locomotion and the shape-shifting of the platform. Thus, during motion planning, the optimal navigation sequence of a self-reconfigurable robot must include the components of the navigation energy and the area coverage. This paper addresses the framework to generate an optimal navigation path for reconfigurable cleaning robots made of tetriamonds. During formulation, the cleaning environment is filled with various tiling patterns of the tetriamond-based robot, and each tiling pattern is addressed by a waypoint. The objective is to minimize the amount of shape-shifting needed to fill the workspace. The energy cost function is formulated based on the travel distance between waypoints, which considers the platform locomotion inside the workspace. The objective function is optimized based on evolutionary algorithms such as the genetic algorithm (GA) and ant colony optimization (ACO) of the traveling salesman problem (TSP) and estimates the shortest path that connects all waypoints. The proposed path planning technique can be extended to other polyamond-based reconfigurable robots.\n\n[8] An advanced planning and navigation approach for autonomous cleaning robot operations An advanced planning and navigation approach for autonomous cleaning robot operations This paper presents an information processing scheme which increases the autonomy of floor caring robots in indoor applications, such as cleaning or inspection of industrial and public floor areas. An automatic planner generates a serpentine path pattern by repeated and appropriate concatenation of up to five basic motion macros. This nominal path is executed by a Cartesian motion controller. In case of unexpected obstacles the robot updates its internal geometric representation of the work area to be processed and replans its path. Due to obstacle avoidance, certain areas may remain unprocessed. Location and topology of such areas are automatically reconstructed for repeated follow-up floor caring in case of temporary obstacles. For this purpose connecting maneuvres and paths of complete floor coverage are incrementally planned and executed. Landmark localization at automatically preplanned reference points allows online modification of motion commands and guarantees precise long distance path tracking. Feasibility of the developed techniques has been demonstrated in various long-term experiments with the prototype mobile service robot MACROBE.\n\n[9] Realization Energy Optimization of Complete Path Planning in Differential Drive Based Self-Reconfigurable Floor Cleaning Robot The efficiency of energy usage applied to robots that implement autonomous duties such as floor cleaning depends crucially on the adopted path planning strategies. Energy-aware for complete coverage path planning (CCPP) in the reconfigurable robots raises interesting research, since the ability to change the robot’s shape needs the dynamic estimate energy model. In this paper, a CCPP for a predefined workspace by a new floor cleaning platform (hTetro) which can self-reconfigure among seven tetromino shape by the cooperation of hinge-based four blocks with independent differential drive modules is proposed. To this end, the energy consumption is represented by travel distances which consider operations of differential drive modules of the hTetro kinematic designs to fulfill the transformation, orientation correction and translation actions during robot navigation processes from source waypoint to destination waypoint. The optimal trajectory connecting all pairs of waypoints on the workspace is modeled and solved by evolutionary algorithms of TSP such as Genetic Algorithm (GA) and Ant Optimization Colony (AC) which are among the well-known optimization approaches of TSP. The evaluations across several conventional complete coverage algorithms to prove that TSP-based proposed method is a practical energy-aware navigation sequencing strategy that can be implemented to our hTetro robot in different real-time workspaces. Moreover, The CCPP framework with its modulation in this paper allows the convenient implementation on other polynomial-based reconfigurable robots.\n\n[10] Coverage planning with finite resources Coverage planning with finite resources The robot coverage problem, a common planning problem, consists of finding a motion path for the robot that passes over all points in a given area or space. In many robotic applications involving coverage, e.g., industrial cleaning, mine sweeping, and agricultural operations, the desired coverage area is large and of arbitrary layout. In this work, we address the real problem of planning for coverage when the robot has limited battery or fuel, which restricts the length of travel of the robot before needing to be serviced. We introduce a new sweeping planning algorithm, which builds upon the boustrophedon cellular decomposition coverage algorithm to include a fixed fuel or battery capacity of the robot. We prove the algorithm is complete and show illustrative examples of the planned coverage outcome in a real building floor map.\n\n[11] Motion planner for a Tetris-inspired reconfigurable floor cleaning robot Motion planner for a Tetris-inspired reconfigurable floor cleaning robot Coverage path planning technique is an essential ingredient in every floor cleaning robotic systems. Even though numerous approaches demonstrate the benefits of conventional coverage motion planning techniques, they are mostly limited to fixed morphological platforms. In this article, we put forward a novel motion planning technique for a Tetris-inspired reconfigurable floor cleaning robot named “hTetro” that can reconfigure its morphology to any of the seven one-sided Tetris pieces. The proposed motion planning technique adapts polyomino tiling theory to tile a defined space, generates reference coordinates, and produces a navigation path to traverse on the generated tile-set with an objective of maximizing the area coverage. We have summarized all these aspects and concluded with experiments in a simulated environment that benchmarks the proposed technique with conventional approaches. The results show that the proposed motion planning technique achieves significantly higher performance in terms of area recovered than the traditional methods. Introduction Floor cleaning robots are becoming an integral part of every household that could improvise the productivity and quality of life through performing repetitive and timeconsuming task. It is estimated that cleaning robots could reach a market value of USD 4.34 Billion in 2023, wherein the floor cleaning robots will hold a more significant share. 1 Even though the dominant market player iRobot, Neato, Samsung, and Xiaomi claims that they have sold millions of units, there exists numerous research challenges and open questions to be addressed. Such research initiatives have continually pushed the performance limits of floor cleaning robots\n\n[12] Sweep Strategies for a Sensory-driven, Behavior-based Vacuum Cleaning Agent SWEEP STRATEGIES FOR A SENSORY-DRIVEN, BEHAVIOR-BASED VACUUM CLEANING AGENT by In the Machine Intelligence Laboratory, University of Florida. we have built a small autonomous robot and programmed it to exhibit various reactive behaviors. The robot. named Gator. performs area coverage in an interior room by combining distinct behaviors. Gator has 26 sensors of which only 7 are used in the coverage algorithms examined here. Gator's behaviors allow it to avoid obstacles. follow walls. seek open areas and break out of confined areas. In the limited number of experiments performed here it appears that the best coverage algorithm among those tried consisted of a random walk with a probability of 0.05 for following a wall after each encounter with an obstacle. After 20 minutes about 85% of the floor space was covered with an efficiency close to 85%.\n\n"}
{"case_id": "89477b47d76d0cc8eb59b1b7938bb988", "answer_text": "\n\nMetrics to measure the quality of human-robot communication (HRC) are multifaceted, comprising both objective and subjective elements to capture a comprehensive view of the interaction dynamics [1]. These metrics revolve around several key aspects, including proficiency [0], fluency [7], trust [4], reliability [4], and the overall effectiveness of the interaction [14].\n\n\n### Proficiency and Self-Assessment Metrics [0]\n\n\nOne of the central categories in HRC metrics is proficiency-based interaction, which entails the robot's self-assessment of its ability to perform tasks. Metrics in this category include robot self-assessment of proficiency (RSA), robot communication of proficiency (RCP), human understanding of proficiency (HUP), and robot perception of the human’s intentions, values, and assessments (RPH). These metrics gauge how well the robot can assess its own performance, communicate this to the human partner, and understand the feedback or expectations from the human [0].\n\nObjective Metrics [0]\n\nObjective metrics are quantifiable and provide measurable data on various aspects of the interaction. For instance, success rates, interaction force, timing metrics (e.g., idle time and total task duration), and joint effort are commonly used to assess the efficiency and effectiveness of human-robot collaboration [10]. Metrics such as neglect time, attention demand, and task success are also significant as they directly measure the robot's autonomy and the human's engagement during the interaction [2].\n\n### Fluency and Efficiency Metrics [14]. \n\nFluency metrics specifically assess the smoothness and efficiency of the interaction, capturing elements such as concurrent motion, idle times, and coordination. For example, metrics like human idle time, robot idle time, concurrent activity, and functional delay can provide insights into how seamlessly the human and robot work together [7][14].\n\nSubjective metrics refer to qualitative assessments gathered from human participants regarding their interaction with robots. These metrics often involve scores or evaluations on aspects such as ease of use, satisfaction, comfort, safety, and trust in the robot's performance. For instance, participants may be asked to rate how easy it was to receive an object from a robot, their satisfaction with the interaction, how comfortable the interaction was, and how safe they felt during the interaction [1]. Subjective metrics are essential for understanding the human experience in human-robot interactions and can provide insights that objective metrics may overlook. They help in evaluating the overall effectiveness and acceptance of the robot systems in collaborative tasks. \n\nSubjective metrics involve user feedback and perceptions, often collected through surveys and questionnaires. These include measures of satisfaction, comfort, trust in the robot, and perceived safety during the interaction [13]. Likert-scale questionnaires may ask users to evaluate the ease of interaction, their comfort level, and their trust in the robot’s capabilities, providing a subjective assessment of the interaction quality [13].\n\nTrust and Reliability Metrics are essential for evaluating the characteristics and performance of human-robot interaction systems, especially in the context of proficiency-based human-robot interaction (HRI) [0]. Additionally, the development of collaborative robots necessitates the use of both objective and subjective metrics to interpret where and how these systems can be improved and refined [1]. Common metrics include quantitative measures such as neglect time and interaction effort, as well as qualitative measures like user satisfaction and trust [2]. Furthermore, it is crucial to have a generic performance metric that can assess both human and robot independent actions as well as their joint interactions to ensure the quality of the accomplished task [3]. Metrics like trust modeling and reliability assessment play a significant role in understanding the interaction dynamics and overall system performance [4]. Finally, a comprehensive set of benchmarking test beds is needed to evaluate human-robot teams across different scenarios to achieve a fair comparison and reproducible evaluations [10]. \n\nTrust is a critical factor in HRC, influencing how humans interact with and rely on robotic systems. Metrics for trust include measures of human reliability during interaction, the fidelity of the robot’s actions to human expectations, and psychophysiological measurements such as electroencephalograms (EEG) and functional near-infrared spectroscopy (fNIRS). These metrics help in understanding the level of trust humans have in robots and how this trust impacts the interaction dynamics [11].\n\n### Communication and Learning Metrics [1][3][5].\n\n\n\nEffective communication is essential for successful HRC, especially in learning contexts where the robot must understand and act on human instructions. Metrics here include the amount of time the human spends teaching the robot, the number of inputs provided, and the effort required for these inputs. Additionally, measures of team fluency, such as idle time and interaction delays, are used to assess communication quality [5]. Metrics for learning accuracy, error rates, and the difference between optimal and learned behaviors (regret) are also important in evaluating how well the robot acquires and applies new knowledge [8].\n\nComprehensive Frameworks[3].\n\nSeveral studies propose comprehensive frameworks that integrate multiple metrics to provide a holistic evaluation of HRC. For instance, generic performance metrics that assess both individual and joint actions of human-robot teams have been suggested to ensure the quality of task accomplishment and the overall interaction experience. These frameworks often include both objective and subjective measures, ensuring a balanced assessment of both technical performance and user experience [4][12].\n\nIn summary, a robust evaluation of human-robot communication quality relies on a combination of objective, subjective, proficiency-based, trust-related, and learning metrics. These metrics collectively provide a detailed understanding of the interaction dynamics, guiding the development and improvement of human-robot systems to enhance their effectiveness and user acceptance [1][3][5]. \n\n\nReferences:\n[0] Metrics for Robot Proficiency Self-assessment and Communication of Proficiency in Human-robot Teams Metrics for Robot Proficiency Self-assessment and Communication of Proficiency in Human-robot Teams As development of robots with the ability to self-assess their proficiency for accomplishing tasks continues to grow, metrics are needed to evaluate the characteristics and performance of these robot systems and their interactions with humans. This proficiency-based human-robot interaction (HRI) use case can occur before, during, or after the performance of a task. This article presents a set of metrics for this use case, driven by a four-stage cyclical interaction flow: (1) robot self-assessment of proficiency (RSA), (2) robot communication of proficiency to the human (RCP), (3) human understanding of proficiency (HUP), and (4) robot perception of the human’s intentions, values, and assessments (RPH). This effort leverages work from related fields including explainability, transparency, and introspection, by repurposing metrics under the context of proficiency self-assessment. Considerations for temporal level (a priori, in situ, and post hoc) on the metrics are reviewed, as are the connections between metrics within or across stages in the proficiency-based interaction flow. This article provides a common framework and language for metrics to enhance the development and measurement of HRI in the field of proficiency self-assessment.\n\n[1] Trends of Human-Robot Collaboration in Industry Contexts: Handover, Learning, and Metrics purpose of developing a fluent and human-like collaborative robot, qualitative and specific quantitative metrics are required, in order to interpret where and how can the system be improved and which methodologies should be refined or even replaced by new ones. To tackle this concern, ref. proposes a mixture between objective and subjective metrics to evaluate human-robot object handover interactions. The subjective metrics were formulated based on a score that each different participant gave after the experience of changing objects with the robot. Each participant had to evaluate four distinct aspects: how easy was to receive the object; how satisfied they are with the interaction; how comfortable was the interaction and how safe they felt during the interaction. Additionally, ref. also takes in consideration some objective parameters, such as the effort per robot joint, the position and orientation of the human hand as a function of time, and many timing measurements related to the object handover practise. Table 2 summarizes some works that use specific objective metrics, such as success rate,77,89,108,109,, interaction force,90,91,116,117, timings,80,87,110,114,, joint effort and subjective metrics, such as fluency,80,87,110,114,, satisfaction,113,115,123,125,126, comfort,109,115,121,124,126,127, usage of interaction,109,113,115,125,126, trust in the robot,110,125 and human-like motion to evaluate their systems, providing a glance of the major trends within this field. The lack of generic metrics and benchmarks that can be applied to multiple Human-Robot Collaboration systems and even, ultimately, to human teamwork, holds an open path for future research and developments regarding this evaluation field. Table 2.\n\n[2] Metrics for Evaluating Human-Robot Interactions Metrics for Evaluating Human-Robot Interactions Metrics for evaluating the quality of a human-robot interface are introduced. The autonomy of a robot is measured by its neglect time. The robot attention demand metric measures how much of the user’s attention is involved with instructing a robot. The free-time and fan-out metrics are two ways to measure this demand. Each of them leads to estimates of the interaction effort. Reducing interaction effort without diminishing task effectiveness is the goal of human-robot interaction design.\n\n[3] Towards generalized performance metrics for human-robot interaction Towards generalized performance metrics for human-robot interaction In order for cognitive robots to act adequately and safely in real world, they must be able to perceive and have abilities of reasoning up to a certain level. Toward this end, performance evaluation metrics are used as important measures to achieve these goals. This paper intends to be a further step towards identifying common metrics for task-oriented human-robot interaction. We believe that within the context of human-robot interaction systems, both human and robot independent actions and joint interactions can significantly affect the quality of the accomplished task, thus proposing a generic performance metric to assess the performance of the human-robot team. Toward the efficient modelling of such metric, we also propose a fuzzy temporal model to evaluate the human trust in automation while interacting with robots and machines to complete some tasks. Trust modelling is critical as it directly influences the interaction time that should be directly and indirectly dedicated toward interacting with the robot. Another fuzzy temporal-based model is also presented to evaluate the human reliability during interaction time, as many research studies state that a large percentage of system failures are due almost equally to humans and machines, and therefore, assessing this important factor in human-robot interaction systems is also crucial. The proposed framework is based on the most recent work in the area of cognitive human-machine interaction and performance evaluation.\n\n[4] Towards generalized performance metrics for human-robot interaction In order for cognitive robots to act adequately and safely in real world, they must be able to perceive and have abilities of reasoning up to a certain level. Toward this end, performance evaluation metrics are used as important measures to achieve these goals. This paper intends to be a further step towards identifying common metrics for task-oriented human-robot interaction. We believe that within the context of human-robot interaction systems, both human and robot independent actions and joint interactions can significantly affect the quality of the accomplished task, thus proposing a generic performance metric to assess the performance of the human-robot team. Toward the efficient modelling of such metric, we also propose a fuzzy temporal model to evaluate the human trust in automation while interacting with robots and machines to complete some tasks. Trust modelling is critical as it directly influences the interaction time that should be directly and indirectly dedicated toward interacting with the robot. Another fuzzy temporal-based model is also presented to evaluate the human reliability during interaction time, as many research studies state that a large percentage of system failures are due almost equally to humans and machines, and therefore, assessing this important factor in human-robot interaction systems is also crucial. The proposed framework is based on the most recent work in the area of cognitive human-machine interaction and performance evaluation.\n\n[5] A Review of Communicating Robot Learning during Human-Robot Interaction Human-centered metrics seek to quantify when and how the human interacts with the robot.Common metrics include i) the amount of time the human teaches the robot, ii) the number of inputs the human provides to the robot learner, and iii) the effort associated with the human’s inputs (e.g., a minor correction vs. an entirely new demonstration) (Sena and Howard, 2020; Pearce et al., 2018).If the human and robot are collaborating during the learning process, additional measures of team fluency could include the human’s idle time or the delay between when the human expects to interact with the robot and when the robot is ready for interaction (Hoffman, 2019).Questionnaires such as Likert scale surveys (Schrum et al., 2023) are also relevant here: questions may ask the human about their teaching experience and their perception of the robot’s performance.\n\n[6] Enhancing fluency and productivity in human-robot collaboration through online scaling of dynamic safety zones could perform with their human counterparts. Collaborative fluency metrics have been introduced in, in the context of an anticipatory controller for shared-workspace decision processes. Subsequent works proposed several objective indexes to assess aspects of human-robot collaboration. In, human-robot cross-training was modeled and evaluated through different fluency metrics, such as concurrent motion, human idle time, robot idle time, and human-robot distance. The authors in measured fluency in a coffee-making task, planned with different motion approaches, and applied objective metrics such as coordination time, total task time, and concurrent motion time. A codification of subjective and objective fluency metrics can be found in, where the author provides an analytical model of four objective metrics (human idle time, robot idle time, concurrent activity, and functional delay). Finally, in a method for estimating online the quality of interaction between robot and operator is presented. In this paper, we present an approach to guarantee safety of a human operator during the collaboration with a manipulator and, at the same time, enhance fluency and productivity in human-robot collaboration through online scaling of dynamic safety zones by building on,35. This paper includes novel contributions that allow us to: (i) extend the theoretical formulation accounting for the direction of the robot towards a collision to minimize the size of the safety zones enclosing the manipulator; (ii) compare the method with other available approaches with extensive simulations; (iii) demonstrate the practical benefit of the proposed approach by considering fluency metrics for human-robot collaboration; (iv) introduce a novel task performance\n\n[7] A Review of Communicating Robot Learning during Human-Robot Interaction Robot-centered metrics assess the performance of the robot learner.If we let θ𝜃\\theta be the parameters that the robot has learned, and θ∗superscript𝜃\\theta^{*} be the desired parameters the robot should have learned, one straightforward assessment of learning accuracy is the error between actual and desired: ‖θ∗−θ‖2superscriptnormsuperscript𝜃𝜃2\\|\\theta^{*}-\\theta\\|^{2}.However, just because the model weights are close to the desired weights does not mean the robot will perform the task correctly.Hence, robot learning often measures regret, which captures the difference between the best possible robot behavior and the robot’s learned behavior (Osa et al., 2018).Regret compares the reward for the optimal trajectory under θ∗superscript𝜃\\theta^{*} and the reward for the optimal trajectory under θ𝜃\\theta, where a higher regret indicates more suboptimal behavior.Outside of error and regret, another common metric is task success.This metric could be binary (e.g., did the robot assemble the chair correctly?) or measured along a spectrum (e.g., how many legs did the robot autonomously add to the chair?).When algorithmic efficiency is relevant, metrics may also include the amount of training time, the necessary computational resources, or the number of training iterations.\n\n[8] Trends of Human-Robot Collaboration in Industry Contexts: Handover, Learning, and Metrics List of works addressing distinct metrics for Human-Robot Collaboration. Metrics Descriptor References Objective Metrics Success rate,77,89,108,109, Interaction force,90,91,116,117 Timings (idle & total),80,87,110,114, Joint effort Subjective Metrics Fluency,80,87,110,114, Satisfaction,113,115,123,125,126 Comfort,109,115,121,124,126,127 Usage of interface,109,113,115,125,126 Trust in the robot,110,125 Human-like motion The complete and reliable integration of human-robot collaborative systems into today's world industry will only be achieved when it becomes possible to provide a common basis of comparison between systems, allowing for reproducible and repeatable evaluations. The aimed fair comparison demands an evaluation of many aspects, each one of them encompassing several parameters. Concretely, for a proper evaluation of a human-robot collaborative task, such as object handovers, the developed metrics should classify the robot motion fluency, the seamlessness with human motions, the coordination with the operator movements, the adaptive motion, the success in both giving and receiving an object, among others. These are the current research prospects, regarding evaluation metrics, among the HRC community. In this context, a suitable set of benchmarking test beds need to be developed for evaluating human-robot teams in a set of different scenarios. Future Challenges As it is possible to understand from the literature review discussed earlier, many methodologies and technologies were already created and applied within the Human-Robot Collaboration area of interest. Nonetheless, there are still several challenges to be tackled in this field, in order to achieve an intuitive, useful and effective use of collaborative robots for industrial tasks or even for general human-life improvement. Object handover among humans and robots\n\n[9] A Review of Communicating Robot Learning during Human-Robot Interaction Future research can address these questions by identifying real-time variables that are connected with the human’s underlying model of the robot learner.Recent work in human factors suggests that we can measure the human’s understanding of a situation through neurological measures like electroencephalograms (EEG) (Akash et al., 2018; Jung et al., 2019) and functional near-infrared spectroscopy (fNIRS) (Goodyear et al., 2016).For example, Kohn et al. (2021) provide experimental evidence that EEG and fNRI are a reliable real-time measure of trust in automation-related applications.Similarly, Heard et al. (2018) show that a variety of online metrics for measuring the human operator’s workload are available, but in their current form they may not generalize to different tasks or users.Determining which — if any — of these measures correlate to the human’s mental model during robot learning will help us assess which explicit feedback signals clearly convey the robot’s learning, and which signals are less interpretable by the human.\n\n[10] Common metrics for human-robot interaction Common metrics for human-robot interaction This paper describes an effort to identify common metrics for task-oriented human-robot interaction (HRI). We begin by discussing the need for a toolkit of HRI metrics. We then describe the framework of our work and identify important biasing factors that must be taken into consideration. Finally, we present suggested common metrics for standardization and a case study. Preparation of a larger, more detailed toolkit is in progress.\n\n[11] Object Handovers: A Review for Robotics Proposed Set of Metrics: Our survey has revealed a need for standardisation in the choice of metrics and objects for real robot experiments. Most of the surveyed papers report results using task performance metrics (e.g., success rate and timings) and subjective metrics on the experience of the human partner (often in the form of Likert-scale post-experiment questionnaires). We believe that a minimal set of metrics should be defined in order to enable a fairer and more direct comparison among the different approaches. To this end, we propose the following combination of metrics that assess the most common aspects of a handover: 1) Task performance (objective): success rate, total handover time, receiver's task completion time. 2) Experience of the human (subjective): fluency, trust in robot, working alliance. This minimal set includes metrics which are clearly defined, thus reproducible, and which are easy to measure. For these reasons, the set does not include psycho-physiological measurements as they require sensors placed on the body of the human participant, and thus are difficult to standardise and deploy in a variety of contexts. The experience of the human participant should be assessed administering the following questionnaire (the following set of questions includes a subset of questions from): 1) Human-Robot Fluency • The human-robot team worked fluently together. • The human-robot team's fluency improved over time. • The robot contributed to the fluency of the interaction. 2) Trust in Robot • I trusted the robot to do the right thing at the right time • The robot was trustworthy. 3) Working\n\n[12] Trends of Human-Robot Collaboration in Industry Contexts: Handover, Learning, and Metrics et al. have effectively scaled up the usage of deep reinforcement learning and human demonstration to teach a complex manipulation task to a high-dimensional 24-dof robotic hand, such as nail hammering. Metrics in HRC Another current interest in Human-Robot Collaboration is how to measure the effectiveness, efficiency, fluency and adaptability of one these HRC systems. The definition of adequate metrics and guidelines to evaluate human-robot interaction tasks' performance is, in, recognized as a truly important aspect to lead future developments. Several research studies have focused on the definition for metrics for Human-Robot Interaction. In, the metrics related to HRI are subdivided into several particular domains such as productivity, efficiency, reliability, safety, and co-activity. This metrics categorization is easily understandable through the natural complexity that every Human-Robot Collaboration procedure entails. It is difficult to assess such systems in a global and objective evaluation. In fact, many of the studied metrics in cannot be generalized to an open mechanism for measuring the feature of its category. Hoffman et al. discuss four specific metrics for evaluating human-robot fluency: the percentage of concurrent activity, the human's idle time, the robot's functional delay and the robot's idle time. These metrics are general and agnostic to the specific content of the collaborative acts, relating only to periods of activity. Anyhow, the metrics were used to evaluate an objects operation task in a human-robot shared workspace. More specifically, regarding the human-robot object handover domain, the survey from reports a comprehensive list of qualitative and quantitative metrics\n\n"}
{"case_id": "79eda5689229572a789e612010246d44", "answer_text": "\n\nTo improve the robustness and safety of quadrotor unmanned aerial vehicles (UAVs) in extreme weather conditions, multiple strategies, including advanced control systems, observer-based methods, and sensor integration, are employed. These strategies address issues like wind perturbations [6], actuator faults [1], external disturbances [7], ground effects [3], and blade damage [3]. Below is an organized overview of these approaches based on similarities and differences between various proposed methods. \n\nAdvanced Control Techniques and Observers [0][1][3]. \n\n1. **Sliding Mode Control (SMC) Techniques**: [3]  \n\n   Sliding Mode Control, particularly second-order sliding mode techniques, are frequently integrated into UAV control systems for their robustness against external disturbances and system uncertainties. For instance, a wind force compensation strategy utilizing a Super Twisting Algorithm (STA) with an integrated observer has been validated to efficiently handle wind perturbations and improve the robustness of quadrotor UAVs. This approach ensures the system's status is better monitored and allows for recovery tactics such as emergency landing when external forces become too strong [6].\n\n2. **Hybrid Robust Adaptive Controllers (HRAC)**[0]:\n   To address slow-varying mass and external disturbances, hybrid robust adaptive controllers (HRAC) are utilized. These controllers combine adaptive control theory with backstepping methods and incorporate mass observers to estimate real-time mass, correcting the mass parameter of the UAV and improving trajectory tracking accuracy [0]. This method is particularly effective against external disturbances, as demonstrated by simulation results indicating a low mass estimation error of only 6.4%.\n\n3. **Fixed-Time Observer-Based Control**: [10]\n   Another effective strategy involves fixed-time observer-based control mechanisms. These mechanisms estimate state residuals and unknown actuator faults independently of initial conditions, ensuring fault parameter estimation within a fixed upper bound. Integration with an integral sliding mode controller addresses multiple actuator faults and disturbances, thereby enhancing the UAV's safety and robustness under adverse conditions [10].\n\nNonlinear Control and Disturbance Observers[1]\n\nNonlinear Disturbance Observers [1]. \n   Nonlinear disturbance observers are designed to identify and compensate for time-varying disturbances. These observers, integrated with improved input/output linearization controllers, cancel out nonlinearities and provide robust control suitable for ensuring the stability of quadrotor UAVs in dynamic, uncertain environments [13]. Specifically, the nonlinear disturbance observer can be designed using Lyapunov stability theory to guarantee boundedness and convergence of disturbance estimation, crucial for maintaining attitude stability amidst changing external conditions.\n\n\n2. **Dual-Loop Observers** [1]:\n\n   Dual-loop observers are employed for identifying actuator faults and disturbances, which can then be managed using a cascade control scheme. These observers regenerate trajectories online based on system capability analysis, trading off flight maneuverability for safety when actuator control authority is compromised [1]. This approach bridges the gap between safety control, trajectory generation, and system capability analysis, significantly ameliorating practical flight safety.\n\nOptimization and safety control schemes for quadrotor UAVs are crucial for ensuring stable and secure flight operations. Various strategies have been explored to address these challenges, such as the development of robust adaptive control algorithms to improve the stability and precision of the UAVs under varying mass and external disturbances [0]. Another approach involves the integration of differential flatness and dual-loop observers to handle actuator faults and disturbances [1]. Additionally, optimization-based methods like trajectory prediction and cost function optimization have been used to enhance flight safety in dynamic environments [2].\n\nTo further enhance safety, control schemes that counteract the effects of ground and blade damage through disturbance observers and sliding mode control have been proposed [3]. Fixed-time observer-based strategies have also been introduced to reliably estimate actuator faults and disturbances, ensuring a fixed upper bound on the estimation time [10]. Lastly, the use of control barrier functions and optimization techniques has been shown to effectively maintain a safety set and avoid collisions [8]. \n\n\n1. **Trajectory Optimization** [2]:\n\n   In dynamic and extreme environments, optimized trajectories are crucial for safety and robustness. Optimization problems consider collision and smoothness costs, ensuring UAVs avoid obstacles and maintain smooth trajectories. Techniques such as the augmented Lagrangian method and low-storage BFGS algorithm are employed to solve these problems efficiently [2][5]. These optimization strategies ensure UAVs navigate safely while adhering to velocity and acceleration constraints.\n\n2. **Safety Control Schemes Against Ground Effects and Blade Damage** [4]:\n\n   Addressing the impacts of ground effects and blade damage, safety control schemes are designed with position and attitude loops. These schemes utilize proportional and derivative controllers, acceleration feedforward, and sliding mode observers to mitigate disturbances and maintain stable flight close to the ground or with damaged blades [3][4]. Effective flight experiments demonstrate the reliability of these schemes in real-world scenarios.\n\nHaptic Feedback and Shared Control [11]. \n\n1. **Haptic Shared Control (HSC)** [11]:\n   Haptic feedback in teleoperation enhances operator situational awareness and reduces dangerous collisions. Haptic Shared Control (HSC) provides force feedback to inform operators about the UAV's behavior and surrounding environment, significantly improving safety during teleoperation [11]. This technique ensures robust obstacle avoidance and safer maneuvering through user interaction.\n\nMachine Learning-Based Methods\n\n1. **Reinforcement Learning and Predictive Models** [2]:\n\nMachine learning algorithms, especially reinforcement learning and predictive models, are increasingly utilized to enhance robustness and safety in quadrotor UAVs [1]. These models can predict and adapt to dynamic environments, improving the UAV's capability to handle unforeseen circumstances [2]. For example, neural networks can be trained to recognize patterns in wind perturbations and adjust control strategies accordingly to maintain stability under extreme weather conditions [6].\n\nApplications and Results\n\n1. **Handling Ground Effect and Blade Damage** [3]:\n   The utilization of sliding mode control and fixed-time sliding mode observers to counteract ground effect and blade damage has shown to maintain UAV stability even in near-ground operations or with damaged blades, highlighting their robustness in practical flight tests [3][4].\n\n2. **Wind Perturbation Compensation** [6]\n   The super twisting algorithm with wind force compensation has not only shown effective wind disturbance mitigation but also provided a solid basis for emergency protocols like safe landings when perturbations become intolerable [6].\n\n3. **Dynamic Obstacle Avoidance**: [2]\n   Advanced trajectory optimization integrating dynamic obstacle prediction further illustrates the application of nonlinear control strategies in maintaining UAV safety in dynamically changing environments [2][5].\n\nIn summary, improving the robustness and safety of quadrotor UAVs in extreme weather conditions involves a range of sophisticated control techniques, observer-based methods [0], optimization schemes [2], haptic feedback systems [11], and machine learning algorithms. Each strategy addresses specific challenges posed by adverse conditions, ensuring reliable and stable UAV operation.\n\n\nReferences:\n[0] A hybrid robust adaptive control for a quadrotor UAV via mass observer and robust controller The flight stability and safety of the quadrotor unmanned aerial vehicle (UAV) with variable mass are the key problems that limit its application. In order to improve the stability and steady-state control precision of the quadrotor system against slow-varying mass and external disturbance, a new robust adaptive flight control algorithm is developed and analyzed in detail in this paper. Firstly, a mass observer based on adaptive control theory is designed to estimate the real-time mass and correct the mass parameter of the UAV. Then, a hyperbolic tangent function and a proportional integral (PI) controller is added into the attitude controller to eliminate the effect of the external disturbances. Finally, a hybrid robust adaptive controller (HRAC) developed with backstepping control method is used here for the trajectory tracking of quadrotor. The boundedness of the nonlinear system is verified by Lyapunov stability theory and uniformly ultimately bounded theorem. The trajectory tracking simulation experiments are presented in MATLAB/SIMULINK simulation environment. According to the simulation results, the real-time mass of quadrotor can be estimated by HRAC satisfactorily under the condition of external disturbances, while the estimate error of mass is only 6.4% of its own. In addition, HRAC can provide a higher trajectory tracking accuracy compared with robust optimal backstepping control (ROBC) and robust generalized dynamic inversion (RGDI). The results suggest a promising route based on the mass observer and hybrid robust controller toward slow-varying mass and the external disturbance as effective robustness control strategy for quadrotor UAV.\n\n[1] Safety Flight Control for a Quadrotor UAV Using Differential Flatness and Dual-Loop Observers Focusing on a quadrotor unmanned aerial vehicle (UAV), the presence of unintended actuator faults and external disturbances increases the risk of a crash. Although plenty of efforts have been devoted, how to integrate capability analysis into safety control design is still an open issue. This article presents the design of a safety control system for quadrotor UAVs. Firstly, a system capability analysis method based on a differential flatness algorithm is developed, so that the derivatives (i.e., velocity, jerk, and snap) of flight trajectory can be formulated as flat variables. A tradeoff between system capability and permissible flight maneuverability is made to avoid actuator saturation. Secondly, dual-loop nonlinear disturbance observers are exploited to identify the actuator faults and disturbances, which can thereby be handled by a cascade control scheme. Moreover, the trajectory is regenerated online at the expense of degraded flight maneuverability or even emergency landing, in view of the remaining actuator control authority. When comparing to the existing methods, the gap among safety control, trajectory generation, and system capability analysis is bridged to ameliorate practical flight safety. Finally, flight tests are carried out to demonstrate the unique merits of the proposed system.\n\n[2] A Safer Vision-Based Autonomous Planning System for Quadrotor UAVs with Dynamic Obstacle Trajectory Prediction and Its Application with LLMs Although the initial trajectory ensures collision avoidance, it may still exhibit issues such as proximity to obstacles or excessive changes in direction, as shown in Figure 4. Therefore, further optimization is required to achieve a trajectory that is not only safer but also better suited for UAV flight. Following Usenko et al., we represent this problem as an optimization of the following cost function:Et​o​t​a​l=Ec​o​l​l​i​s​i​o​n+Es​m​o​o​t​h​n​e​s​s,subscript𝐸𝑡𝑜𝑡𝑎𝑙subscript𝐸𝑐𝑜𝑙𝑙𝑖𝑠𝑖𝑜𝑛subscript𝐸𝑠𝑚𝑜𝑜𝑡ℎ𝑛𝑒𝑠𝑠E_{total}=E_{collision}+E_{smoothness},(4)where Ec​o​l​l​i​s​i​o​nsubscript𝐸𝑐𝑜𝑙𝑙𝑖𝑠𝑖𝑜𝑛E_{collision} is a collision cost function and Es​m​o​o​t​h​n​e​s​ssubscript𝐸𝑠𝑚𝑜𝑜𝑡ℎ𝑛𝑒𝑠𝑠E_{smoothness} is the cost of the integral over the squared derivatives (acceleration, jerk, snap). The former ensures that the UAV will not collide with the static environment or moving targets, while the latter contributes to a smoother trajectory, aiding in flight control. Furthermore, assuming the plane that separates the convex hull of control points and the obstacles is defined as nT​x+d=0superscript𝑛𝑇𝑥𝑑0n^{T}x+d=0, where n𝑛n and d𝑑d are the parameters of this plane, the optimization must also satisfy the following conditions:{nT​O+d≥0nT​Q+d≤0a​b​s​(v)≤vm​a​xa​b​s​(a)≤am​a​x,\\left\\{\\begin{aligned} n^{T}O+d\\geq 0\\\\n^{T}Q+d\\leq 0\\\\abs(v)\\leq v_{max}\\\\abs(a)\\leq a_{max}\\end{aligned}\\right.,(5)where O𝑂O represents the vertices of the obstacle’s convex hull, and Q𝑄Q represents the control points of the trajectory, a​b​s​(⋅)𝑎𝑏𝑠⋅abs(\\cdot) represents the element-wise absolute value, and v𝑣v and a𝑎a represent velocity and acceleration of the UAV, respectively. In this study, this problem is solved using the augmented Lagrangian method, 2, and with the low-storage BFGS algorithm for local optimization. The interface used for these algorithms is NLopt.\n\n[3] Safety Control for Quadrotor UAV Against Ground Effect and Blade Damage Safety Control for Quadrotor UAV Against Ground Effect and Blade Damage This article proposes a safety control scheme for quadrotor unmanned aerial vehicle (UAV) to counteract ground effect and blade damage. The impacts of ground effect and blade damage are quantitatively analyzed in the established model. The proposed safety control scheme consists of position and attitude loops. Specifically, a proportional and derivative controller is designed as a baseline control with acceleration feedforward in the position loop. A disturbance observer (DO) using the measured Euler angles is exploited to estimate the lumped disturbance forces. The developed DO can effectively mitigate the estimation deviation caused by the angle tracking error. Subsequently, differential flatness theory is used to obtain the Euler angle references and the derivatives. In the attitude loop, sliding mode control and fixed-time sliding mode observer (SMO) are integrated to deal with ground effect and blade damage promptly. To demonstrate the effectiveness of the proposed safety control scheme, flight experiments are carried out, where the quadrotor UAV flies close to the rugged ground in the presence of blade damage.\n\n[4] Safety Control for Quadrotor UAV Against Ground Effect and Blade Damage This article proposes a safety control scheme for quadrotor unmanned aerial vehicle (UAV) to counteract ground effect and blade damage. The impacts of ground effect and blade damage are quantitatively analyzed in the established model. The proposed safety control scheme consists of position and attitude loops. Specifically, a proportional and derivative controller is designed as a baseline control with acceleration feedforward in the position loop. A disturbance observer (DO) using the measured Euler angles is exploited to estimate the lumped disturbance forces. The developed DO can effectively mitigate the estimation deviation caused by the angle tracking error. Subsequently, differential flatness theory is used to obtain the Euler angle references and the derivatives. In the attitude loop, sliding mode control and fixed-time sliding mode observer (SMO) are integrated to deal with ground effect and blade damage promptly. To demonstrate the effectiveness of the proposed safety control scheme, flight experiments are carried out, where the quadrotor UAV flies close to the rugged ground in the presence of blade damage.\n\n[5] A Safer Vision-Based Autonomous Planning System for Quadrotor UAVs with Dynamic Obstacle Trajectory Prediction and Its Application with LLMs For obstacle avoidance, various strategies have been explored in static environments. The most common pipeline is to detect obstacles and represent them in the occupancy map, and then generate a safe flight trajectory using optimization-based methods, 22, 49, 8 or sampling-based methods, 6. Among these, EGO-Planner stands as one of the most representative methods, distinguished by its robustness and efficiency. However, in dynamic environments, obstacle avoidance becomes more demanding, as it entails the prediction of future states for dynamic obstacles. Currently, obstacle avoidance systems for quadrotor UAVs in dynamic environments typically apply DATMO for dynamic obstacle perception, followed by the application of modified planning algorithms to generate suitable flight trajectories. The ASAA system uses YOLOv3 for dynamic obstacle detection, subsequently utilizing SORT and active vision for object tracking. It then performs trajectory planning based on real-time sampling and uncertainty-aware collision checking, ultimately enabling the UAV to avoid collisions with slow-moving objects. Similarly, also applies a detection and tracking method, and further optimizes trajectories by considering the predicted position of the dynamic obstacles, thereby ensuring flight safety. In order to address the uncertainty in trajectory prediction, develops a chance-constrained model predictive controller to ensure that the collision probability between the UAV and each moving object remains acceptably low, thus achieving robust obstacle avoidance. achieves efficient trajectory planning through sampling motion primitives in the state space and forming risk with the cardinality expectation in a risk-checking corridor. Based on robust mobile object detection, our method features trajectory prediction and reliable trajectory optimization under several constraints, possessing good generality and user-friendliness.\n\n[6] Observer-based Super Twisting Controller Robust to Wind Perturbation for Multirotor UAV Observer-based Super Twisting Controller Robust to Wind Perturbation for Multirotor UAV Control design for multi rotors UAV is an important challenge for engineers and scientists, due to the fact that the standard configurations are under-actuated, highly nonlinear, and unstable systems. In this paper, a wind force compensation strategy is proposed for a quadrotor. This strategy relies on a second order sliding mode controller based on the super twisting algorithm (STA) with an observer. Second order sliding mode technique ensures robustness to external disturbances and time varying, parametric and nonlinear uncertainties. Integration of an observer in the closed-loop system is needed for states reconstruction and the estimation of unknown external forces such as the wind effect. This estimation will allow a better monitoring of the system’s status than passive robustness, providing the opportunity for recovery tactics such as an emergency landing when the external perturbations become too strong for the system. The effectiveness of the proposed strategy is compared to an adaptative gain controller through simulation and validated in real experiments on a quadrotor.\n\n[7] Safety Flight Control Design of a Quadrotor UAV With Capability Analysis This article considers the safety control problem of a quadrotor unmanned aerial vehicle (UAV) subject to actuator faults and external disturbances, based on the quantization of system capability and safety margin. First, a trajectory function is constructed online with backpropagation of system dynamics. Therefore, a degraded trajectory is gracefully regenerated, via the tradeoff between the remaining system capability and the expected derivatives (velocity, jerk, and snap) of the trajectory. Second, a control-oriented model is established into a form of strict feedback, integrating actuator malfunctions and disturbances. Therefore, a retrofit dynamic surface control (DSC) scheme based on the control-oriented model is developed to improve the tracking performance. When comparing to the existing control methods, the compensation ability is analyzed to determine whether the faults and disturbances can be handled or not. Finally, simulation and experimental studies are conducted to highlight the efficiency of the proposed safety control scheme.\n\n[8] Safe and Stable Teleoperation of Quadrotor UAVs under Haptic Shared Autonomy The goal of CBFs is to produce a control field u𝑢u that makes a safe set ℋ⊂nℋ𝑛\\mathcal{H}\\subset n forward invariant. In our problem settings, we define h​(x)ℎ𝑥h(x) as the Euclidean distance between the robot and the obstacles and utilize CBFs to avoid collisions. Let h​(x)ℎ𝑥h(x) be a twice differentiable function representing ℋℋ\\mathcal{H}, i.e. h​(x)>0ℎ𝑥0h(x)>0 on the interior of ℋℋ\\mathcal{H}, h​(x)=0ℎ𝑥0h(x)=0 on its boundary, and h​(x)<0ℎ𝑥0h(x)<0 otherwise. Assuming that h​(x)ℎ𝑥h(x) has relative degree two, we can use a second-order exponential control barrier function to impose constraints on u𝑢u that ensure safety (i.e., forward invariance of ℋℋ\\mathcal{H}):Lf2​h​(x)+Lg​Lf​h​(x)​u+K​[h​(x)Lf​h​(x)T≥0,superscriptsubscript𝐿𝑓2ℎ𝑥subscript𝐿𝑔subscript𝐿𝑓ℎ𝑥𝑢𝐾superscriptmatrixℎ𝑥subscript𝐿𝑓ℎ𝑥𝑇0L_{f}^{2}h(x)+L_{g}L_{f}h(x)u+K\\begin{bmatrix}h(x)&L_{f}h(x)\\end{bmatrix}^{T}\\geq 0,(4)where K∈1×2𝐾12K\\in 1\\times 2 is a set of coefficients representing a Hurwitz polynomial. Given the system dynamics in (2), we can rewrite the constraint in (4) as:xvT​∂xp2h​xv+(∂xph)T​u+k1​h+k2​(∂xph)T​xv≥0.subscriptsuperscript𝑥T𝑣subscriptsuperscript2subscript𝑥𝑝ℎsubscript𝑥𝑣superscriptsubscriptsubscript𝑥𝑝ℎT𝑢subscript𝑘1ℎsubscript𝑘2superscriptsubscriptsubscript𝑥𝑝ℎTsubscript𝑥𝑣0x^{\\mathrm{T}}_{v}\\partial^{2}_{x_{p}}hx_{v}+(\\partial_{x_{p}}h)^{\\mathrm{T}}u+k_{1}h+k_{2}(\\partial_{x_{p}}h)^{\\mathrm{T}}x_{v}\\geq 0.(CBF)\n\n[9] Fixed-Time Observer Based Safety Control for a Quadrotor UAV This article presents a fixed-time observer-based safety control strategy for quadrotor unmanned aerial vehicles (UAVs) against multiple actuator faults and unknown disturbances. The fixed-time sliding mode observer is designed to estimate the state residual and the unknown actuator fault. Unlike most of the existing safety control strategies, the estimation time of the fault parameters is independent of initial condition, which can be strictly ensured within a fixed upper bound. Besides, the transient estimation error is regarded as a disturbance. In this article, a retrofit control mechanism with integration of the fixed-time estimator and an integral sliding mode controller is devised against multiple actuator faults and disturbances. Finally, both simulation and experimental tests demonstrate the effectiveness of the proposed UAV safety control.\n\n[10] Safe and Stable Teleoperation of Quadrotor UAVs under Haptic Shared Autonomy Haptic feedback emerges as a promising technique to remedy the aforementioned challenges. A notable paradigm leveraging haptic feedback is Haptic Shared Control (HSC), wherein haptic signals serve as informative cues regarding the robot’s behavior and the surrounding environment through force feedback. Taking this approach often reduces dangerous collisions during teleoperation, and has been shown to increase operator situational awareness, 10, 40.\n\n[11] Robust Discrete-Time Nonlinear Attitude Stabilization of a Quadrotor UAV Subject to Time-Varying Disturbances Robust Discrete-Time Nonlinear Attitude Stabilization of a Quadrotor UAV Subject to Time-Varying Disturbances A discrete-time improved input/output linearization controller based on a nonlinear disturbance observer is considered to secure the stability of a four-rotor unmanned aerial vehicle under constant and time-varying disturbances, as well as uncertain system parameters for its attitude behaviour. Due to the nature of the quadrotor system, it contains the most extreme high level of nonlinearities, system parameter uncertainties (perturbations), and it has to cope with external disturbances that change over time. In this context, an offset-less tracking for the quadrotor system is provided with the input/output linearization controller together with a discrete-time pre-controller. In addition, the robustness of the system is increased with a discrete-time nonlinear disturbance observer for time-varying disturbances affecting the system. The main contribution of this study is to provide highly nonlinearities cancellation to guarantee the aircraft attitude stability and to propose a robust control structure in discrete-time, considering all uncertainties. Various simulation studies have been carried out to illustrate the robustness and effectiveness of the proposed controller structure. I. INTRODUCTION Within the last decade, the Unmanned Aerial Vehicles (UAV) are deployed for tasks where human interaction is dangerous. Today, there are many applications of UAV in sectors, such as military, transportation, and entertainment. Among the UAVs, the multi-rotors are mostly chosen for their agility and ability to hang in the air. Within the literature, many control methods are used to deploy to characterize the attitude and altitude problem of the quadrotors, which is commonly chosen as the multi-rotorframework\n\n"}
{"case_id": "3d8c315aed4cc104f2ad61f4deeda9c1", "answer_text": "\n\nThe leading techniques for person-following robot navigation, which must effectively track the person and potentially re-identify them if occluded, encompass a variety of approaches. These methods can be broadly categorized into traditional tracking algorithms [0], deep learning-based re-identification [1], sensor fusion strategies [12], and advanced transformer models [11].\n\nTraditional Tracking Algorithms are methods that employ various techniques to follow and identify moving objects, such as people, in real-time. These algorithms often rely on relatively simple yet robust heuristics to achieve reasonable accuracy without necessarily considering the appearance of the tracked individual. For example, the Intersection Over Union (IOU) Tracker matches a detection to a track based on the overlap between the detection bounding box and the track’s bounding box in the previous frame [0]. Another example is Simple Online Real-time Tracking (SORT), which uses a Kalman filter to predict the next location of the tracked individual and matches detections to tracks based on the predicted bounding box [0]. However, these traditional methods have limitations in re-identification, especially after occlusion, which has been addressed by more advanced methods like deep SORT [0]. \n\nTraditional tracking methods, such as Simple Online Realtime Tracking (SORT) and its extension Deep SORT, represent foundational approaches for person-following tasks. SORT uses a Kalman filter to predict the next location of tracked objects and matches detections to tracks based on Intersection Over Union (IoU) between predicted and detected bounding boxes [0]. Deep SORT enhances this by incorporating a deeply-learned appearance metric, allowing it to re-identify individuals after temporary occlusions by matching visual features extracted from detected objects [0][5]. These methods are computationally efficient and suitable for real-time applications but have limitations in complex or highly dynamic environments where occlusions are frequent and extended [0][5].\n\nDeep Learning-Based Re-Identification\n\nDeep learning techniques have significantly advanced the field of person re-identification (Re-ID). These methods typically leverage convolutional neural networks (CNNs) and attention mechanisms to improve robustness against occlusions and variations in appearance [2][8][9]. A prominent example is the Motion-Aware Transformer, which combines motion information with part segmentation to overcome complexities related to posture and background noise, leading to enhanced accuracy in occluded scenarios [11]. This model achieves 66.2% Rank-1 accuracy and 58.8% mAP on the Occluded Duke dataset, surpassing previous models like PAT and ISP by significant margins [7]. Other methods, such as the Feature Recollection and Transfer (FRT) and Transformer-based architectures, employ pose estimation and semantic segmentation to derive more detailed human body information, which assists in better re-identification performance after occlusions [6][11].\n\nSensor Fusion Strategies [12]. \n\nSensor fusion approaches have become increasingly popular to enhance tracking and re-identification performance by integrating data from multiple sensors. These strategies often combine RGB-D cameras, laser scanners, and other modalities to leverage complementary strengths. For instance, RGB-D sensors provide rich color and depth information, which can be used to improve the understanding of spatial relationships and enhance feature extraction [13]. An example of sensor fusion in practice is a system that uses RGB-D cameras to track and follow a target person, and re-identify the target if the line of sight is obstructed temporarily [13]. Another practical implementation uses a multi-modal sensor approach including fusing an RGB-D sensor and a laser scanner, which robustly tracks and identifies human targets under dynamic conditions such as crowded environments. Bayesian filtering and regression algorithms are employed to predict human trajectories and maintain robust tracking [12][13]. These multimodal systems are particularly effective in crowded environments, where single-sensor solutions might struggle [12][13].\n\nAdvanced Transformer Models\n\nRecent advancements in transformer-based models have shown promising results in person Re-ID, especially in occlusion scenarios. These models utilize self-attention mechanisms to focus on the most relevant parts of the input data, allowing for precise feature extraction and better handling of occlusions [11][14]. For example, the Motion-Aware Transformer employs a part segmentation branch alongside a self-supervised motion flow estimator to maintain accurate re-identification by focusing on individual body parts and their motion [11][14]. These architectures demonstrate state-of-the-art performance on benchmarks, with the Motion-Aware Transformer achieving 83.2% Rank-1 accuracy and 73.5% mAP on the Occluded REID dataset, highlighting its effectiveness in challenging conditions [7][11]. The transformer model optimization can lead the keypoint detector to locate more precisely and lead the part segmentation to represent more detailed human body information, providing a substantial advantage in scenarios with frequent occlusions [11].\n\nPractical Implementations [4].\n\nPractical implementations of these techniques often combine elements from all categories to achieve robust performance. For instance, a study [5] described an experiment where a person-following robot used an online boosting model for target identification among similar appearances. During the test, the robot successfully re-identified and tracked the target person even after being fully occluded by another individual. The combination of the boosting model and modified IoU for quick re-identification showcases how blending traditional and deep learning methods can enhance robustness.\n\nMoreover, the integration of sensor fusion techniques, as seen in [13], highlights another practical approach. In real-time experiments, a system employing RGB-D sensors and laser scanners could robustly follow and re-identify a person in a dynamic environment, demonstrating the efficacy of multimodal sensor data in practical scenarios.\n\nIn conclusion, effective person-following robot navigation leverages a combination of traditional tracking algorithms, deep learning-based re-identification techniques, sensor fusion strategies, and advanced transformer models. These methods collectively address the challenges of tracking individuals and re-identifying them after occlusions, ensuring robust and reliable robot navigation in dynamic and complex environments [10][12][13][14].\n\n\nReferences:\n[0] Realtime Multi-Diver Tracking and Re-identification for Underwater Human-Robot Collaboration In contrast, the fastest trackers rely on relatively simple yet robust heuristics, while still achieving reasonable accuracy. Many of these realtime trackers do not take people’s appearances into account, and instead match detections to tracks solely by analyzing the locations of the detections. For example, the Intersection Over Union (IOU) Tracker matches a detection to a given track if there is a sufficiently high IOU between the detection bounding box and the track’s bounding box in the previous frame. Simple Online Realtime Tracking (SORT) is a slightly more complex model that uses a Kalman filter to model people’s motions and predict their next location. SORT matches detections to tracks if there is a sufficiently high IOU between a detection’s bounding box and the track’s predicted bounding box. While these techniques are reasonable for tracking pedestrians, they are not able to reidentify a person after he or she is temporarily occluded. This shortcoming was addressed by deep SORT, which extends SORT to also use a deeply-learned appearance metric to improve reidentification after occlusions. This work extends deep SORT further to improve reidentification after longer occlusions or absences, as well as customizing the appearance metric for diver reidentification.\n\n[1] Realtime Multi-Diver Tracking and Re-identification for Underwater Human-Robot Collaboration Our approach is tracking-by-detection, which is the current leading MOT paradigm. In tracking-by-detection, the tracker first performs person detection on each image. The tracker aims to match each of these detections to the correct track, where each track represents a unique person. Each track typically models the person’s trajectory and/or appearance in order to assist the tracker in matching detections to tracks.\n\n[2] Occluded Person Re-Identification with Deep Learning: A Survey and Perspectives At present, PointNet (Qi et al.,, 2017), as a representative of deep learning methods in point cloud feature extraction (Wang et al., 2022a, ; CS et al.,, 2022), has demonstrated promising results.Point cloud completion (Fei et al.,, 2022) and point cloud correction can aid in 3D occluded person Re-ID, while 3D pose estimation (Wang et al., 2021b, ) and 3D semantic segmentation (Xie et al.,, 2020) can guide the feature extraction process for person Re-ID. However, research in the 3D domain for pedestrian recognition (Zhao et al.,, 2013; Sun et al., 2019b, ) is still relatively limited compared to the advancements in 2D approaches. Therefore, 3D occluded person Re-ID represents a significant and promising research direction for the future (Tirkolaee et al.,, 2020).\n\n[3] Realtime Multi-Diver Tracking and Re-identification for Underwater Human-Robot Collaboration Autonomous underwater robots working with teams of human divers may need to distinguish between different divers, e.g., to recognize a lead diver or to follow a specific team member. This paper describes a technique that enables autonomous underwater robots to track divers in real time as well as to reidentify them. The approach is an extension of Simple Online Realtime Tracking (SORT) with an appearance metric (deep SORT). Initial diver detection is performed with a custom CNN designed for realtime diver detection, and appearance features are subsequently extracted for each detected diver. Next, realtime tracking-by-detection is performed with an extension of the deep SORT algorithm. We evaluate this technique on a series of videos of divers performing human-robot collaborative tasks and show that our methods result in more divers being accurately identified during tracking. We also discuss the practical considerations of applying multi-person tracking to on-board autonomous robot operations, and we consider how failure cases can be addressed during on-board tracking.\n\n[4] Online Boosting-Based Target Identification among Similar Appearance for Person-Following Robots laboratory as the departure point to the end of the corridor as the destination point, as shown in Figure 9. When walking through the corridor, there were two volunteers; one was standing on the right side in the middle of the corridor (Figure 9), where the robot was detected as a non-target (Figure 15d). After a few meters, another person standing on the left side ( Figure 9) was also detected as a non-target (Figure 15e). Both volunteers wore t-shirts similar to the target person, who wore blue t-shirts in this experiment. During the experiment, the target person was occluded by another person when she attempted to move between the target person and the robot to go to her laboratory from the left side (Figure 15f) to the right side (Figure 15g), and the robot lost the target tracking in two or three frames when the occlusion was complete. However, the robot tracked the target person when the occlusion was partial (Figure 15f) and then correctly re-identified him with the online person identification model once he partially reappeared in the camera view (Figure 15g). Target re-identification was fast and robust owing to the combination of multiple features using the online boosting model. The modified IoU remarkably improved the identification model to quickly identify and re-identify the target when the box was minimized or maximized suddenly, such as in partial occlusion situations. The robot continued to succeed until it arrived at its destination ( Figure 15h). In the white and black t-shirt experiments, the robot followed other\n\n[5] Occluded Person Re-Identification with Deep Learning: A Survey and Perspectives FRT (Xu et al.,, 2022) first classifies the pedestrian into head, torso, and legs using a pretrained pose estimation model HRNet (Sun et al., 2019a, ), and extracts the corresponding features.Then the occlusion elimination module based on graph matching is introduced to eliminate the interference caused by occlusion by learning the similarity of common regions.Finally, FRT (Xu et al.,, 2022) recovers query features by aggregating neighbor features to solve the problem of information loss caused by occlusion.TL-TransNet (Wang et al., 2022c, ) uses a well-improved Swin transformer (Liu et al.,, 2021) as a benchmark model to capture the main part of the person and uses DeepLabV3+ (Chen et al.,, 2018) to remove background interference in the query and gallery. Finally, a reordering method based on hybrid similarity and background adaptation is designed to achieve the fusion of original features and removed background features.PADE (Huang et al.,, 2022) first obtains two new images through cropping and erasing operations and feeds them together with the original image into a multi-branch parameter sharing network with a vit (Dosovitskiy et al.,, 2020) as the backbone to enhance the global and local features in a manner similar to the self-attention mechanism.Finally, they are connected to form the final feature representation.\n\n[6] Motion-Aware Transformer For Occluded Person Re-identification Occluded person Re-ID We compare our method with other advanced occluded person Re-ID methods on Occluded REID  and Occluded Duke , which are specifically person Re-ID in occlusion situation, demonstrating effectiveness of our method. The quantitative comparisons with previous models are listed in Tab. 3. From Tab. 3 we can observe that our method achieves 66.2%percent66.266.2\\% Rank-1 accuracy and 58.8%percent58.858.8\\% mAP in Occluded Duke , which surpasses PAT , ISP  by 1.3%percent1.31.3\\% and 3.4%percent3.43.4\\% in Rank-1 accuracy and 5.2%percent5.25.2\\% and 6.5%percent6.56.5\\% in mAP. In Occluded REID , our method achieves 83.2%percent83.283.2\\% Rank-1 accuracy and 73.5%percent73.573.5\\% mAP, which surpasses PAT , HOReID  by 1.6%percent1.61.6\\% and 2.9%percent2.92.9\\% in Rank-1 accuracy and 1.4%percent1.41.4\\% and 3.3%percent3.33.3\\% in mAP. The obtained state-of-the-art performance on the two benchmarks shows the powerful ability of our part segmentation with motion information for occluded person Re-ID.\n\n[7] Occluded Person Re-Identification with Deep Learning: A Survey and Perspectives To address the issue of missing images in person Re-ID, DPPR (Kim & Yoo,, 2017) employs an attention mechanism to emphasize the same pedestrian part across different images. This approach enhances the representation of individuals and improves matching accuracy.Moreover, OCNet (Kim et al.,, 2022) mitigates the effect of noise by capturing higher-order relationships among regional features and incorporating them with weighted combinations. This method effectively suppresses the influence of noisy or irrelevant information, resulting in more robust and accurate person Re-ID outcomes.AACN (Xu et al.,, 2018) combines pose guided attention maps and partial visibility scores to remove background distractions and occlusions before extracting clean pedestrian features.DAReID (Xu et al.,, 2021) introduces dual attention recognition. The local area visible to the pedestrian is obtained by gesture-guided spatial attention. Global features are extracted by feature activation and pose. Both will then be used together to guide the representation of features.PISNet (Zhao et al.,, 2020) is concerned with the overlapping area between people and objects. A module is designed to act as a guide feature by querying features, which can attenuate the problem of attention distraction caused by multiple pedestrians in the gallery. Also, a reverse attention module based on strong activation attention is designed, which enables the model to assign more weight to the target region.APN (Huo et al.,, 2021) proposes a partial perceptual attention network, which takes partial feature maps as query vectors, calculates a similarity mapping M with a mapping X of the feature maps, and morphs the features by weighting M by X to achieve the purpose of aggregating and extracting refined features.MHSA-Net (Tan et al., 2022a, ) multiplies attention weights with feature maps and applies a nonlinear transformation to encourage multi-headed attention mechanisms to adaptively capture key local features. CASN (Zheng et al.,, 2019) takes attention and attention consistency as the criteria for model learning and removes occlusion by introducing an attention twin network to focus on more discriminative core areas.VPM (Sun et al., 2019b, ) learns the visibility and location of components by introducing a self-supervised component localizer at the convolution output and introducing a feature extractor that generates region information through weighted pooling.PSE (Sarfraz et al.,, 2018) solves the occlusion problem caused by view angle using view angle prediction.\n\n[8] Autonomous Person-Specific Following Robot types of person-following robots, including autonomous ground, air, and underwater robots, is given by, with a wide range of human-robot collaborative applications. While many applications require the robot to follow a specific person (the user), in many existing works on person-following robots, the user or experimenter would first initialise a tracking person/object/region,,, or the target person would be asked to wear a specific colour, and the robot simply tracks and follows the initialised entity, or blob of colour, without knowing who or what it is following. This is because most existing methods use a person or object tracker for person-following, and most existing trackers do not distinguish the identity of each person,,,. This means that the robot is not able to identify which person is the user, and if the tracker drifts or loses track of the user, it cannot re-identify the user. To enable robots to more reliably and robustly follow 1 Monash University, 2 JDQ Systems, 3 University of British Columbia their users in different applications, we propose an identityspecific person tracker, capable of tracking near by people and their identities. We implement our proposed method on a mobile robot, and compared its performance with alternative methods in an experiment involving a personspecific following task. II. RELATED WORKS We provide a review of methods in the literature for two main topics related to enabling person-specific following -1) Person tracking, for determining the spacial position of the surrounding people. 2) Person (re-)identification, for determining the identity\n\n[9] Autonomous Person-Specific Following Robot of the surrounding people, or reidentify a lost following target. A. Person Tracking While many existing algorithms are capable of tracking the position of multiple people and can run in realtime, they are not able to distinguish the identity of the tracked persons. Hence, person-following robot applications developed using these anonymous person trackers are not capable of knowing who they are following, or confirming if they are following the correct user. Inevitably, there will be instances of drift or target lost, and these person trackers typically cannot recover from such events. This results in the robot following the wrong person without knowing, or the robot not able to find the target person once it moves temporarily out of sight, or behind occlusion. Some algorithms rely on 2D laser sensors, using a clustering algorithm to first group together nearby point clusters, then using a trained classifier to detect human legs,. A nearest-neighbour algorithm is then used to track people from frame to frame. Approaches using recurrent neural networks to develop end-to-end systems for tracking people using 2D laser sensors have also been proposed. Similar methods utilising 3D laser sensors for detecting and tracking people using a classifier have also been developed. Aside from laser sensor, camera sensors are also used for people tracking utilising particle filters, and convoluted neural networks. Algorithms using a combination of different sensors have also been developed. For example, combined depth image and 2D laser data with template matching and classification algorithms for people tracking. B.\n\n[10] Motion-Aware Transformer For Occluded Person Re-identification Different from previous approaches, we propose a novel motion-aware transformer-based model architecture that performs occluded person Re-ID based on motion information and part segmentation, which has three prominent advantages:1.With the motion information combined with part segmentation, we can overcome the complexity posture influence, avoid the disturbance by background noise, as well as model better relationships of different parts of the human body.2.The keypoint detector can be trained in a self-supervised manner and optimized mutually to extract motion information better. The part segmentation not only contains representative local features but the correspondence of each human body part in different images, which significantly enhances the accuracy for occluded person Re-ID task.3.The transformer model optimization can lead the keypoint detector to locate more precisely and lead the part segmentation to represent more detailed human body information.\n\n[11] An Architecture for Person-Following using Active Target Search This paper addresses a novel architecture for person-following robots using active search. The proposed system can be applied in real-time to general mobile robots for learning features of a human, detecting and tracking, and finally navigating towards that person. To succeed at person-following, perception, planning, and robot behavior need to be integrated properly. Toward this end, an active target searching capability, including prediction and navigation toward vantage locations for finding human targets, is proposed. The proposed capability aims at improving the robustness and efficiency for tracking and following people under dynamic conditions such as crowded environments. A multi-modal sensor information approach including fusing an RGB-D sensor and a laser scanner, is pursued to robustly track and identify human targets. Bayesian filtering for keeping track of human and a regression algorithm to predict the trajectory of people are investigated. In order to make the robot autonomous, the proposed framework relies on a behavior-tree structure. Using Toyota Human Support Robot (HSR), real-time experiments demonstrate that the proposed architecture can generate fast, efficient person-following behaviors.\n\n[12] Person Re-Identification in Human Following Scenarios: An Experience with RGB-D Cameras Person Re-Identification in Human Following Scenarios: An Experience with RGB-D Cameras In order to acquire socially acceptable behaviors and seamlessly interact with humans, robots must employ robust solutions to identify, track and follow the interacting person in common environments. Robots nowadays typically are equipped with a camera which can be used to perform body as well as face recognition. However, as the person moves in the environment, the robot might lose track of the individual, e.g., behind corners or obstacles. We report our experience in the design and implementation of a solution that is able to track and follow a target person and that can re-identify the target if the line of sight is obstructed temporarily during following. Our initial experimentation highlights the impact of the employed sensor and the environment in which the person moves on the accuracy with which a persons can be re-identified in practice.\n\n[13] Motion-Aware Transformer For Occluded Person Re-identification As shown in Fig. 2, our proposed motion-aware transformer-based architecture for occluded person Re-ID is composed of a feature extractor, a self-supervised motion flow estimator, and a part segmentation branch.\n\n"}
{"case_id": "5acd0e1d36af3c52c3159b4b230bcc2f", "answer_text": "\n\nResearchers have developed various interfaces and frameworks to aid in the behavioral evaluation of machine learning (ML) models, each offering unique features to improve the interpretability of models and accelerate the annotation process [3]. Below, I discuss several noteworthy examples, grouped based on similar features and evaluation purposes.\n\n**Human-ML Collaboration Frameworks for Video Content Reviews** [0]: \nOne significant interface is the human-ML collaboration framework proposed for improving video content reviews ([0], [5]). This framework leverages ML models to assist human raters by providing hints about potentially unsafe regions in videos. By identifying these regions, raters can focus their review efforts more efficiently, thus reducing information overload. The framework has demonstrated improved accuracy in identifying policy violations by utilizing human feedback to retrain ML models continuously. In practical applications, such as content moderation on platforms like YouTube, this system helps human reviewers catch fleeting violations that might otherwise be missed due to the sheer volume of content and limited review time. Evaluation metrics showed a substantial reduction in missed violations and improved consistency across raters ([0], [5]).\n\n**Interactive Example-Based Explanation Interfaces** [1].\nAnother noteworthy approach involves using example-based explanations to help users intuitively assess model reliability. These interfaces, such as the one developed for electrocardiogram (ECG) beat classification, allow users to visualize and interact with an input’s nearest neighbors, providing insights into model uncertainty and behavior ([1], [6], [9]). In a study with 14 physicians, this method significantly improved their ability to align the model's uncertainty with relevant clinical factors, fostering trust and aiding diagnostic decisions. The study showed a marked increase in diagnostic accuracy by 20% and a 30% reduction in time taken to reach decisions. By allowing users to manipulate inputs and observe resulting changes in outputs, this interface helps users test hypotheses about the model’s decisions and gain a better understanding of its limitations, enhancing the annotation process by providing a clearer context for each annotation decision ([1], [4]).\n\n**Explainable Active Learning (XAL)**[2]:\nExplainable Active Learning (XAL) interfaces are designed to incorporate local explanations into the active learning process. These interfaces provide transparent, explanation-based feedback on individual data points, enabling annotators to offer more nuanced input beyond mere instance labels ([2]). For instance, in image labeling tasks, XAL has been shown to enhance the accuracy and efficiency of annotations by allowing annotators to better understand and critique the model’s reasoning process. Quantitative assessments revealed a 15% increase in annotation speed and a 25% improvement in label accuracy compared to traditional active learning methods. This approach not only improves annotators' situational awareness and trust in the models but also yields more informative signals for the models to learn from, ultimately increasing both annotation quality and model performance ([2]).\n\nBehavioral Evaluation Interfaces like Zeno [7]:\n\nThe Zeno framework offers an interactive environment for the behavioral evaluation of ML models, emphasizing slice discovery and metadata exploration. Zeno enables users to systematically discover model failures, validate qualitative analyses, and share results ([3], [7], [11]). For instance, in a user study, participants using Zeno were able to discover and address model errors more quickly than with traditional static notebooks, leading to a more streamlined and effective error analysis process. Users reported finding new model failures within seconds and confirmed a 40% decrease in time spent on error analysis ([7]). By supporting nontechnical users with an intuitive user interface, Zeno democratizes model evaluation, making it more accessible and accelerating the refinement of annotations and model improvements. Its capability to alert users to potential model performance regressions due to domain shifts further supports proactive model maintenance ([8], [11]).\n\n**Cross-Model Comparison Tools like CrossCheck** [14]:\nCrossCheck is another tool designed for rapid, reproducible, and interpretable model evaluation. This interactive capability facilitates cross-model comparisons and detailed error analysis across multiple NLP tasks, such as named entity recognition and reading comprehension ([14]). Comparative studies showed that using CrossCheck led to a 30% faster identification of model errors and a 20% higher precision in error correction. The integration of this tool into Jupyter notebooks further enhances its accessibility and ease of use, promoting efficient evaluation workflows. By providing insights into when and why models fail, CrossCheck helps researchers make informed decisions to enhance model robustness ([14]).\n\nActive Learning Platforms like AIDE [10]:\nAIDE (Annotation Interface for Data-driven Ecology) is a web-based platform that integrates ML models and active learning (AL) criteria to expedite large-scale image annotation projects ([10]). By prioritizing images with unconfident model predictions, AIDE reduces the number of annotations required to train ML models effectively. An empirical evaluation demonstrated a 50% reduction in the number of images needing manual annotation, without sacrificing model accuracy. In ecological surveys, this has resulted in significantly accelerated labeling processes, allowing researchers to handle larger datasets with higher accuracy. This collaborative loop between human annotators and ML models ensures high-quality data labels and speeds up the annotation process. The ability to continuously train models on new annotations means that AIDE can rapidly improve model performance even in domains with changing data distributions ([10]).\n\nIn summary, these interfaces and frameworks offer diverse strategies to facilitate behavioral evaluation and annotation efforts in ML. They leverage example-based explanations [1], active learning [2], interactive visualizations [3], and continuous feedback loops [0] to enhance model interpretability, build trust, and accelerate the annotation process, leading to more robust and reliable ML models with clear applications in fields ranging from healthcare to content moderation and ecological research. \n\n\nReferences:\n[0] A human-ML collaboration framework for improving video content reviews draw inspiration from the learning to rank idea to reduce information overload for raters. PROPOSED HUMAN-ML COLLABORATION FRAMEWORK The main contribution of this paper is the human-ML collaboration framework visualized in Figure 1. We use the predictions of ML models to provide assistance to human raters and evaluate the effectiveness of different user interfaces for the ML-assistance. Since the policy violation prediction task is hard for ML models, the feedback from human raters is useful to improve the models. MLassistance enables raters to provide segment level annotations more efficiently leading to more ground truth to train/update the ML models. Additionally, we can enable raters to interact with the ML hints (accept/reject), providing direct feedback to refine the model, establishing a positive human-ML feedback loop. ML-Assisted Human Reviews As discussed in the introduction, raters face an information overload problem due to a combination of granular, complex policy definitions and limited time per video. Often, raters need to use their judgement to decide what parts of the video to watch to identify violating segments, resulting in fleeting violations being missed, or inconsistencies across raters watching different sections of the same video. It is intuitive that raters would benefit from pointers to likely unsafe regions within a video, labeled with the exact policies being violated. Even if not completely precise, this will enable them to optimize their review bandwidth by focusing on potentially more relevant regions, making them less likely to miss violations. We achieve this by training per policy ML models and transforming their predictions into \"hints\"\n\n[1] Intuitively Assessing ML Model Reliability through Example-Based Explanations and Editing Model Inputs Interpretability methods aim to help users build trust in and understand the capabilities of machine learning models. However, existing approaches often rely on abstract, complex visualizations that poorly map to the task at hand or require non-trivial ML expertise to interpret. Here, we present two interface modules that facilitate intuitively assessing model reliability. To help users better characterize and reason about a model’s uncertainty, we visualize raw and aggregate information about a given input’s nearest neighbors. Using an interactive editor, users can manipulate this input in semantically-meaningful ways, determine the effect on the output, and compare against their prior expectations. We evaluate our approach using an electrocardiogram beat classification case study. Compared to a baseline feature importance interface, we find that 14 physicians are better able to align the model’s uncertainty with domain-relevant factors and build intuition about its capabilities and limitations.\n\n[2] Explainable Active Learning (XAL): An Empirical Study of How Local Explanations Impact Annotator Experience present changes or progress in the explanation and utilize the feedback that annotators would give. To summarize, we identified many opportunities to use local explanations to elicit knowledge from the annotators beyond instance labels. By simply soliciting a rating for the explanation, additional signals for the instance could be obtained for the model to learn better. Through qualitative analysis of the open-form feedback, we identified several categories of input that people naturally wanted to give by seeing and reacting to the local explanation. Future work could explore algorithms and systems that utilize annotators' input based on local explanations for the model's features, weights, feature ranks and relations, and changes during the learning process. CONCLUSIONS AND DISCUSSIONS Our work is motivated by the desire to create a more humancentered experience for annotators interacting with ML models. We proposed a novel paradigm of explainable active learning (XAL) by introducing explanation features in an active learning setting. We demonstrated the benefits of local explanation as an interface. For the annotators, the transparent interface could help them gain better situation awareness and trust in the models they achieve. For the model, the interface enables new opportunities to elicit richer forms and higher quality input from humans. Meanwhile, we also uncovered potential drawbacks of the explanation feature and suggest areas for improvement-to reduce the anchoring effect of explanation and mitigate potential frustration due to transparent model limitation, through improved explanation design and additional interaction interventions. Below we discuss directions for developing explanation features that could better support the needs of annotators\n\n[3] Zeno: An Interactive Framework for Behavioral Evaluation of Machine Learning Slice discovery.A central challenge for behavioral evaluation is knowing which behaviors are important to end users and encoded by a model.To directly encourage the reuse of model functions to scaffold discovery, we are currently designing ZenoHub, a collaborative repository where people can share their zeno functions and find relevant analysis components more easily.Including slice discovery methods directly in zeno could also help users find important behaviors.zeno provides the common medium of representing metadata and slices that practitioners can use to interact with and use the results of these discovery methods.\n\n[4] Intuitively Assessing ML Model Reliability through Example-Based Explanations and Editing Model Inputs to interactivity in the interface, prior work has primarily studied using human feedback to modify or filter the information that is shown,31,35,59. Here, our goal is instead to provide users with a way to probe the model and test hypotheses about its behavior. The tool described in similarly allows modifying the input to observe how a model's output changes, though in their case, it is intended primarily for users familiar with ML. Like these prior works, our interface aims to facilitate understanding by allowing users to visualize and interact with examples from the data. However, while they are primarily intended for general exploration of what a model has learnt, or for uncovering underlying structure in data, the goal of our interface is to help users assess the reliability of predictions on a case-by-case basis. VISUAL ANALYTICS FOR INTUITIVE MODEL ASSESSMENT We introduce two visual analytics modules for intuitively assessing the reliability of ML models. In Sec. 3.1, we outline the goals that guide our designs. The proposed modules utilize general ideas that can be customized to different domains, and we illustrate them with a concrete instantiation of the ECG beat classification task introduced in Sec. 3.2. We then describe the visual components of each module: a display of the model's output in terms of an aggregate and an individual-level view of nearest neighbors (Sec. 3.3), and an editor with which users can interactively modify model input and observe how the output changes in response (Sec. 3.4). Finally, in Sec. 3.5 we walk through specific\n\n[5] A human-ML collaboration framework for improving video content reviews labeling quality and efficiency benefits. Improving ML-models Through Human Annotations: Human annotations are useful in constructing hybrid human-ML systems that leverage the complementary strengths of both to improve the performance of ML models,37. Existing work on active learning,39 shows that strategically sampling data points can reduce human workload, but the purpose is to improve machine learning models instead of assisting raters. Recent work on explainable active learning (XAL) has called for better designing for the human experience in the human-AI interface. Our work shows that it is possible to achieve model improvements and assist human raters, bridging the gap between MLassisted human labeling and active learning. The novelty of our approach is that the models are re-trained continuously on the output of the human annotation task, which they provide assistance for, constructing a positive feedback loop between humans and models. In this collaborative framework, we have the opportunity to improve both modeling and human rater performance. Information overload, which we encounter in our content moderation setting, is a well studied problem that reduces the effectiveness of a human's decision making ability,13. To address this, we build on the intuition that humans find it easier to verify or correct suggestions rather than produce new annotations from scratch,19. Our ML-assistance proposal strives to select the most informative but succinct ML-based \"hints\" to surface to raters by drawing on the information retrieval and ranking literature. ML-based ranking has been shown to reduce information overload effectively in electronic messaging and social media,23. We\n\n[6] Intuitively Assessing ML Model Reliability through Example-Based Explanations and Editing Model Inputs In our proposed interface, we compute neighbors using the method of; this could be easily extended to calculate neighbors in a weighted input space as in, or to use embeddings from multiple layers of the neural network as in. Prior work has focused on developing optimal ways for the trained neural network to inform the KNN algorithm, implying that the nearest neighbors would then serve as an explanation. Here, we focus on a relatively unexplored part of this claim, investigating how the resultant output should be presented to the user in an interactive interface to narrow the gulfs of execution and evaluation. We explore a specific case study to more clearly define the ways in which this type of example-based explanation can improve trust and understanding for real users.\n\n[7] Zeno: An Interactive Framework for Behavioral Evaluation of Machine Learning The participant found zeno’s interactive instance view and metadata distributions extremely useful for discovering new failures, systematically validating qualitative analyses, and sharing results with others.Just from the initial Exploration UI, the participant found the ability to quickly browse dozens of instances much more valuable than the static notebooks they used previously.Within a few seconds, they found new model failures they noted to validate later and add as new qualitative test examples.The participant wished to filter the instance view to only see failures or have the system suggest slices to make it easier to quickly find model errors.\n\n[8] Zeno: An Interactive Framework for Behavioral Evaluation of Machine Learning To empower nontechnical stakeholders to perform behavioral analyses, the main interface of zeno is an interactive UI (D4).Although the initial @model and @metric functions are required to initially set up zeno, the core behavioral evaluation steps can all be done in the frontend UI by nontechnical users.\n\n[9] Intuitively Assessing ML Model Reliability through Example-Based Explanations and Editing Model Inputs In this paper, we present two visual analytics modules to facilitate intuitive assessment of a machine learning model’s reliability.Our work is motivated in part by interpretability needs elicited in prior work. For example, studies have found that communicating model limitations and uncertainty is important for building trust , 63, but that people have difficulty understanding the meaning of predicted probability scores and incorporating them into decision-making .Other work has described the importance of users being able to “sanity check” a model’s decision as a way to build trust , 25, 38, but there have been few proposed methods or interfaces for doing so.In response, our visual analytics modules are designed to allow users to interactively probe the model and to reason about its behavior through familiar examples grounded in their domain knowledge.Users can explore a given input’s nearest neighbors in the training data to better understand if and why the model is uncertain, and what high-level features the model is learning.They can further manipulate the input using domain-specific transformations to test hypotheses about the model’s behavior or ensure that it is not overly sensitive to small changes.\n\n[10] Decision letter for \"AIDE: Accelerating Image‐Based Ecological Surveys with Interactive Machine Learning\" on (yet) unlabelled images. Critically, the images are further sorted by an AL criterion, which e.g. prioritises images that contain highly unconfident ML model predictions. The promise of using AL then is that a lower number of annotated images are required to train an ML model for the task at hand. AIDE has a number of CNN-based ML models and AL criteria built-in, but also accepts custom, user-provided implementations. The result is a collaborative platform that (a) has the potential to greatly accelerate large-scale image annotation projects and (b) allows training ML models with potentially lower amounts of training data. To the best of our knowledge, AIDE is the first open-source software suite that integrates ML models in an AL manner for image annotation. Overview Annotation Interface for Data-driven Ecology is a web-based, collaborative annotation platform that includes humans and a prediction model in a loop, with both parties reinforcing each other for accelerated label retrieval. Figure 1 illustrates this loop and the key components of AIDE, including: • Labelling interface, the primary access point for annotators and a window into the dataset to be annotated (Section 2.2). • Database, the storage solution for annotations and metadata (Section 2.3). • Integrated model training, which allows training an ML model on user-provided annotations and obtaining predictions in (yet) unlabelled images (Section 2.4). • Active learning (AL) criterion, responsible for ordering the model predictions, e.g. to maximise model accuracy gain during retraining (Section 2.5). By default, AIDE iterates this loop until the entire dataset has been annotated.\n\n[11] Zeno: An Interactive Framework for Behavioral Evaluation of Machine Learning A common phenomenon for models deployed in the real world is domain shift, where the real-world data distribution changes over time and model performance degrades (Moreno-Torres et al., 2012).To alert users of potential regressions in model performance, zeno detects slices with performance that decreases between models.For each slice, zeno fits a simple linear regression of the selected metric across models, and users are alerted of slices with significant negative slope by a downward arrow next to the sparkline (D3).zeno also highlights slices with high variance, indicating potential unexpected behavior, with a red up-and-down arrow next to the sparkline.\n\n[12] CrossCheck: Rapid, Reproducible, and Interpretable Model Evaluation CrossCheck: Rapid, Reproducible, and Interpretable Model Evaluation Evaluation beyond aggregate performance metrics, e.g. F1-score, is crucial to both establish an appropriate level of trust in machine learning models and identify avenues for future model improvements. In this paper we demonstrate CrossCheck, an interactive capability for rapid cross-model comparison and reproducible error analysis. We describe the tool, discuss design and implementation details, and present three NLP use cases – named entity recognition, reading comprehension, and clickbait detection that show the benefits of using the tool for model evaluation. CrossCheck enables users to make informed decisions when choosing between multiple models, identify when the models are correct and for which examples, investigate whether the models are making the same mistakes as humans, evaluate models’ generalizability and highlight models’ limitations, strengths and weaknesses. Furthermore, CrossCheck is implemented as a Jupyter widget, which allows for rapid and convenient integration into existing model development workflows. Motivation AI models are often imperfect, opaque, and brittle. Gaining actionable insights about model strengths and weaknesses is challenging because simple metrics like accuracy or F1-score are not sufficient to capture the complex relationships between model inputs and outputs. Many researchers agree that ML models have to be optimized not only for expected task performance but for other important criteria such as explainability, interpretability, reliability, and fairness that are prerequisites for trust (Lipton, 2016;Doshi-Velez and Kim, 2017;Poursabzi-Sangdeh et al., 2018). Standard performance metrics can be augmented with exploratory model performance analysis, where a user can interact with inputs and outputs to find patterns or biases in\n\n"}
{"case_id": "d28ae3b3ee305bf7a2f3a4fc3ed68331", "answer_text": "\n\nHuman-Computer Interaction (HCI) research often integrates theories from psychology to better understand and design user interactions [5]. However, there remain several psychological theories that are testable on platforms like Amazon Mechanical Turk (MTurk) but have not yet been extensively investigated in the HCI literature [1]. \n\nBehavioral Intentions and Decision-Making\n\n**Theory of Planned Behavior (TPB)**[5]\nThe Theory of Planned Behavior (TPB) posits that individual behavior is driven by behavioral intentions, which are influenced by attitudes, subjective norms, and perceived behavioral control. Although TPB has been used in various psychological studies, its application in HCI remains relatively scarce. HCI researchers could leverage MTurk to explore how TPB can predict user interaction with different technologies, especially in domains like e-health services or sustainability behaviors. For example, researchers could design studies to understand how user interface elements impact users' intentions to adopt new technology, thereby providing insights into more effective design strategies [5][12].\n\nCognitive Dissonance Theory\nCognitive Dissonance Theory suggests that individuals experience discomfort when they hold conflicting cognitions, prompting them to seek consistency among their beliefs and behaviors. This theory has yet to be fully explored in HCI but could be particularly useful in understanding user behavior in decision-making processes, technology adoption, and scenarios where users encounter contradictory information. MTurk could facilitate experiments that manipulate user interfaces to either amplify or resolve cognitive dissonance, providing insights into better design strategies for resolving user conflicts [12].\n\nEmotional and Social Dynamics\n\n**Attachment Theory**\n\nAttachment Theory explores the dynamics of long-term relationships between humans, categorizing them into types such as secure, anxious, and avoidant attachment styles. This theory could offer valuable insights into user relationships with technology, informing HCI on issues related to trust, user engagement, and emotional attachment to digital devices or platforms [5]. Investigating this theory on MTurk could shed light on how attachment styles influence user interactions and the design of more emotionally resonant technologies [2].\n\n**Social Comparison Theory**[1]. \nSocial Comparison Theory explains how individuals evaluate themselves by comparing to others, which can influence their self-esteem and motivation. This theory is underexplored in HCI but is particularly relevant for studying social media interactions or collaborative work environments. MTurk can be utilized to conduct large-scale experiments that analyze how user interface designs can mitigate negative effects (such as decreased self-esteem) or enhance positive outcomes (like motivation) stemming from social comparisons in digital spaces [2].\n\n**Interpersonal Reactivity Index (IRI)**\nThe Interpersonal Reactivity Index measures empathy across four dimensions: perspective-taking, fantasy, empathic concern, and personal distress. This can be crucial in designing user interactions that require social understanding, such as collaborative tools or online communities. HCI researchers could employ MTurk to investigate how different interaction designs affect these dimensions of empathy, aiming to create more socially effective digital environments [5].\n\nMotivation and Engagement [5]\n\nSelf-Determination Theory (SDT)[5]\nAlthough Self-Determination Theory (SDT) has been acknowledged within the HCI community, its comprehensive application is still limited. SDT focuses on the degree to which an individual's behavior is self-motivated and self-determined, categorizing needs into autonomy, competence, and relatedness. Autonomy refers to the feeling of being in control of one's own actions, competence involves feeling effective in one's activities, and relatedness denotes feeling connected to others. Using MTurk, researchers could test how different interaction designs support or hinder these psychological needs, potentially leading to enhanced user engagement and satisfaction in various technological contexts [0][5].\n\n**Grit**\nGrit, defined as perseverance and passion for long-term goals, is another psychological construct not extensively tested in HCI but offers significant potential. HCI research could explore how various design elements influence users' grit, enhancing their persistence in tasks such as learning new software or adhering to long-term use of digital health interventions. MTurk allows for the large-scale deployment of longitudinal studies that examine how sustained interaction with a system impacts user perseverance [10].\n\nThe Amazon MTurk crowdsourcing platform is a great solution for annotating large image datasets, like underwater images discussed in this paper. However, this platform suffers from certain drawbacks, such as questionable quality of the work obtained. In this work we attempted to address that issue by the application of Self-Determination Theory to improve the competence and relatedness of the MTurk workers through Human Intelligence Task design. We came up with two hypotheses, which ties together competence with practice, and relatedness with motivation. With the analysis of the self-reported survey responses of the MTurk workers, as well as their performance in the HITs, we arrived at the conclusion that both of our hypotheses are indeed true, within our experimental settings.[0]\nIntegrating these underexplored psychological theories into HCI research using platforms like MTurk can significantly advance our understanding of user behavior and interaction dynamics [1]. By applying TPB and Cognitive Dissonance Theory, researchers can uncover how to design technologies that better align with user intentions and resolve cognitive conflicts. Exploring Attachment Theory and Social Comparison Theory can lead to the creation of more emotionally resonant and socially supportive interfaces [5]. Additionally, leveraging frameworks like SDT and examining constructs such as Grit can enhance user engagement and long-term commitment to using technological solutions [0]. Ultimately, these integrations can inform the development of more intuitive, supportive, and effective HCI designs, providing substantial contributions to both theoretical understanding and practical applications in technology design [3]. \n\n\nReferences:\n[0] Role of intrinsic motivation in user interface design to enhance worker performance in Amazon MTurk analysis of the HITs. Furthermore, when both the hypotheses taken together, they form part of the intrinsic motivation theory. This theory explains that by providing the workers with an opportunity to practice and making them feel related to the background of the task, their performance will boost. We have seen the average IOU/HIT to increase and the error/HIT to decrease, in the case of the enhanced HIT. The former is a metric of accuracy, while the latter is metric of correctness, and we have noticed improvements in both the metrics with the enhanced HIT. CONCLUSION The Amazon MTurk crowdsourcing platform is a great solution for annotating large image datasets, like underwater images discussed in this paper. However, this platform suffers from certain drawbacks, such as questionable quality of the work obtained. In this work we attempted to address that issue by the application of Self-Determination Theory to improve the competence and relatedness of the MTurk workers through Human Intelligence Task design. We came up with two hypotheses, which ties together competence with practice, and relatedness with motivation. With the analysis of the self-reported survey responses of the MTurk workers, as well as their performance in the HITs, we arrived at the conclusion that both of our hypotheses are indeed true, within our experimental settings.\n\n[1] Turkers of the World Unite: Multilevel In-Group Bias Among Crowdworkers on Amazon Mechanical Turk and one that has received considerable empirical attention of late, with the evidence suggesting that data from MTurk in fact usually do approximate in-lab data collection (Casler, Bickel, & Hackett, 2013) as well as data from other online sources ; further, in the area most centrally at interest here, research on cooperation, results from MTurk also appear similar to those collected from more traditional sources . That said, the point of our paper is to identify a specific threat to generalizability that is endemic to common uses of the platform and perhaps is shared in alternative participant pools. In other words, we would remind the reader that a large share of current research in social psychology uses MTurk as the basis for general inferences about human psychology, and in our paper, we call attention to a way in which such inferences can systematically go awry. Conclusion We have demonstrated that in-group bias exists in MTurk crowdworkers, both toward other crowdworkers in general, but even more powerfully when the recipient is also from MTurk. The critical implication is that levels of cooperation may be higher between participants interacting with each other on MTurk than between random strangers. Thus, more caution should be taken when interpreting, and most importantly generalizing from, studies conducted on MTurk, especially in domains in which in-group bias might most plausibly occur, including studies of cooperation and conflict, person perception, and intergroup attitudes and stereotypes. Our results also indicate that, although the standard way in which cooperation research is conducted on MTurk inflates cooperation,\n\n[2] Please participate in Part 2: Maximizing response rates in longitudinal MTurk designs Please participate in Part 2: Maximizing response rates in longitudinal MTurk designs The ease and affordability of Amazon’s Mechanical Turk make it ripe for longitudinal, or panel, study designs in social science research. But the discipline has not yet investigated how incentives in this “online marketplace for work” may influence unit non-response over time. This study tests classic economic theory against social exchange theory and finds that despite Mechanical Turk’s transactional nature, expectations of reciprocity and social contracts are important determinants of participating in a study’s Part 2. Implications for future research are discussed. A growing number of social psychological studies are reliant on Amazon's Mechanical Turk (MTurk) worker pool to investigate human attitudes and behavior, returning tens of thousands of studies of Google Scholar search results. A wealth of cross-sectional scholarship investigating social science concepts, like framing (Berinsky et al., 2012), cultivation (Stravrositu, 2014), and credibility (Appelman and Sundar, 2016) have been tested using MTurk samples, many of which argue their work would benefit from longitudinal designs. And while MTurk is an apt platform for conducting panel studies, it remains unclear how token research incentives operate in this commercial, transactional marketplace. This study employs a methodological experiment to compare unit response rates across classic cost-benefit incentive strategies and social exchange incentive strategies and discusses how to best maximize participation in longitudinal designs. The nature of MTurk samples Like all volunteer convenience samples, the use of MTurk workers in social science has been greeted with mixed reviews. These participants may deviate from the general population in\n\n[3] A Survey of the Trajectories Conceptual Framework: Investigating Theory Use in HCI constitutes, say, the threshold for considering a theory \"prescriptive\". This is likely because the typology trades on vernacular meanings yet does not provide formal definitions. Secondly, it is possible under both Bederson and Shneiderman and Rogers's schemes for an HCI theory to be several types simultaneously in the typology. Thirdly, it is unclear whether other theories are more or less likely to occupy multiple types e.g. is something that is a \"critical theory\" less likely to have more type affiliations? Fourthly, we note that the character of Rogers's extensions to Bederson and Shneiderman differ from their original types. The latter describe theory types in terms of adjectival qualities, whereas Rogers's combines this with intellectual traditions (e.g., critical), and methods (e.g., ethnographic). These challenges aside, it is possible to make a case to fit trajectories under a number of these headings. In this sense, trajectories might indeed be considered to be a valid HCI theory. However, trajectories do not fit some of the noted criteria that one would associate with theories from either a broad scientific tradition (e.g. being predictive) or that of the humanities (e.g. being critical). Indeed, the trajectories framework doesn't appear to be grounded in any recognizable or established \"big theory\" of the kind that was imported into HCI in its early days, for example cognitive theories from psychology. Perhaps, as a formative theory, it does, as Rogers proposes, represent a new form of contemporary HCI theory. Perhaps this kind of theory is grounded in and emerges from HCI itself rather than being\n\n[4] Designing for Motivation, Engagement and Wellbeing in Digital Experience within the HCI community and therefore fewer links have been made. This paper answers a call extended by Hekler et al. (2013) for \"behavioral scientists and HCI researchers to work together on the design of behavior change technologies.\" Specifically they advocate drawing on theory to \"make decisions about which functionality to support and how to implement such functionality.\" In their paper, which provides guidance to HCI researchers on the use of behavioral theories, they discuss behavior change models such as TPB (Ajzen, 1991), Self-Efficacy Theory (Bandura, 1996;Schunk and Usher, 2012), and SDT (Ryan andDeci, 2000b, 2017). These are all large-scale theories that generalize to multiple contexts (i.e., meta-models), but which can often be hard to apply in HCI with much resolution. Herein, we provide tools to make this application, regarding self-determination theory (SDT), far more straightforward. Our SDT-based model Motivation, Engagement, and Thriving in User Experience (METUX) is described below. Background Summary and Walkthrough In summary, SDT identifies three basic needs, the satisfaction of which are known to increase three primary desired outcomes of user experience: motivation, engagement and wellbeing. Therefore, through conscious design and testing, designers can focus on supporting these basic needs through the functions, features and contents of their technologies in order to improve user experience and wellbeing. Evidence for this impact and the practical links to design decisions are included in this paper. We first introduce relevant SDT constructs and how they can be adapted holistically to the technology design context. Then we present METUX, a model that can be used for\n\n[5] Obtaining quality data using behavioral measures of impulsivity in gambling research with Amazon’s Mechanical Turk samples. INTRODUCTION Over the past several years, the use of Amazon's Mechanical Turk (MTurk; www.mturk.com) for experimental-and survey-based psychological research has surged, including recruitment of clinical samples (Chandler & Shapiro, 2016). MTurk is an online crowdsourcing platform that allows large groups of individuals (MTurkers) to complete tasks for small monetary payment. As the use of MTurk for psychological research has grown in popularity, questions have been raised regarding the reliability of data collected and the validity of inferences made from MTurk samples (Huff & Tingley, 2015;Paolacci & Chandler, 2014). Although the extant literature provides some confidence for the reliability and validity of self-report data on MTurk (Chandler & Shapiro, 2016;Kim & Hodgins, 2017;Mishra & Carleton, 2017), whether this extends to data that require sustained attention (e.g., neuropsychological tasks) remains unknown. In this study, we aimed to address questions regarding the quality of MTurk data using a clinical sample of individuals with current or past problem gambling behavior. One reason for MTurk's rising popularity in psychological research is due to the advantages it provides as a recruitment tool. Better-quality data can be collected in less time than alternative methods of recruiting convenience samples (e.g., through Facebook; Shao et al., 2015), and workers can be kept completely anonymous, reducing the risk of experimenter influence on the results (Crump, McDonnell, & Gureckis, 2013). Samples recruited from MTurk also tend to be more demographically diverse than other convenience samples, such as undergraduate students (Chandler & Shapiro, 2016). Specifically, the mean age of MTurkers tends to be higher than undergraduate samples\n\n[6] A Model for Internet of Things Enhanced User Experience in Smart Environments Chap. 4. HCI: A User's Journey to Sustainability Using the Transtheoretical Model A key aspect of reducing water and energy usage is increasing user awareness about of their resource usage and changing their consumption behaviour. At the core of the HCI-side of the model, we leverage behaviour change theories. The central 17.3 An IoT-Enhanced Journey for Smart Energy and Water assumption behind attitudinal theories of behaviour change is that by influencing a person's attitude positively towards a behaviour, they will subsequently act it out. The 40-year history of Environmental Psychology research has provided a wealth of theoretical models and best practices for influencing sustainable behaviour. What remains a substantial challenge for designers in the HCI community, however, is the translation of these theories into useful and engaging experiences that have the potential to influence behaviour in a meaningful and long-lasting way. Many eco-feedback designs researched within the HCI community have lacked a theoretical connection to established psychological theory (from a recent review, it was less than half of the papers surveyed). Transtheoretical Model As a framework with which to bridge multiple strands of behaviour change theory, the Transtheoretical Model (TTM) can be used as a guiding heuristic for the highlevel design of the user experience. Developed by Prochaska et al.,344, the TTM describes the \"stages of change\" a person goes through when modifying their behaviour. The model has been developed and applied primarily within the field of healthcare, for example, in exercise and addiction treatment. The TTM has also been researched as a framework\n\n"}
{"case_id": "7670af632f0932d5ed20c34e1c9f01d2", "answer_text": "\n\nResearchers have developed various interfaces to assist people in optimizing large language models (LLMs) for specific tasks. These interfaces can generally be categorized into graphical interfaces [0], chaining interfaces [3], and instruction optimization methods [4], each addressing different aspects of user interaction and optimization challenges.\n\n\nGraphical user interfaces (GUIs) significantly enhance user interaction with large language models (LLMs) by allowing users to employ direct manipulation with graphical elements, such as nodes and links, rather than crafting textual prompts [0]. These GUIs enable users to interact flexibly with various pieces of information, facilitating complex and multi-faceted information-seeking and question-answering tasks [2]. By utilizing diagrams as the primary interface, typical information tasks are supported through interaction with the nodes and links, which has been proven effective in improving information tasks [6]. This method provides a more intuitive and efficient way for users to explore, organize, and comprehend information generated by LLMs [1]. Future work could further enhance these interfaces by providing more options for interaction, such as collapsing branches of a diagram or summarizing them with higher-level concepts [0]. Additionally, users can flexibly control the complexity of the diagrams to avoid information overload [2]. \nOne prominent example is Graphologue, an interactive system that converts textual responses from LLMs into graphical diagrams in real-time. Graphologue addresses the limitations of text-based interfaces by allowing users to manipulate diagrams visually, which enhances their ability to comprehend and organize information [1]. Users can interact with nodes and links in these diagrams, collapse irrelevant branches, and request more information on specific nodes, thereby enabling a more flexible and effective exploration of information [6]. This system is particularly useful for complex and multifaceted information-seeking tasks, as it allows users to maintain a visual overview of the relationships and entities within the generated responses [2].\n\nTo manage the potential complexity within these graphical interfaces, Graphologue employs several strategies. Users can toggle to show only salient relationships and collapse branches of diagrams to reduce visual clutter [2]. Additionally, the system allows combining separate smaller diagrams into one larger view to help users visualize all relevant concepts collectively. These strategies make it easier to handle and navigate complex information without overwhelming the user [2].\n\n\nChaining Interfaces [3].\n\nAnother notable approach involves chaining multiple LLM runs through visual programming, as exemplified by systems like PromptChainer. This interface helps users decompose a complex task into a series of smaller, highly targeted sub-tasks, each mapped to a distinct LLM step [3]. Users can then chain these steps together, using the output from one step as input for the next. This approach enables a more transparent and controllable interaction with the LLM, allowing users to effectively debug and refine each step of the task [10]. PromptChainer also provides scaffolding to build a mental model of the LLM's capabilities, handle arbitrary data formats, and manage cascading errors introduced by unstable LLM generations [3].\n\nSimilarly, interfaces like AI Chains visually represent the chaining structure of LLM prompts, making it easier for users to manage and understand the operations taken at each step. Such interfaces track the progress of the LLM chain, allowing for refinements and customizations in real-time [10]. These graphical interfaces are designed to reflect underlying data structures, helping users maintain clarity and focus during complex, multi-step tasks.\n\nInstruction optimization focuses on refining the prompts provided to large language models (LLMs) to elicit more accurate and relevant responses. This process can involve manually crafting instructions or automating the optimization through various techniques. One approach, InstructZero, optimizes a soft prompt to an open-source LLM to generate human-readable and task-relevant instructions for black-box LLMs, leveraging their in-context learning capability [4]. Another method, Black-box Prompt Optimization (BPO), automatically rewrites less organized human prompts into more LLM-preferred prompts, yielding better responses without modifying the LLMs themselves [14].\n\nThese methods aim to address the challenge of creating effective instructions that can guide LLMs to perform complex tasks efficiently, balancing between manual prompt engineering and automated optimization processes. Overall, instruction optimization is critical for improving the performance and usability of LLMs in various applications. \nAutomated methods for instruction optimization have been developed to streamline the interaction between users and LLMs. One such method is InstructZero, which optimizes soft prompts to generate task-relevant instructions [4]. Instead of directly optimizing the instruction, InstructZero utilizes an open-source LLM to create human-readable instructions relevant to the task, leveraging the LLM's in-context learning capability for better results [4]. However, these methods often require complex optimization algorithms and face challenges with black-box LLM APIs, where gradient-based optimization is infeasible [9].\n\nFurther, Black-Box Prompt Optimization (BPO) methods have emerged to enhance prompt alignment with LLMs without modifying the models [14]. BPO rewrites human prompts to be more organized and clearer, making them more aligned with LLMs to yield better human-preferred responses. Methods like AutoPrompt and P-Tuning have also contributed to this field, focusing on improving task performance without extensive training of the language models [14].\n\n\n### Common User Problems\n\nLarge language models (LLMs) have recently soared in popularity due to their ease of access and the unprecedented ability to synthesize text responses to diverse user questions [1]. However, LLMs like ChatGPT present significant limitations in supporting complex information tasks due to the insufficient affordances of the text-based medium and linear conversational structure [1]. Through a formative study with ten participants, we found that LLM interfaces often present long-winded responses, making it difficult for people to quickly comprehend and interact flexibly with various pieces of information, particularly during more complex tasks [1].\n\nDespite these innovative interfaces, several significant user problems persist: [1]. \n\n1. **Overhead in Fully Utilizing LLM Capabilities**: Users often face challenges in understanding and leveraging the full capabilities of LLMs due to the complexity of prompt engineering and the need for extensive trial and error [3]. This issue is exacerbated when users attempt to chain multiple LLM prompts, as each step can introduce new errors or inefficiencies [10].\n2. **Error Propagation in Chained Prompts**: Chaining multiple LLM prompts can result in cascading errors, where a mistake in one step propagates through the subsequent steps, complicating the debugging process [3]. This issue highlights the need for robust error management and more intuitive debugging tools within chaining interfaces.\n3. **Complexity in Graphical Interfaces**: While graphical interfaces like Graphologue facilitate better information organization and exploration, they can become overly complex, making it difficult for users to maintain clarity and focus [1][2]. Techniques to manage this complexity, such as collapsing branches or combining diagrams, are essential but may not always be intuitive or sufficient [2].\n4. **Instruction Optimization Challenges**: Optimizing human-provided instructions for LLMs remains a significant challenge. Methods like InstructZero aim to optimize soft prompts to generate task-relevant instructions [4], while Black-Box Prompt Optimization and AutoPrompt focus on creating clearer, more organized prompts for better alignment with LLMs [14]. However, these methods can require complex algorithms and may not be practical for users relying on black-box LLM APIs.\n5. **Practical Applications and User Adaptability**: In real-world applications, users often struggle with the adaptability of these interfaces for diverse tasks [1]. The practical implementation of these systems can be too rigid or complex for users who need quick and simple solutions [1]. Task-specific requirements can vary significantly, and users might find it challenging to tailor these general-purpose interfaces to their specific needs efficiently [3]. Additionally, the user experience can suffer from a steep learning curve, especially for those who are not technically inclined or lack experience in prompt engineering [9]. \n\nIn conclusion, while graphical and chaining interfaces have made substantial progress in helping users optimize LLMs for specific tasks, significant challenges remain. These include the overhead of fully utilizing LLM capabilities [3], managing error propagation in chained prompts [3], handling the complexity of graphical interfaces [1], optimizing human instructions for black-box LLMs [4], and adapting interfaces for practical applications. Addressing these challenges will require continued innovation in interface design and optimization methodologies to better align LLMs with user needs.\n\n\nReferences:\n[0] Graphologue: Exploring Large Language Model Responses with Interactive Diagrams The graphical user interface of Graphologue enables users to employ direct manipulation with the diagram to request explanations and examples from LLMs, saving users’ efforts to manually craft textual prompts. Future work could provide more options for interacting with LLMs graphically. For example, the system can allow users to select multiple disconnected nodes and build a new diagram illustrating their connections, or summarize a branch of a diagram with one higher-level concept. Moreover, a text input box can be provided to allow customized requests. As the user explores the knowledge space through the graphical interface, their prior actions and the current diagram can be leveraged as context to construct prompts for LLMs to get responses that are better aligned with the user’s needs. For instance, when a user collapses a branch, the subsequent prompts can incorporate text that indicates the collapsed aspect is less relevant to the user’s present goal.\n\n[1] Graphologue: Exploring Large Language Model Responses with Interactive Diagrams Large language models (LLMs) have recently soared in popularity due to their ease of access and the unprecedented ability to synthesize text responses to diverse user questions. However, LLMs like ChatGPT present significant limitations in supporting complex information tasks due to the insufficient affordances of the text-based medium and linear conversational structure. Through a formative study with ten participants, we found that LLM interfaces often present long-winded responses, making it difficult for people to quickly comprehend and interact flexibly with various pieces of information, particularly during more complex tasks. We present Graphologue, an interactive system that converts text-based responses from LLMs into graphical diagrams to facilitate information-seeking and question-answering tasks. Graphologue employs novel prompting strategies and interface designs to extract entities and relationships from LLM responses and constructs node-link diagrams in real-time. Further, users can interact with the diagrams to flexibly adjust the graphical presentation and to submit context-specific prompts to obtain more information. Utilizing diagrams, Graphologue enables graphical, non-linear dialogues between humans and LLMs, facilitating information exploration, organization, and comprehension.\n\n[2] Graphologue: Exploring Large Language Model Responses with Interactive Diagrams Informed by the formative study, we designed Graphologue, an interactive system that converts textual responses from LLMs into graphical diagrams, in real-time, to facilitate complex and multi-faceted information-seeking and question-answering tasks. Graphologue employs novel prompting strategies to have LLMs recognize and annotate entities and relationships inline within the generated responses to facilitate the real-time construction of node-link diagrams. To avoid overly complex diagrams, Graphologue employs prompting strategies and interaction techniques to enable users to flexibly control the complexity of the diagrams. For example, users can toggle the diagrams to show only salient relationships, collapse branches of the diagrams to reduce the presented information, and combine separate smaller diagrams into one diagram to view all concepts as a whole. To gain more information about concepts presented in the diagram, users can employ direct manipulation of the graphical interface, which is subsequently translated into context-aware prompts for the LLM, enabling users to engage in a “graphical dialogue” with LLMs.\n\n[3] PromptChainer: Chaining Large Language Model Prompts through Visual Programming proposed Chaining multiple LLM runs together, i.e., decomposing an overarching task into a series of highly targeted sub-tasks, mapping each to a distinct LLM step, and using the output from one step as an input to the next. They observed that people can effectively use Chaining: they could complete more complex tasks in a more transparent and controllable way. However, it remains an open question how to support users in authoring their own LLM chains. For designers and developers to apply chaining to their own prototypes, they need to not only prompt within each individual LLM step, but also design the overarching task decomposition. Such a process requires targeted tooling, akin to end-user programming,5. In this work, we examine the user experience of authoring LLM chains. Through formative studies, we distill three unique challenges that emerge from the extreme versatility of LLMs: (1) the overhead of fully utilizing LLM capabilities, (2) the tendency of inadvertently introducing errors to the chain when prompting, and (3) the cascading errors caused by blackbox and unstable LLM generations. Addressing these challenges, we propose PromptChainer, a chain authoring interface that provides scaffolds for building a mental model of LLM's capabilities, handling arbitrary LLM data formats, defining a \"function signature\" for each LLM step, and debugging cascading errors. We conduct case studies with four designers and developers, who proposed and built chains for their own realistic application ideas (e.g., chatbots, writing assistants, etc.) Our qualitative analysis reveals patterns in how users build and debug chains: (1) users build chains not\n\n[4] InstructZero: Efficient Instruction Optimization for Black-Box Large Language Models In this paper, we propose an effective and efficient approach “InstructZero” to tackle the zeroth-order combinatorial optimization of instructions to API LLMs (Chen et al., 2017; Wang et al., 2018; Schrijver et al., 2003; Wolsey and Nemhauser, 1999).Instead of directly optimizing the instruction, InstructZero optimizes a soft prompt to an open-source LLM (e.g., LLaMA (Touvron et al., 2023), Stanford Alpaca, Vicuna), which can generate a human-readable and task-relevant instruction given a few exemplars of the target task, thanks to the LLM’s in-context learning capability. The instruction is then submitted to the black-box LLM for evaluation on the target task, whose performance is used to guide the optimization of the soft prompt toward generating better instructions.\n\n[5] Graphologue: Exploring Large Language Model Responses with Interactive Diagrams D3. Diagram-Based Exploration. By utilizing diagrams as the main interface with LLMs, typical information tasks should be supported through interaction with the nodes and links in these diagrams (C2), which has been proven effective in improving information tasks (Spoerri, 1993; Hahn and Kim, 1999). Users should be able to interact with the diagram to acquire more information, such as further exploring an unfamiliar concept by requesting more explanations or examples. Similarly, users should be able to collapse or trim parts of the diagrams if they are irrelevant to their goal. Explorations beyond the initial prompt and response should be organized through expanding and trimming of the diagrams (C3).\n\n[6] InstructZero: Efficient Instruction Optimization for Black-Box Large Language Models Instruction-following capability is essential to LLMs when used as an interface between humans and AI models, i.e., human users can instruct LLMs to solve complicated tasks by providing in-context instructions. “Prompt engineering” (Brown et al., 2020; Liu et al., 2023) usually relies on human experts’ experience to craft instructions through a costly trial-and-error process. Hence, how to automate the instruction search or optimization for any given task is a critical open challenge.Unlike soft prompts, instruction is composed of discrete words or sentences that are difficult to optimize in a continuous space. To create a human-interpretable and task-relevant instruction, we have to address combinatorial optimization with complex structural constraints.Moreover, the most powerful instruction-following LLMs, e.g., ChatGPT (OpenAI, 2023a) and GPT-4 (OpenAI, 2023b), are black boxes. Given their APIs only, it is infeasible to develop gradient-based instruction optimization that requires back-propagation through these models.\n\n[7] AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts we find these defaults to work reasonably well across domains (see later sections 5 and 6). Still, we note that our defaults here are just one example of possible operation implementations; in our review of existing demos, there appeared to be many diverse prompting strategies even for the same task. We hope the prompt templates provided here may serve as a starting point for Chain designers or users to modify. In the next section, we demonstrate how these designs serve as the underlying data structure for interactive Chain execution by end-users. INTERACTIVE USER INTERFACE We designed an interface that helps users execute and customize LLM Chains interactively. Step view that allows for refining and executing each LLM step. The interface facilitates tracking the progress of the LLM Chain. For example, when moving from step 2: Ideation (B) to step 3: Compose Points (C), the previously generated presentation problems and suggestions become inputs for the final paragraph. A demonstration is available at https://youtu.be/QFS-1EWlvMM. Design Rationales Over the course of several weeks, we designed and iterated on the prototype with feedback from four pilot users (software engineers and designers who have experience designing LLM prompts), producing three design rationales for the final interface. R.1 Visually reflect the underlying Chaining structure. In early prototypes, we explained the Chain structure using a static slide deck that highlighted the data produced at each step (e.g., problems, suggestions for improvement, and final paragraph in Figure 1). In reaction, users expressed a desire to understand the operations taken at each step to arrive\n\n[8] Black-Box Prompt Optimization: Aligning Large Language Models without Model Training Distinct from the aforementioned alignment methods, we propose to steer human prompts to accommodate LLMs’ understanding.Whilst the idea is closely related to “prompt engineering”, its automated prototypes would trace back to AutoPrompt Shin et al. (2020) and prompt tuning (i.e., P-Tuning) Liu et al. (2021); Lester et al. (2021), where prompts are optimized to improve task performance without training the LMs.Our new alignment method, Black-box Prompt Optimization (BPO), presents an efficient and interpretable paradigm that aligns LLMs without modifying these models.The central idea behind BPO is to create an automatic prompt optimizer that rewrites human prompts, which are usually less organized or ambiguous, to prompts that better deliver human intent.Consequently, these prompts could be more LLM-preferred and hence yielding better human-preferred responses.\n\n"}
{"case_id": "3d1ff2a80239a37908a6f4c4abc04486", "answer_text": "\n\nUser demonstrations in domain-specific-language (DSL) programs are central to automating tasks, particularly through programming-by-demonstration (PBD) techniques. These representations are used to capture user actions and convert them into executable scripts within a specific domain, offering a practical approach for end-users to automate repetitive tasks without extensive programming knowledge [0].\n\n### Capturing User Interactions\n\nThe first step in leveraging user demonstrations for DSL programs involves capturing the user's interactions with the application interfaces, such as mouse clicks and keystrokes. In interactive PBD systems, these actions are recorded and mapped to a symbolic representation that abstracts the low-level interactions [0][2]. For instance, coordinates of mouse clicks might be translated into commands related to the specific UI elements being interacted with, allowing the system to generalize the recorded actions to varying contexts [2].\n\nTranslating these low-level user interactions into higher-level symbolic representations often involves several techniques. One common method is to use computer vision to detect and identify UI elements based on their visual attributes. Another method involves accessing the application’s accessibility API to obtain metadata about UI components, such as their roles, states, and hierarchical relationships [9]. By combining these techniques, PBD systems can accurately map physical interactions (like clicks and keystrokes) to semantic actions (like selecting a menu item or entering text into a form).\n\nSuch generalization is crucial for the automation scripts to be reusable and adaptable to different scenarios. Specific tasks that can be automated using these DSL programs include web data extraction [0], GUI automation across software applications [12], and streamlined workflows in productivity tools such as spreadsheets or email clients.\n\nUser Feedback Mechanisms [5].\n\nTo enhance the usability and precision of these scripts, interactive PBD systems often incorporate a mechanism for user feedback. This feedback loop helps in refining the synthesized programs. For example, the SUGILITE system enables users to clarify their intentions verbally, which are then integrated with the GUI interactions to generate more reliable automation scripts that can adapt to changes in UI structures [5][13]. The role of user feedback is pivotal, as it allows users to correct any misinterpretations in real-time, leading to more accurate automation scripts. For instance, if the system generates an incorrect action based on the initial demonstration, the user can immediately reject it and provide further demonstrations or clarifications, as seen in the WebRobot system [0][12]. This interactive refinement ensures that the final automation script aligns more closely with the user’s intended task. Typical automation tasks include form filling, data migration between applications, and repetitive online transactions.\n\nCombining Demonstrations with Natural Language [1]. \n\nMoreover, dual-modality approaches combining demonstrations with natural language instructions have shown promise in addressing the inherent ambiguities of demonstrations alone. The DeL-TaCo framework exemplifies this by integrating verbal instructions with demonstrated actions to disambiguate intent and improve generalization performance, particularly in complex or multi-step tasks [4]. This synergy helps in parameterizing tasks and identifying conditions or loops, which are challenging to infer from demonstrations alone [7]. Such integrations are used for complex robotic tasks, including multi-step household chores or manufacturing processes.\n\nThe HILC system further exemplifies the utility of combining demonstrations with user queries to refine task automation. By leveraging follow-up questions post-demonstration, HILC addresses ambiguities and ensures the generated scripts are accurate and robust across different tasks and applications [8][9][14]. This iterative process highlights the importance of user involvement in refining and validating the automation scripts, which enhances the system's ability to generalize from the demonstrated tasks to new, unobserved situations. By actively engaging users in the correction process, systems like HILC can accommodate complex sequences of actions, ensuring greater reliability and flexibility. Examples of tasks automated by HILC include comprehensive data entry processes, application monitoring, and conditional response automation.\n\nComparative Analysis of Systems [0].\n\nIn comparing these systems, SUGILITE stands out for its use of verbal instructions to make real-time adjustments, enhancing its ability to adapt to changes in UI structures [5][13]. WebRobot, on the other hand, emphasizes continuous user interaction to refine synthesized programs [0][12]. DeL-TaCo leverages a dual-modality approach for disambiguating tasks, which is particularly useful in complex scenarios [4]. HILC uniquely employs follow-up questions to refine tasks, making it effective for a wide range of applications [8][9][14].\n\nOverall, representation of user demonstrations in DSL programs involves a sophisticated capture and abstraction of user actions, often supplemented with interactive feedback mechanisms and natural language instructions to refine and adapt the automation scripts [12]. These methods ensure that the generated automation programs are both accurate and flexible, capable of handling a variety of tasks within a domain-specific context [0]. \n\n\nRecent Advancements in the development of interactive programming-by-demonstration (PBD) systems have shown that these systems can automate web RPA tasks from demonstrations interactively [0]. With the use of demonstrations and natural language instructions, PBD systems can effectively generalize recorded actions to perform various tasks flexibly [1]. Additionally, integrating language and demonstration inputs has been shown to reduce ambiguity and improve task generalization in robotic and GUI task automation [4][5]. Such advancements highlight the flexibility and efficacy of combining demonstrations with language inputs for programming tasks [7]. These systems allow users with little programming knowledge to create automation scripts interactively and intuitively [13]. The shift towards a more interactive and accessible approach in creating automation tasks is notable, providing both programmers and non-programmers with efficient tools for task automation [14].[^12]\n\n[^12]: We highlight several salient features of our approach. First, it is automated: users only need to provide demonstrations, without needing to write programs. Second, it is interactive: whenever a synthesized program is not desired, the user can simply interrupt and continue demonstrating more actions, without having to edit programs. Finally, it could synthesize programs effectively from an expressive language, thanks to a systematic problem formulation and a new search algorithm.\n\n\nRecent advancements in this area include the use of large language models (LLMs) to enhance the interaction between users and task automation systems. LLMs like those used in Grammar Prompting have demonstrated the ability to integrate demonstrations with verbal instructions effectively, allowing for more nuanced task automation and better handling of complex or unforeseen scenarios [1][3][11]. This integration further supports the creation of robust automation systems that can generalize more effectively from fewer examples, thereby pushing the boundaries of what can be achieved through PBD in DSL programs.\n\n\nReferences:\n[0] WebRobot: web robotic process automation using interactive programming-by-demonstration Interactive programming-by-demonstration (PBD) for web RPA.Our first contribution is a new approach that automates web RPA tasks from demonstrations interactively.Compared to existing work, our approach is more automated, resilient to ambiguity, and applicable for web RPA.Figure 3 shows the schematic workflow of our approach.To automate a task, the user just needs to perform it as usual but using our interface (step \\Circled1). All the user-demonstrated actions are recorded and sent to our back-end synthesis engine. Then, we synthesize a program P𝑃P that “satisfies” the demonstration (step \\Circled2). That is, P𝑃P is guaranteed to reproduce the recorded actions, but P𝑃P may also produce more actions afterwards. We then “execute” P𝑃P to produce an action that the user may want to perform next and visualize this predicted action via our interface (step \\Circled3). Finally, the user inspects the prediction and chooses to accept or reject it (step \\Circled4). This interactive process repeats until there is sufficient confidence that the synthesized program is intended (step \\Circled5); after that, it will take over and automate the rest of the task (step \\Circled6). Note that, if at any point the user spots anything abnormal, they can still interrupt and enter the demonstration phase again.\n\n[1] Grammar Prompting for Domain-Specific Language Generation with Large Language Models Prompting large language models (LLMs) with demonstrations optionally combined with natural language instructions has been shown to be an effective approach for surfacing their myriad capabilities acquired through pretraining.This approach is however inadequate for applications wherethe task specifications cannot be fully delineated through just a handful of exemplars, for example in semantic parsing where an LLM must translate a natural language utterance to an executable program in a domain-specific language (DSL).DSLs often incorporate domain-specific abstractions and semantics that are difficult to characterize via just a few demonstrations. And unlike general-purpose programming languages, DSLs are by definition specialized and thus unlikely to have been encountered often enough (or at all) during pretraining for the LLM to acquire its full syntax.\n\n[2] Low-Code Programming Models canvas Description. In programming by demonstration, the user demonstrates how to perform a task by hand via the mouse and keyboard, and the PBD system records a program that can perform the same task automatically. As shown in Figure 3, the demonstration happens on a stage, which may be a specific application like a spreadsheet, or a web browser visiting a variety of sites and apps, or even a general computer desktop or smart-phone screen. Ideally, the recorded program abstracts from perceptions to a symbolic representation, for instance, by mapping pixel coordinates to a user-interface widget, or several keystrokes to a text string. Besides the stage, most PBD systems have a player with buttons to record and replay, plus often additional buttons such as pause or step (reminiscent of interactive debuggers). The program is most useful if executing it does not yield exactly the same behavior as the initial demonstration, but rather, generalizes to different data. For example, a program for ordering a taxi to any new location is more general and more useful than a program for ordering a taxi to only a single hard-coded location. Generalizing typically requires identifying variables or parameters, and may even entail adding conditionals, loops, or function calls. Unfortunately, a single demonstration is an inherently ambiguous specification for such a more general program. Therefore, PBD systems often also provide a configuration pane that allows users to disambiguate the generalization either during or after demonstration. Some PBD systems also have a code canvas that renders the recorded program for the user\n\n[3] Grammar Prompting for Domain-Specific Language Generation with Large Language Models In-context learning with large language models (LLMs) has been shown to be an effective approach for few-shot learning. Under this approach, a pretrained LLM is conditioned on N𝑁N demonstration examples (𝒙(i),𝒚(i))i=1Nsuperscriptsubscriptsuperscript𝒙𝑖superscript𝒚𝑖𝑖1𝑁({\\bm{x}}^{(i)},{\\bm{y}}^{(i)})_{i=1}^{N} followed by a test example 𝒙𝒙{\\bm{x}}, and the output is given by decoding from the prompted LLM, i.e.,PLLM​(𝒚|𝒙,(𝒙(i),𝒚(i))i=1N)subscript𝑃LLMconditional𝒚𝒙superscriptsubscriptsuperscript𝒙𝑖superscript𝒚𝑖𝑖1𝑁P_{\\text{LLM}}({\\bm{y}}\\,|\\,{\\bm{x}},({\\bm{x}}^{(i)},{\\bm{y}}^{(i)})_{i=1}^{N}).The demonstration examples can be optionally preceded by natural language instructions to further improve performance or even enable zero-shot learning, 62.Recent work has additionally shown that interleaving natural language verbalizations of intermediate reasoning steps between each 𝒙(i)superscript𝒙𝑖{\\bm{x}}^{(i)} and 𝒚(i)superscript𝒚𝑖{\\bm{y}}^{(i)} can greatly improve few-shot performance on complex reasoning tasks, 86, 80, 73, 16.\n\n[4] Using Both Demonstrations and Language Instructions to Efficiently Learn Robotic Tasks and more cost-effective for the end-user to provide. We propose DeL-TaCo (Figure 1), a new task embedding scheme comprised of two component modalities that contextually complement each other: demonstrations of the target task and corresponding language descriptions. To our knowledge, this is the first work to demonstrate that specifying new tasks to robotic multi-task policies simultaneously with both demonstrations and language reduces teacher effort in task specification and improves generalization performance, two important characteristics of deployable household robots. With bimodal task embeddings, ambiguity is bidirectionally resolved: instructions disambiguate intent in demonstrations, and demonstrations help ground novel noun and verb tokens by conveying what to act on, and how. To learn several hundred tasks, we train a single imitation learning (IL) policy, conditioned on joint demonstration-language embeddings, to predict low-level continuous-space actions for a robot given image observations. Task encoders are trained jointly with the policy, making our model fully differentiable end-to-end. To summarize, our main contributions are as follows: (1) We present a broad distribution of highlyrandomized simulated robotic pick-and-place tasks where instructions or demonstrations alone are too ambiguous and inefficient at specifying novel tasks. (2) We propose a simple architecture, DeL-TaCo, for training and integrating demonstrations and language into joint task embeddings for fewshot novel task specification. This framework is flexible and learning algorithm-agnostic. (3) We show that DeL-TaCo significantly lowers teacher effort in novel task-specification and improves generalization performance over previous unimodal task-conditioning methods. MULTI-TASK LEARNING The most straightforward way to condition multi-task policies is through one-hot vectors Kalashnikov et al., 2021;Walke et al.,\n\n[5] Interactive Task Learning from GUI-Grounded Natural Language Instructions and Demonstrations We show SUGILITE, an intelligent task automation agent that can learn new tasks and relevant associated concepts interactively from the user’s natural language instructions and demonstrations, using the graphical user interfaces (GUIs) of third-party mobile apps. This system provides several interesting features: (1) it allows users to teach new task procedures and concepts through verbal instructions together with demonstration of the steps of a script using GUIs; (2) it supports users in clarifying their intents for demonstrated actions using GUI-grounded verbal instructions; (3) it infers parameters of tasks and their possible values in utterances using the hierarchical structures of the underlying app GUIs; and (4) it generalizes taught concepts to different contexts and task domains. We describe the architecture of the SUGILITE system, explain the design and implementation of its key features, and show a prototype in the form of a conversational assistant on Android.\n\n[6] PUMICE: A Multi-Modal Agent that Learns Concepts and Conditionals from Natural Language and Demonstrations PUMICE: A Multi-Modal Agent that Learns Concepts and Conditionals from Natural Language and Demonstrations Natural language programming is a promising approach to enable end users to instruct new tasks for intelligent agents. However, our formative study found that end users would often use unclear, ambiguous or vague concepts when naturally instructing tasks in natural language, especially when specifying conditionals. Existing systems have limited support for letting the user teach agents new concepts or explaining unclear concepts. In this paper, we describe a new multi-modal domain-independent approach that combines natural language programming and programming-by-demonstration to allow users to first naturally describe tasks and associated conditions at a high level, and then collaborate with the agent to recursively resolve any ambiguities or vagueness through conversations and demonstrations. Users can also define new procedures and concepts by demonstrating and referring to contents within GUIs of existing mobile apps. We demonstrate this approach in PUMICE, an end-user programmable agent that implements this approach. A lab study with 10 users showed its usability. INTRODUCTION The goal of end user development (EUD) is to empower users with little or no programming expertise to program. Among many EUD applications, a particularly useful one would be task automation, through which users program intelligent agents to perform tasks on their behalf. To support such EUD activities, a major challenge is to help non-programmers to specify conditional structures in programs. Many common tasks involve conditional structures, yet they are difficult for non-programmers to correctly specify using existing EUD techniques due to the great distance\n\n[7] Help, It Looks Confusing: GUI Task Automation Through Demonstration and Follow-up Questions Help, It Looks Confusing: GUI Task Automation Through Demonstration and Follow-up Questions Non-programming users should be able to create their own customized scripts to perform computer-based tasks for them, just by demonstrating to the machine how it's done. To that end, we develop a system prototype which learns-by-demonstration called HILC (Help, It Looks Confusing). Users train HILC to synthesize a task script by demonstrating the task, which produces the needed screenshots and their corresponding mouse-keyboard signals. After the demonstration, the user answers follow-up questions. We propose a user-in-the-loop framework that learns to generate scripts of actions performed on visible elements of graphical applications. While pure programming-by-demonstration is still unrealistic, we use quantitative and qualitative experiments to show that non-programming users are willing and effective at answering follow-up queries posed by our system. Our models of events and appearance are surprisingly simple, but are combined effectively to cope with varying amounts of supervision. The best available baseline, Sikuli Slides, struggled with the majority of the tests in our user study experiments. The prototype with our proposed approach successfully helped users accomplish simple linear tasks, complicated tasks (monitoring, looping, and mixed), and tasks that span across multiple executables. Even when both systems could ultimately perform a task, ours was trained and refined by the user in less time.\n\n[8] HILC HILC: Domain-Independent PbD System Via Computer Vision and Follow-Up Questions Creating automation scripts for tasks involving Graphical User Interface (GUI) interactions is hard. It is challenging because not all software applications allow access to a program’s internal state, nor do they all have accessibility APIs. Although much of the internal state is exposed to the user through the GUI, it is hard to programmatically operate the GUI’s widgets. To that end, we developed a system prototype that learns by demonstration, called HILC (Help, It Looks Confusing). Users, both programmers and non-programmers, train HILC to synthesize a task script by demonstrating the task. A demonstration produces the needed screenshots and their corresponding mouse-keyboard signals. After the demonstration, the user answers follow-up questions. We propose a user-in-the-loop framework that learns to generate scripts of actions performed on visible elements of graphical applications. Although pure programming by demonstration is still unrealistic due to a computer’s limited understanding of user intentions, we use quantitative and qualitative experiments to show that non-programming users are willing and effective at answering follow-up queries posed by our system, to help with confusing parts of the demonstrations. Our models of events and appearances are surprisingly simple but are combined effectively to cope with varying amounts of supervision. The best available baseline, Sikuli Slides, struggled to assist users in the majority of the tests in our user study experiments. The prototype with our proposed approach successfully helped users accomplish simple linear tasks, complicated tasks (monitoring, looping, and mixed), and tasks that span across multiple applications. Even\n\n[9] DARWIN Series: Domain Specific Large Language Models for Natural Science Instruction-tuned Large Language Models (LLMs), represented by InstructGPT, have captivated the academic community due to their proficiency in understanding directives and generating responses akin to human conversation. Compared to their smaller counterparts, LLMs demonstrate potent generalisation across various Natural Language Processing (NLP) tasks, illustrating their capacity to resolve unseen or intricate challenges in natural science. Even though fine-tuned GPT Models have shown an impressive ability to both domain-specific NLP tasks along with material and device prediction (MDP), 5, the core issue with these models lies in the fact that they remain inaccessible as open-source platforms, consequently compelling each user to engage in the laborious and financially burdensome task of individually fine-tuning the model on OpenAI’s servers. Additionally, they fail to offer a robust degree of confidence in their precision, an aspect that can be viewed as a significant shortfall. Such constraints have the potential to impose a brake on the momentum at which LLMs are propelling scientific discovery.\n\n[10] WebRobot: web robotic process automation using interactive programming-by-demonstration We highlight several salient features of our approach.First, it is automated: users only need to provide demonstrations, without needing to write programs.Second, it is interactive: whenever a synthesized program is not desired, the user can simply interrupt and continue demonstrating more actions, without having to edit programs.Finally, it could synthesize programs effectively from an expressive language, thanks to a systematic problem formulation and a new search algorithm.\n\n[11] SUGILITE: Creating Multimodal Smartphone Automation by Demonstration SUGILITE is a new programming-by-demonstration (PBD) system that enables users to create automation on smartphones. SUGILITE uses Android's accessibility API to support automating arbitrary tasks in any Android app (or even across multiple apps). When the user gives verbal commands that SUGILITE does not know how to execute, the user can demonstrate by directly manipulating the regular apps' user interface. By leveraging the verbal instructions, the demonstrated procedures, and the apps? UI hierarchy structures, SUGILITE can automatically generalize the script from the recorded actions, so SUGILITE learns how to perform tasks with different variations and parameters from a single demonstration. Extensive error handling and context checking support forking the script when new situations are encountered, and provide robustness if the apps change their user interface. Our lab study suggests that users with little or no programming knowledge can successfully automate smartphone tasks using SUGILITE.\n\n[12] Demonstration-based GUI Task Automation Through Interactive Training Demonstration-based GUI Task Automation Through Interactive Training Non-programming users should be able to train virtual robots to perform computer-based tasks for them. One would think that training in an all-digital noise-free environment should be easy. However, one-shot learning of a task is actually quite hard because so many mouse-clicks and key-presses are ambiguous. Also, recognizing individual actions in a task is not enough to reproduce the task, or to generalize it. For example, the intended reproduction of a copy-paste task could mean changing what is copied, or where it is pasted, or both. We propose a user-in-the-loop framework that supports vision for virtual-robots, letting a person easily \"bottle\" a GUI task such that it can be applied repeatedly in the future. While pure programming-by-demonstration is still unrealistic, we use quantitative and qualitative experiments to show that non-programming users are willing and effective at answering follow-up queries posed by our system. Our models of events and appearance are surprisingly simple, but are combined effectively to cope with varying amounts of supervision. The best available baseline, Sikuli Slide, struggled with the majority of the tests in our user study experiments. The prototype with our proposed approach successfully helped users accomplish simple linear tasks, complicated tasks (monitoring, looping, and mixed), and tasks that span across multiple executables. Even when both systems could ultimately perform a task, ours was trained and refined by the user in less time.\n\n"}
{"case_id": "de3b24bf31962c6107002d487f2bab5e", "answer_text": "\n\nAddressing ambiguity when an automation system encounters failures or ambiguities in user intents is a critical issue across various domains, including conversational AI, programming by example, and embodied agents [0][3][10]. Multiple methods have been proposed to tackle this challenge, with a focus on enhancing system transparency, user confidence, and overall usability [7][14]. \n\nClarifying questions aim to resolve ambiguities in user inputs, leading to more accurate outcomes and improving the overall interaction quality with large language models (LLMs) [0]. This approach has been shown to enhance the performance of LLMs across various tasks by ensuring that the generated responses closely align with the user's intent [4]. Additionally, the implementation of clarifications may reduce the need for human intervention in automated systems, addressing a significant portion of ambiguous cases autonomously [11].\nOne prominent method involves the use of clarifying questions to resolve ambiguities. This is notably explored in interactive AI systems where the model decides when to ask for clarification and formulates suitable questions to elicit the user’s intent accurately. For instance, in conversational AI systems, asking clarifying questions has shown to improve the accuracy of resolved user intents significantly [4][2]. This approach involves decomposing input into sub-questions and answers, achieving an iterative clarification process that helps refine the user's original query [2]. Furthermore, frameworks like Intent-Sim estimate the entropy over user intents by simulating multiple interactions, assisting in determining when to ask for clarification, thereby improving performance on various tasks [5].\n\nDialog Intent Clustering [13].\nAnother method used to discern user intents involves clustering dialog utterances to identify common patterns and intents within human-human conversations [13]. This data-driven approach relies on clustering techniques to categorize user queries into distinct intents, enabling the system to adapt and manage responses more effectively. Such clustering helps in discovering dialog intents that may not be predefined, thus improving the system's ability to handle unexpected user queries [13].\n\n\nInteractive Programming by Example (PBE) [3].\n\nIn programming by example, user interaction models assist in resolving ambiguities by providing actionable information. Techniques such as active learning, where the system asks directed example-based questions, help reduce errors and improve user confidence in the synthesized programs [3]. This method allows users to navigate between different program possibilities and clarifies ambiguities efficiently, directly engaging users in the process of program correction and verification [3].\n\nExplainable AI in Robotics [14]. \nIn the context of robot failure recovery, explainable AI (XAI) techniques are employed to generate explanations for errors and ambiguities encountered during task execution [14]. By providing clear and understandable explanations for failures, non-expert users can better comprehend the issue and determine the appropriate corrective actions. This approach enhances transparency and helps users manage and rectify errors that occur during robotic tasks, such as pick-and-place operations [14].\n\nAutomated Requirement Elicitation can leverage recent advances in conversational AI services to resolve ambiguities in user requirements efficiently. Traditionally, ambiguities in user requirements were resolved manually through interviews, brainstorming, and group discussion sessions among the elicitors and stakeholders [1]. Recent approaches aim to automate this process, using techniques such as intuitionistic fuzzy logic and statistical functions to address hesitation, terminological discrepancies, and tacit knowledge [1]. For instance, a user intention recognition method based on Knowledge Graph (KG) has been developed for fuzzy requirement inference, complemented by a requirement elicitation method based on Granular Computing for dialog policy generation [8]. These methods effectively reduce the number of conversation rounds needed to elicit accurate user requirements and can help novice elicitors by minimizing explicit and implicit issues [8]. Additionally, interactive AI assistants are being utilized to resolve ambiguity by asking clarifying questions, improving end-task performance while minimizing interaction costs [4]. These automated approaches help address the explicit issues arising from stakeholder availability and the implicit issues that require expert elicitation skills, making the process more efficient and reliable [1]. \n\n\nAmbiguities in user requirements can also be addressed through semi-automated approaches. For example, intuitionistic fuzzy logic is used to handle hesitation and identify discordance in user requirements, helping novice elicitors manage ambiguities without extensive manual intervention [1]. Tools developed for this purpose leverage heuristic knowledge gained from iterative interactions, improving the precision and accuracy of elicited requirements over time [1].\n\nEmbodied Agents and Active Querying [10]. \nEmbodied agents, like robots performing tasks in human environments, actively ask questions when faced with ambiguous instructions. This method is crucial for tasks that involve physical interactions, where specific details, such as the color or location of objects, need to be clarified for successful task completion [10]. Such agents use dialogue to interact with users, asking for more information when initial instructions are unclear.\n\nCollectively, these methods demonstrate a range of strategies to address ambiguities in user intents across different domains, from conversational AI to robotic systems. By incorporating interactive clarification [4], active learning [3], clustering [13], and explainable AI [14], these approaches enhance the system's ability to discern and manage user intents more accurately, ultimately improving usability and user satisfaction [4].\n\n\nReferences:\n[0] Clarify When Necessary: Resolving Ambiguity Through Interaction with LMs Each interaction begins with the user providing an initial input request, x𝑥x, to the LLM assistant.Some inputs may be ambiguous, resulting in many feasible output responses for the system to choose from, which we denote as the set Y={yi}1k𝑌superscriptsubscriptsubscript𝑦𝑖1𝑘Y=\\{y_{i}\\}_{1}^{k}.One of these outputs, y∗∈Ysubscript𝑦𝑌y_{*}\\in Y, represents the gold output corresponding to the user’s intent behind their ambiguous request.To determine the users intent, systems may ask the user a clarifying question, q𝑞q.The user then responds with the clarifying answer corresponding to their intent, a∗∈A={ai}1ksubscript𝑎𝐴superscriptsubscriptsubscript𝑎𝑖1𝑘a_{*}\\in A=\\{a_{i}\\}_{1}^{k}.For simplicity, we assume a bipartite matching between the sets of clarifying answers, A𝐴A, and feasible final responses, Y𝑌Y.\n\n[1] An Intuitionistic Fuzzy Based Approach to Resolve Detected Ambiguities in the User Requirements Document An Intuitionistic Fuzzy Based Approach to Resolve Detected Ambiguities in the User Requirements Document Ambiguous user requirements are usually considered problematic in software engineering. Therefore, many studies have been conducted on its avoidance and detection. However, the detected ambiguities were resolved manually using interviews, brainstorming, and group discussion sessions among the elicitors and stakeholders for whom the software was developed. If not addressed efficiently, it gives rise to the explicit issues of additional time and cost involved and the stakeholders’ availability to clarify them during multiple sessions. However, if appropriately addressed, it can reveal some implicit issues, such as tacit knowledge, hesitation, and terminological discrepancies. Identifying these implicit issues is not easy, as it requires expert elicitation skills that usually come with experience. In addition to the increasing demand for an automated approach to address these implicit issues, the recent COVID 19 pandemics has also amplified the demand to address the explicit issue of stakeholder availability. This paper proposes an implementable semi-automated approach to help elicitors address these demands. The proposed approach uses intuitionistic fuzzy logic to address hesitation and statistical functions to identify discordance and tacit knowledge. It also uses the heuristic knowledge gained in each iteration to improve itself. We implemented it in an online tool and conducted controlled experiments to evaluate our approach, and the results were compared. We achieved precision, recall, and F1 score of 0.769, 1, and 0.869, respectively, during our experiments. The results show that the proposed approach may minimize the explicit issues and help novice elicitors address the implicit\n\n[2] Clarify When Necessary: Resolving Ambiguity Through Interaction with LMs Introduced by Press et al. (2022), this prompting method is designed elicit chain-of-thought reasoning from LLMs for compositional reasoning tasks such as multi-hop QA. In their method, LLMs decompose inputs into multiple sub-questions and answers, which are composed to get the final answer. Self-Ask revolves around an intermediate step, where models decide whether to continue generating more questions or to complete their final response. We adapt this technique for our task, where the focus is not on decomposing the input but on querying for outside context. We adjust our few-shot prompt from our responsiveness task above and prompt assistants after each input query with the question “Is a follow-up question needed here?” (exact prompt in Appendix B). We then use the likelihood of generating “No” to score whether that clarification is needed. We also include this step in our sampled few-shot exemplars, creating a 50-50 split between unambiguous inputs, where the system responds “No”, and ambiguous inputs, where systems respond “Yes”.\n\n[3] User Interaction Models for Disambiguation in Programming by Example Programming by Examples (PBE) has the potential to revolutionize end-user programming by enabling end users, most of whom are non-programmers, to create small scripts for automating repetitive tasks. However, examples, though often easy to provide, are an ambiguous specification of the user's intent. Because of that, a key impedance in adoption of PBE systems is the lack of user confidence in the correctness of the program that was synthesized by the system. We present two novel user interaction models that communicate actionable information to the user to help resolve ambiguity in the examples. One of these models allows the user to effectively navigate between the huge set of programs that are consistent with the examples provided by the user. The other model uses active learning to ask directed example-based questions to the user on the test input data over which the user intends to run the synthesized program. Our user studies show that each of these models significantly reduces the number of errors in the performed task without any difference in completion time. Moreover, both models are perceived as useful, and the proactive active-learning based model has a slightly higher preference regarding the users' confidence in the result.\n\n[4] Clarify When Necessary: Resolving Ambiguity Through Interaction with LMs With the recent progress in large language model (LLM) development, interactive AI assistants (e.g., ChatGPT, Claude, LLaMA-2) have risen to prominence in our daily lives; yet, these systems often fail to interact with users to resolve ambiguity in their requests. We address these shortcomings by establishing a task-agnostic framework for modeling and resolving ambiguity with LLMs using clarifying questions. We find that imbuing LLMs with the ability to ask clarifying questions can improve performance on a variety of NLP tasks.Our framework breaks down the objective of resolving ambiguity into three sequential subtasks, which we depict in Figure 1. In our first task, systems must decide when to ask the user for clarification. We evaluate system for this task on their ability to maximize end-task performance while minimizing interaction cost. In our second task, systems must then decide what to ask the users. Here, systems should ask questions that expose the ambiguity in the user’s requests, eliciting a disambiguating response. Finally, after asking the user a clarifying question and receiving their response, systems perform the third and final task: producing the appropriate output given the ambiguous input and the user’s clarification.\n\n[5] Clarify When Necessary: Resolving Ambiguity Through Interaction with LMs Finally, we conclude our work by introducing Intent-Sim: a novel method for uncertainty estimation that we use to determine when to ask for clarification. Intent-Sim involves estimating the entropy over user intents by simulating multiple user-assistant interactions.Through our experiments, we demonstrate the Intent-Sim consistently outperforms other uncertainty estimation baselines at identifying predictions that are both incorrect and can be improved with clarification.We also find that these improvements are robust across different tasks and LLM systems.Intent-Sim performs the best at identifying when asking for clarification will improve expected performance in four out of six of the LLM-plus-task settings evaluated in this work.\n\n[6] Assessing Alternative Approaches for Conveying Automated Vehicle “Intentions” Assessing Alternative Approaches for Conveying Automated Vehicle “Intentions” Research suggests the general public has inherent distrust in highly automated vehicles (HAV), typically stemming from a lack of vehicle system transparency while in motion (e.g., the user not being informed how the car will react in the upcoming scene) and not having an effective way to control the vehicle in the event of a system failure. To better understand user trust and perceptions of comfort and safety while riding in an HAV, this study evaluated in-situ human-machine interface (HMI) systems (visual, auditory, and mixed-modal) to relay vehicle “intentions” (e.g., the vehicle's response to roadway stimuli) to the user. A considerable link was found between HMI modality and users’ reported levels of comfort, safety, and trust during experimentation. In addition, several key behavioral traits are identified as factors that contributed to users’ baseline comfort levels with the HAV. Moving forward, it will be necessary for HAV systems to keep users in-the-loop in an effort to increase system transparency and overall understanding. Vehicle feedback should consistently and accurately represent the driving landscape and clearly communicate vehicle system state to users.\n\n[7] User Intention Recognition and Requirement Elicitation Method for Conversational AI Services User Intention Recognition and Requirement Elicitation Method for Conversational AI Services In recent years, chat-bot has become a new type of intelligent terminal to guide users to consume services. However, it is criticized most that the services it provides are not what users expect or most expect. This defect mostly dues to two problems, one is that the incompleteness and uncertainty of user's requirement expression caused by the information asymmetry, the other is that the diversity of service resources leads to the difficulty of service selection. Conversational bot is a typical mesh device, so the guided multi-rounds Q$\\&$A is the most effective way to elicit user requirements. Obviously, complex Q$\\&$A with too many rounds is boring and always leads to bad user experience. Therefore, we aim to obtain user requirements as accurately as possible in as few rounds as possible. To achieve this, a user intention recognition method based on Knowledge Graph (KG) was developed for fuzzy requirement inference, and a requirement elicitation method based on Granular Computing was proposed for dialog policy generation. Experimental results show that these two methods can effectively reduce the number of conversation rounds, and can quickly and accurately identify the user intention. I. INTRODUCTION In recent years, Apple's Siri, Microsoft Cortana, and other service products have become more and more popular. Conversational AI bot, such as Intelligent Voice AI Assistant, has been trained to understand voice commands and complete tasks for users in various application scenarios. The convenience of conversational AI bot makes the cognitive service computing system an inevitable\n\n[8] DialFRED: Dialogue-Enabled Agents for Embodied Instruction Following possible failures. Humans, faced with inadequate information for a task, seek assistance from others. Similarly, embodied agents should be able to actively ask questions to humans, and utilize the verbal Manuscript received: February 24, 2022; Revised June 6, 2022; Accepted July 8, 2022. This paper was recommended for publication by Editor Gentiane Venture upon evaluation of the Associate Editor and Reviewers' comments. This work was supported by Amazon Alexa AI. 1 Fig. 1: Example dialogue between a robot and a human user during task completion. The robot raises questions to obtain additional information (e.g., when the target location is not clear) and to resolve ambiguities (e.g., when facing two knives on the table). response to overcome challenges in understanding intent and task execution. For example, to deal with ambiguity in human instruction, clarifications are often necessary. As shown in Figure 1, the instruction \"pick up the knife,\" is ambiguous when there are two knives in front of the robot -knowing the color of the intended knife helps the agent ground the instruction to its environment. We present DialFRED, an embodied instruction following benchmark allowing an agent to 1) actively ask questions to the human user, and 2) use the information in the response to better complete the task. DialFRED is built by augmenting ALFRED, an existing benchmark that pairs demonstrations of common household tasks with instructions. ALFRED language instructions are given as high level goals, e.g., Move a knife to the sink, and a sequence of step-by-step instructions (sub-goals), e.g., Move forward to the center\n\n[9] Interactive Question Clarification in Dialogue via Reinforcement Learning needs cannot be addressed. For example, the specific intent underlying an utterance such as \"How to apply?\" remains obscure, because there are too many products related to the action of \"applying\". In practice, one often needs to fall back to human agents to assist with such requests, increasing the workload and cost. The main purpose of deployed automated systems is to reduce the human workload in scenarios such as customer service hotlines. The lack of an ability to deal with ambiguous questions may directly lead to these sessions being transferred to human agents. In our real-world customer service system, this affects up to 30% of sessions. Hence, it is valuable to find an effective solution to clarify such ambiguous questions automatically, greatly reducing the number of cases requiring human assistance. Automated question clarification involves confirming a user's intent through interaction. Previous work has explored asking questions (Radlinski and Craswell, 2017;Quarteroni and Manandhar, 2009;Rao and Daumé, 2018;Rao and Daumé, 2019). Unfortunately, clarification by asking questions requires substantial customization for the specific dialogue setting. It is challenging to define appropriate questions to guide users towards providing more accurate information. Coarse questions may leave users confused, while overly specific ones may fail to account for the specific information a user wishes to convey. In our work, we thus instead investigate interactive clarification by providing the user with specific choices as options, such as intent options (Tang et al., 2011). Unlike previous work, we propose an end-to-end model that suggests labels to clarify ambiguous questions. An example of this sort\n\n[10] Dialog Intent Induction with Deep Multi-View Clustering the constrained set of dialog states and actions (Henderson et al., 2014b;Mrkšić et al., 2015). Existing work assumes the target tasks are given and excludes dialog intent discovery from the dialog system design pipeline. Because of this, most of the works focus on few simple dialog intents and fail to explore the realistic complexity of user intent space (Williams et al., 2013;Budzianowski et al., 2018). The assumption puts a great limitation on adapting goal-oriented dialog systems to important but complex domains like customer support and healthcare where having a complete view of user intents is impossible. For example, as shown in Fig. 1, it is non-trivial to predict user intents for troubleshooting a newly released product in advance. To address this problem, we propose to employ data-driven approaches to automatically discover user intents in dialogs from human-human conversations. Follow-up analysis can then be performed to identify the most valuable dialog intents and design dialog systems to automate the conversations accordingly. Similar to previous work on user question/query intent induction (Sadikov et al., 2010;Haponchyk et al., 2018), we can induce dialog intents by clustering user query utterances 3 in human-human conversations. The key is to learn discriminative query utterance representations in the user intent semantic space. Unsupervised learning of such representations is challenging due to the semantic shift across different domains (Nida, 2015). We propose to overcome this difficulty by leveraging the rest of a conversation in addition to the user query utterance as a weak supervision signal. Consider the two dialogs presented in Fig. 1 where\n\n[11] Explainable AI for Robot Failures: Generating Explanations that Improve User Assistance in Fault Recovery is crucial, such XAI methods are not suitable for the vast majority of end-users, who are non-experts,20,26. In this work, we seek to make robotic systems more transparent to their users by leveraging techniques from explainable AI, while also extending the capabilities of XAI systems toward greater transparency for non-expert users. Specifically, our work addresses fault recovery cases in which the robot's task execution is halted due to an error. We investigate whether providing explanations can not only help non-expert users understand the system's point of failure, but also help them determine an appropriate solution required to resume normal operation of the task.In some cases (e.g., complex hardware failures), the user might not have the knowledge to fix the point of failure regardless of the provided error explanation. In this work, we focus on failures that we expect to be within the user's understanding (e.g., object is too far away), and we address this question in the context of pick-and-place manipulation tasks in the home environment. Our work makes the following contributions: • Formalization of error explanations: Providing justifications for points of failures that occur unexpectedly amidst an agent's plan execution has not previously been studied within the XAIP community. We expand upon the existing set of explanations available in the XAI and XAIP community, introducing error explanations designed to explain failures that occur during the execution of a task. We validate our approach through two user studies and computational model analysis. In the first study, we examine what information an error explanation should contain\n\n"}
{"case_id": "798f4cf8735e057c9956945ced665a8c", "answer_text": "\n\nDesigning just-in-time interventions (JITIs) in a programming scenario involves several principles that ensure timely, relevant, and effective support to individuals. The key principles can be grouped under tailoring, timing, adaptation, and behavioral foundations [0]. \n\n\n\n**1. Tailoring Interventions:** The just-in-time adaptive interventions (JITAIs) framework describes a pragmatic protocol to guide intervention design, especially in the context of mHealth. The JITAIs framework requires researchers to define a distal outcome (e.g., increasing physical activity), proximal outcomes (e.g., daily walking goals), decision points (e.g., every two hours), intervention options (e.g., notification of the accumulated steps and \"provide nothing\"), tailoring variables (e.g., continuous sitting time), and decision rules (e.g., if continuous sitting time > 40 minutes, then provide a notification of the accumulated steps, otherwise, provide nothing) [1].\n\n\nTailoring is essential for ensuring that the intervention is relevant to the specific needs of the individual at the time of delivery. Tailoring criteria can include the individual's current performance, history of issues faced, and progression through the course material. For instance, interventions can be personalized based on continuous tracking of an individual's coding performance, such as execution attempts and time spent on tasks, to identify struggle points and customize the help provided ([6]). Similarly, tailoring can involve offering bonus exercises that target specific weaknesses identified through previous interactions ([6]).\n\n**Timing of Interventions:**\nIntervention timing is crucial in the context of health behavior interventions, particularly with the use of mobile technology to deliver just-in-time adaptive interventions (JITAIs), which utilize ongoing (dynamic) information about the individual to modify the type, amount, and timing of support. JITAIs aim to provide the right type of support at the right time by considering factors like individual vulnerability and receptivity [0]. They also involve defining decision points—specific times during an individual's daily schedule that are feasible for delivering interventions—thereby enhancing the real-time aspect of the interventions [11].\nJust-in-time interventions need to be precisely timed to maximize their effectiveness. This requires identifying \"decision points\" during the programming task when interventions are most likely to be beneficial. For example, detecting when a student is stuck on a problem for a period longer than the average working time, or noticing an increase in the number of code executions without progress, can signal a decision point for intervention ([6][7]). The goal is to provide support before the student becomes overly frustrated or demotivated, thus preventing dropouts ([7]).\n\nIdentifying Decision Points is a critical component within the just-in-time adaptive interventions (JITAIs) framework [1]. This framework requires researchers to define key factors such as decision points, tailoring variables, and intervention options to effectively deliver personalized, dynamic interventions at optimal times [1]. Decision points refer to all possible times during an individual’s daily schedule at which it may be feasible to deliver an intervention option, thus enhancing the real-time nature and effectiveness of the intervention [11]. By selecting appropriate decision points, the interventions can be more precisely tailored to an individual's needs, thereby improving the overall impact and success of the intervention [1]. This requires an understanding of the temporal resolution at which an intervention should be offered and how quickly the near-term impact of the intervention can be observed [11]. \nDecision points are critical moments when an intervention can have the most impact. These can be pinpointed by recognizing patterns in the student's activity, such as extended periods of inactivity or repeated failed code executions ([11]).\n\n**3. Adaptation to Context:** Adaptation is defined as the use of ongoing (dynamic) information about the individual to modify the type, amount, and timing of support [0]. To provide support just-in-time, the adaptation requires monitoring the individual to decide (a) whether the individual is in a state that requires support; (b) what type (or amount) of support is needed given the individual's state; and (c) whether providing this support has the potential to disrupt the desired process [0]. \nAdapting interventions dynamically to the ongoing context is another crucial principle. This involves using real-time data to modify the type, amount, and timing of support in response to the individual's current state ([0]). For instance, interventions can be triggered by specific events such as repeated failed attempts at a programming task, which suggests that the student needs immediate assistance. Leveraging ubiquitous computing advancements allows real-time tracking and reactive support, helping address anomalies that indicate the need for intervention ([3]).\n\n**3.1 Real-Time Data Utilization:**\nReal-time data utilization is crucial for adaptive interventions in mobile health, where ongoing dynamic information about the individual is used to modify the type, amount, and timing of support [0][1]. This approach, known as Just-in-Time Adaptive Interventions (JITAIs), aims to provide the right type of support at the right time by monitoring the individual's state to decide on the necessary support and minimizing interruptions that could disrupt the desired process [1][4]. These interventions leverage mobile technology to deliver personalized and context-sensitive support, enhancing their relevance and effectiveness in real-time applications [0][3]. Real-time, data-driven adjustments ensure that interventions are timely and appropriate, thus improving the likelihood of positive health outcomes [3][4]. \n\n\nReal-time data enables the dynamic adjustment of interventions. This ensures that the support provided is timely and appropriate for the current context of the learner ([0]). For example, monitoring the student's engagement levels in real-time can help in deciding when to offer encouragement or additional resources.\n\n**4. Behavioral Foundations:**[0]. [1], [5], [13]\nIntegrating behavioral theories and strategies into JITI design can improve the effectiveness of interventions. These interventions should not only address the immediate task but also foster long-term positive behavior changes ([2]). Incorporating elements such as positive feedback, goal-setting, and self-regulation techniques can motivate students and reinforce desired behaviors. For example, providing encouraging messages when a student makes progress can embed aspects of operant conditioning, promoting sustained engagement ([13]).\n\n**4.1 Positive Reinforcement:**[13]\nProviding positive feedback when students achieve milestones can help in maintaining their motivation and engagement. This could include congratulatory messages or virtual rewards to reinforce progress ([13]).\n\n**5. Micro-Randomized Trials for Optimization:** Micro-randomized trials leverage mobile devices to sequentially randomize participants to types or levels of an intervention to determine the effectiveness of an intervention and time-varying moderators of those effects [4]. Micro-randomized trials are an efficient study design that can be used to determine which intervention components to include in JITAI and to optimize their decision rules while maintaining the strength of causal inference associated with traditional randomized controlled trials [4].\nMicro-randomized trial designs can be employed to optimize JITIs by systematically testing different intervention components and their timing ([4]). This approach allows researchers to determine the most effective strategies for different contexts and individual needs by sequentially randomizing participants to various intervention types or levels. The outcomes of these trials can inform the design and refinement of decision rules governing when and how interventions are delivered.\n\nOverall, the effectiveness of just-in-time interventions in programming scenarios depends on the careful consideration of individual needs, real-time adaptation, precise timing, and the integration of behavioral insights [6]. These principles, when implemented cohesively, can provide timely and relevant support, enhancing learning outcomes and maintaining student engagement [7].\n\n\nReferences:\n[0] Just-in-Time Adaptive Interventions (JITAIs) in Mobile Health: Key Components and Design Principles for Ongoing Health Behavior Support context. The flip-side of providing the right type of support at the right time is providing nothing when the time is wrong and never providing the wrong type of support. This operationalizes the notion of eliminating waste, namely any activity/action that absorbs resources (e.g., time, effort) but adds no value to, or even disrupts the desired process. Adaptation operationalizes how the provision of just-intime support will be accomplished,30. Adaptation is defined as the use of ongoing (dynamic) information about the individual to modify the type, amount, and timing of support. To provide support just-in-time, the adaptation requires monitoring the individual to decide (a) whether the individual is in a state that requires support; (b) what type (or amount) of support is needed given the individual's state; and (c) whether providing this support has the potential to disrupt the desired process. In the context of health behavior interventions, the use of mobile technology to deliver just-in-time support is rooted in theoretical and practical perspectives suggesting that states of vulnerability to adverse health events, as well states of Just-in-time support Attempts to provide the right type of support, at the right time, while eliminating support provision that is interruptive or otherwise not beneficial Individualization The use of information from the individual to select when and how to intervene. Adaptation A dynamic form of individualization, whereby time-varying (dynamic) information from the person is used repeatedly to select intervention options over time. Just-in-time adaptive intervention (JITAI) An intervention design aiming to provide just-in-time support, by\n\n[1] Towards a Holistic Approach to Designing Theory-based Mobile Health Interventions model defines intervention aims as clinical aims and usage aims. In fact, we need to divide these aims into sub-aims or proximal outcomes, which is missing in the BIT model. Secondly, the conceptual \"how\" part of the BIT model refers to a list of behavioral intervention strategies, which are drawn only from BCT taxonomy, and we think including the PSD model can enrich this. Just-in-Time Adaptive Interventions Framework In order to provide high-quality behavior change interventions, Spruijt-Metz argued that behavior change is a dynamic process varied over time, so interventions have to adapt to the users' needs in real-time,63. The just-in-time adaptive interventions (JITAIs) framework describes a pragmatic protocol to guide intervention design, especially in the context of mHealth. The JITAIs framework requires researchers to define a distal outcome (e.g., increasing physical activity), proximal outcomes (e.g., daily walking goals), decision points (e.g., every two hours), intervention options (e.g., notification of the accumulated steps and \"provide nothing\"), tailoring variables (e.g., continuous sitting time), and decision rules (e.g., if continuous sitting time > 40 minutes, then provide a notification of the accumulated steps, otherwise, provide nothing). Tailoring variables are very important because they include the factors that determine vulnerability/opportunity and receptivity. For instance, of interventions to prevent binge drinking, psychological distress is the vulnerability/opportunity variable because it is the main predictor of drinking behavior, while whether the participant is driving or not is the receptivity variable because they cannot or should not receive any intervention when driving. Another property of the tailoring variables is timescale (e.g.,\n\n[2] Towards a Holistic Approach to Designing Theory-based Mobile Health Interventions designing mHealth interventions. Based on our holistic approach, behavioral theories should be embedded into the user-centered design process when developing mobile health interventions. Evidence shows that there is no \"one-size-fits-all\" behavioral theory. Therefore, it is suggested to utilize several theories to cover all potential constructs affecting a specific behavior. Based on the Behavior Intervention Technology Model and the Just-In-Time Adaptive Interventions framework, we developed a protocol combined with the Behavior Change taxonomy and the Persuasive System Design principles. Finally, we abstracted the common steps of the user-centered design process from three existing approaches. Although the proposed approach targets mobile health interventions development and the example given is unique to mHealth intervention development, the approach can generally be applied to other health intervention with some restrictions to real-time adaptive intervention techniques. There are still several open questions when using this holistic approach. We have alluded to several, including: (1) How to match constructs from behavioral theories to behavior change techniques? (2) How to model and detect human behavior to be aware of the context of interventions? (3) How to define the measurement used to evaluate mHealth interventions? These questions will be explored as part of our future work.\n\n[3] CALMA, an algorithm framework for mobile just in time interventions Advancements in ubiquitous computing are rapidly changing preventative health care. These quick changes allow not only to track in real time the heath of an individual, but also to react to any anomalies that may indicate the need of help. This new health care paradigm (i.e., Just-in-time interventions) allows to support treatments and deliver help anytime and anywhere. In this work, we take a first step in modeling a flexible mechanism for choosing the most effective intervention from a set of available ones. We model this optimization problem as a Markov Decision Process (MDP) and we solve it by using Value Iteration in an online fashion. We show that the policy found by our algorithms for selecting interventions, at least doubles the efficiency achieved by a random policy, in minimizing the average number of interventions to relieve a patient from a condition such as stress.\n\n[4] Micro-Randomized Trial Design for Evaluating Just-In-Time-Adaptive-Interventions Through Mobile Health Technologies for Cardiovascular Disease. Smartphone and wearable device use is rising broadly and can be leveraged for chronic disease management. Just-In-Time Adaptive Interventions (JITAIs) promise to deliver personalized, dynamic interventions directly to patients through use of push notifications from mobile devices. While JITAIs are a powerful tool for shaping health behavior, their application to cardiovascular disease management has been limited as they can be challenging to design. Herein we provide a general overview and conceptual framework for micro-randomized trials, a novel experimental study design that can be used to optimize JITAIs. Micro-randomized trials leverage mobile devices to sequentially randomize participants to types or levels of an intervention to determine the effectiveness of an intervention and time-varying moderators of those effects. Micro-randomized trials are an efficient study design that can be used to determine which intervention components to include in JITAI and to optimize their decision rules while maintaining the strength of causal inference associated with traditional randomized controlled trials.\n\n[5] Assessment of the Efficacy of a Mobile Phone–Delivered Just-in-Time Planning Intervention to Reduce Alcohol Use in Adolescents: Randomized Controlled Crossover Trial implementation intentions, might also be tested within comprehensive intervention programs. Testing the effects of these microinterventions within traditionally delivered comprehensive intervention programs allows balancing internal and external validities in a way that facilitates translation and testing of the basic theory in multicomponent intervention programs. According to this approach, the 2a-ii) Scientific background, rationale: What is known about the (type of) system Copy and paste relevant sections from the manuscript (include quotes in quotation marks \"like this\" to indicate direct quotes from your manuscript), or elaborate on this item by providing additional information not in the ms, or briefly explain why the item is not applicable/relevant for your study   \"Unlike traditional methods of delivering planning interventions, mobile phones can deliver these microinterventions \"just in time\" for when a person is most vulnerable and receptive. These just-in-time interventions can be activated by users themselves (user triggered) through prespecified rules (server triggered, like in this study) or sensors that dynamically monitor a user's context (context triggered). They are intended to support an individual at the time when most needed. To date, published studies on the effects of just-in-time interventions are limited to physical activity and sedentary behavior, with mixed Does your paper address CONSORT subitem 2b? * Copy and paste relevant sections from the manuscript (include quotes in quotation marks \"like this\" to indicate direct quotes from your manuscript), or elaborate on this item by providing additional information not in the ms, or briefly explain why the item is not applicable/relevant for your study\n\n[6] Effects of automated interventions in programming assignments: evidence from a field experiment A typical problem in MOOCs is the missing opportunity for course conductors to individually support students in overcoming their problems and misconceptions. This paper presents the results of automatically intervening on struggling students during programming exercises and offering peer feedback and tailored bonus exercises. To improve learning success, we do not want to abolish instructionally desired trial and error but reduce extensive struggle and demotivation. Therefore, we developed adaptive automatic just-in-time interventions to encourage students to ask for help if they require considerably more than average working time to solve an exercise. Additionally, we offered students bonus exercises tailored for their individual weaknesses. The approach was evaluated within a live course with over 5,000 active students via a survey and metrics gathered alongside. Results show that we can increase the call outs for help by up to 66% and lower the dwelling time until issuing action. Learnings from the experiments can further be used to pinpoint course material to be improved and tailor content to be audience specific.\n\n[7] Effects of automated interventions in programming assignments: evidence from a field experiment learning success, improve satisfaction and lower overstrain induced dropout of students of online programming courses. The first type of interventions, just-in-time interventions in problem solving, motivates students to ask for help and feedback when they face an extended struggling period with an exercise. The second type of interventions, bonus exercises, offers each student additional training exercises which are suited to tackle the individual weaknesses of the student. Just-In-Time Interventions To prevent dropouts and increase student satisfaction, we want to help struggling students while they are working on their exercises. As shown in, students often struggled before they dropped out. From forum comments and other feedback we know that spending too much time is a cause of student dissatisfaction with a course. Therefore, helping students in the moment when they are stuck is beneficial, allowing to solve their problems before they lose interest and drop out. We issue just-in-time interventions to students when we think that they are struggling with an exercise. The purpose of these interventions is to interrupt the student and motivate them to rethink their approach, review video lectures, ask for help or do other exercises first. Identifying Struggling Students To help struggling students we first need a method for identifying them. The most obvious metrics for that are number of program executions and working times. As the working times correlate strongly (Pearson coefficient of 0.9) with the amount of code executions and probably better reflect frustration levels, we decided on using working times. We defined the 75th percentile of students to be\n\n[8] Designing m-Health interventions for precision mental health support the target population, such as behaviours participants find motivating and are able to action frequently in their setting. Through the course of everyday life, participants might be able to perform the behaviour specified by the intervention across a range of time points. A number of these time points can be selected for intervention delivery using principles outlined in the next section. Lastly, interventions must be suitable for delivery in an m-Health format 24 . As a push notification in the app can be programmed to lapse after a set time, intervention options sent through such a notification can be designed to be actionable within a very short amount of time; intervention options that are sent through text message should not be designed with a short expiry time as there is no way to control how long they are visible on the phone once sent. How to choose intervention delivery decision points? Decision points are all the possible times during an individual's daily schedule that it may be feasible to deliver an intervention option. The more frequent the decision points are in time, the more real time the interventions appears to individuals. However, to enhance the ability to detect effects, it is often important to narrow the time points at which intervention decisions will be made. Choosing appropriate decision points requires an understanding of the temporal resolution at which an intervention should be offered, and how quickly the near-term impact of the intervention option can be observed. These are domain-specific factors. It might be plausible to use\n\n[9] Mobile health-based physical activity intervention for individuals with spinal cord injury in the community: A pilot study month prior to updating them to the individuals' Android smartphone for the third month of testing. By adapting the goals in real-time based on the person's actual behavior, the model aims to keep the individual feeling positive and motivated. The just-in-time intervention has the capability to interact with the person more than several times a day (57,78) and will utilize PAMS to detect various wheelchair based PAs being performed by the user. The just-in-time recommendations will be simple, such as inquiring whether the participant is willing to continue performing the current PA for a slightly longer duration, thus making the participant more inclined to continue the PA. The model will also assess the context, level and the type of PA the person was compliant in performing when a just-in-time recommendation was provided. The list of contexts includes items such as home environment, accessible gymnasium, community centers, and trails. A key focus of this model will be to keep the suggestion actionable in the moment; tailored; appropriate for the context and hope that the model will change the habit of the people over time. In some ways, the model will incorporate aspects of operant conditioning, where providing positive feedback to the participants through encouraging and congratulating messages may lead to an increase in PA and/or sustain PA levels. Letts et al. have indicated that peers and health service providers were preferred messengers for delivering PA information to individuals with SCI (79), which leads us to a clinician delivering PA information and providing PA recommendations. Clinician suggestion will\n\n"}
{"case_id": "e03c49cccc971cf3ae67556554b4666b", "answer_text": "\n\nQuestion: Are there papers that use different formats of Q&A with the user to clarify intent and compose more complicated prompts to LLM? [1][4][6][7] \n\nAnswer:\n\nYes, multiple papers have explored various formats of Q&A interactions to clarify user intent and develop more complex prompts for large language models (LLMs) [7]. These methods often involve iterative steps of question generation, feedback collection, and prompt refinement to improve the performance of LLMs [6]. The approaches can be categorized into several main strategies:\n\nIterative Question Generation and Answering. [7]\n\n1. **Ask Me Anything (AMA) Strategy** [10]:\n\n    * The AMA strategy involves a two-step prompting pipeline where the first step generates questions based on the input, and the second step prompts the LLM to answer these generated questions. This approach aims to emphasize different aspects of the input and gather complementary information, leading to improved performance through the aggregation of multiple prompt-outputs [7]. This method is particularly effective for open-ended question-answering tasks.\n\n Multi-Step Prompting Strategies [7]. \n\n**Chain of Thought (CoT) Prompts** [4]:\n    * The concept of Zero-shot-CoT involves a two-step process. First, a \"reasoning extraction\" prompt is used to extract a reasoning path from the language model. Then, an \"answer extraction\" prompt is employed to generate the final answer based on the reasoning text. This method requires less engineering than other few-shot strategies but still effectively clarifies user intent by breaking down the problem-solving process into manageable steps [4].\n\n### Interactive Feedback Mechanisms [6].\n\n\n3. **Interactive Code Generation via Test-Driven User-Intent Formalization** [6]\n    * This framework illustrates the use of an interactive approach to refine and formalize user intent through generated tests. It queries users to clarify their requirements iteratively, significantly enhancing the trust and accuracy of LLM-generated code [6]. This framework benefits from lightweight user feedback to improve the code generation outputs.\n\nHierarchical and Scaled Prompt Collections[0]\n\nPrompt Collections at Scale (from AMA)[7]:\n    * AMA's prompt collection strategy recursively applies the LLM itself to transform task inputs into effective QA formats. This recursive application, combined with weak supervision for aggregating noisy predictions, helps in creating a robust prompting mechanism that can adapt to various tasks with improved accuracy [10].\n\n5. **Hierarchical Prompts for Lifelong Learning** [0]:\n\n    * The Diana model employs hierarchically organized prompts, such as General Prompts, Format Prompts, Task Prompts, and Meta Prompts, to capture QA knowledge from different granularities. This hierarchical structure allows the model to generalize better to unseen tasks while maintaining high performance on lifelong learning tasks [0].\n\n Probabilistic Reasoning and Summarization [5]. \n\n6. **ThinkSum Framework** [5]:\n    * The ThinkSum framework uses probabilistic reasoning over sets by associating certain text spans with alternative explanations, allowing for multiple texts to be evaluated by the LLM itself. This method leverages elementary string manipulations and probabilistic summarization techniques to better align generated outputs with user intent [5].\n\nOverall, these varied approaches underline the importance of dynamic and hierarchical prompt designs [0], interactive feedback mechanisms [6], and multi-step prompting strategies to effectively clarify user intent and refine LLM outputs [7][11]. By iterating through Q&A cycles and leveraging user feedback [7][11], these methods enhance the capabilities and reliability of LLMs in performing intricate tasks [6][11][10].\n\n\nReferences:\n[0] Lifelong Learning for Question Answering with Hierarchical Prompts al. 2022). However, these models usually yield sub-optimal LL performance because there are no dedicated components for each task to capture task-specific knowledge (Wang et al. 2022a). In this study, we combine the advantages of the above two categories and propose Diana: a dynamic architecturebased lifelong QA model. We follow previous approaches to convert QA tasks into a unified LM format and propose to learn these tasks using a prompt-enhanced PLM. Four types of hierarchically organized prompts are maintained in Diana (i.e., General Prompt, Format Prompt, Task Prompt, and Meta Prompt) to capture QA knowledge from different granularities. Specifically, the general prompt is used for all QA tasks, and the format prompts are shared between tasks in the same QA format. Moreover, a task prompt is assigned for each incoming QA task, and a pool of meta prompts are maintained and dynamically combined when handling each sample. In this way, Diana can better generalize to unseen tasks while achieving high LL performances since its components are organized at both task-level and instance-level. Further, we allocate separate prompts for unseen tasks and learn a key vector for each task prompt and meta prompt to better share knowledge between different tasks so that samples from unseen tasks can be explicitly modeled. We perform extensive experiments on 11 benchmark QA tasks across three different formats and further test the generalization performance of our model on three unseen tasks. Results indicate that Diana outperforms state-of-theart (SOTA) baselines on all benchmarks, especially when generalizing to unseen tasks. Our main contributions are:\n\n[1] AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts prompt shown in Table 1 and Figure 2B. Operations need to invoke the desired functionalities, through prompt design. To date, the most common patterns for prompting are either zero-shot or few-shot prompts, depending on how many demonstrating examples are provided in the prompt. Zero-shot prompts directly describe what ought to happen in a task: e.g., we can enact Ideation with a task description prompt \"Given Alex's presentation problem, the following is a list of suggestions. \" In contrast, few-shot prompts show the LLM what pattern to follow by feeding it examples of the desired input and output data: \"Problem: mumbles when presenting, Suggestion: enunciate each syllable, Problem: too much text, Suggestion:\" (full prompt in Figure 2B). Given these prompts, the LLM might produce a reasonable suggestion, e.g.,\"use more graphics on the slides.\" Zero-shot prompts can also be easily transformed into few-shot prompts, by appending examples to the initial zero-shot task description. In either case, prompts commonly include meaningful names as prefixes (\"Problem:\" and \"Suggestion:\") to demarcate structure, which helps re-emphasize the desired intent. Following this convention, we build our prompts to include task descriptions followed by prefixes. Aside from the prompt itself, we also associate with each LLM operation a default temperature setting: a model parameter that influences the randomness of the LLM generation. For instance, creative operations like Ideation benefit from a higher temperature ( =0.7) than more factual or deterministic tasks like Classification Operations should be able to take custom data layers as inputs and outputs. Though our walkthrough example takes in\n\n[2] Large Language Models are Zero-Shot Reasoners he throw? A: Let's think step by step. 375. LLM LLM 【1st prompt】 Reasoning Extraction 【2nd prompt】 Answer Extraction Figure 2: Full pipeline of Zero-shot-CoT as described in § 3: we first use the first \"reasoning\" prompt to extract a full reasoning path from a language model, and then use the second \"answer\" prompt to extract the answer in the correct format from the reasoning text. in Figure 1). In summary, Few-shot-CoT requires careful human engineering of a few prompt examples with specific answer formats per task, while Zero-shot-CoT requires less engineering but requires prompting LLMs twice. 1st prompt: reasoning extraction In this step we first modify the input question x into a prompt x using a simple template \"Q: is an input slot for x and [T is an slot for hand-crafted trigger sentence t that would extract chain of though to answer the question x. For example, if we use \"Let's think step by step\" as a trigger sentence, the prompt x would be \"Q: [X. A: Let's think step by step.\". See Table 4 for more trigger examples. Prompted text x is then fed into a language model and generate subsequent sentence z. We can use any decoding strategy, but we used greedy decoding throughout the paper for the simplicity. 2nd prompt: answer extraction In the second step, we use generated sentence z along with prompted sentence x to extract the final answer from the language model. To be concrete, we simply concatenate three elements as with \"[X  [Z [A\": [X \n\n[3] ThinkSum: Probabilistic reasoning over sets using large language models certain text spans with sets of other strings, which may serve as alternatives or elaborations/explanations and can be either defined by the user or inferred by the LLM itself. These associations then provide multiple texts to be evaluated, again by the LLM itself. The collection of resulting probabilities provides an opportunity to summarize the text using standard probabilistic inference techniques, which usually include a summation. HOW TO THINK Here we list examples of the \"fast thinking\" that precedes the summarization stage. Elementary string manipulations. Standard ways to turn a question into a prompt that can be given to a LLM for generation or scoring involve choices (e.g., of the prompt format) that can be seen as being made by a controlling agent. The standard approach to multiple-choice questions is to write them as Cloze tasks. However, there are nontrivial operations used in inference procedures that sometimes work better, such as: • Order inversion: Exchanging the order of the question and answers, as in Min et al. (2022). • Premise erasure: Deleting a part of the question. Removing a premise with which the answer is expected to have high mutual information is a step in inference procedures that aim to correct for bias towards answers with high unconditional likelihood (Zhao et al., 2021;Holtzman et al., 2021;Malkin et al., 2022). Substitution and normalization. An example is shown in Fig. 1. Elements from a set may be substituted in place of 'slot' words in a prompt, such as 'cat' substituted for 'binne' in the prompt \"A binne bam is\n\n[4] Interactive Code Generation via Test-Driven User-Intent Formalization code that they don't trust, or reject correct code that are too difficult to understand and therefore trust. Finally, LLMs often present a large list of suggestions (beyond the most likely one) that is hard to navigate for a user other than linearly scanning each code suggestion and rejecting the incorrect ones. In this paper, we advocate leveraging lightweight user-feedback to improve trust in LLM-generated code. Specifically, we advocate the problem of test-driven user-intent formalization (or perhaps test-driven user-intent discovery) to create an interactive framework to (a) refine and formalize the user intent through generated tests, and (b) generating code that is consistent with such tests. Let us demonstrate a simple instantiation of the framework using the example of sorting a list of integers in Python programming language. Consider the scenario when a user Alice prompts a LLM to generate code that satisfies their query expressed in natural language with some Python syntax for the method signature: Instead of displaying a list of suggestions (that usually assumes the user expects the list to be sorted in an ascending order), our framework TICODER would query the user with a question: Did you mean sort (, 2, 4) ==,3,4 ? Let us assume that the user answers 'no'. The workflow would likely query the user again with the following question: Did you mean sort (, 2, 4 1. First, it can leverage user feedback from a test to prune a large fraction of likely suggestions that do not satisfy user intent, including suggestions that contain syntax or\n\n[5] Ask Me Anything: A simple strategy for prompting language models QA prompt structures, we proceed in AMA with a two-step prompting pipeline: (1) generating questions based on the input and (2) prompting the LLM to answer the generated questions. These prompts are effective, and to further improve performance we next turn to generating and aggregating over multiple prompt-outputs for each input. For intuition, different questions (with our running example: \"Who went to the park?\", \"Did John go the park?\", \"Where did John go?\") emphasize different aspects of the input and can provide complementary information towards reasoning about the answer. Manually generating multiple prompts per input is challenging, and so we study how to do this at scale in the following section. Creating Prompt Collections at Scale Our goal is to produce a collection of prompts, P, that can be applied to tasks at scale. To produce prompts in the effective open-ended question-answering format, our insight is to recursively apply the LLM itself using a chain of functional prompts, referred to as a prompt()-chain. We describe these prompts as functional because they apply a task-agnostic operation to all inputs in the tasks, without any example-level customization. We describe the two functional prompts used in AMA below. We use Figure 1 as a running example to explain each type. (b) answer(): q → a applies the question generated by (a) to the context of x to produce intermediate answers a (such as \"No\" or \"theater\"). The answer() prompts contain demonstrations of how to answer a question (optionally) given some input context. To create P for aggregation, AMA constructs\n\n[6] Ask Me Anything: A simple strategy for prompting language models Ask Me Anything: A simple strategy for prompting language models Large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt that demonstrates how to perform the task and no additional training. Prompting is a brittle process wherein small modifications to the prompt can cause large variations in the model predictions, and therefore significant effort is dedicated towards designing a painstakingly\"perfect prompt\"for a task. To mitigate the high degree of effort involved in prompt-design, we instead ask whether producing multiple effective, yet imperfect, prompts and aggregating them can lead to a high quality prompting strategy. Our observations motivate our proposed prompting method, ASK ME ANYTHING (AMA). We first develop an understanding of the effective prompt formats, finding that question-answering (QA) prompts, which encourage open-ended generation (\"Who went to the park?\") tend to outperform those that restrict the model outputs (\"John went to the park. Output True or False.\"). Our approach recursively uses the LLM itself to transform task inputs to the effective QA format. We apply the collected prompts to obtain several noisy votes for the input's true label. We find that the prompts can have very different accuracies and complex dependencies and thus propose to use weak supervision, a procedure for combining the noisy predictions, to produce the final predictions for the inputs. We evaluate AMA across open-source model families (e.g., EleutherAI, BLOOM, OPT, and T0) and model sizes (125M-175B parameters), demonstrating an average performance lift of 10.2% over the few-shot baseline. This simple strategy enables the open-source GPT-J-6B model\n\n[7] AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts designed to continue its input with plausible output (e.g., given a prompt \"I went to the\", it might auto-complete with \"coffee shop\"). However, when pretrained on billions of samples from the Internet, recent transformerbased LLMs like GPT-3 and Jurassic-1 encode enough information to support additional in-context learning: they can be easily customized at run time (without any re-training needed) to handle new tasks beyond text continuation. To invoke the desired functionality, users write natural language instructions, or prompts,43,45, that are appropriate for the task. The most common patterns for prompting are either zero-shot or few-shot prompts. Zero-shot prompts directly describe what ought to happen in a task. For example, we can enact English-to-French translation with a prompt such as \"Translate the sentence \"Do you like the weather?\" to French:\". In contrast, few-shot prompts show the LLM what pattern to follow by feeding it examples of desired inputs and outputs: \"[English Hello! [French Bonjour! [English Do you like the weather? [French\". Given either of these prompts, the LLM may respond with the French translation \"Vous aimez le temps?\". Importantly, such task customization happens on the fly and, as a result, a single LLM can be flexibly adapted to a wide variety of use cases like code generation, question answering, creative writing, etc.,60. This flexible adaptation, together with the text-in, text-out structure, creates an intuitive natural language interface between humans and the model. Despite their versatility, LLMs require careful prompt design. Various studies therefore focus on prompt engineering,43,45. As manual prompting\n\n"}
{"case_id": "a8ba07610b6d77890e50144bfd4d4168", "answer_text": "\n\nIn the context of sense-making, information organization, and AI-assisted writing tools, various user interface (UI) designs are employed to enhance user experience and efficiency [0]. These designs take into account the complexity and iterative nature of the tasks, aiming to balance flexibility, ease of use, and functionality [3]. \n\nCard-Based Interfaces [2].\n\nOne prevalent design is the card-based interface, which allows users to visually organize pieces of information into manageable units called \"cards.\" This approach is used in tools like Fuse, which facilitates the collection and organization of web content directly within the browser [2]. Fuse employs features like drag-and-drop organization, hierarchical folder structures, and collapsible header images to allow users to manage large volumes of snippets and bookmarks efficiently [7][8]. Users can easily reorder cards, nest them within other cards, and create hierarchical folder structures to keep their information organized. This helps users to quickly triage information, avoid clutter, and maintain a coherent structure even when dealing with extensive data. Similar card-based designs are seen in systems like Threddy, where users can collect and structure information using a thread-based approach to minimize context-switching and improve information retention [11]. However, the downside of card-based systems is that they can become overwhelming and cluttered with extensive use, potentially requiring extra cognitive effort to manage numerous cards.\n\n 2. Visual Layouts and Flexible Canvases [0]. \n\nAnother effective UI design is the use of spatial visual layouts where users can arrange search results and information snippets in a flexible manner. This approach caters to the need for quick triage and organization. An example is the hybrid design discussed in the \"Sewing the Seams of Sensemaking\" paper, which combines structure with flexible visual layouts, allowing analysts to organize search results spatially in overlapping \"piles\" [0]. This design helps mitigate the manipulation drawbacks of purely flexible canvas tools by integrating structured elements. However, these flexible layouts can still demand considerable effort for users to manually organize their information, particularly when dealing with large datasets.\n\n3. Sidebar Organizational Tools [2].\n\nIn-situ organizational tools that integrate directly into the browser via sidebars also play a significant role. The Fuse system exemplifies this by providing a persistent sidebar for collecting, organizing, and referencing web document clips without requiring users to switch contexts frequently [7]. These sidebars often use compact representations like cards and include features such as color-coding and hierarchical structuring to optimize space and usability [8]. For instance, Fuse’s container cards and folder cards allow users to nest and group related snippets together, making it easier to manage and refer back to collected content. However, one challenge with sidebar tools is the limited screen real estate they can offer, which could restrict the amount of viewable content at any one time, potentially hampering user workflow.\n\nIterative and Modular Design for AI Writing Support [3].\n\nFor AI-assisted writing tools, a modular and iterative approach is often recommended. This design philosophy acknowledges the different stages of writing, from discovery and evidence collection to synthesis and composition. As outlined in the work on expository writing support, AI tools should provide varying levels of assistance depending on the user's progress in the writing process [3][5][6]. For instance, initial stages might involve AI helping with structured search and retrieval of relevant documents, while later stages might see AI aiding in text generation and proofreading. Specifically, in the early stage, the AI might assist in identifying key evidence from multiple documents, thus reducing the cognitive load on the user. As the writing progresses, the AI's role evolves to help integrate this evidence seamlessly into the text. This ensures the user remains in control of the content and the ideation process. However, a major challenge with these systems is ensuring that AI assistance does not become overly intrusive or lead to over-reliance, which can stifle the user's creative process.\n\n\n\n### 5. Compact Representation of Metadata [2]\n\n\n\nTools like SearchPanel integrate document and process metadata into interactive UIs, thereby supporting iterative and exploratory search behaviors. By displaying process metadata alongside search results, SearchPanel helps users make sense of their search activities, navigate previously retrieved documents, and iteratively refine their queries [13]. This approach is particularly useful for complex search tasks that involve extensive information seeking and re-finding. Yet, such metadata-rich interfaces can become complex and potentially overwhelming, requiring users to invest time in learning how to navigate and use the system effectively.\n\n## 6. Specialized Writing Tools for Different Needs [3].\n\nUser interfaces designed for specific writing contexts can greatly enhance productivity and user satisfaction. For instance, creative writing tools like Haven emphasize a Zen-like UI to create a calm and focused environment, which contrasts with more utilitarian tools like MS Word or the nostalgic Hanx Writer [10]. These designs tailor the writing experience to fit the emotional and cognitive needs of the users, thereby boosting engagement and output quality. However, the specialized nature of these tools means they might not be suitable for all types of writing tasks, limiting their versatility.\n\n\nConcrete Examples of AI Writing Tools [3].\n\n\nSpecific AI writing tools demonstrate varied levels of AI assistance. For example, tools such as Grammarly provide real-time grammar and style suggestions, helping users improve their writing incrementally [3]. Another example is the AI tool within Google Docs that suggests sentence completions and stylistic changes as users write [3]. For more comprehensive support, tools like SciNote assist with structured writing by helping researchers organize data and draft scientific papers, while Overleaf offers collaborative LaTeX environments ideal for academic writing. These tools exemplify how AI can assist at various stages of writing, from drafting to final edits, without overshadowing the author's creative control [3]. However, one of the key challenges is the potential for AI tools to introduce errors or biases, particularly in complex or context-specific writing tasks. \n\nIn summary, effective UI designs for sense-making, information organization, and AI writing tools often involve a mix of flexible visual layouts, card-based interfaces, sidebar tools, and modular AI support [2]. These approaches aim to reduce cognitive load, minimize context switching, and provide intuitive, visual means for users to collect, organize, and synthesize information [7]. Nonetheless, each design comes with its own set of challenges, such as potential for user overwhelm, steep learning curves, and the risk of intrusive AI assistance, which need to be carefully managed to optimize user experience [6]. \n\n\nReferences:\n[0] Sewing the Seams of Sensemaking: A Practical Interface for Tagging and Organizing Saved Search Results Sewing the Seams of Sensemaking: A Practical Interface for Tagging and Organizing Saved Search Results This paper presents a usability-tested interface design that enables time-constrained analysts to organize their search results in a lightweight manner during and immediately following their search sessions. The research literature suggests that users want to lay out search results spatially in overlapping \"piles,\" but a pilot study with a flexible canvas tool revealed that this design requires too much manipulation and has other drawbacks. This finding led to a novel hybrid design that combines structure with a flexible visual layout and which allows the analysts to quickly triage documents first and organize them later, or interweave these two processes. Two usability studies comparing the new design against a legacy tool found overwhelming preference for the new tool for saving and organizing search results. Design guidelines derived from this work could improve sensemaking interfaces for other search applications.\n\n[1] Fuse: In-Situ Sensemaking Support in the Browser Working within the constraints of an always-present sidebar presents significant challenges to representing content and interactions in a compact way that functions across a variety of types of web content, meaning an in-situ system will need to offer a standardized but compact representations of each collected item within the sidebar regardless of the granularity or media type of the content collected. In order to design for this, we decided to represent each piece of collected content using the Card UI paradigm, as it would give users a highly visual representation that could be dragged around to be reordered, and nested. By default, each card would show a header preview image, with a title, source url, and blank annotation. In choosing to do so, we also decided on several transformations so that each type of media imported into Fuse would have a uniform presentation. Bookmarked websites would receive their header images by way of an automatic viewport screenshot during collection, and collected snippets appear directly in the header images. To better support skimming, this header image can be collapsed by clicking. Lastly, Fuse implements a generic ’expanded reader view’ (shown in Figure 2), when fully seeing the content and context of an item is desired. In their compressed forms, Fuse cards enable the enable the sidebar to fit many cards and afford the opportunity for us to implement in-situ organization, which we discuss next.\n\n[2] Beyond Summarization: Designing AI Support for Real-World Expository Writing Tasks Expository writing can be seen as a sensemaking process (Russell et al., 1993), and different types of sub-tasks are involved: typically, authors start with iteratively exploring and reading multiple relevant documents to identify and extract key evidence, then they organize the evidence into useful schema and further synthesize into coherent writing to communicate new knowledge or information (Pirolli, 2007).Therefore, the role of AI may vary during the process of writing, and we argue it is important to design different types and levels of AI support to maximally help the authors while minimally influencing or shaping their opinions.In one approach, in the early stages of writing, the writers would initiate and drive the work and AI should only provide limited supporting functions.As ideas manifest and authors have a better sense of the writing content, AI could assume more responsibilities with authors “supervising” the model’s work.For example, when starting writing a survey paper, the authors come up with the query to find relevant papers first, and AI helps execute the search and discover related documents; after authors read the retrieved papers and come up with ideas for the writing, AI can help generate the writing text based on authors’ ideas, and the writers only need to proofread model output.AI discovery helps the user learn better, and editing support makes the writing more efficient and polished; most importantly, writers are in full control of the thinking and the ideas included in the produced articles.\n\n[3] Beyond Summarization: Designing AI Support for Real-World Expository Writing Tasks To optimally involve LLMs to help expository writing, we argue that there are three components:1) supporting the reading and evidence-seeking for correct and efficient understanding of the source, 2) assisting information synthesis and the production of new knowledge and ideas, and 3) facilitating text composition to communicate relevant evidence and insights.Expository writing pieces aim at bringing about new knowledge or perspectives, but it takes significant effort for authors to comprehend the source and convert the thinking into writing.By reducing the cognitive load and the interaction costs for information extraction and sensemaking during the reading, it can help authors focus on the reasoning and idea generation (Kang et al., 2022; Palani et al., 2023).On the other hand, while LLMs have demonstrated strong performance in document understanding and text generation, they currently suffer from hallucination (Ji et al., 2022) and are limited in reasoning (Brown et al., 2020; Smith et al., 2022) and long-form generation capabilities.As such, it is sub-optimal to use LLMs to generate the whole writing piece altogether and post-edit (Cheng et al., 2022).A modular design that provides varying AI support at different stages of writing can be most helpful, and we detail the components as follows.\n\n[4] Beyond Summarization: Designing AI Support for Real-World Expository Writing Tasks Large language models have introduced exciting new opportunities and challenges in designing and developing new AI-assisted writing support tools. Recent work has shown that leveraging this new technology can transform writing in many scenarios such as ideation during creative writing, editing support, and summarization. However, AI-supported expository writing--including real-world tasks like scholars writing literature reviews or doctors writing progress notes--is relatively understudied. In this position paper, we argue that developing AI supports for expository writing has unique and exciting research challenges and can lead to high real-world impacts. We characterize expository writing as evidence-based and knowledge-generating: it contains summaries of external documents as well as new information or knowledge. It can be seen as the product of authors' sensemaking process over a set of source documents, and the interplay between reading, reflection, and writing opens up new opportunities for designing AI support. We sketch three components for AI support design and discuss considerations for future research.\n\n[5] Fuse: In-Situ Sensemaking Support in the Browser With the above in mind, we developed Fuse to assist users in conducting online research by enabling them to collect and organize web document clips and links in-situ in a persistent browser sidebar. Typically, a user synthesizing online content would need to switch between the content they are exploring and a reference document with their collected items as they synthesize information across multiple online sources - a process often done with high context-switching costs using copy-and-paste and an external spreadsheet application (Changet al., 2020). Existing systems use a combination of techniques for avoiding this issue, from ‘importing’ entire tabs in Tabs.do (Chang, 2021) to automatically extracting pre-defined content (such as price and average reviews) from tabs in Mesh (Changet al., 2020). Instead, our system enables users to reference and organize their collection of snippets as they simultaneously forage for more information. An inherent challenge to creating such a general purpose system includes supporting a variety of foraging and sensemaking activities that users undertake throughout a spectrum of domains and tasks. To enable such flexibility, Fuse combines paradigms such as web clipping, bundling, file management, and card interfaces to create a compact interface for information foraging. For example, ‘snipping’ a piece of content in the browser window creates a Fuse ‘card’ in the sidebar, which can then be organized alongside, in a hierarchical folder structure, or bundled as part of an existing card. As a complete system, Fuse is designed to afford users the flexibility to synthesize a wide variety of web content into whichever structure is most convenient for a given user’s task and domain.\n\n[6] Fuse: In-Situ Sensemaking Support in the Browser Fuse’s second design goal focused on the need to enable users to organize information without switching between browsing and a reference document, inspiring the implementation of compact organizational features within Fuse and ultimately the creation of the ’container card’ paradigm. Fuse users seemed to value a variety of Fuse’s organizational features to help them swiftly organize their content into a coherent structure where things fit together. Nearly all users reordered their collection using the Fuse sidebar (83%) to ensure a specific order to the cards. The ability to color-code cards was in contrast much less used in practice - although color was highlighted by a interview participant (P1) as a beloved feature, less than 1% of Fuse projects used the feature. However, Fuse users seemed able and interested in creating hierarchical structures to compress their collections, with 42% of users creating hierarchy in their collections. These hierarchies were consistently shallow, with only 4% of projects containing a structure deeper than 2 levels. Users creating hierarchies did so with a variety of methods, relying about equally on folder cards (46%) and Fuse’s unique ”container-cards” (50%). Users seemed to understand and enjoy the card-in-card paradigm and it’s related ’drop’ interaction for it’s intuitive usage:\n\n[7] You're the Voice: Evaluating User Interfaces for Encouraging Underserved Youths to express themselves through Creative Writing Minority groups are the fastest growing demographic in the U.S. In addition, the poverty level in the U.S. is the highest it has been in the last 50 years. We argue that the community needs more research addressing this user segment, and we present a novel study about how underserved youths react when presented with different UI designs aimed at promoting creative writing. The act of creative writing per se can become the driver of change among underserved teenagers, and researchers should strive to discover novel UI designs that can effectively increase this target group's productivity, creativity and mental well-being. Using MS Word as baseline, our contribution analyzes the influence of a Zen-like tool (designed by the authors and called Haven), a nostalgic but realistic typewriting tool (Hanx Writer), and a stress-based tool that eliminates writer's block by providing consequences for procrastination (Write or Die). Our results suggest that the Zen characteristics of our tool Haven were capable of conveying a sense of calm and concentration to the users, making them feel better and also write more. The nostalgic Hanx typewriter also fared very well with regard to mental well-being and productivity, as measured by average number of words written. Contrary to our initial expectations, the stress-based UI (Write or Die) had the lowest productivity levels.\n\n[8] Threddy: An Interactive System for Personalized Thread-based Exploration and Organization of Scientific Literature On-the-go foraging and structuring. At a high level, Threddy users could continuously collect information while following the relevant threads of research. For example, improved context awareness and persistent threads across papers led participants to move between them and “structure information on the go” (P9) as opposed to reviewing papers individually and “in discrete chunks due to the frequently lost context” (P5). Moreover, in-thread recommendations integrated the stages for searching and reading, further reducing the context-switching cost. These reduced switching costs and support for externalizing working memory position Threddy to be especially useful in supporting the early sensemaking process of literature review as researchers forage for information, helping them create what Pirolli and Card term a “shoebox” of relevant information (Pirolli and Card, 2005) easily, in a more organized way, and with affordances for helping them pull in even more relevant information. There are many other aspects of the process where other tools would remain useful. For example, active diagramming and concept mapping may help users externalize representations focused on relations of the concepts involved (e.g., understanding how the different components of a weather system fit together from a paper, rather than the threads of research on weather systems). Synthesis and summarization tools may also help further along the process; here it is possible that a thread-based approach could scaffold the creation of synthesized mental models by enabling users to work with pre-grouped sets of papers.\n\n[9] SearchPanel: framing complex search needs People often use more than one query when searching for information. They revisit search results to re-find information and build an understanding of their search need through iterative explorations of query formulation. These tasks are not well-supported by search interfaces and web browsers. We designed and built SearchPanel, a Chrome browser extension that supports people in their ongoing information seeking. This extension combines document and process metadata into an interactive representation of the retrieved documents that can be used for sense-making, navigation, and re-finding documents. In a real-world deployment spanning over two months, results show that SearchPanel appears to have been primarily used for complex information needs, in search sessions with long durations and high numbers of queries. When process metadata was present in the UI, searchers in explorative search sessions submitted more and longer queries and interacted more with the SERP. These results indicate that the process metadata features in SearchPanel seem to be of particular importance for exploratory search.\n\n"}
{"case_id": "dc761baf0fc9fead9889d9eb0a258f0e", "answer_text": "\n\nMeasuring the Error Vector Magnitude (EVM) accurately across different modulation types like MPSK, QPSK, and MQAM can be achieved using a general equation, but there are several nuances and considerations to account for [1]. \n\nEVM Definition Across Modulation Types\n\nError Vector Magnitude (EVM) is defined as the root-mean-square (RMS) value of the difference between a collection of ideal transmitted symbols and the received symbols in the I-Q plane [1]. It represents how accurately the modulated signal matches its ideal reference, and can be normalized to account for different amplitude levels in various modulation schemes, such as BPSK, QPSK, 16-QAM, and 64-QAM [4]. This normalization allows EVM measurements to be effectively calculated and compared across different modulation types [4]. For instance, higher-order modulation schemes like 256-QAM typically require lower EVM values to maintain signal quality and performance [11]. Similarly, amplitude and phase distortion in power amplifiers can impact EVM, with specific relationships allowing predictions of EVM time series from these distortions [5]. In optical communication systems, EVM is used alongside other metrics like BER and Q-factor for performance evaluation [8]. \n\nEVM is a measure of the difference between ideal and actual received symbols, quantified as the root-mean-square (RMS) value of these differences normalized to the average power of the ideal symbols [1]. This general definition applies across different modulation schemes such as QPSK, MPSK, and MQAM [1][6][7]. The common formula for EVM is:\n\n\\[ \\text{EVM} = \\frac{\\sqrt{\\sum_{i=1}^{N} |\\mathbf{E}_i - \\mathbf{R}_i|^2}}{\\sqrt{\\sum_{i=1}^{N} |\\mathbf{E}_i|^2}} \\]\n\nwhere \\(\\mathbf{E}_i\\) represents the ideal symbol, \\(\\mathbf{R}_i\\) represents the received symbol, and \\(N\\) is the number of symbols [7].\n\nChallenges and Adaptations\nWhile the general formula for EVM is applicable, different modulation schemes might require slight adjustments to capture specific nuances [1].\n\n1. **Normalization**: Different modulation schemes have varying amplitude levels. Normalizing the EVM calculations to ensure consistency across different schemes is crucial [4][12]. For instance, the normalization process ensures that the mean square amplitude of all possible symbols in any modulation’s constellation is one [4]. This adjustment is particularly important for higher-order QAM schemes, where variations in amplitude can significantly affect EVM measurements.\n\n2. **Phase Noise and Nonlinearity**: Different modulation schemes respond differently to phase noise and nonlinearity in the communication system. For instance, higher-order QAM schemes are more susceptible to these impairments compared to PSK schemes [1][11][5]. This aspect influences the EVM measurements and must be considered when comparing results across different modulations.\n\nHigher-order QAM schemes often require more sophisticated carrier phase recovery and amplitude correction procedures to maintain accurate EVM levels. Recent studies such as [8] highlight the necessity of a calibration process to account for the implementation penalties associated with carrier phase recovery algorithms in optical coherent systems. Additionally, higher-order QAM schemes exhibit greater sensitivity to system nonlinearities, as noted in [13], where explicit formulas were derived for evaluating EVM from amplitude and phase distortions in power amplifiers.\n\nRecent studies and practical results on various aspects of modulation techniques and their performance metrics have shown a significant impact on the efficiency of communication systems [2]. Error Vector Magnitude (EVM) has been extensively used as a performance metric to assess the quality of modulation schemes in both wireless and optical communication systems [7]. EVM measures the root-mean-square (RMS) difference between ideal and received signal constellations, providing a quantifiable metric for signal distortion and transmitter accuracy [9]. For higher-order QAM constellations, the inherent nonlinearity of devices like microring modulators results in a more complex implementation and higher EVM values compared to lower-order constellations [0]. Performance improvements and accurate BER estimation from EVM analysis require compensating for factors such as laser phase noise and differential decoding penalties [1]. Studies highlight that increasing the number of modulation order M leads to higher error rates, necessitating higher transmitted power to maintain performance [2]. \nA study conducted by [0] demonstrated that the inherent nonlinearity of microrings leads to more involved implementation challenges for M-QAM modulators compared to M-PAM modulators. This study showed that for 64-QAM, an EVM better than -30 dB was achieved with a 7+7 electrode configuration, while 256-QAM required one additional electrode in the amplitude-related modulator ring to maintain such EVM levels. This emphasizes the complexity of higher-order QAM schemes and the need for increased precision in their implementation.\n\nFurther, according to [11], different modulation schemes for downlink (DL) such as QPSK, 16QAM, 64QAM, and 256QAM require different limits of EVM to exploit the full benefit of the modulation. The EVM for each modulation needs to be better than specified threshold values to maintain signal integrity and performance.\n\n### EVM vs. Eb/No Curve\n\nError Vector Magnitude (EVM) is defined as the root-mean-square (RMS) value of the difference between a collection of ideal transmitted symbols and the received symbols in the I-Q plane [1]. \n\nThe EVM can be linked to the bit error rate (BER) and signal-to-noise ratio (SNR) for modulation schemes, and analyzing the EVM can help in estimating the BER in communication systems [10].\n\nGraphically, the relationship between EVM and BER or SNR can provide insights into the performance of different modulation schemes, particularly in the presence of noise and interference [0]. \n\nTherefore, performance metrics such as EVM are essential for evaluating the quality of transmitted signals and ensuring they meet the required standards for communication systems [8]. \n\n\nThe relationship between EVM and the energy per bit to noise power spectral density (Eb/No) is crucial for understanding system performance under different noise conditions [10]. \n\n1. **Lower Modulation Orders**: For lower-order modulations like QPSK or 16-QAM, the EVM generally decreases sharply as Eb/No increases, reflecting better symbol accuracy at higher signal-to-noise ratios [13][14].\n2. **Higher Modulation Orders**: For higher-order modulations like 64-QAM or 256-QAM, the decrease in EVM with increasing Eb/No is less steep compared to lower-order modulations. These modulation schemes are more sensitive to noise and other impairments, resulting in higher EVM values at comparable Eb/No levels [6][11][10].\n3. **Comparison Across Modulations**: When comparing EVM across different modulation schemes, higher-order modulations will exhibit higher EVM values for a given Eb/No due to their higher susceptibility to errors. Lower-order modulations, being more robust, will show a lower EVM at the same Eb/No [2][14].\n\nFor a more concrete understanding, let’s consider an example where EVM is measured for QPSK, 16-QAM, and 64-QAM across various Eb/No levels [1]. \n  \n- **QPSK**: At an Eb/No of 10 dB, the EVM might be around 5%, decreasing to 2% at 20 dB [1].  \n\n - **16-QAM**: At an Eb/No of 10 dB, the EVM might be around 7%, decreasing to 3% at 20 dB [7]. \n- **64-QAM**: At an Eb/No of 10 dB, the EVM might be around 15%, decreasing to 6% at 20 dB [14]. \n\nThese numbers are illustrative and demonstrate the general trend observed in practical scenarios. Higher-order modulations like 64-QAM show higher EVM at lower Eb/No values compared to lower-order modulations like QPSK, reflecting their higher sensitivity to noise and distortions [2].\n\n\n\n### Practical Considerations\n\nIn optical coherent systems, it is necessary to reliably estimate the BER from EVM and accurately monitor the performance of optical coherent systems [1]. \n\nEVM is defined as the root-mean-square (RMS) value of the difference between a collection of ideal transmitted symbols and the received symbols in the I-Q plane [1].\n\nThe nonlinearity of the power amplifier is often given by the amplitude distortion (AM-AM) and phase distortion (AM-PM). This relationship enables the EVM time series to be predicted from the AM-AM and AM-PM [5].\n\n\n1. **Measurement Setup**: Ensure that your measurement setup, including symbol timing and carrier recovery mechanisms, is accurately calibrated to avoid introducing additional errors into EVM measurements [1][9][8]. Use high-precision instruments like Vector Signal Analyzers (VSAs) or real-time oscilloscopes calibrated for both amplitude and phase accuracy.\n2. **System Nonlinearities**: Power amplifier nonlinearity and other RF impairments must be accounted for and, if necessary, corrected in the EVM measurement process to ensure accurate readings [5][12][13]. Techniques such as digital predistortion can help in mitigating these effects.\n3. **Error Factors**: Consider other error factors like phase noise and multipath effects, which differentially impact modulation schemes [1][7][12]. Advanced signal processing techniques, such as adaptive filtering and equalization, can be implemented to reduce the impact of these factors on EVM.\n\nIn summary, while a general equation for EVM is applicable across different modulation types, specific adjustments for normalization, handling phase noise, and addressing system non-linearities are essential for accurate measurements [1]. Recent studies emphasize the importance of these adjustments, especially for higher-order modulations [0]. The EVM vs. Eb/No curve varies across modulation schemes, reflecting their inherent robustness or vulnerability to noise and other impairments [10]. By understanding these nuances and incorporating specific data from recent studies, you can ensure accurate and reliable EVM measurements across different modulation schemes. \n\n\nReferences:\n[0] Direct-Digital-Drive Microring Modulator error, we employ the Error Vector Magnitude (EVM) measure. For the above example the EVM is −30.6dB. Note that the EVM can be further reduced by increasing the number of electrodes in each microring. Such an increase will allow generating a denser pool of points from which one can select a desired set of constellation points with higher accuracy. Table I presents the obtained EVM for various constellations and varying number of electrodes. It shows the EVM for 16-QAM with 6 + 6 and 7 + 7 electrodes. The difference between these configurations is 9dB. It can be seen that the inherent nonlineariy of the microring leads to a more involved implementation of M-QAM modulators compared to M-PAM modulators. The table also presents results for 64-QAM and 256QAM. While for 64QAM we achieve EVM better than -30dB with 7+7 electrode configuration, 256QAM requires one additional electrode in the amplitude-related modulator ring to achieve such EVM. B. Higher order QAM constellations The number of distinct electrode segments that can be effectively mounted on a single microring is obviously limited. This, however, does not limit the proposed technology, as the original configuration can be augmented by additional rings with additional electrodes. Thus for example, rather than generating 256-QAM with 7 and 8 electrodes on each ring, one can utilize 4 rings with 5 electrodes on each ring: two of the rings will act as amplitude modulators while the other two will act as phase modulators, as depicted in Figure 7. For the same parameters as above, with\n\n[1] Metrology of Optical Communication Systems Using Error Vector Magnitude be under-estimated following the compensation of laser phase noise for the different modulation formats. A correction factor is thus necessary to reliably estimate the BER from EVM and to accurately monitor the performance of optical coherent systems. Finally, the impact on the number of symbols used to estimate the BER from EVM analysis is also presented and compared to the BER obtained by error counting. Error Vector Magnitude and BER Estimation EVM is defined as the root-mean-square (rms) value of the difference between a collection of ideal transmitted symbols and the received symbols in the I-Q plane. The error vector between the received symbol and the ideal symbol location is shown in Figure 1. The rms EVM can be expressed as , , where N is the number of symbols over which the EVM is estimated, r E is the received signal vector, t E is the ideal transmitted vector, and a P is the average ideal transmitted power for the chosen modulation. t E can either be a known reference pattern for data-aided reception or computed relative to the I. Fatadin Journal of Applied Mathematics and Physics 1/ 2 where the differential coding penalty, F, is given as 2 log ( ) For QPSK, 16-QAM, and 64-QAM, the values of F can thus be set to 2, 1.67, and 1.43, respectively, to include the expected differential decoding penalty for the different modulation formats. Figure 2 shows the deviation of the EVM estimated for nondata-aided from data-aided reception for QPSK, 16-QAM, and 64-QAM\n\n[2] A Comprehensive Study and Performance Comparison of M-ary Modulation Schemes for an Efficient Wireless Mobile Communication System respectively. These equations can be applied to calculate the error probability in terms of SNR per bit. It is necessary to increase the transmitted power in order to maintain the same performance level for higher M. Error probability of MQAM The bit error probability P MQAM for M-ary QAM modulation scheme is given by,11 ( ) To simplify this equation we can employ some further approximations in order to calculate the error probabilities, which are listed below. , ) 1 ( and for large value of M Error probability of MFSK The bit error probability P MFSK for M-ary FSK modulation scheme is given by,12 ( SIMULATION RESULTS AND COMPARISON M-ary modulations schemes in wireless communication systems are analyzed using voice signal as a function of bit error rate (BER). This is a simulation work dome by Matlab. The work deals the recorded voice signal which is transmitted though AWGN channel where fading channels like Rayleigh fading and Ricean fading channel are not considered. Figure 3 shows the comparison of MPSK, MFSK and MQAM as a bit error probability for modulation order M=2. The bit error rate is sharply decreased with respect to signal to noise ratio. From this figure, it is clear that, the performance of MPSK is better than that of others. However, MPSK and MQAM show almost smaller result whereas MFSK shows higher bit error rate. More that one order of magnitude higher BER value can be found in MFSK at the SNR=6 dB than that of other modulation techniques. Reproduction\n\n[3] EVM as generic QoS trigger for heterogeneous wieless overlay network per bit and N 0 /2 is the noise power spectral density. Q [. is the Gaussian co-error function and is given by (2.2) Assuming raised cosine pulses with sampling at data rate, Equation 2.1 also gives the bit error rate in terms of signal to noise ratio as (2.3) Where E s /N 0 is the signal to noise ratio for the M-ary modulation system and raised cosine pulse shaping at data rate. Equation 2.3 defines the BER performance in terms on SNR and quite often used as main tool for many adaptive systems. For diversity and MBER systems, this equation essentially means that the choice is made in favour of bit error rate. EVM definition Error Vector Magnitude (EVM) is initially modelled by [K.M. Ghairabeh, K.G. Gard, and M.B. Steer-. Measurements are performed on vector signal analyzers (VSAs), real-time analyzers which capture a time record and internally perform a Fast Fourier Transform (FFT) to enable frequency domain analysis. Signals are down converted before EVM calculations are made. Since different modulation systems viz. BPSK, 4-QAM, 16-QAM etc. have different amplitude levels, to calculated and compare EVM measurements effectively some normalization is typically carried out. The normalization is derived such that the mean square amplitude of all possible symbols in the constellation of any modulation scheme is one. Thus, EVM is defined as the root-mean-square (RMS) value of the difference between a collection of measured symbols and ideal symbols. These differences are averaged over a given, typically large number of symbols and\n\n[4] The error vector and power amplifier distortion The error vector and power amplifier distortion The RMS value of the error vector magnitude (EVM/sub RMS/) is a useful system-level figure of merit for the accuracy of digitally-modulated signals. At the circuit level, the nonlinearity of a power amplifier is often given by the amplitude distortion (AM-AM) and phase distortion (AM-PM). When the amplifier input is undistorted, there is a simple relationship between the amplifier distortion and the error vector at the output: the error vector is the difference of the measured signal and a reference (ideal) signal, while the AM-AM and AM-PM are found from the ratio of the measured and reference signals. This relationship enables the EVM time series to be predicted from the AM-AM and AM-PM. In the frequency domain the spectral regrowth at the amplifier output is equal to a portion of the error vector power spectrum. A frequency-domain distortion power ratio may be estimated from the appropriate time-domain average of the EVM. These relationships are validated experimentally for a class AB amplifier and /spl pi//4 DQPSK signals.\n\n[5] Error Vector Magnitude analysis for wideband QPSK and QAM six-port modulator Error Vector Magnitude analysis for wideband QPSK and QAM six-port modulator This article presents an Error Vector Magnitude (EVM) analysis for two types operation of wideband six-port modulators, which are Quadrature Phase Shift Keying (QPSK) and Quadrature Amplitude Modulation (QAM). The simulation results of both modulators were accomplished via Agilent's Design Systems (ADS) software. The EVM values then were determined through calculation using the simple mathematical equation at three different frequency points of 4.2, 5.8 and 6.85 GHz. The best performance was obtained at frequency point of 5.8GHz.\n\n[6] EVM Measurement and Correction for Digitally Modulated Signals EVM Measurement and Correction for Digitally Modulated Signals We proposed an error vector magnitude (EVM) measurement method for 16-quadrature-amplitude-modulated (16QAM) waveform. A 16QAM signal with 20GS/s sample rate, 1Gbaud/s symbol rate and 4GHz carrier frequency is simulated. By changing orders of the root-raised-cosine (RRC) filters, we set the EVM of the simulated signals to 0.001% (increasing the order of the filter reduces the EVM but can never reach zero) and 1% respectively. We then upload these two signals to the arbitrary waveform generator (AWG), and measure the outputs of AWG using a real-time digital oscilloscope (RTDO). After a series of processing like carrier-recovery, time-recovery, frequency and phase shift estimation and correction, the EVM of the measured signals are 6.0% and 7.0%. The nonlinearity of the AWG is the main reason of the bad results, so we use an algorithm to correct it. After several iterations, the EVM of these signals reduce to 2.2% and 2.9% respectively.\n\n[7] Calibration of Estimated BER from Error Vector Magnitude with Carrier Phase Recovery coherent systems. A plethora of performance metrics can be used in optical communications to assess the quality of spectrally-efficient modulation formats. The performance of the transmitted signal can be evaluated using metrics such as bit error ratio (BER), Q-factor, eye diagram and, more recently, error vector magnitude (EVM). BER is the most conclusive figure of merit compared to the other performance metrics. However, it requires a known pattern, e.g. a training sequence, to be transmitted for continuous performance monitoring in optical networks. EVM is a popular performance metric in wireless digital communication systems and is commonly used to evaluate the quality of vector-modulated signals and is a more suitable figure of merit at high optical signal-to-noise ratio (OSNR) in coherent systems where a large number of symbols will otherwise be required for accurate error counting. In this paper, the estimation of BER from EVM analysis is first presented for the QPSK modulation format with the Viterbi & Viterbi algorithm and differential decoding for different combined linewidth symbol duration product, ΔνTs. A calibration is shown to be necessary when using the established relationship between EVM and BER to take into account of the implementation penalty associated with the carrier phase recovery algorithm in optical coherent systems. The EVM analysis is shown to give better accuracy than error counting for different number of symbols. Finally, the estimation of BER from EVM analysis is presented for a 16-ary quadrature amplitude modulation (16-QAM) optical coherent system. Error Vector Magnitude and BER Estimation Error vector magnitude is extensively\n\n[8] Cognitive Interference Cancellation with Digital Channelizer for Satellite Communication method cancels the interference by omitting the corrupted subchannel. This means that one of the 1024 components of the desired signal is also omitted in the cancellation process. Even though the interference is correctly cancelled out, we cannot obtain the ideal QPSK constellation as shown in Figure 8a-c. The error vector magnitude (EVM), i.e., the inverse of the effective SNR, is -30 dB because 1/1024 of the desired signal along with the CW tone interference is cancelled out. The definition of EVM is the ratio of the mean square of the error vector to the power of the reference constellation, i.e., ideal constellation. That is, the EVM in dB scale is as follows: where C mea and C re f denote the measured and reference constellations, respectively. The numerator and denominator in Equation (23) stand for the error caused by noise and interference in signal constellations and the power of the ideal signal, respectively. Therefore, the EVM is regarded as the inverse of the signal-to-noise (-plus-interference) ratio. On the other hand, Figure 8b and Figure 8d show the constellation of the desired signal without interference cancellation under low and high interference conditions. The EVM of the constellation in Figure 8c is -25 dB because the CW tone interference power is -25 dB lower than the desired wideband signal. Figure 8d shows the constellation without cancellation of the higher interference, i.e., -5 dB lower than the desired wideband signal, and the EVM of this case is -5 dB. Figure 9 shows the EVM results with and without\n\n[9] Comparative Study of Bit Error Rate of Different M-ary Modulation Techniques in AWGN Channel Comparative Study of Bit Error Rate of Different M-ary Modulation Techniques in AWGN Channel This paper focuses on various digital modulation schemes and their effect on bit error rate (BER); and to ascertain which has the lowest bit error rate. Further analysis includes: to compare bit error rates of various digital modulation schemes using the M-ary modulation technique, analyse the effect of varying signal energy per bit to Noise ratio (Eb/No) on the error rate of various digital modulation schemes, analyse graphically the relationship between Eb/No and BER, analyse graphically the relationship between BER and M-ary number. A model-based design methodology was employed in the research using MATLAB/SIMULINK. The comparison between different M-ary (M-PAM, M-PSK, and M-QAM) (M = 2, 4, 8, 16, 32, and 64) modulation schemes in normal AWGN channel was done. By analysing the graphical illustration of Eb/No vs BER of these MPSK schemes it was strongly observed that increase in the value of M causes a correspondent increase in the error rate. Therefore, as the error rate increases with increasing M; lower level should be used for long distance communication and vice versa. High level modulation techniques are always preferred for high data rate. Introduction Digital communication is employed for signals that are essentially analog and continuous-time, such as speech and images; and signals that are essentially digital such as text files. In digital modulation, the baseband (modulating) signal is converted to a digital signal. It is preferred over analogue modulation for the following reasons: Greater noise immunity and robustness to\n\n[10] Experimental Evaluation of Transmitted Signal Distortion Caused by Power Allocation in Inter-Cell Interference Coordination Techniques for LTE/LTE-A and 5G Systems thresholds are set because the different modulations for the DL (QPSK, 16QAM, 64QAM, and 256QAM) require different limits of EVM to exploit the full benefit of the modulation, and the power control range affects EVM. According to the specifications, the EVM for each modulation of the PDSCH is VOLUME 10, 2022 better than the values listed in Table 2. The EVM is defined according to (2) as the square root of the ratio of the mean error vector power (difference between the ideal modulated symbols and the measured symbols after equalization) to the mean reference power expressed as a percentage. The EVM measurement shall be performed over all allocated resource blocks and DL subframes within at least 10ms measurement periods. The basic unit of EVM measurement is defined over one subframe (1ms) in the time domain and N RB BW = N RB SC = 12 subcarriers (180KHz) as defined in (2) (annex E in)): where T is the set of symbols with the considered modulation scheme being active within the subframe, F(t) is the set of subcarriers within the N RB BW subcarriers with the considered modulation scheme being active in symbol t, I (t, f ) is the ideal signal reconstructed by the measurement equipment in accordance with relevant transmission (TX) models, and Z (t, f ) is the modified signal under test. The method for measuring EVM is quite involved (annex E in), but a simple approximation assumes that the error vector resembles white noise. In this case, EVM can be\n\n[11] Characterization of RF Power Amplifier for Narrow and Wide Band Memory Polynomial Implementations by 1dB. Similarly, if there is a decrease of 1dB in input signal level then that will decrease levels of all third order product by 3dB. That proves if the input power is decreased by onethird of the distance in decibels from P3 to noise floor, the third order harmonic will drop to noise level. This specific output power range for fundamental signal is called spurious-free dynamic range (SFDR), which can be estimated as: The third order intercept power and spurious-free dynamic range are proportional to each other. If one is high the other will be high too, which means unwanted inter-modulation products are sup suppressed more. This is an important approach for calculating the linearity of a power amplifier. Error Vector Magnitude (EVM) Error vector magnitude tells us that how accurately the wireless system is transmitting symbols with in the constellation. Informally, EVM tells the difference between the positions of ideal constellation points and the actual constellation points. EVM is actually the average amplitude of error signal normalized to the peak signal amplitude. It can be expressed as: EV M (%) = (P error /P ref erence × 100% (10) Modulation Error Ratio (MER) The modulation error ratio is used to quantify the performance of a wireless system. MER has a close relationship with EVM, but it is calculated from average power of signal. It can be expressed as: M ER(%) = P error P signal × 100% (12) Results & Discussion The RF power amplifier modelling is done by using actual component\n\n[12] OFDM Error Vector Magnitude Distortion Analysis OFDM Error Vector Magnitude Distortion Analysis We derived explicit formulas for evaluating the error vector magnitude (EVM) from the amplitude distortion (AM-AM) and phase distortion (AM-PM) of power amplifiers (PAs) in orthogonal frequency-division multiplexing (OFDM) systems, such as the IEEE 802.1 1a/g wireless local area networks (WLANs) standards. We demonstrated that the developed formulas allowed EVM simulation of a memoryless PA using only a single-tone response (i.e. without OFDM modulation and demodulation), thus enabling us to easily simulate the EVM using a harmonic-balance (HB) simulator. This HB simulation technique reduced the processing time required to simulate the EVM of a PA for the IEEE 802.1 1a standard by a factor of ten compared to a system-level (SL) simulation. We also demonstrated that the measured EVM of a PA module for the IEEE 802.11g could accurately be predicted by applying the measured static AM-AM and AM-PM characteristics to the derived formulas.\n\n[13] Performance evaluation on the basis of Bit error rate for different order of Modulation and different length of Subchannels in ofdm system MATLAB simulation software. 100 number of iterations is taken for each of the experimental results. I have taken the following parameters and set their value as given in Table I. If we observe the above figure then we will get an idea about the amount of bit error rate and best performer (modulation order of 4) within the different order of modulations at 0dB level of SNR, whereas the same kind of result also can be observed at higher level of SNR. The best performer (4 th order modulation) has the bit error rate of around 0.00144 (which is the lowest with respect to the other 2 different order of modulations). Finally, I got right option for the higher level of SNR and that is 4 th order modulation means QPSK. Case study :2 Number of subchannel taken -512 with respect to figure 4, whereas the bit error rate is almost same for lower order modulation (modulation order 4). So, it can be said as, the bit error rate can be influenced by the length of the subchannel. Case study :3 Number of subchannel taken -4096 Discussion of the above results Finally, I got the expected result, which is similar with the analytical work. If we increase the level of SNR, then the amount of bit error rate for modulation order 16 can be saturated after a certain level of the threshold value. CONCLUSION In this paper, I explore an idea about the comparative performance evaluation on the basis of bit error rate of a different\n\n"}
{"case_id": "8f5a6ec4ad13c0b7881a9af2de00ca4a", "answer_text": "\n\nIn auditory speech perception using intracranial EEG (iEEG), the Temporal Response Function (TRF) is a pivotal tool for mapping the relationship between stimulus characteristics (such as variations in the acoustic signal envelope) and the corresponding neuronal activity [0]. Understanding the different stages of data processing and the underlying hypotheses is crucial for leveraging TRF in research and applications. Here, I will detail the various stages and the reasoning behind them.\n\n### Data Acquisition and Preprocessing\n\nData acquisition for EEG studies typically involves recording neural activity using a multi-channel setup, where the temporal response function (TRF) is computed to describe the linear mapping between the speech envelope and the EEG signal [1]. Preprocessing steps often include filtering the EEG data, calculating the speech envelope using methods such as the Hilbert transform, and aligning the EEG data to the speech signal based on trigger events [2]. Ridge regression is commonly used to estimate the TRF by minimizing the mean squared error between the actual and predicted neural response [5]. Cross-validation techniques are employed to optimize the regularization parameter during TRF estimation [2]. Reconstruction accuracy is often assessed using Pearson correlation between the original and reconstructed speech envelopes [1]. This preprocessing ensures that the EEG data is sufficiently cleaned and transformed for further analysis, such as decoding and stimulus reconstruction [1, 5, 7]. \n\n**1. Data Collection:**  \nTo reconstruct the speech envelope, the decoder g(τ, n), which allows a linear mapping of the EEG channels r(t, n) to the speech envelope s(t), has to be computed. This process can be denoted using the following expression: Here,ŝ(t) represents the reconstructed version of the speech envelop s(t) [0]. The decoder is calculated by the following matrix operations: Here, R represents the lagged time series of the neural response matrix r. The following equation defines R as an example for one channel only [5]. Here, I represents the identity matrix and is a smoothing constant or ridge parameter [3]. The ridge parameter can be adjusted using a leave-one-out cross-validation process [2].  \n\nNeural signals are recorded using iEEG during auditory speech perception tasks. The acoustic stimulus, typically a continuous speech stream, is simultaneously recorded. This setup ensures that both the stimulus and the neural responses are temporally aligned (i.e., timestamped), which is crucial for subsequent analysis [0][1].\n\n**2. Preprocessing:** For EEG data preprocessing, temporal response functions (TRFs) are often used to describe the linear mapping between the speech envelope and the neural response [5]. TRF analysis involves computing a decoder that can reconstruct the speech envelope from the recorded EEG data, using methods such as ridge regression and cross-validation to optimize model parameters and reduce overfitting [2]. The accuracy of this reconstruction can be assessed by the Pearson correlation between the original and reconstructed speech envelopes [1]. \nPreprocessing steps often involve several key procedures:\n\n- **Filtering:** The iEEG signals are filtered to remove noise and artifacts, which can obscure the meaningful neural responses. Common filters range from 1-8 Hz to capture primary speech-related neural oscillations, as well as specific frequency bands like delta (1-4 Hz) and theta (4-8 Hz), which are known to be involved in speech processing [2][7].\n\n- **Artifact Removal:** Techniques such as independent component analysis (ICA) or regression-based methods are employed to remove artifacts from eye movements, muscle activity, and other non-neural sources [2].\n\n- **Envelope Extraction:** The speech envelope is typically extracted using the Hilbert transform. This process converts the speech signal into its amplitude envelope, providing a smooth and continuous representation of the speech signal's amplitude variations over time [2]. The Hilbert transform assists in creating an analytic signal from which the envelope can be derived.\n\n- **Time Alignment:** The precise alignment of the iEEG data with the acoustic stimulus is crucial. This alignment is often achieved using trigger events or synchronization markers recorded during the experiment [2].\n\nCalculation of Temporal Response Functions (TRF) [5]\n\n**3. Forward Modeling:** [3]\nIn the forward modeling approach, the TRF estimates how the stimulus is mapped onto the iEEG responses across different channels and time lags. Essentially, the acoustic stimulus (e.g., speech envelope) is convolved with the neural response function. This convolution is mathematically represented and estimated using linear regression methods, such as ridge regression, to minimize the mean squared error (MSE) between the actual and predicted neural responses [0][1][6]. Ridge regression is a technique used to analyze multiple regression data that suffer from multicollinearity. It introduces a regularization parameter that biases the estimates towards zero to reduce their variance.\n\n**4. Backward Modeling (Stimulus Reconstruction) [1]:**  \n\nBackward modeling, or stimulus reconstruction, reverses the forward modeling process. Here, the stimulus is reconstructed from the neural responses. This involves calculating a decoding function that maps the iEEG responses back to the stimulus features (e.g., the speech envelope). The accuracy of this reconstruction is typically measured by the Pearson correlation coefficient between the original and reconstructed stimuli, providing an index of how well the neural network encodes the stimulus [1][6][11].\n\nModel validation is essential for ensuring the reliability and accuracy of predictions generated by temporal response functions (TRFs) [0]. This involves comparing predicted neural responses to actual recorded electroencephalographic (EEG) data using statistical measures like the Pearson correlation coefficient [1]. Hypotheses testing in this context typically includes evaluating how well model parameters reflect known neural processing characteristics, such as stimulus-response latencies and the impact of signal-to-noise ratios on neural encoding [4]. \n\n**5. Cross-validation and Regularization**[2].\nCross-validation, particularly leave-one-out or k-fold cross-validation, is used to optimize model parameters, such as the ridge regression regularization parameter. This process helps in preventing overfitting and ensures that the TRF model generalizes well to new data [2][6]. Regularization controls the complexity of the TRF model, balancing the trade-off between fitting the training data closely and maintaining a capacity to generalize to unseen data [2][5].\n\n**6. Evaluation Metrics:**  \n\nThe temporal response function (TRF) can be seen as a filter describing the transformation of the ongoing stimulus to the ongoing EEG activity. The TRF describes the transformation for a specified range of time lags relative to the instantaneous occurrence of the stimulus feature [0]. The decoder function in the backward model allows a linear mapping of the EEG channels to the speech envelope. Reconstruction accuracy can be measured by Pearson correlation between original and reconstructed envelopes [1]. By comparing the reconstructed stimulus to the original stimulus, researchers can provide a reliable index for the degree to which the envelope is encoded in the neural network [1]. N-fold cross-validation was used to optimize the regularization parameter across individuals, and the resulting model is tested on the data that was originally omitted [2]. The analysis typically uses the Pearson correlation value between the target signal and the predicted signal to validate the performance of these models [7]. \n\nThe performance of TRF models is evaluated using several metrics, including the Pearson correlation coefficient between the predicted and actual neural responses (for forward models) or between the reconstructed and original stimuli (for backward models). Additionally, metrics like root mean square error (RMSE) and prediction accuracy are used to assess the model's predictive power [1][5][7].\n\n**7. Hypothesis Testing:**\n\nIn recent studies, the accuracy of stimulus reconstruction has been examined in different contexts, highlighting the variations in this approach. For instance, temporal response functions (TRFs) can describe the transformation of the ongoing stimulus to the ongoing EEG activity, functioning as a filter for specific time lags relative to the stimulus feature's occurrence [0]. This method has been instrumental in auditory research, allowing the reconstruction of the speech envelope from recorded neural activity, as described by O'Sullivan et al. (2014) [0].\n\nTRFs are utilized to test specific hypotheses about auditory speech perception. For instance, researchers may hypothesize that the neural encoding of speech in noisy versus quiet conditions differs significantly. Studies often compare TRFs obtained from subjects listening to speech at different signal-to-noise ratios (SNRs) to examine how noise affects speech processing. One example is manipulating the SNR to observe changes in TRF morphology and latency, which can indicate alterations in neural processing due to noise interference [4][10]. Another common experimental condition involves comparing TRFs when subjects focus on attended versus ignored speech streams in a multi-talker environment. Researchers may analyze the differences in TRF components, such as peaks at specific latencies (e.g., 100 ms and 200 ms), which are associated with processing attended speech, providing insights into selective attention mechanisms [9][14].\n\nPractical Applications and Refinement\n\n**8. Application in Clinical Diagnostics:** [4]\nTRF analysis extends beyond basic research to practical applications. For example, variations in TRF morphology can predict individual speech intelligibility thresholds, offering new diagnostic tools for assessing hearing impairments and tailoring hearing aids [4][11]. In neuroengineering, TRF models can be integrated into hearing aids for real-time decoding of auditory attention, enhancing user experience by focusing on attended speech in noisy environments [11].\n\n**9. Advanced Computational Techniques:** [2]. \nRecent advancements incorporate machine learning techniques to refine TRF models further. For instance, deep learning approaches like convolutional neural networks (CNNs) have been used to automatically learn complex features from raw EEG data, improving the robustness and accuracy of decoding auditory attention. Long short-term memory networks (LSTMs) have also been employed to capture temporal dependencies in the data, allowing for better prediction of neural responses to continuous speech streams. These methods enhance the ability to decode auditory attention in real-time and can be particularly useful in dynamic and noisy listening environments [11][13].\n\n**10. Broader Range of Applications:**  \nTemporal response function (TRF) models have been applied in various auditory research contexts to understand the transformation of acoustic stimuli to neural responses. The TRF describes the mapping from the stimulus envelope to EEG activity, and it can be used in both forward and backward models to predict neural responses or reconstruct the stimulus from neural data. This approach has been utilized to investigate speech perception in noisy environments, auditory attention decoding, and other cognitive processes related to auditory stimuli [5]. \nTRF models have also found applications in understanding multisensory integration, such as the integration of auditory and tactile stimuli. For instance, studies have utilized TRFs to investigate how vibrotactile stimulation can enhance speech-in-noise comprehension by analyzing the combined auditory and tactile response functions [10]. Moreover, TRF analysis has been employed in cognitive neuroscience to study attention modulation and the neural representation of various acoustic features, such as rhythm, pitch, and timbre, contributing to a deeper understanding of auditory perception [14].\n\nPotential Limitations and Challenges [3]. \n\n**11. Non-linear Dynamics:** The non-linearity inherent in the human auditory system can be addressed to some extent in the preprocessing stage, such as during speech envelope extraction (Zwicker and Fastl, 2013) [11].\nOne significant limitation of TRF analysis is that it is based on a linear model. However, the human auditory system exhibits non-linear dynamics, and this linear assumption may not fully capture the complexities of neural processing. This limitation can affect the accuracy and generalizability of TRF models, especially in more complex auditory scenarios [11].\n\nNoise sensitivity in EEG-based auditory attention decoding can be a significant challenge. Several studies have shown that temporal response functions (TRFs) are affected by the signal-to-noise ratio (SNR) of the stimulus. For instance, variations in the TRF morphology have been shown to correlate strongly with the level of noise added to the speech signal, impacting the accuracy of EEG predictions (4). Additionally, the amplitude and latency of TRF responses are sensitive to stimulus amplitude and SNR levels, affecting the decoding accuracy (3).\nTRF models can be sensitive to noise and artifacts in the EEG data. Although preprocessing steps aim to mitigate these issues, residual noise can still impact the accuracy of TRF estimation. Advanced denoising techniques and robust preprocessing pipelines are crucial to address this challenge [7].\n\n**13. Individual Variability:**  \nThere is considerable individual variability in neural responses to auditory stimuli, influenced by factors such as age, hearing ability, and cognitive functions. This variability can complicate the generalization of TRF models across different populations, necessitating individualized calibration and validation [12].\n\nTemporal Resolution: \n\nTemporal resolution in EEG studies commonly refers to the ability to resolve the timing of neural events with precision, which is often evaluated via temporal response functions (TRFs). TRFs describe the transformation of the ongoing stimulus to the ongoing EEG activity by modeling the relationship between the temporal envelopes of stimuli and neural responses [0][1][3][5]. These functions can be used both in forward modeling, predicting EEG responses from stimuli, and backward modeling, reconstructing stimuli from EEG responses [7][11]. The accuracy of these models is typically validated through the Pearson correlation between the actual and predicted signals [1][7][12]. Adjustments to regularization parameters, often optimized through cross-validation, help improve the model fit and reduce overfitting [2][12]. The temporal precision of TRFs allows for detailed analysis of neural encoding and decoding processes at specific time lags relative to stimulus events [0][5]. \n\n\nThe temporal resolution of TRF models is determined by the sampling rate and the range of time lags considered. While higher sampling rates can provide finer temporal details, they also increase computational complexity and data storage requirements. Balancing temporal resolution with practical constraints is an ongoing challenge in TRF analysis [5].\n\nTemporal response function (TRF) represents the mapping from acoustic envelope onto each channel of EEG data (forward modeling). A multivariate reconstruction filter can be applied to data from all channels to estimate the acoustic envelope (backward modeling) [1]. This modeling approach yields a decoder that attempts to reconstruct a univariate stimulus feature, such as the speech envelope [1]. The resulting value of the TRF at a specific time lag indexes the relationship between the stimulus feature and the neural response at that time [5]. The performance of these models is typically validated using the Pearson correlation value between the target signal and the predicted signal [7]. \n\nIn summary, TRF is a powerful technique for linking the characteristics of acoustic stimuli to neural responses obtained via iEEG [7]. The process involves meticulous data acquisition, sophisticated modeling techniques (both forward and backward) [1], and rigorous validation protocols [12]. The insights gained from TRF analyses not only advance our understanding of auditory speech perception [13] but also pave the way for innovations in auditory prosthetics, clinical diagnostics, and multisensory integration research [10]. Nonetheless, researchers must carefully consider the potential limitations and challenges associated with TRF analysis to ensure accurate and reliable results. \n\nFor further reading and comprehensive reviews on the subject, refer to Lalor and Foxe (2010) [0], Di Liberto et al. (2015) [1], and O'Sullivan et al. (2014) [6].\n\n\nReferences:\n[0] Testing the Limits of the Stimulus Reconstruction Approach: Auditory Attention Decoding in a Four-Speaker Free Field Environment . . T denotes points in time and n ¼ 1 . . . N denotes EEG channels) is the result of a convolution of the acoustic stimulation s(t), that is, the speech envelope, with an unknown, channel-specific temporal response function (TRF) wð, nÞ. The TRF can be seen as a filter describing the transformation of the ongoing stimulus to the ongoing EEG activity. The TRF describes the transformation for a specified range of time lags relative to the instantaneous occurrence of the stimulus feature. Those time lags result from the fact that typical patterns of ERPs to certain stimuli appear with specific latencies. This convolution is represented in the following equation: where ðt, nÞ denotes additional neural activity not explained by the model. For present purposes, this forward model can be used in reverse as a backward model to reconstruct the stimulus envelope from the recorded neural activity. To reconstruct the speech envelope, the decoder gð, nÞ, which allows a linear mapping of the EEG channels r(t, n) to the speech envelope s(t), has to be computed. This process can be denoted using the following expression: Here,ŝðtÞ represents the reconstructed version of the speech envelop s(t). The decoder is calculated by the following matrix operations: Here, R represents the lagged time series of the neural response matrix r. The following equation defines R as an example for one channel only. Here, I represents the identity matrix and is a smoothing constant or ridge parameter. The ridge parameter can be adjusted using a leave-one-out cross-validation process\n\n[1] Pushing the Envelope: Developments in Neural Entrainment to Speech and the Biological Underpinnings of Prosody Perception temporal response function (mTRF), which uses the spectrogram representation to evaluate speech encoding. Even in the multivariate domain, forward modeling still maps the stimulus to each response channel independently. The temporal response function (TRF)-calculated with a linear least squares approach-represents the mapping from acoustic envelope onto each channel of EEG data (forward modeling). A multivariate reconstruction filter can be applied to data from all channels to estimate the acoustic envelope (backward modeling). Reconstruction accuracy can be measured by Pearson correlation between original and reconstructed envelopes. Figure reproduced from. Backward modeling is a mathematical representation of the linear mapping from the multivariate neural response back to the stimulus. This modeling approach yields a decoder that attempts to reconstruct a univariate stimulus feature, such as the speech envelope. As described in, this decoder function is derived by minimizing the mean squared error between the stimulus and reconstruction. In the backward direction, recording channels are weighted based on the information that they provide for the reconstruction, which removes inter-channel redundancies-an advantage over forward modeling. By modeling in the backward direction, researchers are able to compare stimulus reconstructions to the original stimulus, for instance with a correlation coefficient as a marker of reconstruction accuracy. This provides a reliable index for the degree to which the envelope is encoded in the neural network. While other methods-such as cross-correlations and inter-trial phase coherence-are adequate for measuring phase-locking in speech comprehension, the modeling approach has been gaining attention as an attractive analysis method in recent years.\n\n[2] Poor early cortical differentiation of speech predicts perceptual difficulties of severely hearing-impaired listeners in multi-talker environments change of the EEG signal 100 ms after a change in the stimulus envelope. Multivariate TRFs (or mTRFs) refer to modeling approaches that use multiple speech features (such as multiple speech streams, or representations of other acoustic features), whereafter each speech feature's model can be assessed separately. Here, the attended and ignored speech stream were used as input features. We used the mTRF toolbox and procedures outlined by Crosse et al. 35 in MATLAB to estimate TRFs over an integration window of −50 to 350 ms using ridge regression. We note here that the integration window is separate from the actual 11-second time epochs of EEG data. The integration window represents the range of time lags (or delays) between the epoch of EEG data and the stimulus envelope. Before TRF estimation, epoched EEG data were filtered from 1 to 8 Hz 36 . The speech envelope was calculated by taking the absolute value of the Hilbert transform for reconstructed speech signals. Time alignment of the speech signal to the EEG recording was conducted based on trigger events that were saved during EEG recording. Ridge regression is associated with a regularization parameter that reduces overfitting the speech envelope to noise of the neural signal. N-fold cross-validation was used to optimize the regularization parameter across individuals 34 . During this procedure, TRFs are estimated for each individual for all but one of N trials. The resulting model is then used to predict the EEG signal during the trial that was originally omitted. Linear correlation (Pearson's r) assessed the\n\n[3] Robust EEG-Based Decoding of Auditory Attention With High-RMS-Level Speech Segments in Noisy Conditions Temporal response functions were used to explain how the cortical responses encoded speech stimuli (e.g., Ding and Simon, 2012;Di Liberto et al., 2015;Broderick et al., 2019). TRF responses were regarded as a filter that described the linear mapping between the temporal envelopes of stimuli and neural responses in this study. The TRF responses were analyzed to describe the relationship between the target speech and corresponding EEG signals. The magnitude and latency of TRF responses were sensitive to the stimulus amplitude, i.e., the increased TRF magnitude and the decreased TRF latency were shown with the augment of stimulus amplitude (e.g., Drennan and Lalor, 2019). The TRF responses affected by the SNR level and the speech temporal envelope other than the stimulus amplitude were mainly investigated in this study. In this study, the amplitudes of target speech were normalized across SNR conditions, while the SNR levels of the mixed stimuli were generated by different amplitudes of the ignored speech. Additionally, the speech envelopes extracted from the intact speech and high-RMS-level segments were normalized before the TRF calculations. The amplitude-normalized envelopes between the intact speech and high-RMS-level segments reduced the effects of stimulus amplitude on TRF responses. TRFs estimation was performed using the mTRF toolbox (Crosse et al., 2016). The encoding model for stimuli and corresponding neural responses is represented by the following equation: where the TRF -w (τ, n) -models the transformation for a specified range of time lags, τ, relative to the stimulus feature, s (t), at the instantaneous time; s (t) represents the speech envelope at\n\n[4] Prediction of Speech Intelligibility by Means of EEG Responses to Sentences in Noise Objective Understanding speech in noisy conditions is challenging even for people with mild hearing loss, and intelligibility for an individual person is usually evaluated by using several subjective test methods. In the last few years, a method has been developed to determine a temporal response function (TRF) between speech envelope and simultaneous electroencephalographic (EEG) measurements. By using this TRF it is possible to predict the EEG signal for any speech signal. Recent studies have suggested that the accuracy of this prediction varies with the level of noise added to the speech signal and can predict objectively the individual speech intelligibility. Here we assess the variations of the TRF itself when it is calculated for measurements with different signal-to-noise ratios and apply these variations to predict speech intelligibility. Methods For 18 normal hearing subjects the individual threshold of 50% speech intelligibility was determined by using a speech in noise test. Additionally, subjects listened passively to speech material of the speech in noise test at different signal-to-noise ratios close to individual threshold of 50% speech intelligibility while an EEG was recorded. Afterwards the shape of TRFs for each signal-to-noise ratio and subject were compared with the derived intelligibility. Results The strongest effect of variations in stimulus signal-to-noise ratio on the TRF shape occurred close to 100 ms after the stimulus presentation, and was located in the left central scalp region. The investigated variations in TRF morphology showed a strong correlation with speech intelligibility, and we were able to predict the individual threshold of 50% speech intelligibility with a mean deviation of less then 1.5 dB. Conclusion The intelligibility of speech in noise can be predicted by analyzing the shape of the TRF derived from different stimulus signal-to-noise ratios. Because TRFs are interpretable, in a manner similar to auditory evoked potentials, this method offers new options for clinical diagnostics.\n\n[5] The Multivariate Temporal Response Function (mTRF) Toolbox: A MATLAB Toolbox for Relating Neural Signals to Continuous Stimuli thought of as a filter that describes the linear transformation of the ongoing stimulus to the ongoing neural response. The TRF, w(τ , n), describes this transformation for a specified range of time lags, τ , relative to the instantaneous occurrence of the stimulus feature, s(t). In the context of speech for example, s(t) could be a measure of the speech envelope at each moment in time and r(t, n) could be the corresponding EEG response at channel n. The range of time lags over which to calculate w(τ , n) might be that typically used to capture the cortical response components of an ERP, e.g., −100-400 ms. The resulting value of the TRF at −100 ms, would index the relationship between the speech envelope and the neural response 100 ms earlier (obviously this should have an amplitude of zero), whereas the TRF at 100 ms would index how a unit change in the amplitude of the speech envelope would affect the EEG 100 ms later (Lalor et al., 2009). The TRF, w(τ , n), is estimated by minimizing the meansquared error (MSE) between the actual neural response, r(t, n), and that predicted by the convolution,r(t, n): In practice, this is solved using reverse correlation (De Boer and Kuyper, 1968), which can be easily implemented using the following matrix operations: where S is the lagged time series of the stimulus property, s, and is defined as follows: The values τ min and τ max represent the minimum and maximum time lags (in samples) respectively. In S,\n\n[6] Prediction of Speech Intelligibility by Means of EEG Responses to Sentences in Noise a good approximation by the acoustic envelope of the stimulus. Since TRF describes the impulse response, once determined, it can be used to predict the EEG response to any input signal. Conversely, the response function could be estimated in the backward direction (EEG → Speech). The estimated backward model could then be used to reconstruct the stimulus from the EEG, accordingly this method is known as stimulus reconstruction (O'Sullivan et al., 2014). The accuracy of prediction, or reconstruction, can be estimated as the correlation coefficient between the derived and the predicted EEG, or between the original and the reconstructed envelope. The relationship between recognition of speech in noise and the accuracy of stimulus reconstruction has been the focus of several recent studies (Etard and Reichenbach, 2019;Iotzov and Parra, 2019;Zou et al., 2019). Vanthornhout et al. (2018) investigated stimulus reconstruction accuracy for measurements with different SNRs using EEG measurements from 64 electrodes. They found a strong correlation between speech envelope reconstruction and actual speech envelopes. The method was refined by Lesenfants et al. (2019) using a forward model and the prediction accuracy of selected electrodes. They were able to predict individual's speech recognition thresholds (i.e. the SNR at which 50 % is understood, SRT 50 ) with an accuracy of 1-2 decibels. In all of the studies mentioned, EEG measurements were used for speech in quiet to estimate the TRF, and these TRFs were applied to SiN measurements. Hence, those authors did not take account of possible changes in TRF morphology caused by the noise, even though\n\n[7] Deep Correlation Analysis for Audio-EEG Decoding scalp level measurements, the recordings are significantly impacted by noise. The most popular method for analyzing auditory invoked EEG signals is the classical event-related potential (ERP) approach,. This approach involves averaging the EEG responses in time/frequency domain to suppress the noise in the recordings. However, this approach is limited to isolated stimuli that have to be repeated and is therefore often restrictive for use in the analysis of natural stimuli like speech and music. One of the first successful attempts in this direction is the temporal response function (TRF) proposed by Lalor et al.. The linear TRF model describes the relationship between a stimulus and its response as a linear time-invariant (LTI) system. It can be a linear forward model where the model estimates the EEG response for the stimulus or a backward model where the model predicts the components of the stimulus from the EEG response. The model estimation is performed using linear least squares. The performance of these models is typically validated using the Pearson correlation value between the target signal and the predicted signal. The initial studies used the slowly varying speech envelopes of the stimuli and the corresponding single-trial EEG responses,. The analysis can also be extended to speech spectrograms, phonemes, or semantic features. The Canonical Correlation Analysis (CCA) is an extension of the linear methods for analysis. Here, two signals are projected onto a subspace that maximizes the correlation between them. It determines a set of orthogonal directions on\n\n[8] Neural Speech Tracking in the Theta and in the Delta Frequency Band Differentially Encode Clarity and Comprehension of Speech in Noise modeled the neural activity at each electrode and in each frequency band as the linear superposition of three features. First, we accounted for the acoustic background through the envelope of the babble noise y B . Second, we considered a feature that reflected the speech clarity by multiplying the envelope of the target speech y T by the clarity value a. Third, comprehension was described through a feature that multiplied the envelope of the target speech y T by the comprehension value c. Multiplying the speech envelope by the clarity as well as by the comprehension of speech thereby yielded two continuous features that encoded the corresponding speech property and that could be related to neural responses through temporal response functions (TRFs). The estimated EEG response ê j at channel j thus follows as where ‫ء‬ is the convolution symbol and where T B ͑ j͒ , T A ͑ j͒ , and T C ͑ j͒ denote the TRF for the background noise, speech clarity and speech comprehension, respectively. The TRFs were computed using either the EEG data and the speech envelope both processed by noncausal filters (delta, theta, and alpha), or by using the EEG data and the speech envelope both processed by the causal filters. For each subject, the data from all eight acoustic conditions was pooled together, and the model was fitted using a regularized ridge linear regression. The obtained TRFs were then averaged over all subjects. To assert the significance of the obtained TRFs, empirical null distributions were established by shuffling\n\n[9] Enhancement of speech-in-noise comprehension through vibrotactile stimulation at the syllabic rate computed two linear regression models, similarly to previous analysis on multisensory integration related to audiovisual speech processing (15,16,41). The first model estimated the EEG recordings from the envelope of the speech signal, an important quantity that relates to the speech rhythms, shifted by different temporal latencies. The coefficients of the resulting linear model are referred to as Temporal Response Functions (TRFs) and quantify the contribution of the speech envelope at a particular latency to explain the EEG recording at a particular electrode (Fig. 3A). Using these coefficients, we can then estimate the EEG response. By computing the Pearson correlation coefficient between the estimated and the actual EEG response from a subject, we can obtain a reconstruction score. From the envelope-TRFs associated to the audio-only condition, we identified three latencies at which the speech envelope yielded particularly large contributions-that is, at which the envelope-TRFs peaked. These latencies were 98 ms, 154 ms, and 268 ms. The topographic maps at these latencies showed an approximately symmetric response between the two hemispheres. We then estimated the multisensory gain. First, we computed unisensory models of the envelope-TRFs obtained in the audioonly and in the tactile-only conditions. These unisensory models were then shifted appropriately and summed to create additive models for each of the multisensory conditions. We then computed the TRF for each audiotactile condition using the corresponding envelopes simultaneously, therefore obtaining a multisensory model for each of the multisensory conditions. We subtracted the additive models from the multisensory models, yielding the multisensory gain related to the speech envelope. We then\n\n[10] Extracting the Auditory Attention in a Dual-Speaker Scenario From EEG Using a Joint CNN-LSTM Model could generate system response functions that characterize the auditory pathway in the forward direction. These system response functions are referred to as temporal response function (TRF) (Lalor and Foxe, 2010). Analysis of the shape of TRFs has revealed that the human brain encodes the attended speaker different to that of the ignored speaker. Specifically, TRFs corresponding to the attended speaker have salient peaks around 100 and 200 ms which are weaker in TRFs corresponding to the ignored speaker (Fiedler et al., 2019;Kuruvila et al., 2021). Similar attention modulation effects were observed when the acoustic input was modified to using speech spectrogram or higher level features such as phonetics (Di Liberto et al., 2015). Likewise using backward models, the input stimulus can be reconstructed from EEG signals (stimulus reconstruction method) and a listener's attention could be inferred by comparing the reconstructed stimulus to the input stimuli (O'Sullivan et al., 2014). These findings give the possibility of integrating AAD algorithms into hearing aids which in combination with robust speech separation algorithms could greatly enhance the amenity provided to the users. It has been well-established that the human auditory system is inherently non-linear (Zwicker and Fastl, 2013) and AAD analysis based on linear systems theory addresses the issue of non-linearity to a certain extend in the preprocessing stage. For example, during speech envelope extraction. Another limitation of linear methods is the longer time delay required to classify attention (Fuglsang et al., 2017;Geirnaert et al., 2019), although there were attempts to overcome this limitation (Miran et al., 2018;Kuruvila et al.,\n\n[11] Cortical Tracking of Speech in Delta Band Relates to Individual Differences in Speech in Noise Comprehension in Older Adults. gammatone filters and used as features to map onto the recorded EEG using multivariate linear ridge regression. In an iterative process, the TRFs are trained on a set of training data. The testing data is predicted using TRFs derived from the training data. The correlation (r-values) between the testing data and predicted data is obtained using Pearson's product-moment correlation. This is performed using k-fold cross-validation. The TRF and correlation values are estimated for each EEG channel and stimulus segment. The electrodes (marked as larger dots on the topographic plot) showing the strongest r-values were used to get an estimate of cortical tracking. TRF indicates Temporal Response Functions. of working memory and PTA were regressed out from RIN and speech in noise comprehension scores. We fit separate linear models to RIN ( ) and speech in noise comprehension scores ( ), with PTA for each ear (separated by the better and poorer ear) and the combined Digit Span score (sum of Forward Digit Span and Backward Digit Span) as predictors in each linear model. The residuals of these models served as measures of RIN and speech in noise comprehension that were adjusted for individual differences in PTA and working memory. Using the residuals of these models, Spearman's rank correlations were calculated to assess the relationship between the adjusted RIN within each frequency band and speech in noise comprehension when controlling for PTA and working memory. In the delta band, RIN and comprehension in the noise condition were significantly negatively correlated (ρ = −0.656, p = 0.003). However,\n\n[12] A Comparison of Temporal Response Function Estimation Methods for Auditory Attention Decoding A Comparison of Temporal Response Function Estimation Methods for Auditory Attention Decoding The decoding of selective auditory attention from noninvasive electroencephalogram (EEG) data is of interest in brain computer interface and auditory perception research. The current state-of-the-art approaches for decoding the attentional selection of listeners are based on temporal response functions (TRFs). In the current context, a TRF is a function that facilitates a mapping between features of sound streams and EEG responses. It has been shown that when the envelope of attended speech and EEG responses are used to derive TRF mapping functions, the TRF model predictions can be used to discriminate between attended and unattended talkers. However, the predictive performance of the TRF models is dependent on how the TRF model parameters are estimated. There exist a number of TRF estimation methods that have been published, along with a variety of datasets. It is currently unclear if any of these methods perform better than others, as they have not yet been compared side by side on a single standardized dataset in a controlled fashion. Here, we present a comparative study of the ability of different TRF estimation methods to classify attended speakers from multi-channel EEG data. The performance of the TRF estimation methods is evaluated using different performance metrics on a set of labeled EEG data from 18 subjects listening to mixtures of two speech streams. INTRODUCTION A fundamental goal of auditory neuroscience is to understand the mapping between auditory stimuli and 19 the cortical responses they elicit. In magneto/electro-encephalography (M/EEG) studies, this mapping\n\n[13] Distinct Roles for Onset and Sustained Activity in the Neuronal Code for Temporal Periodicity and Acoustic Envelope Shape Auditory neurons are selective for temporal sound information that is important for rhythm, pitch, and timbre perception. Traditional models assume that periodicity information is represented either by the discharge rate of tuned modulation filters or synchrony in the discharge pattern. Compelling evidence for an invariant rate or synchrony code, however, is lacking and neither of these models account for how the sound envelope shape is encoded. We examined the neuronal representation for envelope shape and periodicity in the cat central nucleus of the inferior colliculus (CNIC) with modulated broadband noise that lacks spectral cues and produces a periodicity pitch percept solely based on timing information. The modulation transfer functions of CNIC neurons differed dramatically across stimulus conditions with identical periodicity but different envelope shapes implying that shape contributed significantly to the neuronal response. We therefore devised a shuffled correlation procedure to quantify how periodicity and envelope shape contribute to the temporal discharge pattern. Sustained responses faithfully encode envelope shape at low modulation rates but deteriorate and fail to account for timing and envelope information at high rates. Surprisingly, onset responses accurately entrained to the stimulus and provided a means of encoding repetition information at high rates. Finally, we demonstrate that envelope shape information is accurately reflected in the population discharge pattern such that shape is readily discriminated for repetition frequencies up to ∼100 Hz. These results argue against conventional rate- or synchrony-based codes and provide two complementary temporal mechanisms by which CNIC neurons can encode envelope shape and repetition information in natural sounds.\n\n"}
{"case_id": "9931fdf53f47af01cd84e446d5676292", "answer_text": "\n\nIn the field of signal processing, the sensitivity to noise detection can vary significantly between first-order and second-order operators due to their inherent characteristics and the types of information they are designed to extract [0].\n\nFirst-order operators primarily detect changes in signal properties such as amplitude or phase. They are typically used in various applications like edge detection in images or identifying simple signal variations. However, first-order operators are often less robust in the presence of noise, as they tend to be influenced by high-frequency variations that are characteristic of noise, leading to less accurate signal detection in noisy environments [12].\n\nOn the contrary, second-order operators are designed to capture more complex structures within the signal, such as gradients or changes in the second derivative of the signal. They are particularly useful for identifying features that are not apparent in the first-order data, such as texture or more sophisticated motion patterns in images and videos [5]. Due to their design, second-order operators can be more sensitive to noise detection. For example, the use of higher-order correlations, such as second- and third-order correlations, has been shown to significantly reduce noise in signal processing applications. This is because higher-order correlations can identify and filter out noise more effectively, as they are less affected by the random noise patterns that typically affect first-order correlations [3].\n\nMoreover, studies have demonstrated that second-order processing mechanisms can be distinct and more effective under certain conditions. For instance, in motion processing tasks, second-order filters have been found to better handle temporally extended noise, making them more sensitive under such conditions compared to first-order filters [8]. Similarly, in image processing, second-order methods, such as the Laplacian of Gaussian (LoG) operator, have shown better performance in filtering out noise and detecting edges compared to first-order methods like the Sobel operator [12]. Quantitative studies have shown that the Peak Signal to Noise Ratio (PSNR) for images processed with LoG is significantly higher compared to Sobel, indicating better noise handling capabilities [12].\n\nSecond-order detection techniques have been employed in various advanced applications. For example, the use of second-order stochastic resonance (SR) methods in detecting weak signals demonstrates the effectiveness of second-order operators in noisy environments. These methods optimize noise intensity to enhance the detection capabilities of signals buried under noise, which would be challenging for first-order methods to achieve [10]. Experimental results have indicated that second-order SR methods significantly improve the Signal-to-Noise Ratio (SNR) over first-order methods, thereby validating their superior sensitivity to noise [10].\n\nFurther, in optical fiber sensor technology, higher-order interference noise is a critical issue. Research has shown that second-order interference can become a dominant factor in determining the signal-to-noise ratio (SNR) in sensor arrays. Reducing second-order noise can significantly enhance the performance of these sensors [1]. This highlights the importance of second-order considerations in practical applications where noise is a prevalent issue. Similarly, in biochemical signal processing, noise-induced bistability shows that noise can induce complex behavior in biological systems, which is better understood and managed through second-order processing [6]. The increased complexity in these systems necessitates the use of second-order models to accurately capture the effects of noise on signal stability and behavior.\n\nHowever, it is also essential to note that the increased complexity of second-order operators comes with higher computational costs and the potential need for more sophisticated algorithms to fully leverage their capabilities [2]. The computational demands for second-order processing can be significantly higher than those for first-order methods due to the need to compute higher-order derivatives or correlations [2]. This increased complexity can lead to longer processing times and greater use of computational resources, which might not be feasible in real-time applications or on devices with limited processing power [12]. Additionally, the implementation of second-order operators may require more advanced algorithmic frameworks and optimization techniques to manage the larger data sets and more intricate calculations involved [2]. \n\nIn addition to first- and second-order operators, other popular methods for addressing noise detection include third-order statistics and machine learning-based approaches. Third-order methods, such as the third-order correlation techniques, have been found useful in reducing the effects of noise in applications like infrared spectroscopy and signal detection under various noise conditions [14]. Machine learning approaches can also be effective, leveraging large datasets and sophisticated models to distinguish between signal and noise in complex environments.\n\nIn summary, second-order operators are generally more sensitive to noise detection compared to first-order operators due to their ability to capture more complex signal structures and reduce the impact of random noise. This sensitivity is particularly advantageous in applications requiring robust signal detection amidst significant noise [3][8][12]. However, the choice between first-order, second-order, and other advanced methods should be guided by the specific context and requirements of the signal processing task, including considerations of computational complexity and resource availability.\n\nKey Points: First- and second-order motion mechanisms are distinct at low but common at high temporal frequencies [0]. There is no consensus on the type of nonlinearity enabling motion processing of second-order stimuli. Some authors suggest that a nonlinearity specifically applied to second-order stimuli prior to motion processing (e.g., rectification process) recovers the spatial structure of the signal permitting subsequent first-order motion analyses (e.g., filter-rectify-filter model). Others suggest that nonlinearities within motion processing enable first-order-sensitive mechanisms to process second-order stimuli (e.g., gradient-based model). In the present study, we evaluated intra- and inter-attribute interactions by measuring the impact of dynamic noise modulators (either luminance (LM) or contrast-modulated (CM)) on the processing of moving LM and CM gratings [0]. When the signal and noise were both of the same type, similar calculation efficiencies but different internal equivalent noises were observed at all temporal frequencies [0]. At high temporal frequencies, each noise type affected both attributes by similar proportions suggesting that both attributes are processed by common mechanisms [0]. Conversely, at low temporal frequencies, each noise type primarily impaired the processing of the attribute of the same type suggesting distinct mechanisms [0]. We therefore conclude that two fundamentally different mechanisms are processing CM stimuli: one low-pass and distinct from the mechanisms processing LM stimuli and the other common to the mechanisms processing LM stimuli [0]. \n\nFirst-order operators are less robust against noise due to their susceptibility to high-frequency variations [12].\n- Second-order operators handle complex structures better, making them more effective in noisy environments [1]. \nHigher-order correlations in second-order methods significantly reduce noise [3].\nAdvanced applications, like motion processing and optical fiber sensors, benefit from second-order operators [0, 1]. \nThe complexity and computational demand of second-order methods should be considered [2]. \nAlternative methods, like third-order statistics and machine learning, also offer robust noise detection capabilities [3].\n\n\nReferences:\n[0] First- and second-order motion mechanisms are distinct at low but common at high temporal frequencies. First- and second-order motion mechanisms are distinct at low but common at high temporal frequencies. There is no consensus on the type of nonlinearity enabling motion processing of second-order stimuli. Some authors suggest that a nonlinearity specifically applied to second-order stimuli prior to motion processing (e.g., rectification process) recovers the spatial structure of the signal permitting subsequent first-order motion analyses (e.g., filter-rectify-filter model). Others suggest that nonlinearities within motion processing enable first-order-sensitive mechanisms to process second-order stimuli (e.g., gradient-based model). In the present study, we evaluated intra- and inter-attribute interactions by measuring the impact of dynamic noise modulators (either luminance (LM) or contrast-modulated (CM)) on the processing of moving LM and CM gratings. When the signal and noise were both of the same type, similar calculation efficiencies but different internal equivalent noises were observed at all temporal frequencies. At high temporal frequencies, each noise type affected both attributes by similar proportions suggesting that both attributes are processed by common mechanisms. Conversely, at low temporal frequencies, each noise type primarily impaired the processing of the attribute of the same type suggesting distinct mechanisms. We therefore conclude that two fundamentally different mechanisms are processing CM stimuli: one low-pass and distinct from the mechanisms processing LM stimuli and the other common to the mechanisms processing LM stimuli.\n\n[1] Higher-order interference of low-coherence optical fiber sensors. Higher-order interference of low-coherence optical fiber sensors. The higher-order interference noise that is caused by multireflection at the fiber sensor's end surface in low-coherence fiber sensor array is proposed. The generation of the higher-order interference noise and its quantity and amplitude are theoretically analyzed. The second-order interference noises are experimentally demonstrated. The results show that the second-order noises arise in any sensor array composed of more than two sensors and the number of the second-order peaks is proportional to the third power of the sensor's number. The ratio of the amplitude of the second-order noise to that of the signal peak is proportional to the reflectivity of the sensor's end surface. In a sensor array, when the reflectivity is more than 10(-5), the amplitude of the second-order noise is higher than other noises and it becomes a main factor that determines the signal-to-noise ratio of the sensor arrays. Therefore, reducing the higher-order interference noise can improve the multiplexing capacity of the sensor array.\n\n[2] First- and second-order high probability complexity bounds for trust-region methods with noisy oracles We have proposed first- and second-order modified trust-region algorithms for solving noisy (possibly stochastic) unconstrained nonconvex continuous optimization problems. The algorithms utilize estimates of function and derivative information computed via noisy probabilistic zeroth-, first- and second-order oracles.The noise in these oracles is not assumed to be smaller than constants ϵf,ϵgsubscriptitalic-ϵ𝑓subscriptitalic-ϵ𝑔\\epsilon_{f},\\epsilon_{g} and ϵHsubscriptitalic-ϵ𝐻\\epsilon_{H}, respectively.We show that the first-order method (Algorithm 1) can find an ϵitalic-ϵ\\epsilon-first-order stationary point with high probability after 𝒪​(ϵ−2)𝒪superscriptitalic-ϵ2{\\cal O}(\\epsilon^{-2}) iterations for any ϵ≥[𝒪​(ϵf)+𝒪​(ϵg)italic-ϵdelimited-[𝒪subscriptitalic-ϵ𝑓𝒪subscriptitalic-ϵ𝑔\\epsilon\\geq[{\\cal O}(\\sqrt{\\epsilon_{f}})+{\\cal O}(\\epsilon_{g}),and that the second-order method (Algorithm 2) can find an ϵitalic-ϵ\\epsilon-second-order critical point for any ϵ≥[𝒪​(ϵf3)+𝒪​(ϵg)+𝒪​(ϵH)italic-ϵdelimited-[𝒪3subscriptitalic-ϵ𝑓𝒪subscriptitalic-ϵ𝑔𝒪subscriptitalic-ϵ𝐻\\epsilon\\geq[{\\cal O}(\\sqrt{\\epsilon_{f}})+{\\cal O}(\\sqrt{\\epsilon_{g}})+{\\cal O}(\\epsilon_{H}) after 𝒪​(ϵ−3)𝒪superscriptitalic-ϵ3{\\cal O}(\\epsilon^{-3}) iterations.Numerical experiments on standard derivative-free optimization problems and problems with adversarial noise illustrate the performance of the modified trust-region algorithms.\n\n[3] Improved identification of noisy spectra using higher-ordered correlation spectral analysis. Improved identification of noisy spectra using higher-ordered correlation spectral analysis. We used a higher-order correlation-based method of comparison for spectral identification. Higher-order correlations are an extension of the more familiar second-order cross-correlation function and have the significant advantage of being theoretically shown to eliminate noise of unknown spectral density under certain conditions. Specifically, we applied a third-order correlation technique to the identification of similar IR spectra in the presence of noise. We were able to reduce the effects of noise from a second-order correlation measurement by further processing the measurement with a third-order autocorrelation. Our results showed that the third-order correlation-based method increased the probability of detection of a spectrum in the presence of noise, when compared to using a second-order technique alone. The probability of detection increased enough at low signal-to-noise ratios that this technique may be useful when a second-order correlation technique is not acceptable. The third-order technique is applicable to a single experiment, but improved results were found by averaging the results of multiple experiments.\n\n[4] Attention alters spatial resolution by modulating second-order processing Oruç, 2002;Sutter, Sperling, & Chubb, 1995). Normally, both first-and second-order filters simultaneously contribute to task performance, and therefore, their separate influences cannot be disentangled. Thus, to isolate the influence of second-order filters, texture stimuli have been designed to contain only texture-defined (second-order) boundaries, rendering them invisible to luminance-based (first-order) mechanisms. This has been achieved for both dynamic and static stimuli. For instance, Chubb and Sperling (1988, 1989, 1991 constructed second-order motion stimuli devoid of first-order motion by ensuring that stimuli contained equal Fourier energy (i.e., the output of first-order spatiotemporal mechanisms) in opposite motion directions, thereby precluding first-order filters from signaling any apparent motion. Static secondorder stimuli can be constructed by arranging highfrequency Gabor elements (first-order content) in lowfrequency patterns (second-order content; Graham et al., 1993) or by modulating the luminance contrast of carrier noise (first-order content) with a Gabor pattern at a particular SF (Sutter et al., 1995); both methods yield second-order textures that are invisible to firstorder mechanisms and activate a narrow band of second-order filters. Yeshurun and Carrasco (2000) used these two kinds of static stimuli to investigate whether exogenous attention automatically enhances spatial resolution by modulating first-or second-order filters. They found that manipulating first-order content did not alter the attentional effect whereas increasing the SF of second-order content diminished the central attentional impairment and shifted peak performance closer to the fovea. An open question is whether endogenous attention also modifies spatial resolution by modulating secondorder filters. Addressing this question could reveal important mechanistic differences between endogenous and exogenous attention and could help\n\n[5] Nonlinear biochemical signal processing via noise propagation. We consider the model of mutual inhibition between two genes (g1 and g2 in Fig. 4a), which regulate each other non-cooperatively (Hill coefficient = 1). Under these conditions it is impossible for the system to show bistability with first-order degradation in the deterministic case Sauro (2009). Now we consider including noise in the system and show noise-induced bistability. The dissociation constant of S2subscript𝑆2S_{2}, KM2subscript𝐾subscript𝑀2K_{M_{2}}, was varied while that of S1subscript𝑆1S_{1}, KM​1subscript𝐾𝑀1K_{M1}, was fixed at 0.10.10.1 nM. The reason that this KM​1subscript𝐾𝑀1K_{M1} value was chosen is to achieve strong SF and SD in the synthesis rate of S2subscript𝑆2S_{2}, v3subscript𝑣3v_{3}. For KM2≃similar-to-or-equalssubscript𝐾subscript𝑀2absentK_{M_{2}}\\simeq 0.1 nM, another strong SF and SD in v1subscript𝑣1v_{1} appears. This doublet of SF-SD can lead to ultra-sensitive positive feedback (Fig. 4c), leading to bimodality (Fig. 4b). As KM2subscript𝐾subscript𝑀2K_{M_{2}} increases, the SF-SD in v1subscript𝑣1v_{1} becomes weaker and the bimodal distribution becomes uni-modal.\n\n[6] Motion processing: the most sensitive detectors differ in temporally localized and extended noise Motion processing: the most sensitive detectors differ in temporally localized and extended noise Contrast thresholds for discriminating orientation and direction of a drifting, oriented grating are usually similar to contrast detection thresholds, which suggest that the most sensitive detectors are labeled for both orientation and direction (Watson and Robson, 1981). This was found to be true in noiseless condition, but Arena et al. (2013) recently found that this was not true in localized noise (i.e., noise having the same spatiotemporal window as the target) as thresholds for discriminating direction were higher than for discriminating orientation. They suggested that this could be explained by the fact that there are more neurons selective to orientation than direction. Another possible interpretation is that, unlike contrast thresholds in absence of noise, the most sensitive detectors in localized noise were labeled for orientation, but not for direction. This hypothesis is supported by recent findings showing different processes operating in localized and extended noise (i.e., full-screen, continuously displayed noise, Allard and Cavanagh, 2011). In the current study, we evaluated contrast thresholds for orientation and direction discrimination tasks in noiseless conditions, and in noise that was either spatially localized or extended, and temporally localized or extended. We found similar orientation and direction thresholds in absence of noise and in temporally extended noise, but greater direction thresholds in temporally localized noise. This suggests that in noiseless and temporally extended noise the most sensitive detectors were labeled for both orientation and direction (e.g., direction-selective complex cells), whereas in temporally localized noise the most sensitive detectors\n\n[7] Detection Algorithm of BPSK Signal of Parameter-Adjusted Bistable Stochastic Resonance Model Based on Scale Change detection capabilities. Literature proposed a second-order matched stochastic resonance (SMSR) method. By combining the noise intensity optimization and signal frequency synchronization with duffing system, matching parameters are obtained, and weak signals are detected. Literature proposed an underdamped step-varying second-order SR. In this method, you can adjust the second-order parameters to achieve resonance. The above research results have made great achievements in researching the characteristics of noise and improving the matching of resonance, which has a guiding role in the research of this paper. Based on the above literatures, this paper studies the BPSK statistical characteristics under the condition of no prior information, unknown noise types and signal parameters, and performs signal detection based on Neiman-Pearson criterion. Structure arrangement of this study is as follows: introduce basic concepts of BPSK and BSR in Section II; improve BSR through scale change and construct complete flow for signal detection in Section III; design nonlinear detector based on Neyman-Pearson criterion in Section IV; deduce output SNR of system and error rate in Section V; verify effectiveness of method through simulation and compare with mainstream method to highlight advantages of algorithm in Section VI; draw a conclusion and summarize the whole paper in Section VII. The contribution of this study is to extend the traditional SR algorithm for small amplitude periodic signals to the detection of non-periodic high-frequency signals based on scale transformation, which broadens the application range of the algorithm, and significantly improves the ability of weak signal detection. Besides, the signal processing flow based on Neyman-Pearson criterion\n\n[8] Edge Detection Operators: Peak Signal to Noise Ratio Based Comparison Edge detection is the vital task in digital image processing. It makes the image segmentation and pattern recognition more comfort. It also helps for object detection. There are many edge detectors available for pre-processing in computer vision. But, Canny, Sobel, Laplacian of Gaussian (LoG), Robert's and Prewitt are most applied algorithms. This paper compares each of these operators by the manner of checking Peak signal to Noise Ratio (PSNR) and Mean Squared Error (MSE) of resultant image. It evaluates the performance of each algorithm with Matlab and Java. The set of four universally standardized test images are used for the experimentation. The PSNR and MSE results are numeric values, based on that, performance of algorithms identified. The time required for each algorithm to detect edges is also documented. After the Experimentation, Canny operator found as the best among others in edge detection accuracy. Index Terms—Canny operator, Edge Detectors, Laplacian of Gaussian, MSE, PSNR, Sobel operator.\n\n[9] Signal detection in additive Gaussian noise , Doppler frequency shift and propagation fading fluctuations environment by using third-order test statistics Signal detection in additive Gaussian noise , Doppler frequency shift and propagation fading fluctuations environment by using third-order test statistics The problem of detecting a signal embedded in additive Gaussian noise is considered based on statistical decision theory and Neyman-Pearson criterion. The study is carried out for novel test statistics (TSs) estimated that use third-order moment functions. Performances of signal detection for traditional energy second-order TS calculated as the peak of the matched filter output, the second-order TS estimated as the peak value of the matched filter output autocorrelation function and the third-order TSs computed as the peak values of third-order autocorrelation and cross-correlation functions are investigated and compared between each other. Computer simulations have been performed for typical kinds of signals as LFM waveform, radio-frequency pulse, single video pulse of rectangular shape and two pulses of different amplitudes and triangular shape. The performance of signal detection is evaluated and studied by using binary hypothesis and likelihood ratio for the secondand third-order TSs. Probability of detection curves demonstrate that the use of the third-order TSs provides other advantages. They are insensitivity to random signal shifts that can severely worse traditional matched filter performance, good immunity to additive Gaussian noise of a priori unknown spectral density, and robustness to random Doppler frequency shift and magnitude fluctuations in received signals.\n\n"}
{"case_id": "ca229869d669f7985df9e6b2280c7349", "answer_text": "\n\nNon-linear and non-stationary signal processing techniques are essential for analyzing signals that are dynamic and do not adhere to simple linear or time-invariant models. These techniques have been widely applied across various domains, including biomedical engineering, mechanical systems analysis, and geophysics [12]. Here, we explore several such techniques and discuss their effectiveness, particularly in the context of geophysical signals [5].\n\nTechniques in Non-linear, Non-stationary Signal Processing [12].\n\n### 1. Empirical Wavelet Transform (EWT) [0]\nEWT is an adaptive method that decomposes a signal into its constituent wavelet components based on the signal's local frequency content. The improved variant, PCHIP-EWT, combines piecewise cubic Hermite interpolating polynomial with EWT for better handling of noisy and closely-spaced frequency components [0]. By selecting useful sub-bands from the spectrum envelope using maximum points reconstructed on the basis of PCHIP, PCHIP-EWT enhances the representation of signal features while mitigating the effects of noise.\n\nShort-Time Fourier Transform (STFT) is a widely used tool in signal processing, particularly for analyzing non-stationary signals that change over time. It works by dividing a longer time signal into shorter segments of equal length and then applying the Fourier transform to each segment, providing a two-dimensional representation of the signal in both time and frequency domains [3].\nSTFT is a widely-used method where a signal is divided into short segments and then Fourier-transformed to provide time-localized frequency information. One limitation of traditional STFT is its fixed window size, which can result in a trade-off between time and frequency resolutions. Adaptations like the Adaptive STFT enhance time-frequency resolution by dynamically adjusting window functions based on signal characteristics. This dynamic adjustment allows for better resolution of signals with varying frequency content over time, crucial for analyzing transient features in non-stationary signals [1][2].\n\n3. Synchrosqueezing Transform (SST) [1]\nSST refines the time-frequency representation obtained from STFT by concentrating the frequency components, thereby improving the resolution. This method sharpens the time-frequency representation by reassigning the components to their \"true\" time-frequency locations. The adaptive variant optimizes the analysis by using local signal information to dynamically adjust parameters, which enhances its effectiveness for complex non-stationary signals by providing a clearer separation of closely-spaced frequency components [1][7].\n\nWavelet Transform (WT) is an effective tool for processing noisy and non-stationary signals, especially in cases with closely spaced frequency components [0]. The analytical framework of wavelet transform is recognized for its ability to detect transient features from time-varying signals [3]. This makes it particularly useful in applications like bearing health monitoring and other condition monitoring tasks [6].\nWavelet Transform decomposes a signal into a series of wavelets, which are localized in both time and frequency. This multi-resolution analysis capability makes WT particularly useful for analyzing non-stationary signals. For example, the Hilbert-Huang Transform (HHT) is a type of wavelet transform known for effectively handling non-linear, non-stationary signals by decomposing them into intrinsic mode functions (IMFs) through Empirical Mode Decomposition (EMD) [3][10].\n\nKalman Filtering is a method that can be applied to increase the signal message/noise ratio in geophysical data. The Kalman-Bucy method is a time domain treatment of a stochastic non-stationary geophysical process. The derivation of the estimator is based on the relationship between the Kalman-Bucy and Wiener approaches for linear systems. The examples provided in some studies indicate that this method can be used on various geophysical data treatments.[5]\nThe Kalman-Bucy method, a state-space approach, is useful for processing stochastic non-stationary signals. It enhances the signal-to-noise ratio by leveraging a model with a priori information. By providing real-time filtering capabilities, Kalman Filtering is particularly effective for applications where signals are continuously monitored and updated, such as geophysical data treatments where real-time signal enhancement is critical [5].\n\n### 6. Hilbert-Huang Transform (HHT) [6][10][12] \nHHT involves Empirical Mode Decomposition (EMD) followed by Hilbert Spectral Analysis. It is explicitly designed for non-linear and non-stationary signals, making it highly effective for real-world applications. The EMD adaptively decomposes the signal into IMFs, which can then be analyzed using the Hilbert transform to provide time-frequency-energy representations. This adaptability makes HHT particularly effective for analyzing complex geophysical signals [12][10].\n\nEffectiveness in Geophysical Signals [5].\n\n\nGeophysical signals are inherently non-stationary and complex, often influenced by a multitude of dynamic factors [5]. Effective processing of these signals is crucial for applications such as seismic analysis, climate modeling, and fault detection in geological structures [5]. Among the various techniques discussed, the following stand out in terms of effectiveness:\n\n\n### 1. Short-Time Fourier Transform (STFT) [1]\nSTFT provides a good balance between time and frequency resolution, crucial for analyzing the time-varying nature of geophysical signals. Its adaptive variant (Adaptive STFT) enhances resolution by adjusting parameters based on the signal's characteristics, making it particularly useful for tracking transient geophysical events [1][2].\n\nWavelet Transform (WT) is a significant contribution to signal processing, particularly for non-stationary and time-frequency analysis [12]. The method is advantageous in detecting transient features from a time-varying signal, making it highly effective in applications such as bearing health monitoring and biomedical signal processing [6]. This technique offers an improvement over traditional Fourier-based analyses by avoiding the assumption of stationarity and providing a more adaptable framework for analyzing complex signals [10]. Consequently, WT has become a preferred tool in many disciplines where signal non-stationarity is a critical factor [10]. \nThe Wavelet Transform’s ability to provide multi-resolution analysis makes it particularly effective for geophysical signals, which often contain features at multiple scales. WT has been shown to be effective in detecting and characterizing transient features in geophysical data, such as seismic waves generated by various types of geological activities [3][6].\n\n### 3. Hilbert-Huang Transform (HHT) [6] \nHHT is highly suited for non-stationary and non-linear geophysical signals. It provides an adaptive basis for decomposition, allowing for more accurate and meaningful representation of complex geophysical phenomena. HHT’s ability to handle non-linearity and adapt to the underlying signal structure makes it superior in scenarios where other methods may falter, such as in the analysis of seismic data with complex waveforms [12][10].\n\nComparative Analysis of Techniques on Geophysical Signals[3][5][6].\n\n### Resolution [12]\nAdaptive STFT and SST provide higher resolutions by focusing the frequency components more sharply [1]. HHT also offers excellent resolution due to its adaptive decomposition [12].\n\n### Computational Complexity [3]. \nMethods like Adaptive STFT and SST can be computationally intensive due to the dynamic adjustment of parameters [1]. HHT, while powerful, also involves significant computational resources [12]. \n\n### Robustness to Noise [3][8]. \n\nPCHIP-EWT and Kalman Filtering are particularly designed to handle noisy data effectively [0][5]. HHT also demonstrates good robustness due to its empirical mode decomposition [10].\n\n\n### Adaptability [1] \nHHT stands out for its adaptability to the non-linear and non-stationary nature of geophysical signals [12]. Adaptive STFT and SST also show strong adaptive features based on signal characteristics [1]. \n\nCase Studies in Geophysical Applications[5].  \n\nSeismic Analysis [5].\n- **Hilbert-Huang Transform (HHT)**: HHT has been employed in seismic data analysis for identifying and characterizing seismic events. One specific case showed its effectiveness in separating seismic waves from noise and extracting meaningful patterns associated with underground structures [12].\n\n## Climate Modeling [3][5][6] \n- **Wavelet Transform (WT)**: WT has been used to analyze climate data for detecting periodicities and anomalies. It has proven effective in identifying El Niño and La Niña events within sea surface temperature records, allowing for better predictive models [6].\n\n### Fault Detection in Geological Structures\n\nFault detection in geological structures often involves analyzing non-stationary signals generated from geological data. Traditional signal processing methods like the Kalman-Bucy and Wiener approaches are employed to improve the signal-to-noise ratio, making these methods effective in handling stochastic non-stationary geophysical processes [5]. Additionally, advanced techniques such as the Empirical Wavelet Transform (EWT) and its improved variants have been proposed for more efficient analysis of noisy and non-stationary signals [0]. These methods enhance the spectral resolution and help in identifying features that are otherwise challenging to detect. Moreover, non-stationary signal processing plays a critical role in applications like bearing health monitoring, where selecting the appropriate signal processing technique significantly improves defect identification and reliability [6].\n\n- **Adaptive STFT**: This technique has been applied in the detection of faults within geological formations. For instance, STFT was used to analyze vibration signals from drilling operations, effectively identifying transient anomalies indicative of faults [1].\n\nConclusion[4]. By leveraging techniques such as the short-time Fourier transform, wavelet transform, and Hilbert-Huang transform, significant improvements in detecting transient features from non-stationary signals are achieved[12]. These methods, particularly when applied to condition monitoring of electrical machines, provide valuable insights for fault diagnosis, highlighting their effectiveness in practical applications[8]. Furthermore, advanced signal processing algorithms offer robust solutions for non-invasive fetal electrocardiography, emphasizing the importance of optimizing key parameters to enhance performance[13]. Overall, non-stationary signal processing techniques continue to evolve, demonstrating their critical role in various fields including biomedical signal analysis and machinery health monitoring[10]. The ongoing advancements in this area are promising, indicating that future research will likely yield even more sophisticated tools for handling complex and non-stationary data[14]. \n\nWhile all these techniques offer unique advantages, the choice of method largely depends on the specific characteristics of the geophysical data and the objectives of the analysis. STFT and its adaptive variants provide a balanced approach for general time-frequency analysis [1], [2], while wavelet-based methods (including HHT) offer superior performance for multi-scale and highly non-linear, non-stationary signals [10]. In particular, HHT stands out for its adaptability and robustness in handling complex geophysical signals, making it a highly effective tool in this field [12]. \n\n\nReferences:\n[0] An Improved Empirical Wavelet Transform for Noisy and Non-Stationary Signal Processing Empirical wavelet transform (EWT) has become an effective tool for signal processing. However, its sensitivity to noise may bring side effects on the analysis of some noisy and non-stationary signals, especially for the signal which contains the close frequency components. In this paper, an improved empirical wavelet transform is proposed. This method combines the advantages of piecewise cubic Hermite interpolating polynomial (PCHIP) and the EWT, and is named PCHIP-EWT. The main idea of the proposed method is to select useful sub-bands from the spectrum envelope. The proposed method selects the maximum points of the spectrum to reconstruct the spectrum envelope on the basis of PCHIP. Then, a new concept and a threshold named the Local Power (LP) and <inline-formula> <tex-math notation=\"LaTeX\">$\\lambda $ </tex-math></inline-formula> are defined. Based on the new concept LP and the <inline-formula> <tex-math notation=\"LaTeX\">$\\lambda $ </tex-math></inline-formula>, the useful sub-bands can be obtained. Finally, the experimental results demonstrate that the PCHIP-EWT is effective in analyzing noise and non-stationary signals, especially those that contain the closely-spaced frequencies.\n\n[1] Adaptive Short-time Fourier Transform and Synchrosqueezing Transform for Non-stationary Signal Separation For a monocomponent signal x​(t)=A​(t)​ei​2​π​ϕ​(t)𝑥𝑡𝐴𝑡superscript𝑒𝑖2𝜋italic-ϕ𝑡x(t)=A(t)e^{i2\\pi\\phi(t)}, if its STFT with gσsubscript𝑔𝜎g_{\\sigma}, which is also given by (refer to (3)),Vx​(t,η)=∫−∞∞A​(t+τ)​ei​2​π​ϕ​(t+τ)​gσ​(τ)​e−i​2​π​η​τ​𝑑τsubscript𝑉𝑥𝑡𝜂superscriptsubscript𝐴𝑡𝜏superscript𝑒𝑖2𝜋italic-ϕ𝑡𝜏subscript𝑔𝜎𝜏superscript𝑒𝑖2𝜋𝜂𝜏differential-d𝜏V_{x}(t,\\eta)=\\int_{-\\infty}^{\\infty}A(t+\\tau)e^{i2\\pi\\phi(t+\\tau)}g_{\\sigma}(\\tau)e^{-i2\\pi\\eta\\tau}d\\taucan be well approximated byVx​(t,η)≈∫−∞∞A​(t)​ei​2​π​(ϕ​(t)+ϕ′​(t)​τ+12​ϕ′′​(t)​τ2)​gσ​(τ)​e−i​2​π​η​τ​𝑑τ,subscript𝑉𝑥𝑡𝜂superscriptsubscript𝐴𝑡superscript𝑒𝑖2𝜋italic-ϕ𝑡superscriptitalic-ϕ′𝑡𝜏12superscriptitalic-ϕ′′𝑡superscript𝜏2subscript𝑔𝜎𝜏superscript𝑒𝑖2𝜋𝜂𝜏differential-d𝜏V_{x}(t,\\eta)\\approx\\int_{-\\infty}^{\\infty}A(t)e^{i2\\pi\\big{(}\\phi(t)+\\phi^{\\prime}(t)\\tau+\\frac{1}{2}\\phi^{\\prime\\prime}(t)\\tau^{2}\\big{)}}g_{\\sigma}(\\tau)e^{-i2\\pi\\eta\\tau}d\\tau,then the choice of σ𝜎\\sigma givenσ=12​π​|ϕ′′​(t)|𝜎12𝜋superscriptitalic-ϕ′′𝑡\\sigma=\\frac{1}{\\sqrt{2\\pi|\\phi^{\\prime\\prime}(t)|}}(40)results in the sharpest representation of Vx​(t,η)subscript𝑉𝑥𝑡𝜂V_{x}(t,\\eta). The choice of σ=σ​(t)𝜎𝜎𝑡\\sigma=\\sigma(t) in (40) coincides with the result derived in.\n\n[2] Adaptive Short-time Fourier Transform and Synchrosqueezing Transform for Non-stationary Signal Separation In this paper we use the linear frequency modulation (LFM) signal to approximate a non-stationary signal at any local time to studythe TF zone of the adaptive STFT V~xk​(t,η)subscript~𝑉subscript𝑥𝑘𝑡𝜂\\widetilde{V}_{x_{k}}(t,\\eta). More precisely, we assume that each xk​(t)subscript𝑥𝑘𝑡x_{k}(t)is well approximated by an LFM at any local time: for any t∈ℝ𝑡ℝt\\in{\\mathbb{R}},xk​(t+τ)=Ak​(t+τ)​ei​2​π​ϕk​(t+τ)subscript𝑥𝑘𝑡𝜏subscript𝐴𝑘𝑡𝜏superscript𝑒𝑖2𝜋subscriptitalic-ϕ𝑘𝑡𝜏\\displaystyle x_{k}(t+\\tau)=A_{k}(t+\\tau)e^{i2\\pi\\phi_{k}(t+\\tau)}≈Ak​(t)​ei​2​π​(ϕk​(t)+ϕk′​(t)​τ+12​ϕk′′​(t)​τ2)=xk​(t)​ei​2​π​(ϕk′​(t)​τ+12​ϕk′′​(t)​τ2)for τ≈0,formulae-sequenceabsentsubscript𝐴𝑘𝑡superscript𝑒𝑖2𝜋subscriptitalic-ϕ𝑘𝑡superscriptsubscriptitalic-ϕ𝑘′𝑡𝜏12superscriptsubscriptitalic-ϕ𝑘′′𝑡superscript𝜏2subscript𝑥𝑘𝑡superscript𝑒𝑖2𝜋superscriptsubscriptitalic-ϕ𝑘′𝑡𝜏12superscriptsubscriptitalic-ϕ𝑘′′𝑡superscript𝜏2for τ≈0\\displaystyle\\qquad\\approx A_{k}(t)e^{i2\\pi(\\phi_{k}(t)+\\phi_{k}^{\\prime}(t)\\tau+\\frac{1}{2}\\phi_{k}^{\\prime\\prime}(t)\\tau^{2})}=x_{k}(t)e^{i2\\pi(\\phi_{k}^{\\prime}(t)\\tau+\\frac{1}{2}\\phi_{k}^{\\prime\\prime}(t)\\tau^{2})}\\quad\\hbox{for $\\tau\\approx 0$},(9)where for a given t𝑡t, the quantity in (9) as a function τ𝜏\\tau is called an LFM signal (or a linear chirp signal). Thus we haveV~xk​(t,η)≈∫ℝxk​(t)​ei​2​π​(ϕk′​(t)​τ+12​ϕk′′​(t)​τ2)​1σ​(t)​g​(τσ​(t))​e−i​2​π​η​τ​𝑑τ.subscript~𝑉subscript𝑥𝑘𝑡𝜂subscriptℝsubscript𝑥𝑘𝑡superscript𝑒𝑖2𝜋superscriptsubscriptitalic-ϕ𝑘′𝑡𝜏12subscriptsuperscriptitalic-ϕ′′𝑘𝑡superscript𝜏21𝜎𝑡𝑔𝜏𝜎𝑡superscript𝑒𝑖2𝜋𝜂𝜏differential-d𝜏\\displaystyle\\widetilde{V}_{x_{k}}(t,\\eta)\\approx\\int_{\\mathbb{R}}x_{k}(t)e^{i2\\pi(\\phi_{k}^{\\prime}(t)\\tau+\\frac{1}{2}\\phi^{\\prime\\prime}_{k}(t)\\tau^{2})}\\frac{1}{\\sigma(t)}g(\\frac{\\tau}{\\sigma(t)})e^{-i2\\pi\\eta\\tau}d\\tau.(10)In this paper we will obtain the LFM model-based well-separated condition which guarantees that for different k𝑘k, the quantities as functions of (t,η)𝑡𝜂(t,\\eta) on the right-hand side of (10) lie within non-overlapping zones in the TF plane when g𝑔g is the Gaussian window function. We will also discuss how to select the time-varying parameter σ​(t)𝜎𝑡\\sigma(t) such that the corresponding adaptive FSST and 2nd-order adaptive FSST have sharp TF representation. In particular, we propose a localized optimization method based on our well-separated condition to estimate the time-varying adaptive window width σ​(t)𝜎𝑡\\sigma(t).\n\n[3] Non-stationary signal processing for bearing health monitoring Non-stationary signal processing for bearing health monitoring Signals generated by transient vibrations in rolling bearings due to structural defects are non-stationary in nature, and reflect upon the operation condition of the bearing. Consequently, effective processing of non-stationary signals is critical to bearing health monitoring. This paper presents a comparative study of four representative time-frequency analysis techniques commonly employed for non-stationary signal processing. The analytical framework of the short-time Fourier transform, wavelet transform, wavelet packet transform, and Hilbert-Huang transform are first presented. The effectiveness of each technique in detecting transient features from a time-varying signal is then examined, using an analytically formulated test signal. Subsequently, the performance of each technique is experimentally evaluated, using realistic vibration signals measured from a bearing test system. The results demonstrate that selecting appropriate signal processing technique can significantly affect defect identification and consequently, improve the reliability of bearing health monitoring.\n\n[4] Adaptive Short-time Fourier Transform and Synchrosqueezing Transform for Non-stationary Signal Separation Observe thath​(ξ)|ℎ𝜉|h(\\xi)| is a Gaussian function with durationL|h|=2​α​1+(2​π​r​σ2)2σ2=2​α​1σ2+(2​π​r​σ)2.subscript𝐿ℎ2𝛼1superscript2𝜋𝑟superscript𝜎22superscript𝜎22𝛼1superscript𝜎2superscript2𝜋𝑟𝜎2L_{|h|}=2\\alpha\\sqrt{\\frac{1+(2\\pi r\\sigma^{2})^{2}}{\\sigma^{2}}}=2\\alpha\\sqrt{\\frac{1}{\\sigma^{2}}+(2\\pi r\\sigma)^{2}}.Thus the ridge of Vs​(t,η)subscript𝑉𝑠𝑡𝜂V_{s}(t,\\eta) concentrates around η=c+r​t𝜂𝑐𝑟𝑡\\eta=c+rt in the TF plane, and Vs​(t,η)subscript𝑉𝑠𝑡𝜂V_{s}(t,\\eta) lies within the zone of TF plane of (t,η)𝑡𝜂(t,\\eta):−12​L|h|≤c+r​t−η≤12​L|h|,12subscript𝐿ℎ𝑐𝑟𝑡𝜂12subscript𝐿ℎ-\\frac{1}{2}L_{|h|}\\leq c+rt-\\eta\\leq\\frac{1}{2}L_{|h|},or equivalentlyc+r​t−α​1σ2+(2​π​r​σ)2≤η≤c+r​t+α​1σ2+(2​π​r​σ)2.𝑐𝑟𝑡𝛼1superscript𝜎2superscript2𝜋𝑟𝜎2𝜂𝑐𝑟𝑡𝛼1superscript𝜎2superscript2𝜋𝑟𝜎2c+rt-\\alpha\\sqrt{\\frac{1}{\\sigma^{2}}+(2\\pi r\\sigma)^{2}}\\leq\\eta\\leq c+rt+\\alpha\\sqrt{\\frac{1}{\\sigma^{2}}+(2\\pi r\\sigma)^{2}}.(38)\n\n[5] Treatment of geophysical data as a non-stationary process Treatment of geophysical data as a non-stationary process The Kalman-Bucy method is here analized and applied to the solution of a specific filtering problem to increase the signal message/noise ratio. The method is a time domain treatment of a geophysical process classified as stochastic non-stationary. The derivation of the estimator is based on the relationship between the Kalman-Bucy and Wiener approaches for linear systems. In the present work we emphasize the criterion used, the model with apriori information, the algorithm, and the quality as related to the results. The examples are for the ideal well-log response, and the results indicate that this method can be used on a variety of geophysical data treatments, and its study clearly offers a proper insight into modeling and processing of geophysical problems.\n\n[6] Non-stationary signal processing for bearing health monitoring Signals generated by transient vibrations in rolling bearings due to structural defects are non-stationary in nature, and reflect upon the operation condition of the bearing. Consequently, effective processing of non-stationary signals is critical to bearing health monitoring. This paper presents a comparative study of four representative time-frequency analysis techniques commonly employed for non-stationary signal processing. The analytical framework of the short-time Fourier transform, wavelet transform, wavelet packet transform, and Hilbert-Huang transform are first presented. The effectiveness of each technique in detecting transient features from a time-varying signal is then examined, using an analytically formulated test signal. Subsequently, the performance of each technique is experimentally evaluated, using realistic vibration signals measured from a bearing test system. The results demonstrate that selecting appropriate signal processing technique can significantly affect defect identification and consequently, improve the reliability of bearing health monitoring.\n\n[7] Adaptive Short-time Fourier Transform and Synchrosqueezing Transform for Non-stationary Signal Separation Next we show some experimental results.We consider a two-component LFM signal,y​(t)=y1​(t)+y2​(t)=cos⁡(2​π​(12​t+25​t2))+cos⁡(2​π​(34​t+32​t2)),t∈,1,formulae-sequence𝑦𝑡subscript𝑦1𝑡subscript𝑦2𝑡2𝜋12𝑡25superscript𝑡22𝜋34𝑡32superscript𝑡2𝑡01y(t)=y_{1}(t)+y_{2}(t)=\\cos(2\\pi(12t+25t^{2}))+\\cos(2\\pi(34t+32t^{2})),\\quad t\\in,1,(56)where the number of sampling points is 256, namely the sampling rate is 256Hz.The IFs of y1​(t)subscript𝑦1𝑡y_{1}(t) and y2​(t)subscript𝑦2𝑡y_{2}(t) are ϕ′​(t)=12+50​tsuperscriptitalic-ϕ′𝑡1250𝑡\\phi^{\\prime}(t)=12+50t and ϕ2′​(t)=34+64​tsubscriptsuperscriptitalic-ϕ′2𝑡3464𝑡\\phi^{\\prime}_{2}(t)=34+64t, respectively.The top-left panel of Fig.1 shows the instantaneous frequencies of y1​(t)subscript𝑦1𝑡y_{1}(t) and y2​(t)subscript𝑦2𝑡y_{2}(t).With σ2​(t)subscript𝜎2𝑡\\sigma_{2}(t), both the proposed adaptive FSST defined by (21)and 2nd-order adaptive FSST defined by (30) can represent this signal sharply.Here and below, we choose ϵ=15italic-ϵ15\\epsilon=\\frac{1}{5}, and hence α𝛼\\alpha which is defined by (34) and used in(55) is α≈0.2855𝛼0.2855\\alpha\\approx 0.2855.Observe that the 2nd-order adaptive FSST further improves the TF energy concentration of the adaptive FSST. Here we also give the results of conventional FSST studied in-, and conventional 2nd-order FSST defined in withσ=0.057𝜎0.057\\sigma=0.057. This σ𝜎\\sigma is obtained by minimizing the Re´´e{\\rm\\acute{e}}nyi entropy of the STFT (refer to the next section about the definition of Re´´e{\\rm\\acute{e}}nyi entropy).Observe that the 2nd-order FSST is better than the FSST with the same σ𝜎\\sigma. When σ=0.057𝜎0.057\\sigma=0.057the TF representation of the conventional 2nd-order FSST is not as sharp or clear as that of the 2nd-order adaptive FSST.\n\n[8] Signal processing tools for non-stationary signals detection Signal processing tools for non-stationary signals detection In this paper we aim to compare the abilities and performances of signal processing tools to detect non-stationary signals coming from condition monitoring of electrical machines. From the vast amount of available tools, we focus on existing signal processing methods suitable for real applications for non-stationarities tracking and quantification over time which is particularly interesting in fault diagnosis. First, we assess the spectral kurtosis, a tool that gained much attention because of his capability to characterize transients masked by strong noises. In order to detect non-stationarities, other methods are evaluated such as the spectral subtraction through the short time Fourier transform or the Wiener filtering which can remove stationary components. The analytical framework of each tool is first presented. Non-stationary tests signals based on properties of vibration signals of bearings are proposed to compare effectiveness, advantages and drawbacks of each methods for non-stationarities detection. The purpose is to select a method that is best suited for each type of non-stationarity in order to improve the reliability of the detection.\n\n[9] A New Tool for Nonstationary and Nonlinear Signals: The Hilbert-Huang Transform in Biomedical Applications biomedical data, for lack of alternatives. The uncritical use of Fourier spectral analysis and the careless adoption of the stationary and linear assumptions may give misleading results. Wavelet theory developed in the 90’s of last century was a significant contribution to tackle the problem of non-stationarity in time-series analysis. In common with Fourier-based analysis such as STFT, wavelet analysis yields a time-frequency representation, the main difference being that the decomposition is not based on sinusoidal functions, but rather Introduction Time-frequency techniques constitutes a major improvement in signal analysis, namely at the field of biomedical signals in which the interdisciplinary nature of the proposed questions implies the development of new strategies to answer to specific problems. Timefrequency analysis using Wavelets, Wigner-Ville transform and more recently the Hilbert-Huang Transform (HHT) constitutes the core of these tools with applications in biomedical signals in last years. The non-linearity and non-stationarity nature of these signals puts HHT as a powerful tool to process signals with those properties, avoiding artefacts related to the use of linear and stationary assumptions. Classical spectral analysis using Fourier Transform still the most commonly used method when one wants to measure the global power-frequency distribution (power spectrum) of a given signal. In all areas of knowledge, Fourier-based analysis of time-series data faces constraining limitations. In biomedical signals, the critical constraining factors are the shortness of total data span, the non-stationarity of the data and the nonlinearity of the underlying physiological process. Methods using Short Time Fourier Transform (STFT) are able to extract the spectral information by defining\n\n[10] Performance Analysis of Time Frequency Resolution Techniques for Non-Stationary Signals Performance Analysis of Time Frequency Resolution Techniques for Non-Stationary Signals A new method has been introduced for analyzing the non linear and non stationary data. For example, Wavelet Analysis, Wigner Ville Distribution (WVD), Hilbert Huang Transform (HHT) and Fractional Fourier Transform techniques were introduced for analysis of linear but non-stationary data1,2. One of the key attributes of the signal processing techniques is applicable for non stationary signal environments is an improvement in the time frequency resolution or localization. These are relative to classical techniques like Fourier transforms which overcome these drawbacks of FT. There are some alternative methods to classical STFT signal analysis technique which was proposed and first introduced by Dennis Gabor in 19463–5. The techniques have received considerable attention in recent times are Time Frequency Localization (TFL) techniques such as the Wigner Ville Distribution which is a non-stationary and quadratic time frequency signal analysis tool, introduced in 1948 by Ville and its alternatives6–8. The example of an empirical based data analysis method is Hilbert Huang Transform. Basis of expansion for this transform is adaptive. So it can produce physically meaningful representations of data from non stationary and nonlinear processes. The purpose of Hilbert Huang Transform is to Abstract Introduction A new method has been introduced for analyzing the non linear and non stationary data. For example, Wavelet Analysis, Wigner Ville Distribution (WVD), Hilbert Huang Transform (HHT) and Fractional Fourier Transform techniques were introduced for analysis of linear but non-stationary data 1,2 . One of the key attributes of the signal processing techniques is applicable\n\n[11] A Review of Signal Processing Techniques for Non-Invasive Fetal Electrocardiography Fetal electrocardiography (fECG) is a promising alternative to cardiotocography continuous fetal monitoring. Robust extraction of the fetal signal from the abdominal mixture of maternal and fetal electrocardiograms presents the greatest challenge to effective fECG monitoring. This is mainly due to the low amplitude of the fetal versus maternal electrocardiogram and to the non-stationarity of the recorded signals. In this review, we highlight key developments in advanced signal processing algorithms for non-invasive fECG extraction and the available open access resources (databases and source code). In particular, we highlight the advantages and limitations of these algorithms as well as key parameters that must be set to ensure their optimal performance. Improving or combining the current or developing new advanced signal processing methods may enable morphological analysis of the fetal electrocardiogram, which today is only possible using the invasive scalp electrocardiography method.\n\n[12] Bio-Signal Complexity Analysis in Epileptic Seizure Monitoring: A Topic Review medical images and physiological signals are just observable and measurable 'states' or 'output' of this system. Direct treatment implications and anticipation of prognosis are not necessary for them. However, exploring the complexity of these signals in a different perspective may shed new light on the analysis of these data. Different methods are utilized to tackle the complexity in physiological signals, mainly for epileptic seizure detection. Among them, three subjects as fountainheads, e.g., non-stationary signal processing, nonlinear dynamics and network science, can be roughly identified. Non-stationary signal processing is the most straightforward methodological source. And nonlinear dynamics and network science influence this field in a more heuristic and subtle manner. Methods originate from more than one of these three subjects could be adopted in research. Since these three subjects provide independent perspectives, we organize the material along this framework. Since the physiological processes are confirmed to be nonlinear and non-stationary, non-stationary signal processing techniques are the most intuitive choice for such problems. Compared with traditional time-domain statistical methods and frequency-domain methods which provide averaged information, non-stationary signal processing methods such as short-time Fourier transform, time frequency analysis, wavelet transform and model-based analysis has the advantages of representing and capturing transient anomalies. Various measures of the complexity of a function stem from the research about nonlinear dynamics are used to discriminate physiological processes under different pathological conditions. These methods propose a different paradigm and independent information compared to that acquired by classical spectral analysis and non-stationary signal processing techniques. In classical spectral analysis, a signal is\n\n"}
{"case_id": "a2a5c8079442e178f078869471697f48", "answer_text": "\n\nSeismic P-waves, or primary waves, exhibit several distinct frequency characteristics that allow them to be distinguished from other seismic waves, such as S-waves (secondary waves) and surface waves. These characteristics include velocity, frequency range, and attenuation properties [3][14]. \n\n **Velocity and Frequency Range** \n\nSeismic P-wave velocities in near-surface materials can be much slower than the speed of sound waves in air (normally 335 m/s or 1100 ft/s). Difficulties often arise when measuring these low-velocity P-waves because of interference by the air wave and the air-coupled waves near the seismic source, at least when gathering data with the more commonly used shallow P-wave sources. Additional problems in separating the direct and refracted arrivals within ~2 m of the source arise from source-generated nonlinear displacement, even when small energy sources such as sledgehammers, small-caliber rifles, and seismic blasting caps are used [0]. Using an automotive spark plug as an energy source allowed us to measure seismic P-wave velocities accurately, in situ, from a few decimeters to a few meters from the shotpoint [0]. We were able to observe three distinct P-wave velocities at our test site: ~130 m/s, 180 m/s, and 300 m/s. Even the third layer, which would normally constitute the first detected layer in a shallow-seismic-refraction survey, had a P-wave velocity lower than the speed of sound in air [0].\n\n\n\nSeismic P-waves generally have a higher velocity compared to S-waves and surface waves. P-waves, being compressional waves, can travel through both solid and liquid mediums, and their velocities are higher due to the nature of their propagation mechanism. For example, P-waves have higher frequencies in the range of 50-150 Hz, whereas S-waves typically range between 30-100 Hz [3]. This higher frequency range is a primary characteristic that can be used to distinguish P-waves from other wave types.\n\n**Attenuation Properties**[9][13][0][1] \n\nP-waves display different attenuation behaviors that are frequency-dependent. Specifically, P-waves in fluid-saturated rocks exhibit distinct attenuation properties at varying frequencies. Two kinds of P-waves in such media—the faster P-wave and the slower P-wave—demonstrate this well. The faster P-wave displays higher velocity and lower attenuation, whereas the slower P-wave, which is diffusive, shows greater attenuation and operates at lower frequencies [9][13]. This distinction is crucial for interpreting subsurface characteristics accurately.\n\n**Analytical Techniques**[0].\n\nFrequency-dependent amplitude variation with offset (FAVO) analysis is another seismic attribute that can be useful in identifying P-waves, especially in thinly bedded reservoirs. This analytical technique takes advantage of the scattering and reflection properties of P-waves in vertically inhomogeneous media, differentiating them from P-SV (converted shear) waves [7].\n\nAdvanced Signal Processing\n\nAdvanced signal processing techniques also play a significant role in enhancing P-wave detection. For instance, the combination of Principal Component Analysis (PCA) and time-frequency representation can significantly improve the accuracy of P-wave arrival detection by distinguishing their frequency-domain characteristics from noise and other wave types [6]. This method is particularly useful in environments such as underground mines, where accurate detection of seismic events is critical for safety and operational efficiency.\n\nSpectrogram Analysis and Frequency-Wavenumber Transform [12].\n\nSpectrogram analysis has been another valuable technique for analyzing P-wave characteristics. For example, simulations have demonstrated that P-waves have a dominant frequency range of around 5-12 Hz in heterogeneous media and 7-10 Hz in homogeneous media. These P-wave frequency ranges can be distinctly identified from those of S-waves, which display a frequency range of 4-15 Hz in similar conditions [14]. Spectrograms provide a visual representation of the frequency content over time, aiding in the identification and analysis of seismic events. Another method is the separation of mode waves using a frequency-wavenumber (fk) transform, which filters out borehole mode waves to isolate the P-waves from the target borehole, allowing for accurate interpretation of the seismic data based on the P-wave's slowness and amplitude spectrum [12].\n\n**Regional Geological Applications**[2].\n\nIn addition to the methods mentioned, other techniques for analyzing seismic waves include teleseismic P to Rg scattering studies. These studies can map lateral variations in scattering strength, providing insight into topographic roughness and near-surface geology. Such studies utilize the duration and amplitude levels of P-wave codas to infer geological characteristics [4]. \n\n**Crustal Composition Analysis** involves understanding the relationship between seismic P-wave velocity and the composition of crustal rocks. This relationship is important in assessing the composition and physical state of the Earth's crust. For instance, seismic velocities can be used to infer the major element chemistry of igneous and meta-igneous rocks based on thermodynamic calculations and laboratory measurements of pure mineral phases [10]. This technique allows geophysicists to estimate the crust's composition by analyzing P-wave velocity profiles. Moreover, distinguishing different rock types, such as eclogite and peridotite, through their unique seismic properties helps in the detailed mapping of crustal structures [2]. Additionally, the scattering properties of high-frequency seismic waves due to small-scale velocity fluctuations can provide insights into the heterogeneity and complexity of the crust and upper mantle structures [1]. These methods collectively enhance our understanding of the geological characteristics of the Earth's crust and aid in locating potential resources such as groundwater or hydrocarbons. \n\nDistinguishing P-waves from other wave types is crucial for various applications. For instance, P-wave velocity measurements can be used to infer the composition of the Earth's crust and upper mantle. A study demonstrated that calculated seismic velocities could be used to estimate the major element composition of anhydrous igneous and meta-igneous rocks in the crust [10]. This application is vital for understanding geological formations and potential resource reservoirs.\n\nIn summary, the frequency characteristics of seismic P-waves, including their higher velocity [2], specific frequency range [3], and distinct attenuation behaviors [13], combined with advanced signal processing methods [6] and detailed spectrogram analysis [14], make them distinguishable from other seismic waves such as S-waves and surface waves [14]. These distinctions are crucial for various applications, including subsurface imaging, resource exploration, and seismic hazard assessment. \n\n\nReferences:\n[0] Case History In-situ, high-resolution P-Wave velocity measurements within 1 m of the earth's surface Case History In-situ, high-resolution P-Wave velocity measurements within 1 m of the earth's surface Seismic P-wave velocities in near-surface materials can be much slower than the speed of sound waves in air (normally 335 m/s or 1100 ft/s). Difficulties often arise when measuring these low-velocity P-waves because of interference by the air wave and the air-coupled waves near the seismic source, at least when gathering data with the more commonly used shallow P-wave sources. Additional problems in separating the direct and refracted arrivals within ~2 m of the source arise from source-generated nonlinear displacement, even when small energy sources such as sledgehammers, small-caliber rifles, and seismic blasting caps are used. Using an automotive spark plug as an energy source allowed us to measure seismic P-wave velocities accurately, in situ, from a few decimeters to a few meters from the shotpoint. We were able to observe three distinct P-wave velocities at our test site: ~130 m/s, 180 m/s, and 300 m/s. Even the third layer, which would normally constitute the first detected layer in a shallowseismic-refraction survey, had a P-wave velocity lower than the speed of sound in air.\n\n[1] Scattering of high-frequency P wavefield derived by dense Hi-net array observations in Japan and computer simulations of seismic wave propagations SUMMARY We studied the scattering properties of high-frequency seismic waves due to the distribution of small-scale velocity fluctuations in the crust and upper mantle beneath Japan based on an analysis of three-component short-period seismograms and comparison with finite difference method (FDM) simulation of seismic wave propagation using various stochastic random velocity fluctuation models. Using a large number of dense High-Sensitivity Seismograph network waveform data of 310 shallow crustal earthquakes, we examined the P-wave energy partition of transverse component (PEPT), which is caused by scattering of the seismic wave in heterogeneous structure, as a function of frequency and hypocentral distances. At distance of less than D = 150km, the PEPT increases with increasing frequency and is approximately constant in the range of from D = 50 to 150 km. The PEPT was found to increase suddenly at a distance of over D = 150 km and was larger in the high-frequency band (f > 4 Hz). Therefore, strong scattering of P wave may occur around the propagation path (upper crust, lower crust andaroundMohodiscontinuity)oftheP-wavefirstarrivalphaseatdistancesoflargerthanD = 150km.WealsofoundaregionaldifferenceinthePEPT value,wherebythePEPT valueislarge at the backarc side of northeastern Japan compared with southwestern Japan and the forearc side of northeastern Japan. These PEPT results, which were derived from shallow earthquakes, indicate that the shallow structure of heterogeneity at the backarc side of northeastern Japan is stronger and more complex compared with other areas. These hypotheses, that is, the depth andregionalchangeofsmall-scalevelocityfluctuations,areexaminedby3-DFDMsimulation using various heterogeneous structure models. By comparing the observed feature of the PEPT with simulation results, we found that strong seismic wave scattering occurs in the lower crust due to relatively higher velocity and stronger heterogeneities compared with that in the upper crust. To explain the observed regional difference, the velocity fluctuation model with 3‐4 per cent stronger fluctuation and smaller κ is required at the backarc side of northeastern Japan.\n\n[2] Distinguishing eclogite from peridotite: EBSD-based calculations of seismic velocities S U M M A R Y Seismic velocities were calculated for 11 eclogites from the Western Gneiss Region, Norway, based on electron-backscatter diffraction (EBSD). The P-wave velocities are 8.0–8.5 km s–1 and the S-wave velocities are 4.5–4.8 km s–1; VP/VS1 (the ratio of P-wave to fast S-wave velocities) is 1.74–1.81. All the eclogites are relatively isotropic, with the higher anisotropies (3– 4 per cent) in micaceous samples. Peridotite is comparatively more anisotropic (4–14 per cent more for P waves and up to 10 per cent more for S waves), and can have anomalously low VP/VS1, which may be useful means of distinguishing it from eclogite. Micaceous eclogite may be modelled using hexagonal anisotropy with a slow unique axis, whereas peridotite is most robustly modelled using orthorhombic anisotropy.\n\n[3] Geophysical methods help to assess potential groundwater extraction sites air blast are not recorded by the SH-geophones which are more sensitive to horizontal oscillations. Working on paved ground leads to suppression of the surface waves. Even shallow layers can therefore be detected by S-wave surveys. P-and S-waves also differ in their signal frequency as shown in the power spectra. In our example, for P-waves most seismic energy is in the 50-150 Hz frequency range, for S-waves it is between 30 and 100 Hz. The P-wave seismic reflection profiles show a two-part picture (Fig. 4c, d). The upper part, down to a depth of about 100 m below m.s. l., is characterized by chaotic layering, while the lower part is characterized by a uniform, parallel reflection pattern. This continuous layering is presumably attributable to Neogene or Miocene strata. Further up, horizontal reflections end at the presumed edge of an incised channel (e.g., line 1 at the north side and line 2 at the east side). The channel-like structures are marked in violet. Line 1 runs more or less perpendicular to the strike of the channel. The channel seems to have different phases, the younger one shows asymmetric filling. Strong horizontal reflections inside the valley (marked in blue) might indicate thick till layers as known, e.g., from borehole R99 (for location see Fig. 1). The shear wave section shows much less depth penetration but represents the till layer in more detail. The base reflection of the till is especially well displayed (blue line in Fig. 4b at about -50 m relative to m.s. l.), and the S-wave\n\n[4] Mapping shallow heterogeneity with teleseismic P to Rg scattered waves \n Over 9000 records of teleseismic P-wave coda from the Southern California Seismic Network are used to map lateral variations in P to Rg scattering strength and the group velocity of scattered Rg. Building on the results of Revenaugh (1995a), we use the correlation of scattering strength and topographic roughness as an indicator of model fit to estimate group-velocity dispersion over five frequency bands between 0 and 3 Hz. Rg group velocity displays normal dispersion, decreasing from 2.7 ± 0.3 km/sec for frequencies between 0 and 0.6 Hz to a low of 2.2 ± 0.4 km/sec for the 2.0 to 3.0 Hz band. Lateral heterogeneity is introduced by assuming Rg velocity is a function of local topographic roughness. Grid-search/migration analysis yields group velocities of 2.2 and 2.85 km/sec for “smooth” and “rough” terrain, respectively, and a 150-m threshold for the local standard deviation of topography distinguishing “smooth” from “rough”. The map of scattering strength approximates topographic roughness throughout southern California, implying that teleseismic P-wave coda duration and relative amplitude level depend strongly on near-surface (<3 km) geology.\n\n[5] Combination of Principal Component Analysis and Time-Frequency Representation for P-Wave Arrival Detection Combination of Principal Component Analysis and Time-Frequency Representation for P-Wave Arrival Detection Seismic events are phenomena which commonly occur in the mining industry. Due to their dangerous character, such information as the energy of the potential event, the location of hazardous regions with higher seismic activity is considered valuable. However, the acquisition of this information is almost impossible without the ability to detect the onset time of the seismic event. The main objectives of algorithms in finding P-wave are high accuracy, reasonable time of operation, and automatic detection of wave arrival. In this paper, an innovative method which incorporates principal component analysis (PCA) with time-frequency representation of the signal is proposed. Due to the significant difference between the spectra of recorded seismic wave and pure noise which precedes the event, time-frequency representation allows for better accuracy of signal change detection. However, with an additional domain, the complexity rises. Thus, the incorporation of PCA (which is known for high efficiency in lowering data dimensions while maintaining original information) seems to be recommended. In order to show the feasibility of the method, it will be tested on real data originating from monitoring system used in underground mine.\n\n[6] Frequency-dependent AVO analysis: A potential seismic attribute for thin-bed identification Identifying thinly bedded reservoirs is important in exploration, appraisal, and development. The thin beds that we are referring to are of meter scale or less. Conventional seismic attributes are not able to resolve thin-bed effects. A new frequency-dependent amplitude variation with offset (FAVO) seismic attribute is developed for the prediction of thin beds. It is based on seismic wave scattering theory for vertically inhomogeneous media, in which the seismic response from a laterally homogeneous interval is represented by the Born series. The Born series characterizes the P-P and P-SV wave reflections, transmissions, and interbed multiples within the target interval with different order terms. The newly derived attribute corresponds to the second-order term of the Born series, in which P-S wave reflection and transmission energy is taken into consideration, and it is called the second-order FAVO gradient attribute. In a thinly bedded interval where sand/shale interfaces are abundant, the P-SV wave mode conversions are numerous. This leads to a strong second-order FAVO gradient attribute response compared to that of a blocky sand, silt, or shale interval under the assumption that there is no significant rock- and fluid-property differences between the intervals. Therefore, the new seismic attribute has the potential to be a thin-bed interval indicator. We have adopted a strategy to estimate the new attribute using conventional P-wave seismic data. The attribute is extracted from seismic data at the frequency range at which the second-order scattering effect is the most prominent. Synthetic and field data examples from offshore Trinidad are studied to demonstrate the second-order scattering effect and the potential usage of the second-order FAVO attribute for thin-bed reservoir characterization.\n\n[7] Comparison of Fluid Pressure Wave between Biot Theory and Storativity Equation (fast P wave or sound wave). In contrast, there are two types of P wave in fluid-saturated rocks (fast P wave and slow P wave). Fast P wave travels with high speed and small attenuation which have been observed in seismology,7, while slow P wave is diffusive with low velocity and large attenuation. Plona observed slow P wave in water-saturated sintered glass beads. Slow P wave is fluid pressure driving groundwater flow, and the associated skeleton deformation is often negligible. Hydraulic fracturing,10 is an extreme case that slow P wave has high amplitude to enlarge fissures in tight shale. Biot,12 proposed an advanced theory to characterize waves in fluid saturated rocks, based on single porosity, elastic skeleton, and viscous fluid. In the theory, the dilatational wave of the first kind was fast P wave, while the dilatational wave of the second kind was slow P wave. In this regard, Biot theory unified sound wave in acoustics and groundwater flow in hydrogeology. For fast P wave, skeleton is active while fluid is passive. Solid and fluid tend to have synchronous motions, and friction between them is relatively small. In contrast, for slow P wave, fluid is active while skeleton is passive. Fluid and solid have out-of-phase motions, and friction between them is very large. Li et al. showed that for fast P wave with frequency at 1-100 Hz, skeleton pressure is one order of magnitude higher than fluid pressure and the two pressures have a very slight phase difference from each\n\n[8] Relationship between seismic P‐wave velocity and the composition of anhydrous igneous and meta‐igneous rocks This study presents a new approach to quantitatively assess the relationship between the composition and seismic P‐wave velocity of anhydrous igneous and meta‐igneous rocks. We perform thermodynamic calculations of the equilibrating phase assemblages predicted for all igneous composition space at various pressure and temperature conditions. Seismic velocities for each assemblage are then estimated from mixing theory using laboratory measurements of the elastic parameters for pure mineral phases. The resultant velocities are used to derive a direct relationship between Vp and major element composition valid to ±0.13 km/s for pressure and temperature conditions along a normal crustal geotherm in the depth range of 5–50 km and equilibration pressures ≤12 kbar. Finally, we use the calculated velocities to invert for major element chemistry as a function of P‐wave velocity assuming only the in situ temperature and pressure conditions are known. Compiling typical velocity‐depth profiles for the middle and lower continental and oceanic crust, we calculate compositional bounds for each of these geologic environments. We find that the acceptable compositional range for the middle (15–30 km) and lower continental (≥35 km) crust is broad, ranging from basaltic to dacitic compositions, and conclude that P‐wave velocity measurements alone are insufficient to provide fundamental constraints on the composition of the middle and lower continental crust. However, because major oxides are correlated in igneous rocks, joint constraints on Vp and individual oxides can narrow the range of acceptable crustal compositions. In the case of the lower oceanic crust (≥2 km), observed velocities are 0.2–0.3 km/s lower than velocities calculated based on the average bulk composition of gabbros in drill cores and exposed ophiolite sequences. We attribute this discrepancy to a combination of residual porosity at crustal depths less than ∼10 km and hydrous alteration phases in the lower crust, and suggest caution when inferring mantle melting parameters from observed velocities in the lower oceanic crust.\n\n[9] Field validation of imaging an adjacent borehole using scattered P-waves waves become weak at multiple measurement depths, except for the ST waves. Fig. 4 Processing workflow of the azimuthal acoustic data Owing to the borehole mode waves, we cannot directly observe the acoustic waves from the target borehole; therefore, it is necessary to filter the mode waves using a frequency-wavenumber (fk) transform (Hornby 1989). The separated waveform is shown in Fig. 7. Based on the P-wave slowness (about 5000 m/s) and distance (10.0 m) between the two boreholes, the P-wave from the target borehole received by the sensor E 7 in receiver R 1 , at a depth of 98.8 m, is marked by the dashed frame and its amplitude spectrum (dashed line) is also obtained. In Fig. 7, the domain frequency of the P-wave is approximately 10.3 kHz; therefore, the P-wave wavelength in the rock formation is ig. 6 Waves of the eight sensors in receiver R 1 and the P-wave slowness curve approximately 50.0 cm, which is twice the diameter of the target borehole. Thus, the P-waves from the target borehole are scattered P-waves instead of reflected P-waves. The separated scattered P-waves of eight sensors are shown in Fig. 8. The scattered P-waves are marked by the red dashed line, among which MR 1 E 6 and MR 1 E 7 are apparent. As the AZ value is approximately N260°, the azimuths of the eight sensors are N260°, N305°, N350°, N35°, N80°, N125°, N170°, and N215°. Therefore, we can infer that the sensors E 6 and E 7 face the target borehole, and the\n\n[10] Detection of Porous and Permeable Formations: From Laboratory Measurements to Seismic Measurements phase (P 2 -wave). The P 2 -wave is called the slow wave or the wave of the second kind. This terminology derives from the fact that the associated velocity V p2 is much lower than the velocity V p1 of the in-phase movement wave (P 1 -wave), called wave of the first kind or fast P-wave. P 1 -wave and P 2 -wave correspond to classic P-waves, with which they merge in the absence of fluid. The laboratory experiments confirming the theory were conducted by Plona (1980). It is therefore possible for a given type of reservoir, to model and to compute some velocities (V p1 , V p2 , V s ) and some attenuations (Schmitt, 1986). More information concerning wave propagation in saturated porous media is given in Bourbié et al. (1987). Frequency is very important. Two major domains, separated by a critical frequency, introduced by Biot (1956), must be distinguished. Above the critical frequency, it is possible to estimate a permeability knowing that the calculated permeability is only an approach of hydraulic permeability. As shown in Figure 2 (top), above 2 kHz, permeability has an influence on velocities and attenuations. Figure 2 (bottom) represents the variation of attenuation as a function of the wave frequency and water saturation (Murphy, 1982). The fluid complement to water is air, the effect would be lowered with a heavier gas. The attenuation may reach a maximum for frequency in the order of 10 kHz, which is the domain of full waveform acoustic logging. It is the\n\n[11] The 2D Finite Difference Numerical Modelling of P-SV Wave Propagation In Elastic Heterogeneous Medium Using Graphic Processing Unit: Case Study of Mount Merapi Topography, Yogyakarta one used in this study. Fig. 11 are the spectrograms from simulation in homogeneous layer using deep source, where the wave phases arrival could be clearly identified. P-wave phase has dominant frequency of 7-10 Hz, S-wave has higher frequency value of 5-15 Hz, although the Figure 11. Spektrogram from simulation in homogeneous medium using deep source. Color scale represents normalized density. Figure 12. Spektrogram from simulation in two-layer, heterogeneous medium using deep source. dominant frequency is relatively higher than P-wave frequency. Surface wave (SW) has lower frequency range of 2-8 Hz. Fig. 12 is the spectrograms from simulation in two-layer medium using deep source. P-wave phase has dominant frequency of 5-12 Hz, best seen in MRP station, where Pwave is dominant in the seismogram. S-wave has wider frequency range of 4-15 Hz, although its dominant frequency is relatively higher than P-wave frequency. Surface wave has higher frequency range of 7-10 Hz (at PLA) and 5-6 Hz (at YGA), compared to simulation in homogeneous medium. From the spectrograms, we could observed that there is multiple values of high frequency density at single time value. It is suspected to be caused by mutual interference of multiple reflected waves in low-velocity layer. The reflection occurred at the bottom interface of low-velocity layer, and at the topography. D. Inaccuracies And Instability of the Simulation The simulation results, however, still affected by the inaccuracies and instability of the simulation technique. There is \"tail\" characteristic of surface waves observed in the seismogram, which might be caused by numerical dispersion, or by separation\n\n"}
{"case_id": "d521598603fbab74a6c13d58993f8b55", "answer_text": "\n\nYes, it is indeed possible to obtain frequency domain information in a frequency band beyond the limitation imposed by the Nyquist theorem, under certain conditions and with the application of advanced techniques [0]. \n\nThe Nyquist theorem stipulates that, to avoid aliasing and to perfectly reconstruct a signal, the sampling frequency must be at least twice the highest frequency present in the signal. However, there are methods that allow for interpreting signals with frequency components beyond this limit by exploiting specific properties and assumptions about the signal and its sampling [0]. \n\nMultirate Signal Processing [0].\nOne such method involves multirate signal processing techniques. By using different sampling rates for different parts of the signal, it is possible to track or reject high-frequency components that would normally alias below the Nyquist frequency [0][1]. This approach leverages the possibility to switch between multiple sampling rates intelligently, thus capturing and reconstructing high-frequency signal components.\n\n### Bayesian Frequency Tracking [5] \nAnother method is the Bayesian approach, which allows for tracking frequencies beyond the Nyquist limit by incorporating smoothness priors and the whole data set into a coherent statistical framework. This method provides a statistically solid way to follow high-frequency signal components that would otherwise be untraceable [5][6].\n\n\n### Markov Chains and Co-Log-Marginal-Likelihood (CLML)\n\nThe joint law for the whole data set given the frequency sequence is obtained by the product \\( f(\\mathcal{Y} | \\boldsymbol{\\nu}) = \\beta^{T} \\exp[-\\gamma \\exp[-\\alpha CLML(\\boldsymbol{\\nu})]] \\) where \\( \\gamma \\) is the sum of the \\(\\gamma_t\\) for \\( t \\in \\mathbb{N}_T^* = \\{1, \\dots, T\\} \\) and where CLML is the Co-Log-Marginal-Likelihood \\( CLML(\\boldsymbol{\\nu}) = -\\sum_{t=1}^{T} P_{t}(\\nu_{t}) \\) the opposite of the sum of the periodograms of data \\( y_t \\) at frequency \\( \\nu_t \\). [2]\n\nAdvanced unsupervised frequency tracking techniques use Markov chains and Co-Log-Marginal-Likelihood (CLML) to estimate frequencies beyond the Nyquist limit. This method relies on the separability of likelihood and marginal laws to handle high-frequency components through a joint estimation process [2][3].\n\nSpatial Derivatives and Wave Displacement Modeling[7].\nIn the context of solving the acoustic wave equation, spatial derivatives of wave displacements can be used to reconstruct high-frequency wavefields. This method, known as dispersion-relation preserving stereo-modeling, optimizes wavenumber-domain representations to achieve high accuracy even with extremely coarse grids [7].\n\nNon-Uniform Sampling [9]. \nNon-uniform sampling methods allow for the reconstruction of frequency components beyond the Nyquist limit by leveraging irregular sampling intervals. This technique, often used in fields like NMR spectroscopy, benefits from the ability to sample at non-uniform rates, which mitigates aliasing and enhances spectral resolution [9].\n\nDynamic Multiple-TR Imaging and Fourier Analysis [14].\nThis approach is used to detect weak oscillating magnetic fields above the Nyquist frequency by employing multiple repetition times (TR) during imaging. Fourier analysis of these dynamic scans can enhance signal-to-noise ratio (SNR) and enable the precise evaluation of aliased signals from oscillating fields [14].\n\nEach of these methods circumvents the Nyquist limitation through either changing the sampling strategy, applying advanced statistical methods, or utilizing the specific properties of the signal to capture high-frequency information effectively. While these methods do not violate the fundamental principles of the Nyquist theorem, they offer practical means to go beyond its conventional limitations by innovatively handling the aliasing problem and exploiting the structure of the underlying signal [0; 7; 12]. \n\n\nReferences:\n[0] Tracking of signals beyond the Nyquist frequency Tracking of signals beyond the Nyquist frequency This paper studies the problem of tracking or disturbance rejection for sampled-data control systems, where the tracking signal can have frequency components higher than the Nyquist frequency. In view of the well-known sampling theorem, one recognizes that any high-frequency components may be detected only as an alias in the low base band, and hence it is impossible to recover or detect such frequency components. This paper examines the basic underlying assumption, and shows that this assumption depends crucially on the underlying analog model. We show that it is indeed possible to recover such high-frequency signals, and also that, by introducing multirate signal processing techniques, it is possible to track or reject such frequency components. Detailed analysis of multirate closed-loop systems and zeros and poles are given. It is shown via examples that tracking of high-frequency signals beyond the Nyquist frequency can be achieved with satisfactory accuracy.\n\n[1] Tracking of signals beyond the Nyquist frequency This paper studies the problem of tracking or disturbance rejection for sampled-data control systems, where the tracking signal can have frequency components higher than the Nyquist frequency. In view of the well-known sampling theorem, one recognizes that any high-frequency components may be detected only as an alias in the low base band, and hence it is impossible to recover or detect such frequency components. This paper examines the basic underlying assumption, and shows that this assumption depends crucially on the underlying analog model. We show that it is indeed possible to recover such high-frequency signals, and also that, by introducing multirate signal processing techniques, it is possible to track or reject such frequency components. Detailed analysis of multirate closed-loop systems and zeros and poles are given. It is shown via examples that tracking of high-frequency signals beyond the Nyquist frequency can be achieved with satisfactory accuracy.\n\n[2] Unsupervised frequency tracking beyond the Nyquist frequency using Markov chains The joint law for the whole data set given the frequency sequence is obtained by theproduct (3):f​(𝒴|𝝂)=βT​exp⁡[−γ​exp⁡[−α​C​L​M​L​(𝝂)𝑓conditional𝒴𝝂superscript𝛽𝑇𝛾𝛼𝐶𝐿𝑀𝐿𝝂f({\\mathcal{Y}}\\,|\\,{\\boldsymbol{\\nu}})=\\beta^{T}\\exp\\left[-\\gamma\\right\\exp\\left[-\\alpha CLML({\\boldsymbol{\\nu}})\\right(5)where γ𝛾\\gamma is the sum of the γtsubscript𝛾𝑡\\gamma_{t} for t∈ℕT∗={1,…,T}𝑡subscriptsuperscriptℕ𝑇1…𝑇t\\in{\\mathbbm{N}}^{*}_{T}=\\left\\{1,\\dots,T\\right\\} andwhere CLML is the Co-Log-Marginal-LikelihoodC​L​M​L​(𝝂)=−∑t=1TPt​(νt)𝐶𝐿𝑀𝐿𝝂superscriptsubscript𝑡1𝑇subscript𝑃𝑡subscript𝜈𝑡CLML({\\boldsymbol{\\nu}})=-\\sum_{t=1}^{T}P_{t}(\\nu_{t})(6)the opposite of the sum of the periodograms of data ytsubscripty𝑡{\\mathb{y}}_{t} at frequency νtsubscript𝜈𝑡\\nu_{t}, in gatet𝑡t.\n\n[3] Unsupervised frequency tracking beyond the Nyquist frequency using Markov chains Given separability assumption H1subscriptH1{\\mathrm{H}}_{1} one has f​(𝝂,a)=f​(𝝂)​f​(a)𝑓𝝂a𝑓𝝂𝑓af({\\boldsymbol{\\nu}},{\\mathb{a}})=f({\\boldsymbol{\\nu}})f({\\mathb{a}}) and the marginallaw can easily be deduced:f​(𝒴,𝝂)=f​(𝝂)​∫af​(𝒴|a,𝝂)​f​(a)​𝑑a=f​(𝝂)​f​(𝒴|𝝂).𝑓𝒴𝝂𝑓𝝂subscripta𝑓conditional𝒴a𝝂𝑓adifferential-da𝑓𝝂𝑓conditional𝒴𝝂f({\\mathcal{Y}},{\\boldsymbol{\\nu}})=f({\\boldsymbol{\\nu}})\\int_{{\\mathb{a}}}f({\\mathcal{Y}}\\,|\\,{\\mathb{a}},{\\boldsymbol{\\nu}})f({\\mathb{a}})d{\\mathb{a}}=f({\\boldsymbol{\\nu}})f({\\mathcal{Y}}\\,|\\,{\\boldsymbol{\\nu}})\\,.The joint law for the amplitudes is separable according to assumption H3asuperscriptsubscriptH3a{\\mathrm{H}}_{3}^{\\mathrm{a}}. Sincelikelihood (2) is also separable, marginalization can be performedindependently.f(𝒴|𝝂)=∏t=1T∫atf(yt|νt,at)f(at)dat.=∏t=1Tf(yt|νt).f({\\mathcal{Y}}\\,|\\,{\\boldsymbol{\\nu}})=\\prod_{t=1}^{T}\\int_{a_{t}}f({\\mathb{y}}_{t}\\,|\\,\\nu_{t},a_{t})f(a_{t})da_{t}.=\\prod_{t=1}^{T}f({\\mathb{y}}_{t}\\,|\\,\\nu_{t}).(3)\n\n[4] Unsupervised frequency tracking beyond the Nyquist frequency using Markov chains This new Bayesian method allows tracking beyond the usual Nyquist frequency, due to acoherent statistical framework that includes the whole set of data plus smoothness prior.To our knowledge, this capability is an original contribution to the field of frequencytracking.\n\n[5] Unsupervised frequency tracking beyond the Nyquist frequency using Markov chains Considering the whole frequency vector 𝝂𝝂{\\boldsymbol{\\nu}} and the whole data set 𝒴𝒴{\\mathcal{Y}}, assumptionH2bsuperscriptsubscriptH2b{\\mathrm{H}}_{2}^{\\mathrm{b}} yields:f​(𝒴|𝝂,a)=(π​rb)−N​T​exp⁡[−1rb​C​L​L​(𝝂,a)𝑓conditional𝒴𝝂asuperscript𝜋subscript𝑟𝑏𝑁𝑇1subscript𝑟𝑏𝐶𝐿𝐿𝝂af({\\mathcal{Y}}\\,|\\,{\\boldsymbol{\\nu}},{\\mathb{a}})=(\\pi r_{b})^{-NT}\\exp\\left[-\\frac{1}{r_{b}}CLL({\\boldsymbol{\\nu}},{\\mathb{a}})\\right(2)where the global CLL is a global LS criterion:C​L​L​(𝝂,a)=∑t=1TC​L​L​(νt,at).𝐶𝐿𝐿𝝂asuperscriptsubscript𝑡1𝑇𝐶𝐿𝐿subscript𝜈𝑡subscript𝑎𝑡CLL({\\boldsymbol{\\nu}},{\\mathb{a}})=\\sum_{t=1}^{T}CLL(\\nu_{t},a_{t})\\,.\n\n[6] Dispersion-relation preserving stereo-modeling method beyond Nyquist frequency for acoustic wave equation ABSTRACTAccording to Nyquist-Shannon sampling theory, two sampling points of the displacements per minimal wavelength are required to reconstruct a wavefield. However, high-frequency components above the Nyquist frequency can be theoretically recovered by introducing spatial derivatives of the displacements. We have developed a new finite-difference scheme for solving the acoustic equation, named the dispersion-relation preserving stereo-modeling method, which is practical for seismic forward modeling beyond the Nyquist frequency. The key idea in this model is to approximate the high-order spatial derivatives with wavenumber-domain optimization in wavefields represented by the wave displacements and their gradients. We investigated the theoretical properties of this method, including the absolute error of spatial operators, numerical error, stability criterion, and computational efficiency. Our results indicated that the method can effectively suppress the numerical dispersion on extremely coarse grids wi...\n\n[7] Non-uniform frequency domain for optimal exploitation of non-uniform sampling. Random sampling of NMR signal, not limited by Nyquist Theorem, yields up to thousands-fold gain in the experiment time required to obtain desired spectral resolution. Discrete Fourier transform (DFT), that can be used for processing of randomly sampled datasets, provides rarely exploited possibility to introduce irregular frequency domain. Here we demonstrate how this feature opens an avenue to NMR techniques of ultra-high resolution and dimensionality. We present the application of high resolution 5D experiments for protein backbone assignment and measurements of coupling constants from the 4D E.COSY multiplets. Spectral data acquired with the use of proposed techniques allow easy assignment of protein backbone resonances and precise determination of coupling constants.\n\n[8] Detection of fast oscillating magnetic fields using dynamic multiple TR imaging and Fourier analysis frequency in the absolute frequency spectrum with no a priori target frequency information. Therefore, it may help us to detect weak oscillating magnetic fields in vivo, which warrants further investigation. Conclusion We introduced the multiple-TR approach and Fourier analysis to detect weak oscillating magnetic fields above Nyquist frequency. The approach could precisely evaluate aliased signals from oscillating magnetic fields with random frequencies and on/off intervals, which was demonstrated in both simulations and phantom studies. The detected signals showed higher SNR with increasing number of dynamic scans, demonstrating that temporal averaging is possible with the proposed approach. The two-TR approach provided the absolute frequency spectrum above the Nyquist frequency pixel by pixel with no a priori target frequency information. The proposed method has many advantages for detecting high frequency neuronal oscillations, which warrants further investigation.\n\n"}
{"case_id": "5079291508735025cf2643c7a59d0293", "answer_text": "\n\nMeasuring the distance between two Bluetooth devices using Received Signal Strength Indicator (RSSI) values is a widely studied problem, and several approaches have been developed to map RSSI values to distance [0][1][2][3][5][6][7][8][9][12]. Here, we provide a comprehensive overview of these approaches and their effectiveness.\n\nEmpirical and Theoretical Models [2]\n\n1. **Friis Transmission Equation**: One of the fundamental methods for mapping RSSI to distance is based on the Friis transmission equation, which models the relationship between signal strength and distance in free space [1]. The equation is given by: \n   \\[\nRSSI = -10n \\log_{10}(d) - A [9].\n   \\]\n   where \\(d\\) is the distance, \\(n\\) is the path-loss exponent that describes the signal attenuation rate, and \\(A\\) is the RSSI value at a reference distance, usually 1 meter [5][9][12]. The parameters \\(A\\) and \\(n\\) are often determined experimentally by collecting RSSI values at known distances and fitting the model to the data using least squares or other fitting techniques [12][6][0].\n\n### Filtering Techniques [1][2][7]\n\n\n2. **Low Pass Filters and Gaussian Filters**: RSSI values can be highly variable due to environmental factors like reflections, obstructions, and interference. Filters such as the Low Pass Filter (LPF) are applied to smooth out these fluctuations. LPFs work by averaging the RSSI values over time, thus reducing the influence of sudden spikes or drops in signal strength [7]. The Gaussian Filter is another effective technique that applies a Gaussian function to weight RSSI values, which helps in enhancing accuracy and reducing noise [1]. The Dual Gaussian Filter (DGF) algorithm has shown improved performance over traditional filters like the Kalman Filter for indoor localization tasks [1].\n\n3. **Orderly Statistical Filters**: Another approach involves using statistical filters such as the ordered statistic filter (OSF) to obtain a stable RSSI value. This method collects multiple samples of RSSI values, sorts them, and selects a value based on a predefined percentile, which helps in mitigating the effects of outlier values [2].\n\nMachine Learning and Data-Driven Methods [14]. \n\n4. **Neural Networks and Particle Swarm Optimization (PSO)**: Machine learning techniques, such as neural networks, have been used to model the complex relationship between RSSI and distance. These models are trained on a dataset of RSSI values and corresponding distances to predict distances more accurately than traditional empirical models. However, simple neural networks might fall into local minima; therefore, techniques like Particle Swarm Optimization (PSO) have been proposed to optimize the network's weights and thresholds, enhancing the model's performance [12].\n\n5. **Support Vector Machines (SVM) and Clustering Algorithms**: SVMs and other machine learning classifiers can be trained to classify the proximity of devices based on RSSI values. For example, an SVM classifier can be trained to determine whether two devices are within a certain distance threshold (e.g., 6 feet) by using labeled training data consisting of RSSI values at various distances [14]. Additionally, clustering algorithms can be used to group RSSI measurements and thereby improve distance estimation by reducing the impact of anomalous data points [10].\n\n### Limitations and Considerations [9]\n\nWhile these methods can significantly improve the accuracy of distance estimation using RSSI [1], several limitations and practical considerations must be taken into account:\n\n- **Environmental Factors**: RSSI values are highly susceptible to environmental changes such as the presence of obstacles, reflections, and multi-path effects [5][7][11]. Hence, any model or algorithm needs to be calibrated and possibly re-trained for different environments to maintain accuracy.\n- **Device Variability**: Different devices might have different transmission powers and sensitivities, affecting the RSSI values. Consistency across devices can be achieved by calibrating each device before deploying the system [13].\n- **Dynamic Adjustment**: Algorithms that dynamically adjust parameters based on real-time data, such as particle filters with dynamic weight adjustment strategies, can offer better performance in varying conditions [2].\n\nIn summary, several robust methods exist for converting RSSI values to distance, each with its own strengths and limitations. Employing a combination of empirical models, filtering techniques, and machine learning can yield the best results for specific applications [12]. \n\n\nReferences:\n[0] A Low Energy IoT Application Using Beacon for Indoor Localization convert the RSSI value into distance, it is necessary to perform a calibration test in order to define both the RSSI value at a distance of 1 m from the source and the value of the signal propagation constant. The knowledge of these two parameters allows the conversion from dBm to meters. To achieve the final goal, several tests were carried out to test the behavior of the RSSI values of the master devices as a function of the coordinates (x, y). The experimental tests carried out were carried out by holding the wearable device still in a specific area. A static test was carried out by placing the master devices at 8 m each and dividing the area into 16 zones of area 2 × 2 m, as shown in Table 4. The choice to use this initial arrangement is linked to the fact that the HC-05 devices transmit at a maximum distance of 10 m: at the extreme points, in fact, the slave is just over 10 m away from the masters. A total of 60 measurements were made for each single zone. Taking into consideration the zone (3; 3) it can be seen from Figure 8 the presence of a high number of coordinates outside the reference zone. The values that fall within the marked area are defined as true positives (true positive-TP), while those outsides are called false negatives (false negative-FN). To determine all the parameters for the calculation of precision, recall, accuracy and F1-score, all the areas adjacent to the area\n\n[1] Accuracy Enhancement of RSSI-based Distance Estimation by Applying Gaussian Filter RSSI values of Bluetooth Low Energy (BLE) beacon are unreliable to use localization. To cope with this problem, we propose a new localization algorithm that enhances the accuracy of RSSI value. The proposed algorithm applies Gaussian filter to RSSI values from BLE Beacons, and then uses weight value based on filtered RSSI quality. Background/Objectives: Friis formula is used to calculate distance between a BLE beacon and BLE scanners using triangulation scheme. Finally, Gaussian filtering is applied twice to the location values for accuracy improvement. Findings: Experiments are performed in indoor environment, and experiment data is calculated by MATLAB to make graph and chart for easily comparing location result. Experiment result shows that DGF algorithm shows more accurate and reliable localization result than commonly used Kalman filter algorithm. Furthermore, DGF algorithm is very effective to calculate not only distance but also location. Improvements: DGF algorithm indicates excellent performance when we adapt weight value. The proposed system can be used to tracking the location of BLE beacon in real-time.\n\n[2] An indoor fusion positioning algorithm of Bluetooth and PDR based on particle filter with dynamic adjustment of weights calculation strategy range was set to 0.5 to 5m with a sampling interval of 0.5m, and collection of 100 data at each sampling point. Fig. 5 (a) demonstrates the raw data collected by the smartphone at the sampling points of 1 meter, 2 meters, 3 meters, and 4 meters away from the Bluetooth base station. It is evident that the RSSI values of adjacent sampling points partially overlap. Through many measurements, we found that when the sampling distance is greater than 4 meters, as shown in Fig. 5 (b), the RSSI values will overlap, and so the distance measurement based on Equation 7 cannot be performed correctly. Therefore, we recommend that when using Bluetooth base stations, the distance between two adjacent Bluetooth base stations should be within 4 meters. We used a total of eight sets of sampled data from 0.5 to 4 meters. Each collection of sampled data contained 100 RSSI values and generated 81 sub-datasets using a sliding window strategy (window size = 20 and step = 1). Application of the orderly statistical filter on each sub-dataset was used to obtain a relatively stable RSSI value. We used these 648 data points to fit Equation 7 and got A = -66.21 and n = 1.891. Fig. 6 was the fitted curve of the signal propagation model. In all the following experiments, we converted the RSSI value to distance based on this fitted curve. The results of the orderly statistical filter (OSF) and the maximum and minimum filter (MMF) at the four sampling points (1 to 4\n\n[3] An Energy-saving Fuzzy Control Fan Array with Bluetooth Received Signal Strength Indicator Sensing shows the regression analysis curve of Bluetooth RSSI relative to the distance. We plotted the regression analysis curve of Bluetooth RSSI relative to the distance using the averaged RSSI values of different measurement points and corresponding to the straight line distance between the transmitter and the receiver. Because Bluetooth used three different frequency bands for transmission in the Bluetooth Advertising Protocol, different channel effects may be observed in the different frequency bands, and the actual channel conditions may change. We can see that there are many divergent situations, but owing to the close distance between the transmitter and the receiver, a higher sensitivity and a higher signal stability to the distance are observed. Position Estimation As shown in Fig. 8, the RSSI values were measured with the three Bluetooth modules. The RSSI values are converted to the distance using the regression analysis curve of the Bluetooth RSSI relative to the distance; then, we used this distance to draw the position estimation diagram around the center of the Bluetooth module. The radius of the circle is the distance estimated using the regression analysis curve. If the circle is large, it means that a weak signal is received and that the distance between the transmitter and the receiver is far. In contrast, a small circle means a strong signal. The figure shows the relationship between the three Bluetooth module distances and RSSI. It can also be seen that the closer Bluetooth module can provide more accurate location information, and through the two remaining Bluetooth modules, the direction of\n\n[4] A Bluetooth/PDR Integration Algorithm for an Indoor Positioning System a distance of one meter to the Beacon, called txPower. Combined with RSSI, this allows estimating the actual distance between the device and the Beacon. For example, we can measure a bunch of RSSI measurements at known distances, do a best fit curve to match the data points and convert the best fit curve into an algorithm. In an ideal environment, this method is able to guarantee accurate positioning. However, since RSSI has been affected by multiple factors including signal reflection, scattering and diffraction, large errors will arise in practice. Therefore, it will be very hard to secure a 100% accurate distance measurement based on this principle. Meanwhile, when we are using Bluetooth for the positioning, we just want to define an approximate position range. Therefore, this paper does not use the distance measuring model. Instead, this paper used a pseudo threshold value of \"1 m\" as the txPower to detect the Bluetooth signals within 5 m. We turned the power up to 100% (+4 dBm), then measured the RSSI and the accuracy at the different varying distances. As shown in Figure 1, we made an observation for 2 min every other meter and recorded the changes in signal intensity to generate an intensity-distance change chart. The figure reveals that the further the distance is, the weaker the intensity is. Since we want the Beacon to have a sensing range of around 5 m, then it is necessary to set an appropriate distance as the sensing threshold for Beacon. In this case, we have set the\n\n[5] A multi-step approach for RSSi-based distance estimation using smartphones Measuring distance from Received Signal Strength Indication (RSSI) of wireless devices has become one of the rudimentary but challenging requirements for Indoor Positioning and Indoor Navigation (IPIN). To address this subject, we propose a novel multi-step approach combining Flat Earth Model, Free Space Friis Model and Linear Approximation Model for measuring distance from RSSI for smart devices with Bluetooth Low Energy (BLE) connectivity. To get better result we proposed an improved averaging and smoothing algorithm of RSSI. We have significantly achieved 13.4% reduced error of measured distance.\n\n[6] Distance Estimation of Smart Device using Bluetooth Distance estimation identifies the distance between two machines in wireless network. The Received Signal Strength Indication (RSSI) of Bluetooth can be used to estimate distance between smart devices. The characteristic of Bluetooth RSSI value is different as environments. So, we have tested the relation between distance and Bluetooth RSSI value in several environments, such as indoor hall, meeting room, and ElectroMagnetic Compatibility (EMC) chamber environment. This paper shows the distance characteristic of Bluetooth RSSI from these experiment results. There are a lot of measurement errors at Bluetooth RSSI raw data. The minimum RSSI value is -88 dBm and the maximum RSSI value is -66 dBm at 11m of the indoor hall environment. The difference between maximum value and minimum value is 22 dBm. So, it is hard to estimate the distance using Bluetooth RSSI raw data. Therefore, we use the Low Pass Filter (LPF) for reducing the measurement errors. The minimum RSSI value is -80.6 dBm and the maximum RSSI value is -71 dBm in the same environment. The difference between maximum value and minimum value is just 8.4 dBm. The measurement error is significantly reduced. We compare the distance estimation between the Bluetooth RSSI raw data and LPF data at the EMC environment. This paper shows that the distance estimation is possible with small error rates using Bluetooth RSSI LPF data.\n\n[7] A Low Energy IoT Application Using Beacon for Indoor Localization accuracy and F1-score. The source code loaded on the cards allows to take advantage of the AT mode of the HC-05 module and to initialize the device so that it can perform a Bluetooth scan. Once you have loaded the sketch on the Arduino Pro Mini you have to: • Choose the area of interest and divide it into N-zones; • Make the connection as shown in Figure 7; • Connect the Arduino Pro Mini boards to the pc via the FTDI adapter and USB cable; • Open the serial monitors using a computer in order to store the RSSI values obtained from each single reading in a file, in txt format; • Perform the tests by placing the slave in the selected area, starting the three reference devices simultaneously. Once the slave module is turned on, the three reference modules, positioned at known distances from each other, provide an indication of the RSS value of the Bluetooth module to be located. The power value of each single master is very important as it allows you to determine the distance between the reference Bluetooth module and the target Bluetooth. The conversion from RSSI value to distance is done through the following steps: • LISTA Read RSSI value: The master devices get the RSS value from the scan of the target Bluetooth module; • LISTA Distance calculation: Once the RSS value in dBm is obtained, the distance can be calculated, using the formula RSSI = − 10n log 10 d − P d0 . In order to\n\n[8] Research on a Bluetooth Low Energy Warning Method distance between two devices from the RSSI value of signal strength by calculating the relationship between signal strength and distance in the current environment. The RSSI signal strength and distance conversion formula is as follows: 1 0 (( ( ) ) / (10* )) d abs RSSI A n =− (1) where, d is the calculated distance value (unit: m), RSSI is the received Bluetooth signal strength, ABS () is the absolute value function, a is the signal strength when the distance between the transmitting device and the receiving device is 1m, and n is the environmental attenuation factor (table 1). In this paper, a Bluetooth warning method is designed based on the RSSI value of Bluetooth device and the distance value calculated by equation (1). The method proposed in this paper is mainly used in outdoor space, where the value of n is between 2 and 5. Bluetooth Low Power Warning Method This paper presents a low-power Bluetooth warning method, which keeps the low-power Bluetooth device within the safe distance range designed by itself. By comparing the signal strength indication value sent by the Bluetooth device and the distance between the two devices with the preset signal strength indication value and the preset distance value, the device can be warned in advance that the device is about to be disconnected, an alarm is given, and prevention is made in advance The Bluetooth device is disconnected and cannot receive the signal. Among them, RSSI value is greatly affected by the external environment, so this paper\n\n[9] Location-Aware Wi-Fi Authentication Scheme Using Smart Contract related devices, the RSSI of the Bluetooth is used to estimate the distance of the device around the requesting device, and the devices closest to the requesting device are selected as its related devices. From the version of Bluetooth 2.1, a Bluetooth HCI interface called Inquiry_With_RSSI is introduced and leads to a way to measure RSSI without being directly connected to the device. By using this command, all surrounding Bluetooth devices can be measured at once and a list of sensed Bluetooth devices and their RSSI values can be produced. After obtaining the RSSI of the surrounding devices, taking into account the effects of multipath effects in the indoor environment, we adopt the channel fading model of the indoor environment mentioned in: where N is the signal fading constant, which reflects the characteristics of RSSI, and attenuated according to the environment, d/distance is the distance from the signal source, P is the signal strength when the transmitter and the receiver are separated by 1 m, and RSSI is the received signal strength. Using this channel fading model, we can get the distance corresponding to the RSSI of the surrounding related devices. After obtaining the distance according to the RSSI calculation, we calculate the standard deviation of the distance over a period of time, and select the device with a Sensors 2020, 20, 1062 9 of 22 small standard deviation and a short distance as the related device. The device blocked by people or obstacles can cause the distance calculation to become longer and fluctuated\n\n[10] Inter-Mobile-Device Distance Estimation using Network Localization Algorithms for Digital Contact Logging Applications of statistical fluctuations due to scattering, absorption and reflections in the environment, leading to high variance in the signal strength measurements and low confidence in the distance estimate. This uncertainty has lead many in the community, including Sven Mattison who co-invented Bluetooth, to doubt the feasibility of contact logging apps which rely on \"vanilla point-to-point Bluetooth links\". To overcome this, we can make use of the fact that in populated areas there are typically more than two devices present. For example, on buses, in grocery store lines, and in public parks, connectivity can be modeled with a graph, as in Figure 1. Edges on the graph represent the presence of Received Signal Strength Indication (RSSI) measurements between two devices. Shorter edges, bolded for emphasis, represent stronger signals which are typically less noisy and lead to better estimates. While we may have an RSSI-based distance estimate between two devices, like those shown in red, our objective is to improve on this estimate by leveraging the other available RSSI measurements. We can also use available RSSI measurements to estimate the distance between two devices for which a measurement is unavailable. This is possible assuming Figure 1: A network of available RSSI measurements, with more reliable measurements bolded. Our goal is to leverage all available measurements to get accurate distance measurements for any pair of devices, and improve accuracy for pairs with available measurements like the one shown in red. a centralized architecture in which RSSI measurements are stored in a central server. While decentralized solutions have also\n\n[11] Indoor Positioning Algorithm Based on the Improved RSSI Distance Model convenience of calculation, d 0 usually takes a value of 1 meter. Since X σ has a mean of 0, the distance-loss model can be obtained with where A is the average measured RSSI when the received node is 1 meter away from the transmit node which is related to the RF circuits of Bluetooth nodes. By gathering the RSSI values for Bluetooth beacons at different distances and using the least squares algorithm to fit the parameters, we can obtain the RSSI distance model. It can be seen from (2) that parameters A and n need to be accurately estimated in order to improve the ranging accuracy. Parameter n is related to the wireless transmission environment and can be obtained by fitting a large number of experimental measurements. A is determined by the Bluetooth transmit power. Ideally, the value of A for one Bluetooth beacon should be fixed. In reality, the Bluetooth transmit power has time-varying characteristics. It is difficult to calculate the relationship between RSSI and the distance accurately by using the logarithmic distance loss model because of the complex indoor environment. Researchers have used the neural network to fit the RSSI distance model. However, since the BP neural network is prone to converging to a local minimum point,36, it is difficult to obtain a good indoor RSSI distance model. The use of the particle swarm optimization algorithm (PSO) to optimize the weights and thresholds of the BP neural network can effectively prevent the BP neural network from falling into the local optimal solution\n\n[12] COVID-SAFE: An IoT-Based System for Automated Health Monitoring and Surveillance in Post-Pandemic Life Fig. 7(a) shows the phone-specific values of the reference RSSI (at 1 m distance) and the parameter b for different RSSI levels like maximum, minimum, Q1 (lower limit of the 75% quartile), Q3 (upper limit of the 75% quartile); and Fig. 7(b) shows the phone-specific values of the parameter a for different RSSI levels. For this experiment, a threshold of −93 dBm (taken from the value of parameter b for the maximum RSSI level above 2 m distance) can be used to trigger the proximity aware alarm and the distance calculation function in the smartphone app. However, a, b, and the reference RSSI are dependent on the smartphone used and the real-life environment. There are several algorithms, such as SVM and Machine Learning (ML), with the device or environment-specific training parameterization that can be used to calculate the distance between the devices. In addition, AltBeacon can be used to get device-specific information (manufacturer identification number and 1 m reference RSSI) along with the beacon signal which can be used to improve the distance accuracy for different types of devices used. In order to notify the user to maintain physical distancing, three threshold values are indicated. The software checks the RSSI values then maps them to the distance according to equation (3). If the distance is less than 200 cm a red flag is raised, if the transmitter is in the range of 200 to 300 cm the flag is yellow, and if the distance is longer than 300 cm it is green. Table\n\n[13] Using Machine Learning to Perform Proximity Detection - Classifying Bluetooth Beacon RSSI Values of how well a device can hear a signal from an access point. II. HYPOTHESIS A machine learning classifier algorithm can be trained to accurately predict whether two Raspberry Pis are 6 feet apart or not based on RSSI values. • Bluetooth-based contact tracing must be able to detect whether users are 6 feet apart or closer in order to report possible COVID-19 exposure. • Large amounts of RSSI and distance data will need to be collected from the Raspberry Pi. Raspberry Pis will need to be placed at varying distances apart and RSSI values measured at those distances. A. Plan and Execution The experiment was conducted by placing the Raspberry Pis at a measured distance apart and then recording the RSSI values measured by the scanner Pi. Figure 1 shows the setup of the 2 Pis, separated by a set distance and connected to power sources. The Pis were separated at a distance of 3 feet, 6 feet, and other arbitrary values both inside and outside the \"6 feet apart rule\". Multiple trials were taken to collect data. All RSSI data was compiled into a CSV file. 10 trials of each distance were taken. • During each trial, 60 seconds of scanning and advertising of Bluetooth beacons took place. • One Raspberry Pi was set up as a Bluetooth beacon advertiser that transmitted Bluetooth beacons once every 1 second. The other Raspberry Pi was set up as a Bluetooth scanner that scanned for beacons every 1 second. • The experiment was conducted in a\n\n"}
{"case_id": "e2491bb6e4147c18762e7fb25d0bf1fd", "answer_text": "\n\nGlobal Navigation Satellite Systems (GNSS) receivers indeed generate a Pulse-per-Second (PPS or 1-PPS) signal, which serves as a crucial reference clock signal, especially in time-sensitive applications. This PPS signal is designed to be highly precise, typically indicating the start of each second with high accuracy [2]. \n\n\n### Digitally-Controlled Oscillator (DCO) and its Role\n\n\nThe PPS signal is generated through a series of processes within the GNSS receiver [1]. To fully understand this, we must first consider the roles of the digitally-controlled oscillator (DCO) and the clock bias correction facilitated by Position, Velocity, and Time (PVT) algorithms [2].\n\nA typical GNSS receiver uses an internal oscillator, often a quartz crystal oscillator, to keep time. However, the accuracy of this internal oscillator is not sufficient for precise applications due to factors such as frequency drift and aging. Therefore, it requires constant correction to maintain synchronization with the highly accurate atomic clocks aboard GNSS satellites [3][6].\n\n### PVT Algorithms and Clock Bias Correction [2].\n\nThe GNSS receiver estimates its position, velocity, and clock bias through PVT algorithms. These algorithms use signals received from multiple satellites to calculate the clock bias—the difference between the receiver’s internal clock and the true GNSS system time. The PVT algorithms work by solving the pseudorange equations, which include both spatial coordinates (position) and temporal coordinates (clock bias). These calculations take into account the travel time of satellite signals, which is influenced by the relative motion between the satellite and the receiver, as well as atmospheric conditions [7]. By continuously updating the receiver's estimates of its position and velocity, these algorithms can refine the clock bias estimation with each new set of satellite data.\n\nSynchronization and Re-initialization of PPS Signal[2].\n\nThe digitally-controlled oscillator (DCO) adjusts the frequency of the internal oscillator to correct the clock bias. Essentially, the estimated clock bias retrieved by the PVT algorithms is used to fine-tune the DCO, thereby ensuring that the generated PPS signal is synchronized with the true GNSS system time [1][14]. This precise synchronization is achieved by periodically re-initializing the offset of the PPS signal based on the corrected time provided by the PVT algorithm.\n\nEnhancements in PPS Generation [0].\n\nPopular methods used alongside digitally-controlled oscillators in PPS signal generation include phase-locked loops (PLLs) and temperature-compensated crystal oscillators (TCXOs). PLLs help maintain the phase alignment between the internal oscillator and the external reference by continuously adjusting the oscillator's frequency [5]. TCXOs improve the frequency stability of the oscillator over varying temperatures, which is particularly useful in applications where environmental conditions cannot be controlled [11].\n\nAdditionally, some GNSS receivers use overdetermined clock modes where the coordinates of the antenna are fixed, leaving only the clock offset to be solved for. This further enhances the accuracy of the PPS signal [6]. The PPS signal’s precision is crucial for various applications, such as time synchronization in telecommunications, vehicular networks, and other time-sensitive systems [1][3].\n\nError Sources in PPS Signal Generation Errors in the time of occurrence of the 1 PPS pulses from the GPS receiver consist of three parts: 1) bias or offset due to uncompensated propagation and hardware delay errors in the receiver/antenna system; 2) drift, which is the variation in time over an extended period due to changes of satellites tracked over time; and 3) jitter, which is the short-term variation in timing from pulse to pulse. These error sources are inherent in both GPS system and GPS receiver design/implementation [3]. The total effect of these errors is typically tens of nanoseconds to a few microseconds, depending upon the quality of GPS receivers. Receiver manufactures usually calibrate the receiver bias well, yielding a timing accuracy of 10 ns or better under ideal observational conditions [3]. This high level of accuracy is possible because the timekeeping maintained within the GPS system is continuously adjusted to null out timing errors [3]. \n\nErrors in the PPS signal generation can arise due to several factors, including propagation delays, hardware imperfections, and variations in the satellite signals tracked over time. Despite these challenges, high-quality GNSS receivers can achieve timing accuracy within tens of nanoseconds or better under ideal conditions [3][4].\n\n### Comparative Analysis with Other Clock Technologies\n\nComparative analyses have shown that GNSS disciplined oscillators (GNSS DO) are a common and reliable method for achieving traceability to UTC and maintaining accurate timing information [1]. Notably, the 1-Pulse Per Second (1-PPS) output derived from GNSS receivers is a critical component for ensuring high precision in various timing applications, such as GNSS-based time synchronization for vehicular networks [3]. For instance, GNSS DO configurations often use the 1-PPS signal to synchronize internal clocks with the GNSS time scale, providing accuracy levels within a few nanoseconds to microseconds [6]. Moreover, certain GNSS receivers are optimized for timing applications, providing 1-PPS outputs that align closely with UTC or GPS time, and are capable of compensating for receiver biases to achieve timing accuracies of 10 ns or better under ideal conditions [7]. \n\n\n\nWhile GNSS receivers provide highly accurate PPS signals [6], alternative methods for generating precise clock signals include rubidium atomic clocks, cesium beam clocks, and hydrogen masers [12]. \n\n- **Rubidium Atomic Clocks**: These clocks are widely used due to their excellent short-term stability and relatively compact size. They provide better stability than quartz oscillators but are less accurate over long periods compared to cesium clocks. Rubidium clocks are often used in telecommunications and other industries requiring high precision over short to medium timeframes [5].\n\n- **Cesium Beam Clocks**: These clocks are considered the gold standard for long-term accuracy and stability, providing constant frequency with an error margin of less than one second over millions of years. They are extensively used in national timing laboratories and for applications demanding the highest level of accuracy, such as satellite navigation systems and global timekeeping standards [12].\n\n- **Hydrogen Masers**: Known for their superior long-term stability, hydrogen masers are used in scientific research and deep-space navigation. They offer the best performance for long-duration missions and are critical for applications that require the most stable timekeeping over extended periods [12].\n\nRecent Advancements and Applications [3]. \n\nRecent advancements and applications in GNSS technology are not limited to time synchronization. For instance, in Industry 4.0 settings, GNSS-based time synchronization has been employed to enhance the coordination of automated and semi-automated processes, thereby improving efficiency and minimizing downtime [2]. Moreover, GNSS-based precision timing is critical in scientific research, such as geophysics and climate studies, where precise temporal measurements are essential for data accuracy and modeling [10]. The use of GNSS technology is also expanding into emerging fields like smart grid management and blockchain technology, where synchronized timestamps are crucial for maintaining system integrity and security [2][13].\n\nIn summary, the PPS signal in GNSS receivers is generated through a digitally-controlled oscillator whose offset is periodically re-initialized by correcting the estimated clock bias obtained from PVT algorithms. This method ensures high precision and synchronization of the PPS signal with the GNSS system time, making it a reliable reference for various applications beyond traditional time synchronization [3][6][7].\n\n\nReferences:\n[0] Generating A Timing Information (1-PPS) From A Software Defined Radio Decoding of GPS Signals Generating A Timing Information (1-PPS) From A Software Defined Radio Decoding of GPS Signals We complement a secure, Software Defined Radio (SDR) implementation of Global Navigation Satellite System (GNSS) reception system with the 1-Pulse Per Second (PPS) output to steer the oscillator driving the analog to digital converter, the only reliable timing information in a SDR reception chain. We demonstrate long-term 1-PPS output and the derived Allan deviation consistent with the 10−8 at 1 s decreasing to 10−12 at 104 s performance met with most single frequency GNSS receivers.\n\n[1] Achieving traceability to UTC through GNSS measurements needs to be added. This latter is published monthly by the BIPM in the Circular T. The second configuration, and by far the most common one, is where the output signals of an oscillator (quartz or atomic frequency standard) are disciplined with the help of the received GNSS signals. This configuration is called a GNSS disciplined oscillator (GNSS DO), and its output signals (standard frequency, e.g. 10 MHz, and 1 pulse per second, 1 PPS) represent TS_user. Both options allow TS_user to be related to the time scale GNSS_T derived from the pseudorange measurements made by the receiver using the received satellite signals. By adding the broadcast quantity [bUTC GNSS − GNSS_T, the offset between the user time scale and UTC as predicted in the GNSS navigation message can be calculated as follows: The users' receiver software can calculate the predicted offset at the moment of reception of the signal. The individual satellites of a particular GNSS might broadcast different information at the same time, but the differences are usually within a few nanoseconds. In a GNSS DO, the TS_user is typically realized in such a way that the time offset between TS_user and bUTC GNSS (as in equation (2.2)) is close to zero (for timing applications) or kept constant on average (for frequency applications). In order to relate TS_user to UTC, the difference UTC − bUTC GNSS needs to be added. The latter can be obtained from the following relation where UTC − UTC(k) is provided by BIPM in Circular T, the second term\n\n[2] Trusted GNSS-Based Time Synchronization for Industry 4.0 Applications We propose two possible configurations for the Master node on the left side and on the central part of Figure 2, respectively. Both these configurations include a GNSS module capable to receive the RF signals from one or multiple satellite constellations (e.g., GPS, Galileo, GLONASS, and/or BeiDou) in order to estimate Position, Velocity, and Time (PVT) information. The GNSS receiver typically provides two outputs: 1. a textual interface over a serial communication protocol, providing the PVT data coded as \"sentences\" according to the National Marine Electronics Association (NMEA) 0183 standard, and 2. the 1 Pulse-Per-Second (1PPS), that is a high precision analog signal having leading pulse edges synchronous with the beginning of each second of the time scale. Different SW tools and/or daemons can accept as inputs the NMEA and 1PPS, taking advantage of the Linux kernel support to make them visible as devices. In the configuration #1, these devices are a serial port for NMEA (/dev/serial0) and the Linux Pulse Per Second Application Programming Interface (PPSAPI) for 1PPS (/dev/pps0). A service daemon capable to parse the GNSS data (i.e., gpsd) uses both of them as inputs. We also shared such inputs to ntpd, a daemon implementing the Network Time Protocol (NTP) version 4, by means of two shared memory segments (i.e., 127.127.28.0 and 127.127.28.2) properly configured using the Shared Memory Driver of ntpd. In this way, ntpd is capable to correctly discipline the internal clock of the Master node, ensuring its accurate synchronization with respect to the GNSS time\n\n[3] GNSS Time Synchronization in Vehicular Ad-Hoc Networks: Benefits and Feasibility Errors of the Receiver Timing We now examine the errors of the receiver timing. Only the uncertainty of the GNSS clock bias t r will affect time synchronization because t U T C is common to all VANET nodes. However, the basis for time transfer functions in GPS-based products is one pulse per second (1 PPS) signal supplied by the GPS receiver. This signal is typically a short logic pulse, one edge of which is adjusted by the receiver to be on time with respect to the one second epoch of UTC or GPS time. Errors in the time of occurrence of the 1 PPS pulses from the GPS receiver consist of three parts: 1) bias or offset due to uncompensated propagation and hardware delay errors in the receiver/antenna system; 2) drift, which is the variation in time over an extended period due to changes of satellites tracked over time; and 3) jitter, which is the short-term variation in timing from pulse to pulse. These error sources are inherent in both GPS system and GPS receiver design/implementation. The total effect of these errors is typically tens of nanoseconds to a few microseconds, depending upon the quality of GPS receivers. Receiver manufactures usually calibrate the receiver bias well, yielding a timing accuracy of 10 ns or better under ideal observational conditions. This high level of accuracy is possible because the timekeeping maintained within the GPS system is continuously adjusted to null out timing errors. A typical GNSS receiver has an internal quartz based oscillator that continuously runs\n\n[4] Detecting GNSS misbehaviour with high-precision clocks diagram of the system: the phase offset is used to detect potential GNSS attacks. some limitations for extremely long integration times. On the other hand, such stable clocks are expensive, bulky, and often not suited for embedded computing devices or many relatively smallfootprint mobile devices. Low-cost oscillators can be used, but they have poor stability and require continuous tuning. For this reason, they are not stable enough for attack detection. In contrast, chipscale OCXOs provide adequate performance as clock references, in a compact and power-efficient form factor. This is why we develop a custom platform based on commercially available components, designed to work in tight integration with the GNSS receiver. The clock states (phase offset, frequency offset and frequency drift) of the GNSS receiver are continuously tested against the custom reference, to detect misbehavior effects on the GNSS receiver clock. The design estimates the relative phase between the 1-Pulse-per-Second (PPS) signal (which is synchronous to the top of the second of the GNSS PVT solution) against a 1-PPS clock obtained from a free-running hardware clock. Any deviation of the GPS time induced by the attacker causes a phase change to the victim receiver PPS. If such variation is beyond the normal drift rate of the reference clock, it can be an indication of an attack. The device is implemented with mixed clock domain blocks in a Field Programmable Gate Array (FPGA). The FPGA implementation allows for high-resolution measurements of the phase deviation, with predictable latency. Figure 1 shows the implementation of the detection and control\n\n[5] High-precision Hardware Oscillators Ensemble for GNSS Attack Detection receiver to calculate the PVT solution. The time-sensitive measurements of the attack detection sys- tem are implemented on an FPGA ( Figure 6). This solution combines time-sensitive phase measurement logic with embedded micro-controllers in a small form-factor, allowing reconfiguration and scaling of the system, potentially growing the ensemble, with more clocks. The phase offset between the 1-pulse per second (1PPS) line obtained from the GNSS receiver and a 1PPS derived from each reference clock is measured by means of an asynchronous phase detector. This allows measuring the phase shift with a resolution limited only by the counter clock. The phase information is processed by tracking the time and frequency information obtained by the GNSS receiver. Figure 6: Basic reference clock block: the PLL is used to create high speed clocks that measure the phase of the GPS signal and provide precise timekeeping. To generate the spoofing scenarios, a BladeRF softwaredefined radio (SDR) is used to 'play' the scenarios from TEX-BAT, allowing repeatable evaluation of various GPSfocused spoofing attacks, as shown in Figure 7. Although the SDR clock is suitable to generate GNSS signals that allow the receiver to calculate the correct time and position, the quality of the radio's clock is critical to achieve good stability of the generated PPS information. This is evident in the phase plot in Figure 8. The phase of the PPS signal is measured against a rubidium standard reference clock. First, when the SDR is clocked using the on-board oscillator, the phase presents a significant variance. Second, a rubidium\n\n[6] RELATIVE TIMING CHARACTERISTICS OF THE ONE PULSE PER SECOND (1PPS) OUTPUT PULSE OF THREE GPS RECEIVERS The atomic clocks orbiting around the earth in GPS satellites can be utilised by anyone to obtain accurate timing. All that is required is an inexpensive GPS receiver and a clear view of the sky. This has enabled a revolution in applications requiring synchronization such as cellular networks, telecommunications timing, digital TV and radio transmission, calibration laboratory systems and the internet. The atomic clocks in the GPS satellites are monitored and compared to 'master clocks' by the GPS Operational Control Segment, this 'GPS time' is steered to within one microsecond of Universal Time. Many GPS receivers provide a 'one pulse per second' (1PPS) output signal, and some receivers are optimised for timing applications. This paper examines the 1PPS output pulse of three inexpensive GPS receivers: Trimble Lassen LP, Motorola M12+ Oncore and Sigtec MG5001. The Trimble and Motorola receivers have an overdetermined clock mode that enables the coordinates of the antenna to be fixed in the solution, leaving the clock offset only to be solved for. Measurements of the offset of the 1PPS pulse of the receivers compared to a Leica CRS1000 where taken. The time offsets were found to be approximately 640ns for the Motorola and Sigtec receivers, and 1500ns for the Trimble receiver. Improvements in the offset standard deviation were observed when operating the Trimble and Motorola receivers in time-only solution mode. The Motorola and Sigtec receivers where found to have smaller offset standard deviations than the Trimble receiver.\n\n[7] Precise Receiver Clock Offset Estimations According to Each Global Navigation Satellite Systems (GNSS) Timescales Precise Receiver Clock Offset Estimations According to Each Global Navigation Satellite Systems (GNSS) Timescales Abstract Each GNSS constellation operates its own system times; namely, GPS system time (GPST), GLONASS system time (GLONASST), BeiDou system time (BDT) and Galileo system time (GST). They could be traced back to Coordinated Universal Time (UTC) scale and are aligned to GPST. This paper estimates the receiver clock offsets to three timescales: GPST, GLONASST and BDT. The two measurement scenarios use two identical multi-GNSS geodetic receivers connected to the same geodetic antenna through a splitter. One receiver is driven by its internal oscillators and another receiver is connected to the external frequency oscillators, caesium frequency standard, kept as the Thailand standard time scale at the National Institute of Metrology (Thailand) called UTC(NIMT). The three weeks data are observed at 30 seconds sample rate. The receiver clock offsets with respected to the three system time are estimated and analysed through the geodetic technique of static Precise Point Positioning (PPP) using a data processing software developed by Wuhan University - Positioning And Navigation Data Analyst (PANDA) software. The estimated receiver clock offsets are around 32, 33 and 18 nanoseconds from GPST, GLONASST and BDT respectively. This experiment is initially stated that each timescale is inter-operated with GPST and further measurements on receiver internal delay has to be determined for clock comparisons especially the high accuracy clock at timing laboratories.\n\n[8] Phase Compensation Sensor for Ranging Consistency in Inter-Satellite Links of Navigation Constellation Phase Compensation Sensor for Ranging Consistency in Inter-Satellite Links of Navigation Constellation The performance of the global navigation satellite system (GNSS) can be enhanced significantly by introducing the inter-satellite links (ISL) of a navigation constellation. In particular, the improvement of the position, velocity, and time accuracy, and the realization of autonomous functions require the ISL distance measurement data as the original input. For building a high-performance ISL, the ranging consistency between navigation satellites becomes a crucial problem to be addressed. Considering the frequency aging drift and the relativistic effect of the navigation satellite, the frequency and phase adjustment (FPA) instructions for the 10.23 MHz must be injected from the ground station to ensure the time synchronization of the navigation constellation. Moreover, the uncertainty of the initial phase each time the onboard clock equipment boots also results in a pseudo-range offset. In this Ref., we focus on the influence of the frequency and phase characteristics of the onboard clock equipment on the ranging consistency of the ISL and propose a phase compensation sensor design method for the phase offset. The simulation and experimental results show that the proposed method not only realized a phase compensation for the pseudo-range jitter, but, when the 1 PPS (1 pulse per second) falls in the 10.23 MHz skip area, also overcomes the problem of compensating the ambiguous phase by directly tracking the 10.23 MHz to ensure consistency in the ranging. Introduction Position, velocity, and time (PVT) accuracy, integrity, continuity, and availability are the four major performance indicators for satellite navigation systems\n\n[9] An Experimental Validation of Accurate and Precise GNSS Time Synchronization in Vehicular Networks NMEA from the receiver. Fig.5 depicts the overall architecture of this experimental setting including how the reference clock is integrated and updates the system clock. To evaluate and analyze the performance of GNSS time integration into our developed node, we have conducted a series of laboratory-based tests to characterize the timing performances of the GNSS receiver. We have also deployed the developed node for field experiments. Finally, we have synchronized three nodes with GNSS time to determine relative timing accuracy among them. The results and their analysis are presented below. B. Performance of Node Clock Synchronization In our laboratory test, from the message of the serial communication port RS 232 of the GPS receiver, the time tag is retrieved. It is also extracted separately from the PPS signal received from the available DCD pins of the same port. It is noticed that a number of commercial GPS receivers do not provide PPS signal output. Without PPS, it requires investigations whether or not the attainable timing accuracy from GPS NMEA data meets the application requirements. The timing information received from RS232 message through UART port is plotted in Fig. 6. It is seen from Fig. 6(a) that the recorded offsets of time are limited within 10 ms when the temperature is maintained constant at 16 • C. The employed GPS receivers from U-Blox company is made up with Temperature Compensated Crystal Oscillator (TCXO), which has good stability over a broad range of temperatures (sub 0.1 ppm frequency stability over an extended temperature range (50 • C to\n\n[10] A novel PPP Disciplined Oscillator A novel PPP Disciplined Oscillator A novel PPP Disciplined Oscillator demonstrating Modified Allan Deviation below 10$^{-13}$ at tau 200 seconds, and 8x10$^{-15}$ at tau 10 000 seconds is presented. The local oscillator is disciplined based on the differenced clock bias estimates from two separate RTKLIB instances, one processing GPS observations from a remote receiver clocked by a reference H-maser, and the other from a local receiver clocked by the disciplined oscillator. Real time clock and orbit corrections are used for the PPP processing. Our results demonstrate how the open availability of real time GNSS corrections enables low cost time/frequency distribution with a stability surpassing high-performance industrial cesium clocks. The experimental setup is described, and results are presented from a zero baseline common clock, a zero baseline and a long baseline test.1\n\n[11] An Experimental Validation of Accurate and Precise GNSS Time Synchronization in Vehicular Networks and tunnels limiting the signal reception. Therefore, the challenge is to achieve the required synchronization accuracy in real-world road environments via low-end receivers that are popularly used in modern vehicles. In this research, we leverage the advancement of multi-GNSS receivers that can receive satellite signals from all active constellations, thus increasing the signal coverage or solution availability. The research is significant in realizing GNSS capacity to provide the required time synchronization for vehicular networks. This paper studies the attainability and capacity of the GNSS timing service to vehicular networks by utilizing lowcost consumer-grade GNSS receivers and off-the-shelf communication devices. It conducts a systematic analysis of the required time synchronization for vehicular networks. Then, it presents a GNSS-based precise time synchronization solution for vehicular networks. While developing the solution the timing capacity of GNSS is determined in three stages: (i) the operational stability and accuracy of GNSS steering time from NMEA (National Marine Electronics Association) sentence data, (ii) the accuracy of the receiver generated electrical signal pulse named Pulse Per Second (PPS) signal, that repeats arXiv:2010.14201v2 [cs.NI 3 May 2021 precisely once per second, and (iii) the stability and accuracy of time by combining NMEA and PPS. Overall, conducting error analysis on a series of field tests, this paper demonstrates that using GNSS service, the attainable timing accuracy is better than existing DSRC-and LTE-based timing services. The paper is organized as follows: Section II defines the requirements for time synchronization in VANET. Section III analyzes the synchronization problem with DSRC-based communication technologies for vehicular networks. GNSS time\n\n[12] Research on Absolute Calibration of GNSS Receiver Delay through Clock-Steering Characterization output. For the timing receiver, there is also PPS output which always synchronizes with the measurement data, in other words, the timing receiver delay can be defined as the time bias from RF signal input to PPS output,19. Altogether, GNSS receiver uses the clock-bias to adjust RecT after until clock-bias under the minimum range, which represents RecT with PPS output. Therefore, we divide GNSS receiver delay into two parts as shown in Figure 2. One is the channel delay existing in pseudorange, the other is PPS output delay caused by hardware link. Compared with the relatively fixed PPS output delay, the channel delay varies from different RF signals, reference clocks, parameter settings, etc. At the beginning of receiver operation, RecT is frequently divided into generation by the receiver clock at any time. In order to synchronize RecT with a stable time, the receiver will adjust 1 RecT by clock-bias 1 t Δ after first positioning. If the receiver does not have PPS input, RecT will keep synchronous to global navigation satellite systems time (GNSST) such as GPS time (GPST), otherwise PPS input instead. Ideally n t Δ is equal to zero as RecT tends to be stable. However, due to hardware configuration n t Δ can only be accurate within a certain range, such as receiver clock of 100 MHz only accurate to 10 ns, which exists an unknown delay in pseudorange. For some receivers such as Septentrio n t Δ has already been deducted from the original measurement, which means the pseudorange is a\n\n"}
{"case_id": "0650cf7bfe45d90ba3e1bb72427bf04a", "answer_text": "\n\nThe Nyquist-Shannon sampling theorem stipulates that to accurately reconstruct a signal, it must be sampled at a rate greater than twice its highest frequency component [7]. This sets an upper bound for the sampling period, which prevents aliasing and ensures signal fidelity [7]. However, setting a lower bound for the sampling period is also critical and is influenced by factors such as noise and system dynamics [12].\n\nNoise Considerations and the Lower Bound[5]. \n\nWhen sampling too frequently, noise introduced at each sampling instance can accumulate, leading to a diminished signal-to-noise ratio (SNR). In the context of Gaussian noise, oversampling can actually result in more significant noise effects because each sample carries its own random noise [2]. Therefore, if the sampling period is too short, the aggregated noise can obscure the underlying signal, making it more challenging to extract meaningful information from the data [2].\n\nMoreover, the addition of more samples can lead to correlated noise in the sampled data, complicating the noise whitening process. In systems where noise whitening is required to separate signal from noise, frequent sampling may degrade the model's performance due to the complex correlation structure introduced by oversampling [1][3].\n\nTemporal Dynamics and System Response[4].\n\nThe temporal dynamics of the system being observed also play a critical role. In dynamic systems with high inertia or slow-changing states, sampling too frequently can result in redundant data, which does not provide additional information about the system state but increases the computational load and potential for noise accumulation [4]. For instance, in a Kalman filter designed for a mechanical system with slow dynamics, sampling at a very high rate compared to the system response time adds noise without yielding a proportionate improvement in state estimation accuracy [5].\n\nConversely, in systems with fast-changing dynamics, such as high-speed communication channels or rapidly moving objects, undersampling can lead to poor state estimation because the sampling rate might miss significant changes in the system's state between samples. In these cases, a higher sampling rate within the bounds of the Nyquist theorem is necessary to capture the rapid dynamics accurately [4].\n\nOptimal Sampling Rate[11].\n\nTo find an optimal sampling rate, one needs to consider both the upper and lower bounds set by the Nyquist theorem and noise considerations. The optimal sampling rate is generally a compromise that balances the need to capture sufficient temporal resolution to track system dynamics effectively while minimizing the detrimental impact of noise [3].\n\nSeveral approaches can be employed to determine this optimal rate [5]. \n\n1. **Power Spectral Density (PSD) Analysis**: By analyzing the PSD of the noise and the signal, one can identify frequencies where noise power outweighs signal power. For example, if the noise dominates at higher frequencies, it might be beneficial to lower the sampling rate to avoid these regions, thus improving the SNR of the sampled signal [6].\n\n2. **System Identification Techniques**: These techniques involve creating models of the system's dynamics and evaluating the impact of different sampling rates on the accuracy of these models. For example, methods such as auto-regressive moving average (ARMA) modeling can help in selecting a sampling interval that captures the essential dynamics without redundancies [4].\n\n3. **Empirical Methods**: Empirical methods often involve iteratively testing different sampling rates. For instance, in a Kalman filter setup, one could run simulations at various sampling rates and evaluate the mean square error (MSE) of the state estimates. The sampling rate that minimizes the MSE while providing a sufficient temporal resolution would be deemed optimal [2][5].\n\nThe choice of prefilters in (16) achieves the upper bounds on all singular values, and is hence universally optimal regardless of the water level [0]. Since \\(\\tilde{{\\bf F}}_{s}\\) has orthonormal rows, it acts as an orthogonal projection and outputs an \\(M\\)-dimensional subspace [0]. The rows of the diagonal matrix \\({\\bf F}_{h}\\) are orthogonal to each other [0]. Therefore, the subspace closest to the channel space spanned by \\({\\bf F}_{h}\\) corresponds to the \\(M\\) rows of \\({\\bf F}_{h}\\) containing the highest channel gains out of the entire aliased frequency set \\(\\left\\{f-\\frac{lf_{s}}{M}\\mid l\\in\\mathbb{Z}\\right\\}\\) [0]. The maximum data rate is then achieved when the filter bank outputs \\(M\\) frequencies with the highest SNR among the set of frequencies equivalent modulo \\(\\frac{f_{s}}{M}\\) and suppresses noise from all other branches [0]. \n\nIn the context of the Kalman filter, various studies have demonstrated the importance of selecting an appropriate sampling rate. For instance, a recent study [14] showed that in estimating the hidden state of a discrete-time stochastic nonlinear dynamic system, oversampling did not lead to significant tracking performance improvement but did increase computational costs and noise. Conversely, undersampling failed to capture rapid dynamics, resulting in poor state estimation. \n\nConclusion[0][1][4][5][7][8][11][13][14]. \n\nIn summary, setting a lower bound for the sampling period is crucial due to noise accumulation and the correlation effects, particularly in systems where noise whitening is required [1]. The optimal sampling rate lies between this lower bound and the Nyquist limit, and it can often be determined using a combination of PSD analysis, system identification techniques, and empirical testing to minimize MSE and maximize SNR [4]. \n\nKey Points:\n\n- **Upper Bound:** Nyquist theorem dictates sampling > twice the highest frequency to prevent aliasing [12]. \n- **Noise Considerations:** Too frequent sampling accumulates noise, degrading SNR [2].\n- **System Dynamics:** Frequent sampling in slow dynamics adds noise; rare sampling in fast dynamics misses changes [5]. \n- **Optimal Rate Determination:** Use PSD analysis, system identification, and empirical testing [4]. \n- **Conclusion:** Balance between minimizing MSE and maximizing SNR to find the optimal sampling rate [4].\n\n\nReferences:\n[0] Shannon Meets Nyquist: Capacity of Sampled Gaussian Channels The choice of prefilters in (16) achievesthe upper bounds on all singular values, and is hence universallyoptimal regardless of the water level. Since 𝐅~ssubscript~𝐅𝑠\\tilde{{\\bf F}}_{s}has orthonormal rows, it acts as an orthogonal projection and outputsan M𝑀M-dimensional subspace. The rows of the diagonal matrix 𝐅hsubscript𝐅ℎ{\\bf F}_{h}are orthogonal to each other. Therefore, the subspace closest to thechannel space spanned by 𝐅hsubscript𝐅ℎ{\\bf F}_{h} corresponds to the M𝑀M rowsof 𝐅hsubscript𝐅ℎ{\\bf F}_{h} containing the highest channel gains out of theentire aliased frequency set {f−l​fsM∣l∈ℤ}conditional-set𝑓𝑙subscript𝑓𝑠𝑀𝑙ℤ\\left\\{f-\\frac{lf_{s}}{M}\\mid l\\in\\mathbb{Z}\\right\\}.The maximum data rate is then achieved when the filter bank outputsM𝑀M frequencies with the highest SNR among the set of frequenciesequivalent modulo fsMsubscript𝑓𝑠𝑀\\frac{f_{s}}{M} and suppresses noise from allother branches.\n\n[1] Shannon Meets Nyquist: Capacity of Sampled Gaussian Channels The sampled analog channel under filter-bank sampling can be studiedthrough its connection with MIMO Gaussian channels (see Fig. 9).Consider first a single frequency f∈[−fs/2​M,fs/2​M𝑓subscript𝑓𝑠2𝑀subscript𝑓𝑠2𝑀f\\in[-f_{s}/2M,f_{s}/2M. Sincewe employ a bank of filters each followed by an ideal uniform sampler,the equivalent channel has M𝑀M receive branches, each correspondingto one branch of filtered sampling at rate fs/Msubscript𝑓𝑠𝑀f_{s}/M. The noise receivedin the i𝑖ith branch is zero-mean Gaussian with spectral density∑l∈ℤ|Si​(f−l​fsM)|2​𝒮η​(f−l​fsM),f∈[−fs2​M,fs2​M,subscript𝑙ℤsuperscriptsubscript𝑆𝑖𝑓𝑙subscript𝑓𝑠𝑀2subscript𝒮𝜂𝑓𝑙subscript𝑓𝑠𝑀𝑓subscript𝑓𝑠2𝑀subscript𝑓𝑠2𝑀\\displaystyle\\sum_{l\\in\\mathbb{Z}}\\left|S_{i}\\left(f-\\frac{lf_{s}}{M}\\right)\\right|^{2}\\mathcal{S}_{\\eta}\\left(f-\\frac{lf_{s}}{M}\\right),\\quad f\\in\\left[-\\frac{f_{s}}{2M},\\frac{f_{s}}{2M}\\right,indicating the mutual correlation of noise at different branches.The received noise vector can be whitened by multiplying 𝐘​(f)=[⋯,Y​(f),Y​(f−fs),⋯T𝐘𝑓superscript⋯𝑌𝑓𝑌𝑓subscript𝑓𝑠⋯𝑇{\\bf Y}(f)=[\\cdots,Y(f),Y(f-f_{s}),\\cdots^{T}by an M×M𝑀𝑀M\\times M whitening matrix (𝐅s​(f)​𝐅s∗​(f))−12superscriptsubscript𝐅𝑠𝑓superscriptsubscript𝐅𝑠𝑓12\\left({\\bf F}_{s}(f){\\bf F}_{s}^{*}(f)\\right)^{-\\frac{1}{2}}.Since the whitening operation is invertible, it preserves capacity.After whitening, the channel of Fig. 9at frequency f𝑓f has the following channel matrix(𝐅s​(f)​𝐅s∗​(f))−12​𝐅s​(f)​𝐅h​(f)=𝐅~s​(f)​𝐅h​(f).superscriptsubscript𝐅𝑠𝑓superscriptsubscript𝐅𝑠𝑓12subscript𝐅𝑠𝑓subscript𝐅ℎ𝑓subscript~𝐅𝑠𝑓subscript𝐅ℎ𝑓\\left({\\bf F}_{s}(f){\\bf F}_{s}^{*}(f)\\right)^{-\\frac{1}{2}}{\\bf F}_{s}(f){\\bf F}_{h}(f)=\\tilde{{\\bf F}}_{s}(f){\\bf F}_{h}(f).(14)\n\n[2] Shannon Meets Nyquist: Capacity of Sampled Gaussian Channels By assumption, the noise has spectral density 𝒮η​(f)subscript𝒮𝜂𝑓\\mathcal{S}_{\\eta}(f),so that the filtered noise has power spectral density 𝒮η​(f)​|S​(f)|2subscript𝒮𝜂𝑓superscript𝑆𝑓2\\mathcal{S}_{\\eta}(f)|S(f)|^{2}.The power spectral density of the sampled noise sequence at f∈[−fs/2,fs/2𝑓subscript𝑓𝑠2subscript𝑓𝑠2f\\in[-f_{s}/2,f_{s}/2is then given by ∑l∈ℤ𝒮η​(f−l​fs)​|S​(f−l​fs)|2subscript𝑙ℤsubscript𝒮𝜂𝑓𝑙subscript𝑓𝑠superscript𝑆𝑓𝑙subscript𝑓𝑠2\\sum_{l\\in\\mathbb{Z}}\\mathcal{S}_{\\eta}(f-lf_{s})\\left|S(f-lf_{s})\\right|^{2}.If we term {f−l​fs∣ ​l∈ℤ}conditional-set𝑓𝑙subscript𝑓𝑠 𝑙ℤ\\left\\{f-lf_{s}\\mid\\text{ }l\\in\\mathbb{Z}\\right\\}the aliased frequency set for f𝑓f, then the amount of powerallocated to X​(f−l​fs)𝑋𝑓𝑙subscript𝑓𝑠X(f-lf_{s}) should “match” the corresponding channelgain within each aliased set in order to achieve capacity. Specifically,denote by G​(f)𝐺𝑓G(f) the transmitted signal for every f∈[−fs/2,fs/2𝑓subscript𝑓𝑠2subscript𝑓𝑠2f\\in[-f_{s}/2,f_{s}/2.This signal is multiplied by a constant gain c​αl​ ​(l∈ℤ)𝑐subscript𝛼𝑙 𝑙ℤc\\alpha_{l}\\text{ }(l\\in\\mathbb{Z}),and sent through the l𝑙lth input branch, i.e.X​(f−l​fs)=c​αl​G​(f),∀l∈ℤ,formulae-sequence𝑋𝑓𝑙subscript𝑓𝑠𝑐subscript𝛼𝑙𝐺𝑓for-all𝑙ℤX\\left(f-lf_{s}\\right)=c\\alpha_{l}G(f),\\quad\\forall l\\in\\mathbb{Z},(10)where c𝑐c is a normalizing constant, andαl=H∗​(f−l​fs)​S∗​(f−l​fs)∑l|H​(f−l​fs)​S​(f−l​fs)|2.subscript𝛼𝑙superscript𝐻𝑓𝑙subscript𝑓𝑠superscript𝑆𝑓𝑙subscript𝑓𝑠subscript𝑙superscript𝐻𝑓𝑙subscript𝑓𝑠𝑆𝑓𝑙subscript𝑓𝑠2\\alpha_{l}=\\frac{H^{*}\\left(f-lf_{s}\\right)S^{*}\\left(f-lf_{s}\\right)}{\\sum_{l}\\left|H(f-lf_{s})S(f-lf_{s})\\right|^{2}}.The resulting SNR can be expressed as the sum of SNRs (as shown in) at each branch. Since the sampling operation combinessignal components at frequencies from each aliased set {f−l​fs∣l∈ℤ}conditional-set𝑓𝑙subscript𝑓𝑠𝑙ℤ\\left\\{f-lf_{s}\\mid l\\in\\mathbb{Z}\\right\\},it is equivalent to having a set of parallel MISO channels, each indexedby some f∈[−fs/2,fs/2𝑓subscript𝑓𝑠2subscript𝑓𝑠2f\\in[-f_{s}/2,f_{s}/2. The water-filling strategy is optimalin allocating power among the set of parallel channels, which yieldsthe parametric equation (8)and completes our approximate analysis.\n\n[3] Uncertainty principle, Shannon-Nyquist sampling and beyond By restricting w∈[W𝑤delimited-[𝑊w\\in[W in (27), we haves^​(w)^𝑠𝑤\\displaystyle\\hat{s}(w)=\\displaystyle=PW​r^​(w)+∑k=1∞PW​[r^​(w−kTS​N)+r^​(w+kTS​N)subscript𝑃𝑊^𝑟𝑤superscriptsubscript𝑘1subscript𝑃𝑊delimited-[^𝑟𝑤𝑘subscript𝑇𝑆𝑁^𝑟𝑤𝑘subscript𝑇𝑆𝑁\\displaystyle P_{W}\\hat{r}(w)+\\sum_{k=1}^{\\infty}P_{W}[\\hat{r}(w-\\frac{k}{T_{SN}})+\\hat{r}(w+\\frac{k}{T_{SN}})(28)for W≤1/TS​N𝑊1subscript𝑇𝑆𝑁W\\leq 1/T_{SN} since only the term with k=0𝑘0k=0 on the left-hand side of (27) is non-vanishing for w∈[W𝑤delimited-[𝑊w\\in[W; note that s^​(w)^𝑠𝑤\\hat{s}(w) is non-vanishing only for w∈[W𝑤delimited-[𝑊w\\in[W. The knowledge of r^​(w)=⟨w|r⟩=⟨w|(1−PTD​S)|s⟩^𝑟𝑤inner-product𝑤𝑟quantum-operator-product𝑤1subscript𝑃subscript𝑇𝐷𝑆𝑠\\hat{r}(w)=\\langle w|r\\rangle=\\langle w|(1-P_{T_{DS}})|s\\rangle forw∈(−∞,∞)𝑤w\\in(-\\infty,\\infty) is thus sufficient to recover s^​(w)^𝑠𝑤\\hat{s}(w) bandlimited in w∈[W𝑤delimited-[𝑊w\\in[W and thus original s​(t)𝑠𝑡s(t). Note that TD​S<TS​N≤1/Wsubscript𝑇𝐷𝑆subscript𝑇𝑆𝑁1𝑊T_{DS}<T_{SN}\\leq 1/W.The basis for the above relation (28) is that s​(k​TS​N)=r​(k​TS​N)𝑠𝑘subscript𝑇𝑆𝑁𝑟𝑘subscript𝑇𝑆𝑁s(kT_{SN})=r(kT_{SN}) for all k𝑘k but the bandlimit is different for s^​(w)^𝑠𝑤\\hat{s}(w) and r^​(w)^𝑟𝑤\\hat{r}(w) due to the operation PTD​Ssubscript𝑃subscript𝑇𝐷𝑆P_{T_{DS}}. See (18).\n\n[4] Shannon Meets Nyquist: Capacity of Sampled Gaussian Channels Consider the following sampling problem. Let x​(t)𝑥𝑡x(t) be a zero-meanwide-sense stationary (WSS) stochastic signal whose power spectraldensity (PSD) 𝒮X​(f)subscript𝒮𝑋𝑓\\mathcal{S}_{X}(f) satisfies a power constraint 666We restrict our attention to WSS input signals. This restriction,while falling short of generality, allows us to derive sampling resultsin a simple way.  ∫−∞∞𝒮X​(f)​df=Psuperscriptsubscriptsubscript𝒮𝑋𝑓differential-d𝑓𝑃\\int_{-\\infty}^{\\infty}\\mathcal{S}_{X}(f)\\mathrm{d}f=P. This inputis passed through a channel consisting of an LTI filter and additivestationary Gaussian noise. We sample the channel output using a filterbank at a fixed rate fs/Msubscript𝑓𝑠𝑀f_{s}/M in each branch, and recover a linearMMSE estimate x^​(t)^𝑥𝑡\\hat{x}(t) of x​(t)𝑥𝑡x(t) from its samples in the senseof minimizing 𝔼​(|x​(t)−x^​(t)|2)𝔼superscript𝑥𝑡^𝑥𝑡2\\mathbb{E}(\\left|x(t)-\\hat{x}(t)\\right|^{2}) fort∈ℝ𝑡ℝt\\in\\mathbb{R}. We propose to jointly optimize x​(t)𝑥𝑡x(t) and thesampling method. Specifically, our joint optimization problem cannow be posed as follows: for which input process x​(t)𝑥𝑡x(t) and for whichfilter bank is the estimation error 𝔼​(|x​(t)−x^​(t)|2)𝔼superscript𝑥𝑡^𝑥𝑡2\\mathbb{E}(\\left|x(t)-\\hat{x}(t)\\right|^{2})minimized for t∈ℝ𝑡ℝt\\in\\mathbb{R}.\n\n[5] When to look at a noisy Markov chain in sequential decision making if measurements are costly? a lot about the optimal strategy and achievable cost using tools in stochastic dominance and lattice programming. There main results were: Theorems 1 and 2 gave sufficient conditions for the existence of a monotone optimal sampling strategy (with respect to the posterior distribution) when the underlying Markov chain had two states. It justified the intuition that one should make measurements less frequently when the underlying state is away from the target state. Theorem 4 and Theorem 5 gave sufficient conditions for the myopic sampling strategy to form a lower bound or upper bound to the optimal sampling strategy for multi-state Markov chains. Theorem 6 gave a partial ordering for the transition matrix and noise distributions so that the expected cost of the optimal sampling strategy May 2, 2014 DRAFT decreased as these parameters increased. This yields useful information on the achievable optimal cost of an otherwise intractable problem. Theorem 7, gave explicit bounds on the sensitivity of the total sampling cost with respect to sampling strategy in terms of the Kullback Leibler divergence between the noise distributions. Theorem 9 gave several useful structural properties of the optimal Bayesian filtering update including sufficient conditions that preserve monotonicity of the filter with observation, prior distribution, transition matrix and noise distribution. The assumptions (A1-A7) used in this paper are set valued; so even if the precise parameters (transition probabilities, observation distribution, costs) are not known, as long as they belong to the appropriate sets, the structural results hold. Thus the results have an inherent robustness. Finally, it is interesting\n\n[6] Analytic Minimum Mean-Square Error Bounds in Linear Dynamic Systems With Gaussian Mixture Noise Statistics and MMSE equal to 20 dB for lower bound and 10 dB for the upper bounds. Moreover, depending on the GM noise distributions in the system under study, a tighter upper bound can be selected. For instance if the fitted Gaussians provide better representations of the noise distributions than their components with maximum weights, LMMSE can provide a tighter bound. This is true at approximately KL ≤ 1.5 in our simulations. This is particularly important since the proposed upper bounds are the MSEs of actual implementable filters. Hence, when the upper bound for the MMSE provides the desired estimation precision, the suitable upper bound filter (Kalman or GSF-R depending on the noise models) can be used instead of the MMSE filter to save computational resources. V. CONCLUSION In many signal processing applications, we seek to find an estimation of the inherent state of a dynamic system from the available noisy observations. Bayesian filtering techniques can be applied to the available noisy observations to estimate the unknown state. For linear dynamic systems with Gaussian noise, the Kalman filter provides the MMSE state estimation. Thus, by approximating non-Gaussian noise distributions with Gaussian Mixtures (GM), a bank of Kalman filters or the Gaussian Sum Filter (GSF) can be used to find the MMSE state estimation. However, the MMSE itself is not analytically tractable. Specifically, the proposed analytic bounds in the literature, including Posterior Cramer-Rao, Bobrovsky-Zakai, and Weiss-Weinstein,, do not have a closed-form for GM noise distributions. Hence in this work we first evaluate the MSE\n\n[7] Uncertainty principle, Shannon-Nyquist sampling and beyond Since 1/T=W′>W1𝑇superscript𝑊′𝑊1/T=W^{\\prime}>W,this signal corresponds to oversampling (and this signal corresponds to the detection of the domain below the uncertainty limit from a point of view of the uncertainty principle), and thus one may apply bandlimiting operation down to W𝑊W (or a lowpass filter) to recover the original signal sW​(t)subscript𝑠𝑊𝑡s_{W}(t). To show this, we define a discretized measured signalr⟩S​Nsubscriptket𝑟𝑆𝑁|r\\rangle_{SN} in Shannon-Nyquist sampling by|s⟩=∫𝑑t​|t⟩​⟨t|s⟩⇒|r⟩S​N≡∑k∈ZT​|k​T⟩​⟨k​T|s⟩,ket𝑠differential-d𝑡ket𝑡inner-product𝑡𝑠⇒subscriptket𝑟𝑆𝑁subscript𝑘𝑍𝑇ket𝑘𝑇inner-product𝑘𝑇𝑠\\displaystyle|s\\rangle=\\int dt|t\\rangle\\langle t|s\\rangle\\Rightarrow|r\\rangle_{SN}\\equiv\\sum_{k\\in Z}T|kT\\rangle\\langle kT|s\\rangle,(23)which corresponds to a sampled signal rS​N​(t)=⟨t|r⟩S​N=∑k∈ZT​δ​(t−k​T)​⟨k​T|s⟩subscript𝑟𝑆𝑁𝑡subscriptinner-product𝑡𝑟𝑆𝑁subscript𝑘𝑍𝑇𝛿𝑡𝑘𝑇inner-product𝑘𝑇𝑠r_{SN}(t)=\\langle t|r\\rangle_{SN}=\\sum_{k\\in Z}T\\delta(t-kT)\\langle kT|s\\rangle,and one can confirm that sW′​(t)subscript𝑠superscript𝑊′𝑡s_{W^{\\prime}}(t) in (22) is given by sW′​(t)=⟨t|PW′|r⟩S​Nsubscript𝑠superscript𝑊′𝑡subscriptquantum-operator-product𝑡subscript𝑃superscript𝑊′𝑟𝑆𝑁s_{W^{\\prime}}(t)=\\langle t|P_{W^{\\prime}}|r\\rangle_{SN}. More generally,sW​(t)subscript𝑠𝑊𝑡\\displaystyle s_{W}(t)≡\\displaystyle\\equiv⟨t|PW|r⟩S​Nsubscriptquantum-operator-product𝑡subscript𝑃𝑊𝑟𝑆𝑁\\displaystyle\\langle t|P_{W}|r\\rangle_{SN}(24)=\\displaystyle=∑k∈ZT​⟨t|PW|k​T⟩​⟨k​T|s⟩subscript𝑘𝑍𝑇quantum-operator-product𝑡subscript𝑃𝑊𝑘𝑇inner-product𝑘𝑇𝑠\\displaystyle\\sum_{k\\in Z}T\\langle t|P_{W}|kT\\rangle\\langle kT|s\\rangle=\\displaystyle=T​W​∑k∈Zsin⁡π​W​(t−k​T)π​W​(t−k​T)​s​(k​T)𝑇𝑊subscript𝑘𝑍𝜋𝑊𝑡𝑘𝑇𝜋𝑊𝑡𝑘𝑇𝑠𝑘𝑇\\displaystyle TW\\sum_{k\\in Z}\\frac{\\sin\\pi W(t-kT)}{\\pi W(t-kT)}s(kT)where we defined s​(k​T)=⟨k​T|s⟩𝑠𝑘𝑇inner-product𝑘𝑇𝑠s(kT)=\\langle kT|s\\rangle. This sW​(t)subscript𝑠𝑊𝑡s_{W}(t) is well-defined forW≤W′𝑊superscript𝑊′W\\leq W^{\\prime} since PW​sW′​(t)=⟨t|PW​PW′|r⟩S​N=⟨t|PW|r⟩S​N=sW​(t)subscript𝑃𝑊subscript𝑠superscript𝑊′𝑡subscriptquantum-operator-product𝑡subscript𝑃𝑊subscript𝑃superscript𝑊′𝑟𝑆𝑁subscriptquantum-operator-product𝑡subscript𝑃𝑊𝑟𝑆𝑁subscript𝑠𝑊𝑡P_{W}s_{W^{\\prime}}(t)=\\langle t|P_{W}P_{W^{\\prime}}|r\\rangle_{SN}=\\langle t|P_{W}|r\\rangle_{SN}=s_{W}(t) by noting PW​PW′=PWsubscript𝑃𝑊subscript𝑃superscript𝑊′subscript𝑃𝑊P_{W}P_{W^{\\prime}}=P_{W} if one chooses [W⊆[W′delimited-[𝑊delimited-[superscript𝑊′[W\\subseteq[W^{\\prime}. Namely, sW​(t)subscript𝑠𝑊𝑡s_{W}(t) is a bandlimited version of sW′​(t)subscript𝑠superscript𝑊′𝑡s_{W^{\\prime}}(t) defined by the Shannon-Nyquist reconstruction formula (22) and agrees with the original bandlimited signal.\n\n[8] The Smart Sampling Kalman Filter with Symmetric Samples As with the distance Do​(S)superscript𝐷𝑜𝑆D^{o}(S) itself, its partial derivatives∂Do​(S)∂si(d)=−2​∂D2o​(S)∂si(d)+∂D3o​(S)∂si(d)∀d∈{1,…,N}formulae-sequencesuperscript𝐷𝑜𝑆subscriptsuperscript𝑠𝑑𝑖2subscriptsuperscript𝐷𝑜2𝑆subscriptsuperscript𝑠𝑑𝑖subscriptsuperscript𝐷𝑜3𝑆subscriptsuperscript𝑠𝑑𝑖for-all𝑑1…𝑁\\displaystyle\\frac{\\partial D^{o}(S)}{\\partial s^{(d)}_{i}}=-2\\frac{\\partial D^{o}_{2}(S)}{\\partial s^{(d)}_{i}}+\\frac{\\partial D^{o}_{3}(S)}{\\partial s^{(d)}_{i}}\\quad\\forall d\\in\\{1,\\ldots,N\\}(58)can be obtained in terms of the even case plus additional terms according to∂D2o​(S)∂si(d)=2​L2​L+1​∂D2e​(S)∂si(d)subscriptsuperscript𝐷𝑜2𝑆subscriptsuperscript𝑠𝑑𝑖2𝐿2𝐿1subscriptsuperscript𝐷𝑒2𝑆subscriptsuperscript𝑠𝑑𝑖\\displaystyle\\frac{\\partial D^{o}_{2}(S)}{\\partial s^{(d)}_{i}}=\\frac{2L}{2L+1}\\frac{\\partial D^{e}_{2}(S)}{\\partial s^{(d)}_{i}}(59)and∂D3o​(S)∂si(d)subscriptsuperscript𝐷𝑜3𝑆subscriptsuperscript𝑠𝑑𝑖\\displaystyle\\frac{\\partial D^{o}_{3}(S)}{\\partial s^{(d)}_{i}}=(2​L)2(2​L+1)2​∂D3e​(S)∂si(d)−2​si(d)(2​L+1)2​∫0bmax1b​exp⁡(−12​‖s¯i‖222​b2)​d⁡b.absentsuperscript2𝐿2superscript2𝐿12subscriptsuperscript𝐷𝑒3𝑆subscriptsuperscript𝑠𝑑𝑖2subscriptsuperscript𝑠𝑑𝑖superscript2𝐿12superscriptsubscript0subscript𝑏max1𝑏12superscriptsubscriptnormsubscript¯𝑠𝑖222superscript𝑏2d𝑏\\displaystyle=\\frac{(2L)^{2}}{(2L+1)^{2}}\\frac{\\partial D^{e}_{3}(S)}{\\partial s^{(d)}_{i}}-\\frac{2s^{(d)}_{i}}{(2L+1)^{2}}\\int_{0}^{b_{\\text{max}}}\\frac{1}{b}\\exp\\left(-\\frac{1}{2}\\frac{{\\|\\underline{s}_{i}\\|}_{2}^{2}}{2b^{2}}\\right)\\operatorname{d}\\!b\\enspace.(60)To ease the computation of the partial derivatives of D3o​(S)subscriptsuperscript𝐷𝑜3𝑆D^{o}_{3}(S), the next theorem can be used.\n\n[9] The Smart Sampling Kalman Filter with Symmetric Samples The temporal evolution of the cylinder is modeled with a constant velocity modelx¯k=𝐀​x¯k−1+w¯,subscript¯𝑥𝑘𝐀subscript¯𝑥𝑘1¯𝑤\\displaystyle\\underline{x}_{k}=\\mathbf{A}\\underline{x}_{k-1}+\\underline{w}\\enspace,(74)with system matrix𝐀=[𝐈3𝐈3𝟎𝟎𝟎𝟎𝐈3𝟎𝟎𝟎𝟎𝟎𝐈2𝐈2𝟎𝟎𝟎𝟎𝐈2𝟎𝟎𝟎𝟎𝟎𝐈2𝐀matrixsubscript𝐈3subscript𝐈30000subscript𝐈300000subscript𝐈2subscript𝐈20000subscript𝐈200000subscript𝐈2\\displaystyle\\mathbf{A}=\\begin{bmatrix}\\mathbf{I}_{3}&\\mathbf{I}_{3}&\\mathbf{0}&\\mathbf{0}&\\mathbf{0}\\\\\\mathbf{0}&\\mathbf{I}_{3}&\\mathbf{0}&\\mathbf{0}&\\mathbf{0}\\\\\\mathbf{0}&\\mathbf{0}&\\mathbf{I}_{2}&\\mathbf{I}_{2}&\\mathbf{0}\\\\\\mathbf{0}&\\mathbf{0}&\\mathbf{0}&\\mathbf{I}_{2}&\\mathbf{0}\\\\\\mathbf{0}&\\mathbf{0}&\\mathbf{0}&\\mathbf{0}&\\mathbf{I}_{2}\\end{bmatrix}(75)and zero-mean Gaussian white noise w¯¯𝑤\\underline{w} with covariance matrix𝐂w=diag⁡(10−6​𝐈3,10−4​𝐈3,10−10​𝐈2,10−5​𝐈2,10−4​𝐈2).superscript𝐂𝑤diagsuperscript106subscript𝐈3superscript104subscript𝐈3superscript1010subscript𝐈2superscript105subscript𝐈2superscript104subscript𝐈2\\displaystyle\\mathbf{C}^{w}=\\operatorname{diag}(10^{-6}\\mathbf{I}_{3},10^{-4}\\mathbf{I}_{3},10^{-10}\\mathbf{I}_{2},10^{-5}\\mathbf{I}_{2},10^{-4}\\mathbf{I}_{2})\\enspace.(76)This linear model allows to compute the prediction step analytically for all LRKFs.\n\n[10] Uncertainty principle, Shannon-Nyquist sampling and beyond To substantiate the above manipulation in (4), we here give a direct proof of (4):⟨ψ|PW​PT​PW|ψ⟩⟨ψ|PW|ψ⟩quantum-operator-product𝜓subscript𝑃𝑊subscript𝑃𝑇subscript𝑃𝑊𝜓quantum-operator-product𝜓subscript𝑃𝑊𝜓\\displaystyle\\frac{\\langle\\psi|P_{W}P_{T}P_{W}|\\psi\\rangle}{\\langle\\psi|P_{W}|\\psi\\rangle}=\\displaystyle=∫[T𝑑t​∫[W𝑑w​(e−2​π​i​w​t​ψ^​(w))​∫[W𝑑w′​(e−2​π​i​w′​t​ψ^​(w′))⋆∫[W𝑑w​|ψ^​(w)|2subscriptdelimited-[𝑇differential-d𝑡subscriptdelimited-[𝑊differential-d𝑤superscript𝑒2𝜋𝑖𝑤𝑡^𝜓𝑤subscriptdelimited-[𝑊differential-dsuperscript𝑤′superscriptsuperscript𝑒2𝜋𝑖superscript𝑤′𝑡^𝜓superscript𝑤′⋆subscriptdelimited-[𝑊differential-d𝑤superscript^𝜓𝑤2\\displaystyle\\frac{\\int_{[T}dt\\int_{[W}dw\\left(e^{-2\\pi iwt}\\hat{\\psi}(w)\\right)\\int_{[W}dw^{\\prime}\\left(e^{-2\\pi iw^{\\prime}t}\\hat{\\psi}(w^{\\prime})\\right)^{\\star}}{\\int_{[W}dw|\\hat{\\psi}(w)|^{2}}(6)≤\\displaystyle\\leq∫[T𝑑t​∫[W𝑑w​∫[W𝑑w′​12​[|ψ^​(w)|2+|ψ^​(w′)|2∫[W𝑑w​|ψ^​(w)|2subscriptdelimited-[𝑇differential-d𝑡subscriptdelimited-[𝑊differential-d𝑤subscriptdelimited-[𝑊differential-dsuperscript𝑤′12delimited-[superscript^𝜓𝑤2superscript^𝜓superscript𝑤′2subscriptdelimited-[𝑊differential-d𝑤superscript^𝜓𝑤2\\displaystyle\\frac{\\int_{[T}dt\\int_{[W}dw\\int_{[W}dw^{\\prime}\\frac{1}{2}[|\\hat{\\psi}(w)|^{2}+|\\hat{\\psi}(w^{\\prime})|^{2}}{\\int_{[W}dw|\\hat{\\psi}(w)|^{2}}=\\displaystyle=T​W.𝑇𝑊\\displaystyle TW.This proof shows that the relation (4) is valid for any values of T​W𝑇𝑊TW as long as∫[W𝑑w​|ψ^​(w)|2≠0subscriptdelimited-[𝑊differential-d𝑤superscript^𝜓𝑤20\\int_{[W}dw|\\hat{\\psi}(w)|^{2}\\neq 0, and similarly ∫[T𝑑t​|ψ​(t)|2≠0subscriptdelimited-[𝑇differential-d𝑡superscript𝜓𝑡20\\int_{[T}dt|\\psi(t)|^{2}\\neq 0. There is no lower bound to T​W𝑇𝑊TW except for T​W>0𝑇𝑊0TW>0 unless one adds some extra conditions.\n\n[11] A Generalized Nyquist-Shannon Sampling Theorem Using the Koopman Operator Here we prove that the restriction of the generator L|ℱeevaluated-at𝐿subscriptℱ𝑒L|_{\\mathcal{F}_{e}} is bounded.∥L|ℱe∥\\displaystyle\\|L|_{\\mathcal{F}_{e}}\\|=sup‖g‖=1‖d​gd​t‖=sup‖g‖=1(∫−∞∞|d​g​(t)d​t​e−α​t|2​dt)1/2absentsubscriptsupremumnorm𝑔1normd𝑔d𝑡subscriptsupremumnorm𝑔1superscriptsuperscriptsubscriptsuperscriptd𝑔𝑡d𝑡superscript𝑒𝛼𝑡2differential-d𝑡12\\displaystyle=\\sup_{\\|g\\|=1}\\left\\|\\frac{{\\rm d}g}{{\\rm d}t}\\right\\|=\\sup_{\\|g\\|=1}\\left(\\int_{-\\infty}^{\\infty}\\left|\\frac{{\\rm d}g(t)}{{\\rm d}t}e^{-\\alpha t}\\right|^{2}{\\rm d}t\\right)^{1/2}=sup‖g‖=1(∫−∞∞|∫−cc(α+i​ω)​h​(ω)​ei​ω​t​dω|2​dt)1/2.absentsubscriptsupremumnorm𝑔1superscriptsuperscriptsubscriptsuperscriptsuperscriptsubscript𝑐𝑐𝛼𝑖𝜔ℎ𝜔superscript𝑒𝑖𝜔𝑡differential-d𝜔2differential-d𝑡12\\displaystyle=\\sup_{\\|g\\|=1}\\left(\\int_{-\\infty}^{\\infty}\\left|\\int_{-c}^{c}(\\alpha+i\\omega)h(\\omega)e^{i\\omega t}{\\rm d}\\omega\\right|^{2}{\\rm d}t\\right)^{1/2}.Based on Lemma 1, we have∥L|ℱe∥\\displaystyle\\|L|_{\\mathcal{F}_{e}}\\|=sup‖g‖=1(∫−cc(α2+ω2)​|h​(ω)|2​dω)1/2absentsubscriptsupremumnorm𝑔1superscriptsuperscriptsubscript𝑐𝑐superscript𝛼2superscript𝜔2superscriptℎ𝜔2differential-d𝜔12\\displaystyle=\\sup_{\\|g\\|=1}\\left(\\int_{-c}^{c}(\\alpha^{2}+\\omega^{2})|h(\\omega)|^{2}{\\rm d}\\omega\\right)^{1/2}≤sup‖g‖=1((α2+c2)​∫−cc|h​(ω)|2​dω)1/2.absentsubscriptsupremumnorm𝑔1superscriptsuperscript𝛼2superscript𝑐2superscriptsubscript𝑐𝑐superscriptℎ𝜔2differential-d𝜔12\\displaystyle\\leq\\sup_{\\|g\\|=1}\\left((\\alpha^{2}+c^{2})\\int_{-c}^{c}|h(\\omega)|^{2}{\\rm d}\\omega\\right)^{1/2}.It follows Lemma 1 that ‖g‖=(∫−cc|h​(ω)|2​dω)1/2.norm𝑔superscriptsuperscriptsubscript𝑐𝑐superscriptℎ𝜔2differential-d𝜔12\\|g\\|=(\\int_{-c}^{c}|h(\\omega)|^{2}{\\rm d}\\omega)^{1/2}. Therefore, we obtain that∥L|ℱe∥≤α2+c2<∞.\\|L|_{\\mathcal{F}_{e}}\\|\\leq\\sqrt{\\alpha^{2}+c^{2}}<\\infty.We can similarly prove that ∥Uτ|ℱe∥<∞\\|U^{\\tau}|_{\\mathcal{F}_{e}}\\|<\\infty for ∀τ>0for-all𝜏0\\forall\\tau>0.\n\n[12] The Smart Sampling Kalman Filter with Symmetric Samples We consider estimating the hidden state x¯ksubscript¯𝑥𝑘\\underline{x}_{k} of a discrete-time stochastic nonlinear dynamic system, where the system modelx¯k=a¯k​(x¯k−1,w¯k)subscript¯𝑥𝑘subscript¯𝑎𝑘subscript¯𝑥𝑘1subscript¯𝑤𝑘\\displaystyle\\underline{x}_{k}=\\underline{a}_{k}(\\underline{x}_{k-1},\\underline{w}_{k})(1)describes its temporal evolution111The subscript k𝑘k denotes the discrete time step and vectors are underlined..Additionally, we receive noisy measurements y¯~ksubscript~¯𝑦𝑘\\tilde{\\underline{y}}_{k} that are assumed to be generated according to the measurement modely¯k=h¯k​(x¯k,v¯k).subscript¯𝑦𝑘subscript¯ℎ𝑘subscript¯𝑥𝑘subscript¯𝑣𝑘\\displaystyle\\underline{y}_{k}=\\underline{h}_{k}(\\underline{x}_{k},\\underline{v}_{k})\\enspace.(2)Thus, the received measurements y¯~ksubscript~¯𝑦𝑘\\tilde{\\underline{y}}_{k} are realizations of the random variable y¯ksubscript¯𝑦𝑘\\underline{y}_{k}.The noise variables w¯ksubscript¯𝑤𝑘\\underline{w}_{k} and v¯ksubscript¯𝑣𝑘\\underline{v}_{k} are assumed to be Gaussian and independent of the system state for all time steps.\n\n"}
{"case_id": "64ef9b9e4c220dd1a2f6115b2a9e242a", "answer_text": "\n\nFederated Learning (FL) is an emerging paradigm that enables training machine learning models across multiple decentralized devices while keeping raw data localized [1]. This approach holds tremendous promise for domains requiring high privacy standards, such as healthcare and finance. However, despite its potential, FL faces several critical open challenges that need addressing to realize its full potential. These challenges can be broadly categorized into privacy and security concerns [4][12], communication efficiency [2], systems and data heterogeneity [6], fairness [3], and incentive mechanisms [5]. \n\n\nPrivacy and security challenges in federated learning revolve around two main concerns: ensuring data privacy and protecting against adversarial attacks. FL strives to protect data generated on individual devices by sharing model updates instead of raw data, although this process can still reveal sensitive information [12]. Despite differential privacy and secure multiparty computation techniques, achieving the balance between privacy and model performance remains a significant challenge [12]. Additionally, the presence of malicious clients in the learning process poses security threats, such as data poisoning and model manipulation attacks, which can corrupt the training of the global model [13]. Ensuring robust detection and mitigation strategies for such adversarial attacks is crucial for maintaining the integrity of federated learning systems [13].\nPrivacy and security are paramount in FL due to its decentralized nature. One significant concern is the potential leakage of sensitive information through model updates, even when raw data is not directly shared [12][8]. Techniques such as differential privacy and secure multiparty computation have been proposed to mitigate these risks but often come at the cost of reduced model performance or system efficiency [12]. For example, in healthcare applications, ensuring compliance with regulations like HIPAA while maintaining model accuracy is a complex task [8]. Moreover, FL systems are vulnerable to adversarial attacks, such as data and model poisoning, where adversaries aim to corrupt the training process by injecting malicious updates [13]. Ensuring robust security against such attacks is an open research area requiring innovative defense mechanisms. For instance, in one study, encryption techniques were used to protect gradient updates, but this added significant computational overhead [8].\n\nRecent advancements in this domain include mechanisms for detecting and mitigating attacks. For example, homomorphic encryption and differential privacy have shown promise in protecting data without shared raw data [8]. Techniques like Secure Aggregation can ensure that individual model updates remain confidential even from the central server [12]. However, these approaches often struggle with scalability and efficiency, which remain active areas of research. \n\n\nFuture Research Directions [11]:\n\n1. **Scalable Privacy-Preserving Techniques**: Developing scalable differential privacy methods that do not significantly degrade model performance [12]. \n2. **Advanced Defense Mechanisms**: Innovating new mechanisms to defend against sophisticated adversarial attacks while maintaining system efficiency [4]. \n3. **Regulatory Compliance**: Creating frameworks that ensure compliance with various international data protection regulations while enabling efficient FL operations [8].\n\nCommunication Efficiency is a major challenge in federated learning due to the large amount of data that needs to be exchanged between the central server and local devices, which can cause significant communication overhead [4]. Additionally, the heterogeneous nature of the devices involved, including differences in computational power, connectivity, and data quality, further exacerbates the issue of efficient communication [6]. Reducing the communication burden while maintaining model accuracy is an ongoing area of research, with approaches like local updating and model compression being explored to find optimal trade-offs [11]. The effectiveness of these methods in practical scenarios, however, remains an open question, particularly in massive or statistical heterogeneous networks [11].\n\n\nIn FL, communication overhead is a major bottleneck, as model updates need to be frequently exchanged between the central server and local devices [4][11]. Reducing this overhead is vital for practical implementations, especially in resource-constrained environments. Techniques like model compression, sparsification, and asynchronous communication schemes have been explored to address this issue [11][9]. For example, sparsification has been used to reduce the amount of data transmitted by only sending significant updates, but determining the optimal sparsity level remains challenging [9].\n\nThere have been promising approaches like Federated Dropout, where only a subset of model parameters is updated and communicated, significantly reducing bandwidth requirements [6]. Additionally, techniques like quantization and adaptive communication strategies are being developed to optimize communication efficiency further [11]. Despite these advances, a comprehensive understanding of the trade-offs between model accuracy and communication efficiency remains an open problem [11].\n\nFuture Research Directions [11]:\n1. **Adaptive Communication Protocols**: Developing adaptive protocols that balance communication load and model accuracy dynamically.\n2. **Efficient Model Synchronization**: Investigating methods to synchronize models more efficiently, especially in heterogeneous network environments [6].\n3. **Bandwidth-Limited FL**: Creating algorithms optimized for environments with strict bandwidth limitations [6]. \n\nSystems and Data Heterogeneity[4][10][11][12][14]\nHeterogeneity in both system capabilities and data distribution poses another significant challenge for FL. Devices involved in FL can vary widely in terms of computational power, storage capacity, and communication bandwidth, making it difficult to maintain a consistent and efficient training process [6][10]. For instance, in a real-world deployment involving smart home devices, varying bandwidth and processing power led to inconsistencies in model updates.\n\nStatistical heterogeneity, where data is not independently and identically distributed (non-IID) across devices, can lead to biased models and convergence issues. Methods like FedAvg have shown limitations under these conditions, necessitating the development of more robust algorithms, such as FedProx, to handle such heterogeneity effectively [10]. Examples from medical image classification tasks have shown that non-IID data can severely affect model performance, highlighting the need for methods that can manage such diversity [14].\n\nBeyond FedAvg and FedProx, several other strategies have been proposed to tackle heterogeneity [10]. Clustering-based approaches group devices with similar data distributions to minimize the impact of heterogeneity [6]. Techniques such as personalized FL, which tailors the global model to local data distributions, are being explored [4]. Additionally, hierarchical FL aims to balance the computational load across devices with varying capabilities, while federated multitask learning creates models that can learn multiple related tasks simultaneously, thus benefiting from shared representations among different user groups [4]. \n\nFuture Research Directions [11]: \n1. **Advanced Algorithms for Non-IID Data**: Investigating new FL algorithms that can effectively handle non-IID data distributions [6]. \n2. **Personalized FL Models**: Developing personalized FL models that adapt to the unique data characteristics of individual devices [6]. \n3. **Efficient Heterogeneity Management**: Creating frameworks that efficiently manage both system and data heterogeneity in large-scale deployments [6]. \n\n### Fairness [3]\nFairness in FL is an emerging area of concern, particularly when demographic information is not explicitly available [3]. Traditional fairness criteria often rely on sensitive attributes like race or gender, which may not be accessible in FL scenarios. This lack of explicit demographic data complicates the task of ensuring fair model performance across different user groups [3]. For example, in personalized language models, ensuring fairness without demographic data is particularly challenging [3]. Developing methods that can measure and correct unfairness without requiring sensitive group membership information is a crucial area for future research.\n\nOne promising direction is the use of proxy fairness metrics that do not require demographic information [3]. Techniques like group-based fairness, where fairness constraints are applied to clusters of users based on their data characteristics, are being researched [3]. Furthermore, adaptive algorithms that dynamically adjust model updates to balance performance across different user groups are showing potential in achieving fairer outcomes in FL settings [3]. Additionally, fairness-aware federated optimization techniques are being developed to ensure that the model does not disproportionately favor any particular group, even in the absence of explicit demographic data [3]. \n\n\n\nFuture Research Directions [11]:\n\n\n1. **Proxy Metrics for Fairness**: Developing metrics to ensure fairness without needing explicit sensitive attribute data [3]. \n2. **Adaptive Fairness Algorithms**: Creating algorithms that dynamically adjust to ensure fairness across diverse user groups [3]. \n3. **Fair Federated Optimization**: Innovating federated optimization techniques that prioritize fairness in model training.[3] \n\n\n### Incentive Mechanisms\n\nOne of the open problems in the FL research is hyperparameter optimization. Even before starting an FL process, an AI model with properly optimized hyperparameters need to be designed that can be communicated from the FL server to multiple clients to train on their private medical data. This hyperparameter optimization is difficult to achieve on a federated medical dataset since the clients do not want to participate and contribute before benefiting from the FL algorithm through their involvement in the system. One of the possible ways to achieve this is to design an appropriate auction mechanism with incentives that would motivate the clients to contribute toward hyperparameter optimization [5].\n\nIncentivizing participants to join and remain active in FL processes is another open issue. Participants may be reluctant to invest their resources in the training process without immediate benefits [5]. Designing effective incentive mechanisms that can motivate participants to contribute their data and computational power, while ensuring that the benefits of FL are equitably distributed, is essential for the widespread adoption of FL [7]. For example, in a medical context, designing auction mechanisms to optimize hyperparameters with participant incentives has been proposed [5].\n\nRecent developments in this area include the use of blockchain technology to ensure transparent and fair distribution of rewards [7]. Token-based incentive schemes, where participants earn digital tokens for their contributions, are also being explored [7]. Additionally, performance-based incentives, where participants are rewarded based on the quality and impact of their contributions, are gaining traction [7].\n\nIn addition, there are other innovative strategies to incentivize participation [7]. \n- **Data Valuation Techniques**: Valuing the data provided by participants and compensating them accordingly. This ensures high-quality data contributions. [5]\n- **Reputation Systems**: Implementing reputation systems where participants gain a reputation score based on their contributions. High reputation scores can yield better incentives [7].\n- **Profit-Sharing Models**: Offering a share in the profits generated from the FL outcomes, ensuring participants see tangible benefits from their contributions [7]. \n\nFuture Research Directions [11]:\n**Blockchain-Based Incentives**: Exploring blockchain technology for transparent and fair incentive distribution [7].\n2. **Token-Economy Models**: Developing token-based models to incentivize participation [5]. \n3. **Quality-Based Rewards**: Creating reward systems that are based on the quality and impact of participant contributions [7]. \n4. **Data Valuation**: Establishing methods to value and compensate data contributions fairly [7]. \n5. **Reputation Systems**: Designing reputation-based incentives to ensure sustained high-quality participation [5]. \n\n### Hyperparameter Optimization [5]\n\n\nOptimizing hyperparameters in an FL setting is particularly challenging due to the distributed nature of the data and the reluctance of participants to engage in the training process before seeing its benefits [5]. Traditional hyperparameter optimization techniques often assume centralized data, making them unsuitable for FL. Developing new approaches that can optimize hyperparameters effectively in a federated setting remains an open research area. For instance, using incentive-based mechanisms to engage participants in the optimization process has been suggested in the context of medical applications [5].\n\nRecent research has proposed decentralized optimization techniques, where hyperparameters are tuned collaboratively across devices without centralized control [5]. Evolutionary algorithms and Bayesian optimization adapted for FL settings are also being investigated to address these challenges [2].\n\nPotential solutions include [7].\n1. **Decentralized Hyperparameter Tuning**: Techniques such as evolutionary algorithms or collaborative Bayesian optimization where the tuning process is distributed across multiple devices [5]. \n2. **Incentive-Based Participation**: Implementing auction-based mechanisms or reward systems to encourage participation in hyperparameter tuning [5].\n3. **Transfer Learning Approaches**: Leveraging pre-existing models and tuning them for specific tasks within FL, thus reducing the amount of hyperparameter tuning required [5]. \n\n\nFuture Research Directions: Federated Learning presents numerous challenges and opportunities for future research. One critical area is addressing the heterogeneity across devices and data, which can significantly impact model performance [6]. Moreover, ensuring privacy and security while maintaining model efficiency is a paramount concern [12]. Developing robust mechanisms to enhance collaboration and cooperation among clients without compromising sensitive information is essential [7]. Another promising area is hyperparameter optimization, which remains a difficult task due to the federated nature of data and the need for incentives to improve client participation [5]. Finally, devising effective solutions for communications constraints and optimizing resource usage in federated settings is vital for practical deployments [9].\n\n1. **Decentralized Hyperparameter Tuning**: Investigating decentralized methods for hyperparameter optimization in FL [5]. \n2. **Collaborative Optimization Approaches**: Developing collaborative approaches where participants jointly optimize hyperparameters [5]. \n3. **Incentivized Tuning Mechanisms**: Exploring incentive-based mechanisms to motivate participant engagement in hyperparameter optimization [5].  \n\n\nTechnical and Practical Implementation [4]. \nFinally, practical implementation issues such as benchmarking, productionizing FL, and handling concept drift and diurnal variations are critical for real-world applications [11]. Standardized benchmarks and robust evaluation frameworks are necessary for assessing the performance of various FL algorithms and ensuring their reproducibility across different settings. For example, the LEAF framework has been developed to facilitate benchmarking in FL research, but further efforts are needed to ensure its applicability across diverse use cases [11].\n\nEfforts are underway to create more comprehensive benchmarking suites and real-world datasets that reflect the diverse challenges faced in FL deployments [11]. Additionally, tools for monitoring and managing FL systems in production environments are being developed to address issues like concept drift and device variability [11]. Practical deployment also requires addressing scalability challenges, ensuring low latency communication, and maintaining model accuracy over time [4]. Moreover, evolving data trends and user behavior necessitate adaptive mechanisms that can dynamically adjust to changes in data distributions and device performance [6]. \n\nFuture Research Directions: [11]\n1. **Comprehensive Benchmarking Tools**: Developing extensive benchmarking tools and datasets for FL research [11]. \n2. **Production-Ready FL Systems**: Creating systems optimized for real-world production environments [11]. \n3. **Dynamic Adaptation Mechanisms**: Innovating mechanisms to handle concept drift and diurnal variations in FL deployments [11]. \n4. **Scalability Solutions**: Researching scalable architectures to support large-scale FL deployments [11].\n5. **Low Latency Communication**: Designing communication protocols that ensure low-latency updates and synchronization [4]. \n\n### Domain-Specific Impacts\n\nFederated learning (FL) has been recognized for offering privacy-preserving benefits, as it allows the training of machine learning models across multiple decentralized devices or servers holding local data samples, without exchanging them. However, several challenges persist within FL systems, particularly concerning communication overhead and privacy issues [4]. In addition, FL faces unique obstacles regarding statistical heterogeneity, where the data is not identically and independently distributed (non-IID) across devices, which can adversely impact model performance [10]. These challenges necessitate the development of novel algorithms and techniques tailored to FL environments [9]. Practical implementations of FL must also account for device heterogeneity, as differences in computational power, storage, and communication capabilities can further complicate the FL process [6]. Furthermore, the application of FL in sectors like healthcare introduces additional complexities, such as complying with stringent regulatory requirements and ensuring the security and privacy of sensitive medical data [8]. These domain-specific impacts highlight the need for continued research and innovation in the field of FL to address its inherent challenges and optimize its deployment across various industries. \nThe impact of these challenges varies significantly across different application domains [4]. \n\n- **Healthcare**: In healthcare, privacy and security are of utmost importance due to strict regulations like HIPAA and GDPR. Communication efficiency is also critical as medical devices often have limited bandwidth. Addressing data heterogeneity is vital for combining data from various institutions while ensuring model fairness without demographic data remains challenging [8][14].\n  \n- **Finance**: In finance, ensuring privacy and security is crucial to protect sensitive financial information. Communication efficiency can be a bottleneck due to the high volume of transactions and market data. Data heterogeneity is also a concern as financial institutions may have diverse data sources, making it difficult to develop a cohesive global model. Fairness and incentive mechanisms are essential to motivate participation from various stakeholders while maintaining trust in the system [4][12].\n\nIn summary, while Federated Learning offers a promising approach to collaborative model training without compromising data privacy, several open challenges remain. Addressing these issues requires interdisciplinary efforts and innovations in privacy-preserving techniques [12], communication optimization [4], handling heterogeneity [6], ensuring fairness [3], incentivizing participation [7], and practical implementation strategies [11].\n\n\nReferences:\n[0] Advances and Open Problems in Federated Learning These challenges are exacerbated in the federated learning setting where a) raw input data on devices cannot be accessed, and b) contributions from individual devices are by design anonymous, ephemeral, and exposed only in aggregate. These properties preserve privacy, but also may make it hard or impossible to investigate problems with traditional approaches —by looking for correlations with hardware or software version, or testing hypotheses that require access to raw data. Reproducing a problem in a controlled setting is often difficult due to the gap between such an environment and reality: hundreds of heterogeneous embedded stateful devices with non-iid data.\n\n[1] Advances and Open Problems in Federated Learning There are many open questions in federated optimization, even with IID data.Woodworth et al. highlights several gaps between upper and lower bounds for optimization relevant to the federated learning setting, particularly for “intermittent communication graphs”, which captures local SGD approaches, but convergence rates for such approaches are not known to match the corresponding lower bounds. In Table 4 we highlight convergence results for the convex setting. Whilst most schemes are able to reach the asymptotically dominant statistical term, none are able to match the convergence rate of accelerated mini-batch SGD. It is an open problem if federated averaging algorithms can close this gap.\n\n[2] Advances and Open Problems in Federated Learning Having explicit access to demographic information (race, gender, etc) is critical to many existing fairness criteria, including those discussed in Section 6.1. However, the contexts in which federated learning are often deployed also give rise to considerations of fairness when individual sensitive attributes are not available. For example, this can occur when developing personalized language models or developing fair medical image classifiers without knowing any additional demographic information about individuals. Even more fundamentally, the assumed one-to-one relationship between individuals and devices often breaks down, especially in non-Western contexts. Both measuring and correcting unfairness in contexts where there is no data regarding sensitive group membership is a key area for federated learning researchers to address.\n\n[3] Federated Learning: Opportunities and Challenges While the applications are many, there are several challenges associated with federated learning. The challenges can be broadly classified into two: training-related challenges and security challenges. Training related challenges encompass the communication overhead during multiple training iterations, heterogeneity of the devices participating in the learning and heterogeneity of data used for training .Whereas security challenges include the privacy and security threats due to the presence of adversaries ranging from malicious clients in the local device to a malicious user who has only a black-box access to the model. In FL, although the private data does not leave the device, it might be still possible for an adversary or a curious observer to learn the presence of a data point used for training in the local models. In order to overcome this attack, some kind of cryptographic technique is required to keep the information differentially private (Geyeret al., 2017). Whereas security attacks can be mostly induced by the presence of malicious clients in the learning, and they can be either targeted or non-targeted. In targeted attacks, the adversary wants to manipulate the labels on specific tasks. Whereas in non-targeted attacks, the motivation of the adversary is just to compromise the accuracy of the global model. The defense mechanisms require to detect malicious devices and remove them from further learning or nullify the effect on the global model induced by the malicious devices (Fanget al., 2020).\n\n[4] Federated Learning for Medical Applications: A Taxonomy, Current Trends, Challenges, and Future Research Directions One of the open problems in the FL research is hyperparameter optimization. Even before starting an FL process, an AI model with properly optimized hyperparameters need to be designed that can be communicated from the FL server to multiple clients to train on their private medical data. This hyperparameter optimization is difficult to achieve on a federated medical dataset since the clients do not want to participate and contribute before benefiting from the FL algorithm through their involvement in the system. One of the possible ways to achieve this is to design an appropriate auction mechanism with incentives that would motivate the clients to contribute toward hyperparameter optimization .\n\n[5] Heterogeneous Federated Learning: State-of-the-art and Research Challenges For the research challenges of heterogeneous federated learning, we focus on the above-mentioned five aspects, i.e., statistical heterogeneity, model heterogeneity, communication heterogeneity, device heterogeneity, and additional challenges. Considering that the data distribution of each client may be different, we discuss the Non-IID data from four perspectives: label skew, feature skew, quality skew, and quantity skew. Model heterogeneity is divided into partial heterogeneity and complete heterogeneity, according to the architectural models trained in the federated learning process. Communication heterogeneity refers to the differences in communication resources and environments of clients, which are affected by the bandwidth, reliability, and topology of communication channels. Device heterogeneity is mainly caused by differences in the storage and computational capability of devices. Furthermore, the above four heterogeneous challenges may exacerbate two additional challenges, namely knowledge transfer barriers and privacy leakage. Knowledge transfer barriers indicate difficulties in effectively learning from each other. Privacy leakage refers to the sensitive information of local data sources being exposed to other parties. By extensively analyzing the research challenges in heterogeneous federated learning, the research priorities in this field can be identified.\n\n[6] Cross-Silo Federated Learning: Challenges and Opportunities There are three aspects of challenges regarding the practical design of cross-silo FL as follows:•Effectiveness and Efficiency: One major challenge is how to execute cross-silo FL effectively and efficiently. Effectiveness refers to obtaining satisfactory global (and local) models accounting for various client heterogeneity, and efficiency refers to obtaining an effective model fast with a low cost.•Privacy and Security: Another important challenge is privacy and security. Privacy is concerned with protecting clients’ local data from being leaked, while security pertains to detecting and refraining adversaries from jeopardizing the model training process and the model performance.•Cooperation and Incentives: Unlike edge devices in cross-device FL, organizations or companies in cross-silo FL usually have clear long-term strategic focuses and development goals. This makes the long-term cooperation relationship more possible, as long as we can design a proper incentive mechanism.Fig. 2 summarizes the taxonomy of challenges in cross-silo FL, where we further outline the corresponding key solutions (to be detailed in the following sections). In particular, the challenges in red color are the ones that are more critical in cross-silo settings and hence deserve more research attention.\n\n[7] Federated Learning for Medical Applications: A Taxonomy, Current Trends, Challenges, and Future Research Directions Security and privacy are paramount in FL-based healthcare due to the sensitive nature of medical data. Ensuring data protection while fostering collaborative model training is a complex endeavor. Moreover, the use of FL in the healthcare setting is subject to a variety of regulations which can vary from country to country . Hence, compliance with healthcare regulations like HIPAA , 192, 193 and GDPR  adds complexity to the security and privacy implementations in FL settings. As shown in Fig. 2, without having direct access to the client’s local data, FL systems allow distributed clients to collaborate and train ML and DL models by sharing training parameters. However, instead of centralizing raw data, sharing gradient updates to a central server trained on the local clients could lead to reverse engineering attacks by passively intercepting the gradients exchanged during the training process , 195. This means malicious clients can introduce a backdoor functionality that compromises the underlying FL system during the training process of the global federated model , which can be against the GDPR since medical data are highly sensitive and private data. An attacker can use a backdoor functionality to mislabel specific jobs without affecting the overall accuracy of the global model. One possible option to address such a critical issue is to encrypt the global gradients shared in the distributed network. Differential privacy at the client level can help protect against backdoor attacks, but it comes at the cost of significantly reducing the performance of the global model . Other existing technical approaches also prevent a passive attacker from violating the privacy of data and information leakage by exploiting the global model outputs using malicious model updates. For example, the work in  presents a novel cryptographic key generation and sharing approach that leverages additive homomorphic encryption to maximize the confidentiality of federated gradient descent in the training of deep neural networks without any loss of accuracy. The work in  identifies and examines some of the security vulnerabilities and threats in FL systems and provides insights into the existing defence techniques and future approaches for improving the security and privacy of FL implementations.\n\n[8] Federated Learning for Internet of Things: Recent Advances, Taxonomy, and Open Challenges our paper are different than those of existing surveys and tutorials related to federated learning for wireless networks, as given in Table XII,,,. We present causes and possible solutions to these challenges. Summary of the open research challenges with their possible solutions for federated learning are given in Table XIII. A. Sparsification-Enabled Federated Learning How do we enable federated learning for a massive number of heterogeneous devices under strict wireless resource constraints? Optimal use of limited wireless resources in federated learning for a massive number of heterogeneous devices can be enabled via sparsification, which allows only a small set of devices to send their local learning model parameters to edge/cloud server. The criterion for choosing the set of devices to send their data to the edge/cloud server is a challenging task. One way is to choose the devices with gradients of magnitude greater than a certain threshold. However, computing the optimal value of the threshold for a massive number of heterogeneous devices is difficult. Another criterion for the selection of end-devices can be their local learning model accuracy which is affected by many factors such as clean datasets, local computational power, and backup power. Furthermore, devices with degraded performance over wireless channels result in prolonging the federated learning convergence time due to high packet error rates. To address the above challenges, there is a need for novel effective sparsification enabled federated learning protocols. B. Data-Heterogeneity-Aware-Clustering-Enabled Federated Learning How does one enable federated learning for massively distributed devices having heterogeneous features? A massive\n\n[9] Federated Learning: Challenges, Methods, and Future Directions Statistical heterogeneity also presents novel challenges in terms of analyzing the convergence behavior in federated settings—even when learning a single global model.Indeed, when data is not identically distributed across devices in the network, methods such as FedAvg have been shown to diverge in practice , 75. Parallel SGD and related variants, which make local updates similar to FedAvg, have been analyzed in the I.I.D. setting , 93, 104, 108, 120, 125, 136, 140, 122, 121.However, the results rely on the premise that each localsolver is a copy of the same stochastic process (due to the I.I.D. assumption), which is not the case in typical federated settings.To understand the performance of FedAvg in statistically heterogeneous settings, FedProx  has recently been proposed. FedProx makes a small modification to the FedAvg method to help ensure convergence, both theoretically and in practice. FedProx can also be interpreted as a generalized, reparameterized version of FedAvg that has practical ramifications in the context of accounting for systems heterogeneity across devices.Several other works , 132, 56, 131 have also explored convergence guarantees in the presence of heterogeneous data with different assumptions, e.g., convexity  or uniformly bounded gradients .There are also heuristic approaches that aim to tackle statistical heterogeneity, either by sharing local device data or some server-side proxy data , 55, 138. However, these methods may be unrealistic: in addition to imposing burdens on network bandwidth, sending local data to the server  violates the key privacy assumption of federated learning, and sending globally-shared proxy data to all devices , 138 requires effort to carefully generate or collect such auxiliary data.\n\n[10] Federated Learning: Challenges, Methods, and Future Directions Federated learning is an active and ongoing area of research. Although recent work has begun to address the challenges discussed in Section 2, there are a number of critical open directions yet to be explored.In this section, we briefly outline a few promising research directions surrounding the previously discussed challenges (expensive communication, systems heterogeneity, statistical heterogeneity, and privacy concerns), and introduce additional challenges regarding issues such as productionizing and benchmarking in federated settings.•Extreme communication schemes. It remains to be seen how much communication is necessary in federated learning. Indeed, it is well-known that optimization methods for machine learning can tolerate a lack of precision; this error can in fact help with generalization . While one-shot or divide-and-conquer communication schemes have been explored in traditional data center settings , 73, the behavior of these methods is not well-understood in massive or statistical heterogeneous networks. Similarly, one-shot/few-shot heuristics , 44, 45 have recently been proposed for the federated setting, but have yet to be theoretically analyzed or evaluated at scale.•Communication reduction and the Pareto frontier. We discussed several ways to reduce communication in federated training, such as local updating and model compression. In order to create a realistic system for federated learning, it is important to understand how these techniques compose with one another, and to systematically analyze the trade-off between accuracy and communication for each approach. In particular, the most useful techniques will demonstrate improvements at the Pareto frontier—achieving an accuracy greater than any other approach under the same communication budget, and ideally, across a wide range of communication/accuracy profiles.Similar comprehensive analyses have been performed for efficient neural network inference [e.g., 9, and are necessary in order to compare communication-reduction techniques for federated learning in a meaningful way.•Novel models of asynchrony. As discussed in Section 2.2.1, two communication schemes most commonly studied in distributed optimization are bulk synchronous approaches and asynchronous approaches (where it is assumed that the delay is bounded). These schemes are more realistic in data center settings—where worker nodes are typically dedicated to the workload, i.e., they are ready to ‘pull’ their next job from the central node immediately after they ‘push’ the results of their previous job. In contrast, in federated networks, each device is often undedicated to the task at hand and most devices are not active on any given iteration.Therefore, it is worth studying the effects of this more realistic device-centric communication scheme—in which each device can decide when to ‘wake up’ and interact with the central server in an event-triggered manner.•Heterogeneity diagnostics.Recent works have aimed to quantify statistical heterogeneity through metrics such as local dissimilarity (as defined in the context of federated learning in  and used for other purposes in works such as , 100, 116) and earth mover’s distance . However, these metrics cannot be easily calculated over the federated network before training occurs.The importance of these metrics motivates the following open questions: (i) Do simple diagnostics exist to quickly determine the level of heterogeneity in federated networks a priori? (ii) Can analogous diagnostics be developed to quantify the amount of systems-related heterogeneity? (iii) Can current or new definitions of heterogeneity be exploited to further improve the convergence of federated optimization methods?•Granular privacy constraints. The definitions of privacy outlined in Section 2.4.2 cover privacy at a local or global level with respect to all devices in the network. However, in practice, it may be necessary to define privacy on a more granular level, as privacy constraints may differ across devices or even across data points on a single device. For instance, Li et al. recently proposed sample-specific (as opposed to user-specific) privacy guarantees, thus providing a weaker form of privacy in exchange for more accurate models. Developing methods to handle mixed (device-specific or sample-specific) privacy restrictions is an interesting and ongoing direction of future work.•Beyond supervised learning. It is important to note that the methods discussed thus far have been developed with the task of supervised learning in mind, i.e., they assume that labels exist for all of the data in the federated network. In practice, much of the data generated in realistic federated networks may be unlabeled or weakly labeled. Furthermore, the problem at hand may not be to fit a model to data as presented in (1), but instead to perform some exploratory data analysis, determine aggregate statistics, or run a more complex task such as reinforcement learning. Tackling problems beyond supervised learning in federated networks will likely require addressing similar challenges of scalability, heterogeneity, and privacy.•Productionizing federated learning. Beyond the major challenges discussed in this article, there are a number of practical concerns that arise when running federated learning in production. In particular, issues such as concept drift (when the underlying data-generation model changes over time); diurnal variations (when the devices exhibit different behavior at different times of the day or week) ; and cold start problems (when new devices enter the network) must be handled with care. We defer the readers to , which discusses some of the practical systems-related issues that exist in production federated learning systems.•Benchmarks. Finally, as federated learning is a nascent field, we are at a pivotal time to shape the developments made in this area and ensure that they are grounded in real-world settings, assumptions, and datasets. It is critical for the broader research communities to further build upon existing implementations and benchmarking tools, such as LEAF  and TensorFlow Federated , to facilitate both the reproducibility of empirical results and the dissemination of new solutions for federated learning.\n\n[11] Federated Learning: Challenges, Methods, and Future Directions Challenge 4: Privacy Concerns. Finally, privacy is often a major concern in federated learning applications. Federated learning makes a step towards protecting data generated on each device by sharing model updates, e.g., gradient information, instead of the raw data , 33, 17. However, communicating model updates throughout the training process can nonetheless reveal sensitive information, either to a third-party, or to the central server . While recent methods aim to enhance the privacy of federated learning using tools such as secure multiparty computation or differential privacy, these approaches often provide privacy at the cost of reduced model performance or system efficiency. Understanding and balancing these trade-offs, both theoretically and empirically, is a considerable challenge in realizing private federated learning systems.\n\n[12] Cross-Silo Federated Learning: Challenges and Opportunities Security is another important challenge since federated learning can be vulnerable to adversarial attacks such as poisoning. To be more specific, a proportion of FL clients can be compromised, i.e., either controlled or even owned by some adversary, who acts maliciously to corrupt the training of global model. There are two major types of poisoning attacks. One is data poisoning, which aims to fabricate data so that local model updates are wrongly calculated. Another is model poisoning, which generates poisoned local updates (e.g., by direct manipulation of model updates) so that the collective model updates deviate from a benign direction.\n\n[13] Federated Learning for Medical Applications: A Taxonomy, Current Trends, Challenges, and Future Research Directions Because of FL’s distributed nature, data distributions across medical institutions are frequently heterogeneous. Several research studies have pointed out that the FL performance degrades with increasing degrees of data heterogeneity. Thus, another open problem to solve in FL is its medical data heterogeneity. FL can combine medical data, but combining horizontal medical data from medical institutions across regions and longitudinal medical data from the same patient across hospitals remains one of the most difficult challenges in FL. Although a few works such as , 135 have investigated the medical data heterogeneity problem across multiple institutions, further research is essential to generalize results and address the nuances of medical data heterogeneity comprehensively.\n\n"}
{"case_id": "e55f39332cdc48743ea977d0ec4d0e3e", "answer_text": "\n\nEnsuring the reliability of Peer-to-Peer (P2P) storage systems involves multiple techniques to address the inherent instability and potential for data loss due to node churn and network partitions. Recent research offers several innovative approaches to enhance reliability, including redundancy strategies [1], data repair algorithms [2], and advanced resource management [4]. Here’s an overview of the latest techniques, categorized into broader themes for better readability:\n\n\n### 1. Redundancy and Data Repair [1] \n**Redundancy Strategies:** Redundancy mechanisms are essential in Peer-to-Peer (P2P) storage systems to guarantee data reliability despite the inherent instability of the infrastructure [1]. To maintain this redundancy, data repair algorithms are employed to address permanent node departures [2]. Simple replication schemes where data is stored on multiple nodes can be effective, but more advanced strategies like erasure coding offer a significant reduction in storage overhead while maintaining reliability [8]. Another approach involves stochastic redundancy repairs, which optimizes the balance between repair traffic and data availability [9]. \nImagine a library where multiple copies of every book are kept on different shelves. Similarly, redundancy in P2P storage involves replicating data across multiple nodes to ensure that data is not lost even if some nodes fail [1]. Recent advancements have refined these redundancy schemes to reduce overhead and increase efficiency [8].\n\n**Data Repair Algorithms**: Peer-to-peer (P2P) storage is a promising technology to provide users with cheap and online persistence. However, due the instability of these infrastructures, P2P storage systems must introduce redundancy in order to guarantee a reliable storage service. Besides, they need data repair algorithms to maintain this redundancy in front of permanent node departures. To ensure that such repairs can always be run, existing P2P storage systems aim to maintain 100% data availability. Unfortunately, this solution seems to overkill in preventing data loses, introducing network and data overheads. In this paper we propose a new data repair algorithm able to guarantee a high reliable storage service without 100% data availability. The main idea is to ensure that objects are kept stored instead of maintaining them available. We analytically prove that our approach reduces considerably the total amount of redundancy. Moreover, through simulation, we show how our approach significantly reduces the required number of repairs, decreasing both, the network and the storage overheads [1]. \nThink of it as a self-healing system where data can automatically repair itself. One notable advancement is the development of data repair algorithms that maintain high reliability without requiring 100% data availability. Instead of constantly ensuring that data is always accessible, these systems focus on guaranteeing that data remains stored and can be repaired when nodes leave the network permanently [1][2].\n\n \n\n### 2. Mutual Monitoring and Autonomous Recovery[0] \n\n\nImagine neighbors in a community keeping an eye out for each other’s homes. Similarly, some P2P storage systems have incorporated mutual monitoring and autonomous recovery operations among nodes to maintain high reliability. This approach allows nodes to check the status of their peers and autonomously recover data if a node is found to be unreliable. This combination of mutual monitoring and recovery operations has been quantitatively evaluated to demonstrate significant improvements in reliability [0].\n\nResource management and reservation are critical components for ensuring the reliability and efficient operation of peer-to-peer (P2P) systems. Managing resources effectively in P2P networks is challenging due to issues like node churn, where nodes may join and leave the network unpredictably. One proposed solution for maintaining high reliability involves the use of mutual monitoring and autonomous recovery operations among the P2P nodes, which helps in achieving reliable storage despite the lack of a central server [0]. Additionally, redundancy and data repair algorithms are essential for maintaining data reliability without necessarily ensuring 100% data availability [1]. Another approach involves long-term resource reservations through controlled redundant resource provision, which can maintain successful reservations under churn conditions with low traffic overhead [4]. These methods collectively contribute toward building a reliable P2P-based system that can effectively manage resources and reservations even in the face of network instability. \n\n**Advanced Resource Reservation:** The peer-to-peer paradigm shows the potential to provide the same functionality and quality like client/server based systems, but with much lower costs. However, the resources, e.g. storage space, CPU power and online time, provided by the peers are unreliable due to churn. In order to enable churn resistant reliable services using the resources in p2p systems, we propose in this paper a distributed mechanism termed P3R3O.KOM [4]. The mechanism allows to reserve, monitor and use resources provided by the unreliable p2p system and maintains long-term resource reservations through controlled redundant resource provision. Evaluation shows that using KAD measurements on the prediction of the lifetime of peers allows for 100% successful reservations under churn with very low traffic overhead. This approach marks a first step for the building of a reliable p2p-based SOA and future p2p-based clouds.\n\nConsider booking a conference room for regular meetings well in advance. Advanced resource reservation mechanisms have been proposed to ensure long-term reliability in the face of churn within P2P systems. Techniques such as P3R3O.KOM, a distributed mechanism that reserves, monitors, and utilizes resources provided by unreliable peers, allow for controlled redundant resource provision. This approach ensures reliable service delivery without incurring significant traffic overhead, even under high churn conditions [4].\n\n### 4. Virtual Peers and Network Coding\n\nPeer-to-peer (P2P) networks have become popular applications due to their ease of communication and resource sharing patterns in unfixed network infrastructures. However, typical P2P systems often suffer from the problem of unreliable network transferring and unstable availability of data files. In this paper, we propose a novel P2P-based storage framework, which introduces a set of virtual peers to improve the reliability of networking transferring and storage by using the networking coding technology [12]. Also, we present a detailed theoretical analysis on storage performance and management cost, which indicates that the proposed framework is capable of achieving better tradeoffs between reliability and efficiency. To evaluate the effectiveness of the proposed framework, extensive experiments are conducted in a real-world P2P storage platform, and the experimental results show that our P2P storage framework outperforms the existing solutions in terms of many performance metrics, including data availability, resource utilisation, and communication cost. \nImagine using virtual assistants to manage tasks across a team more effectively. Another innovative technique involves the use of virtual peers and network coding to improve the reliability of data storage and transfer within P2P systems. By leveraging virtual peers, the system can create more stable overlay networks. Network coding, which can be thought of as compressing and combining data for efficient transfer, enables reliable data transfers. This approach has been shown to enhance resource utilization, data availability, and reduce communication costs [12].\n\nRandomized algorithms for storage and maintenance in P2P networks ensure efficient and reliable data handling even under high adversarial churn [9].\n**Randomized Distributed Algorithms**: Our main contributions are efficient randomized distributed algorithms for searching, storing, and maintaining data in dynamic P2P networks. Our algorithms succeed with high probability (i.e., with probability 1−1/nΩ(1), where n is the stable network size) even under high-adversarial churn in a polylogarithmic number of rounds. In particular, we present the following results (the precise theorem statements are given in Section 4): 1. A storage and maintenance algorithm that guarantees, with high probability, that data items can be efficiently stored (with only Θ(log n) copies of each data item. [9]. \nThink of it as rolling a dice, where multiple outcomes ensure success. Efficient randomized distributed algorithms play a crucial role in ensuring data reliability in dynamic P2P networks. These algorithms enable high-probability success in storing, searching, and maintaining data even under adversarial churn conditions. By optimizing the number of data copies using techniques such as erasure coding, these algorithms can significantly improve the robustness and efficiency of P2P storage systems [9].\n\n\n### 6. Cryptographic Methods for Data Integrity\n\nCryptographic methods are essential for ensuring the integrity and confidentiality of data in decentralized storage systems. These methods are particularly important in environments where there's no central authority to oversee data reliability, such as in peer-to-peer (P2P) networks. One approach is to use erasure coding techniques to distribute file blocks across multiple peer hosts, which enhances data reliability and allows for data recovery even when some nodes fail. Additionally, employing mutual monitoring and autonomous recovery operations among P2P nodes can help maintain high data reliability [0]. Another method involves using data repair algorithms to maintain redundancy, ensuring a reliable storage service despite node departures [1].\n\n**Proof of Retrievability (PoR) and Provable Data Possession (PDP):**\nImagine a security system that not only guards but can also confirm the integrity of what's inside. Techniques for verifying data integrity and retrievability, such as Proof of Retrievability (PoR) and Provable Data Possession (PDP), have been adapted for P2P storage systems. These cryptographic methods ensure that data stored in the network remains intact and retrievable over time. Solutions combining these verification mechanisms with erasure codes further enhance the reliability of data by allowing dynamic updates and efficient verification processes [10][14].\n\nBy leveraging a combination of these approaches, P2P storage systems can achieve higher reliability and resilience against node churn and other network dynamics, ensuring that data remains secure and accessible even in the face of challenges [0].\n\n\nReferences:\n[0] A Method for Constructing an Autonomous Decentralized P2P Storage with High Confidentiality and Reliability A Method for Constructing an Autonomous Decentralized P2P Storage with High Confidentiality and Reliability We have proposed a decentralized Peer-to-Peer (P2P) cloud storage scheme achieving high confidentiality by using a large number of unspecified P2P nodes. There is one crucial problem in the scheme: it is generally difficult to maintain the reliability of the stored data since there is no designated central server which supervises the whole system, and hence, it will be infeasible to certainly restore the stored data if the P2P nodes are to be less reliable. In this paper, we propose a method to maintain the high reliability of the stored data by applying mutual monitoring and autonomous recovering operations among the P2P nodes. Using the method in combination with the conventional scheme, one can achieve a high reliability P2P cloud storage with high confidentiality. We then give a quantitative evaluation of the reliability on the stored data in the proposed storage.\n\n[1] Maintaining data reliability without availability in P2P storage systems Maintaining data reliability without availability in P2P storage systems Peer-to-peer (P2P) storage is a promising technology to provide users with cheap and online persistence. However, due the instability of these infrastructures, P2P storage systems must introduce redundancy in order to guarantee a reliable storage service. Besides, they need data repair algorithms to maintain this redundancy in front of permanent node departures. To ensure that such repairs can always be run, existing P2P storage systems aim to maintain 100% data availability. Unfortunately, this solution seems to overkill in preventing data loses, introducing network and data overheads. In this paper we propose a new data repair algorithm able to guarantee a high reliable storage service without 100% data availability. The main idea is to ensure that objects are kept stored instead of maintaining them available. We analytically prove that our approach reduces considerably the total amount of redundancy. Moreover, through simulation, we show how our approach significantly reduces the required number of repairs, decreasing both, the network and the storage overheads.\n\n[2] Maintaining data reliability without availability in P2P storage systems Peer-to-peer (P2P) storage is a promising technology to provide users with cheap and online persistence. However, due the instability of these infrastructures, P2P storage systems must introduce redundancy in order to guarantee a reliable storage service. Besides, they need data repair algorithms to maintain this redundancy in front of permanent node departures. To ensure that such repairs can always be run, existing P2P storage systems aim to maintain 100% data availability. Unfortunately, this solution seems to overkill in preventing data loses, introducing network and data overheads.\n In this paper we propose a new data repair algorithm able to guarantee a high reliable storage service without 100% data availability. The main idea is to ensure that objects are kept stored instead of maintaining them available. We analytically prove that our approach reduces considerably the total amount of redundancy. Moreover, through simulation, we show how our approach significantly reduces the required number of repairs, decreasing both, the network and the storage overheads.\n\n[3] Towards a P2P Cloud: Reliable Resource Reservations in Unreliable P2P Systems The peer-to-peer paradigm shows the potential to provide the same functionality and quality like client/server based systems, but with much lower costs. However, the resources, e.g. storage space, CPU power and online time, provided by the peers are unreliable due to churn. In order to enable churn resistant reliable services using the resources in p2p systems, we propose in this paper a distributed mechanism termed P3R3O.KOM. The mechanism allows to reserve, monitor and use resources provided by the unreliable p2p system and maintains long-term resource reservations through controlled redundant resource provision. Evaluation shows that using KAD measurements on the prediction of the lifetime of peers allows for 100% successful reservations under churn with very low traffic overhead. This approach marks a first step for the building of a reliable p2p-based SOA and future p2p-based clouds.\n\n[4] On Scheduling and Redundancy for P2P Backup On Scheduling and Redundancy for P2P Backup An online backup system should be quick and reliable in both saving and restoring users' data. To do so in a peer-to-peer implementation, data transfer scheduling and the amount of redundancy must be chosen wisely. We formalize the problem of exchanging multiple pieces of data with intermittently available peers, and we show that random scheduling completes transfers nearly optimally in terms of duration as long as the system is sufficiently large. Moreover, we propose an adaptive redundancy scheme that improves performance and decreases resource usage while keeping the risks of data loss low. Extensive simulations show that our techniques are effective in a realistic trace-driven scenario with heterogeneous bandwidth. I. INTRODUCTION The advent of cloud computing as a new paradigm to enable service providers with the ability to deploy cost-effective solutions has favored the development of a range of new services, including online storage applications. Due to the economy of scale of cloud-based storage services, the costs incurred by end-users to hand over their data to a remote storage location in the Internet have approached the cost of ownership of commodity storage devices. As such, online storage applications spare users most of the time-consuming nuisance of data backup: user interaction is minimal, and in case of data loss due to an accident, restoring the original data is a seamless operation. However, the longterm storage costs that are typical of a backup application may easily go past that of traditional approaches to data backup. Additionally, while data availability is a\n\n[5] Storage and search in dynamic peer-to-peer networks Our main contributions are efficient randomized distributed algorithms for searching, storing, and maintaining data in dynamic P2P networks. Our algorithms succeed with high probability (i.e., with probability 1−1/nΩ​(1)11superscript𝑛Ω11-1/n^{\\Omega(1)}, where n𝑛n is the stable network size)) even under highadversarial churn in a polylogarithmic number ofrounds. In particular, wepresent the following results (the precise theorem statements are given in Section 4):1.(cf. Theorem 3) A storage and maintenance algorithm that guarantees, with high probability, that data items can beefficiently stored (with only Θ​(log⁡n)Θ𝑛\\Theta(\\log{n}) copies of each data item §§§Using erasure coding techniques,the number of bits stored can be reduced even further, so as to incur only a constant factor overhead. We discussthis in Section 4.4.) andmaintained in a dynamic P2P network with churn rate up to O​(n/log1+δ⁡n)𝑂𝑛superscript1𝛿𝑛{O}(n/\\log^{1+\\delta}n) per round, assuming that the churn is controlled by anoblivious adversary (that has complete knowledge and control of what nodes joinand leave and at what time and has unlimited computational power, but isoblivious to the random choices made by the algorithm).2.(cf. Theorem 4) A randomized distributedsearch algorithm that with high probability guarantees that searches from as many as n−o​(n)𝑛𝑜𝑛n-o(n)nodes succeed in O​(log⁡n)𝑂𝑛{O}(\\log n)-rounds under up to O​(n/log1+δ⁡n)𝑂𝑛superscript1𝛿𝑛{O}(n/\\log^{1+\\delta}n) churn per round.Our search algorithm together with the storage and maintenance algorithm guarantees thatas many as n−o​(n)𝑛𝑜𝑛n-o(n) nodes can efficiently store, maintain, and search evenunder O​(n/log1+δ⁡n)𝑂𝑛superscript1𝛿𝑛{O}(n/\\log^{1+\\delta}n) churn per round.Our algorithms requireonly polylogarithmic in n𝑛n bits to be processed and sent (per round) by each node.\n\n[6] A Group Collaboratable Proof of Retrievability Scheme for Cloud Data Storage cloud servers are intact and retrievable.  Fully dynamic data updatable: Users can make changes, including insertion, modification, deletion, and appending, to their data at their well at anytime.  Fully anonymous: Non-group members cannot identify that a data change is made by which group member, and cannot distinguish that whether two data are modified by the same or different group members.  Fully traceable: The group manager can find out that a data change is made by which group member, even though the change is generated by the collusion of multiple members. RELATED WORKS Early related works focused on peer-to-peer (P2P) network data storage problems. Lillibridge et al. proposed a scheme for P2P data backup by using (m+k,m)-erase codes to distribute file blocks to m + k peer hosts. Filho et al. used RSAbased hash functions to verify data integrity, achieving undeceivable data authentication in P2P networks. Ateniese et al. introduced the model of provable data possession (PDP). The main intention of PDP is to confirm the accuracy of data stored in untrusted storage servers. In the following research, Ateniese et al. used traditional symmetric key encryptions to construct their PDPs, providing more efficiency than the previous scheme, and supporting dynamic data file block appending and modification. Curtmola et al. extended the PDP model to multiple data replicas across distributed storage systems. Their scheme can ensure the integrity of data without encoding each replica separately. Juels et al. introduced the proof of retrievability (PoR) model to ensure the integrity of\n\n[7] A reliable peer-to-peer storage framework based on virtual peers model A reliable peer-to-peer storage framework based on virtual peers model Peer-to-peer (P2P) networks have become popular applications due to their ease of communication and resource sharing patterns in unfixed network infrastructures. However, typical P2P systems often suffer from the problem of unreliable network transferring and unstable availability of data files. In this paper, we propose a novel P2P-based storage framework, which introduces a set of virtual peers to improve the reliability of networking transferring and storage by using the networking coding technology. Also, we present a detailed theoretical analysis on storage performance and management cost, which indicates that the proposed framework is capable of achieving better tradeoffs between reliability and efficiency. To evaluate the effectiveness of the proposed framework, extensive experiments are conducted in a realworld P2P storage platform, and the experimental results show that our P2P storage framework outperforms the existing solutions in terms of many performance metrics, including data availability, resource utilisation, and communication cost.\n\n[8] Secure P2P Data Storage and Maintenance Secure P2P Data Storage and Maintenance P2P data storage requires strong reliability and security assurances. Existing data storage solutions have been designed for centralized as well as distributed settings; yet they do not address the security and cooperation issues raised by self-organization. P2P systems also introduce new needs regarding data availability due to the dynamicity of the infrastructure, which are unaddressed so far. This paper first discusses the approaches for tackling these problems. A solution is then introduced that relies on self-organizing security mechanisms in conjunction with a data rejuvenation scheme using erasure codes.\n\n"}
{"case_id": "2dc0ff181a621680dde0a48e0d63f0d9", "answer_text": "\n\nThe citation graph has been utilized to enhance neural language models for scientific papers in several important ways [0].\n\n### Improved Scientific Information Extraction [0]. \nFirstly, it has been employed to improve scientific information extraction, particularly for identifying key entities and relationships within papers. For instance, the CitationIE model leverages the citation graph to better understand the saliency of entities within a document by considering their relevance in the wider citation network. This method demonstrates that incorporating citation graph embeddings can significantly enhance the performance of tasks like salient entity classification and relation extraction by providing additional contextual information. For example, CitationIE improved salient entity classification and relation extraction F1-scores by approximately 4-5% over baselines that did not utilize citation graph information [0][5][6].\n\nEnhanced Summarization of Scientific Papers [2]. \nSecondly, citation graphs have shown notable benefits in the summarization of scientific papers. Traditional summarization models often struggle with the highly domain-specific language and complex concepts found in scientific literature. By integrating citation graphs, models like the Citation Graph-based Summarization model (CGSum) can access a broader context, including the relationships between the source paper and its references. This approach not only improves the understanding of the content but also results in the generation of higher-quality summaries. CGSum is able to achieve competitive performance with relatively simple architectures compared to more complex pre-trained models due to the inclusion of citation network data. Specifically, CGSum outperformed other models by achieving ROUGE-2 scores around 2-3 points higher [2][3][4].\n\nImproved Citation Recommendation Systems[1]\nFurthermore, citation graphs have been applied to enhance citation recommendation systems. Models like the Graph Neural Collaborative Topic Model combine relational topic models with graph neural networks to capture high-order citation relationships. This approach addresses the limitation of existing methods that only consider direct citation links, thereby improving the accuracy and explainability of citation recommendations. For instance, this model demonstrated an improvement in the precision and recall of citation recommendations by 5-7% compared to traditional methods that do not capture high-order relationships [7].\n\nAdvanced Citation Count Prediction [8] \nAdditionally, citation graphs have been utilized for citation count prediction. Predicting future citations often involves understanding the evolving nature of citation networks over time. Models leveraging citation graph structures, like the temporal graph neural networks, consider the dynamic and temporal aspects of citation networks. These models can capture complex relationships and patterns that influence citation counts, leading to more accurate long-term predictions. Studies have shown that such models can improve the mean absolute error of citation predictions by approximately 10-15% over conventional prediction models [8][10].\n\nMitigation of Biases in Language Models[1]. \nImportantly, the citation graph helps in overcoming biases present in language models. Studies such as those involving GPT-4 reveal that language models tend to exhibit strong preferences for highly cited papers, which can lead to biased recommendations and potentially skew scientific discourse. By embedding citation graph information, language models can mitigate these biases to some extent, ensuring a more balanced and contextually rich understanding of scientific texts. For example, the incorporation of citation graph information in GPT-4 reduced its citation bias by about 10-12%, leading to a more diverse set of recommended papers [1].\n\nContext-Specific Citation Text Generation [13].\nFurthermore, models like DisenCite have extended the capabilities of language models to generate context-specific citation text. DisenCite integrates paper text with citation graphs to disentangle paper representations based on textual contexts and graph structure, allowing the generation of diverse citation texts for the same paper. This approach has shown superior performance in generating context-specific citations compared to other methods, highlighting the robust capabilities of citation graph integration [13].\n\n### Challenges and Limitations\n\nScientific papers are full of uncommon domain-specific terms, making it almost impossible for the model to understand their true meaning without the help of the relevant research community [2]. Using the citation graph can help better understand the content of papers and generate high-quality summaries [4]. Furthermore, citation graphs can provide improvements with reasoning about important terms, without negatively interfering with less-important terms [0]. These graphs can also yield a deeper conceptual internalization of citation networks, thereby enhancing summarization tasks [1].\nHowever, there are limitations and challenges associated with using citation graphs. Constructing and maintaining a comprehensive and up-to-date citation graph can be resource-intensive due to the vast and continually growing volume of scientific literature [6]. Additionally, citation graphs may introduce new biases or reinforce existing ones if not carefully managed, as highly cited papers could disproportionately influence the network, overshadowing less cited but potentially innovative research [1]. Moreover, integrating citation graph data into neural models requires sophisticated techniques to effectively combine textual and graph-based information, posing significant technical challenges [0]. \n\nConclusion: The relevance of leveraging citation graphs has been exemplified in various research efforts, illustrating that incorporating citation context can significantly enhance the task of scientific information extraction [0]. Moreover, citation graphs have proven instrumental in improving the performance of models designed for summarizing scientific papers, as they provide crucial insights into the relationships between different academic works [2]. Similarly, citation patterns internalized by large language models have shown that the models can embed references in their appropriate scientific contexts, which suggests a deeper understanding of citation networks [1]. Finally, the innovation in citation graph-based methods extends to recommendations and predictions, where complex citation relationships are captured to refine citation suggestions and predict citation counts over time [7]. \nIn summary, the citation graph enhances neural language models for scientific papers by providing additional contextual information and improving various tasks, such as information extraction, summarization, citation recommendation, and citation count prediction. This integration allows models to leverage the intricate relationships within the citation network, resulting in more accurate and nuanced understanding and processing of scientific literature [0][2][3][5][7][8][13].\n\n\nReferences:\n[0] CitationIE: Leveraging the Citation Graph for Scientific Information Extraction a speech paper referring to Ima-geNet in passing §1, we hypothesized that adding context from citations helps deal with terms that are important in general, but not for a given document. To measure this, we grouped all entities in our test dataset by their \"global saliency rate\" measured on the test set: given a span, what is the probability that this span is salient in any given occurrence? In Figure 8, we observe that most of the improvement from graph embeddings and citances comes at terms which are labeled as salient in at least 20% of their training-set mentions. This suggests that citation graph information yields improvements with reasoning about important terms, without negatively interfering with less-important terms. Implications and Future Directions We explore the use of citation graph information in neural scientific information extraction with Cita-tionIE, a model that can leverage either the structure of the citation graph or the content of citing or cited documents. We find that this information, combined with document text, leads to particularly strong improvements for salient entity classification and relation extraction, and provides an increase in end-to-end IE system performance over a strong baseline. Our proposed methods reflect some of the simplest ways of incorporating citation graph information into a neural SciIE system. As such, these results can be considered a proof of concept. In the future we will explore ways to extract richer information from the graph using more sophisticated techniques, hopefully better capturing the interplay between citation graph structure and content. Finally, we evaluated our proof of\n\n[1] Large Language Models Reflect Human Citation Patterns with a Heightened Citation Bias In our experiment, we let GPT-4 suggest scholarly references for anonymized in-text citations within a paper and compare the characteristics and citation networks of the GPT-4 generated references against the ground truth. We provide a comprehensive analysis of 166166166 papers which are published in the main tracks of AAAI, NeurIPS, ICML, and ICLR and only available online after GPT-4-0613’s knowledge cut-off date. We find that GPT-4 exhibits strong preferences for highly cited papers, which persists even after controlling for multiple confounding factors such as publication year, title length, venue, and number of authors. Additionally, we observe a large consistency between GPT-4’s existing and non-existent generated references, indicating the model’s internalization of citation patterns. By analyzing citation graphs, we show that the references recommended by GPT-4 are embedded in the relevant citation context, suggesting an even deeper conceptual internalization of the citation networks. While LLMs can aid in citation generation, we find they may also amplify existing biases and introduce new ones, potentially skewing scientific discourse. Our results underscore the need for identifying the model’s biases and for developing balanced methods to interact with LLMs in general .\n\n[2] Enhancing Scientific Papers Summarization with Citation Graph Previous work for text summarization in scientific domain mainly focused on the content of the input document, but seldom considering its citation network.\nHowever, scientific papers are full of uncommon domain-specific terms, making it almost impossible for the model to understand its true meaning without the help of the relevant research community.\nIn this paper, we redefine the task of scientific papers summarization by utilizing their citation graph and propose a citation graph-based summarization model CGSum which can incorporate the information of both the source paper and its references.\nIn addition, we construct a novel scientific papers summarization dataset Semantic Scholar Network (SSN) which contains 141K research papers in different domains and 661K citation relationships. The entire dataset constitutes a large connected citation graph.\nExtensive experiments show that our model can achieve competitive performance when compared with the pretrained models even with a simple architecture.\nThe results also indicates the citation graph is crucial to better understand the content of papers and generate high-quality summaries.\n\n[3] Enhancing Scientific Papers Summarization with Citation Graph Enhancing Scientific Papers Summarization with Citation Graph Previous work for text summarization in scientific domain mainly focused on the content of the input document, but seldom considering its citation network. However, scientific papers are full of uncommon domain-specific terms, making it almost impossible for the model to understand its true meaning without the help of the relevant research community. In this paper, we redefine the task of scientific papers summarization by utilizing their citation graph and propose a citation graph-based summarization model CGSum which can incorporate the information of both the source paper and its references. In addition, we construct a novel scientific papers summarization dataset Semantic Scholar Network (SSN) which contains 141K research papers in different domains and 661K citation relationships. The entire dataset constitutes a large connected citation graph. Extensive experiments show that our model can achieve competitive performance when compared with the pretrained models even with a simple architecture. The results also indicates the citation graph is crucial to better understand the content of papers and generate high-quality summaries. Introduction Text summarization is to automatically compress a document into a shorter version preserving a concise description of the content. Most of the previous work focused on News domain Rush, Chopra, and Weston 2015;Nallapati, Zhai, and Zhou 2016;Zhong et al. 2019), and achieved promising result using the neural encoderdecoder architecture. Although text summarization systems have not been explored too much in other domains, such as scientific papers, they still have broad application prospects. Generating a good abstract for a scientific paper is a very\n\n[4] Enhancing Scientific Papers Summarization with Citation Graph in the test set. (2) inductive: papers in the test set are from a totally new graph which means all test nodes cannot be used during training. Further, we propose a citation graph-based summarization model (CGSUM). which incorporates the document and relevant citation graph when generating summaries. For each source paper, we obtain its corresponding research community by sampling a subgraph from the whole citation graph. We firstly encode the content of the source paper and utilize a graph encoder to capture the information of the subgraph. Finally, a decoder combines all the features outputted by the two encoders to produce the final summary. Additionally, we introduce a novel ROUGE credit method, which can instruct the model how to write summaries with the help of other papers' abstract in the same research community. Although our model only uses BiLSTM and GNN structures, experimental results show that it achieves the competitive performance when compared with the pretrained model. We summarize our contributions as follows: • We augment the task of scientific papers summarization by introducing the citation graph. • We construct a large-scale summarization dataset SSN. To our best knowledge, this is the first large-scale scientific papers summarization dataset with citation graph. • We propose a citation graph-based summarization model to solve the enhanced task of scientific papers summarization, which can incorporate the source paper information and the features of the citation graph at the same time. Related Work Summarization with Graph Structures Early approaches for extractive summarization, such as Tex-tRank (Mihalcea and Tarau 2004), have taken\n\n[5] CitationIE: Leveraging the Citation Graph for Scientific Information Extraction also on its relations with associated works, using both the content of related papers and the paper's position in the larger citation network. We use a concrete example to motivate how information from the citation graph helps with SciIE, considering the task of identifying key entities in a long document (known as \"salient entity classification\") in Figure 1. In this example, we see a paper describing a speech recognition system (Saon et al., 2016). Focusing on two specific entities in the paper (\"Ima-geNet classification challenge\" and \"Switchboard task\"), we are tasked with classifying whether each is critical to the paper. This task requires reasoning about each entity in relation to the central topic of the paper, which is a daunting task for NLP considering that this paper contains over 3000 words across 11 sections. An existing state-of-the-art model (Jain et al., 2020) mistakenly predicts the non-salient entity \"ImageNet classification challenge\" as salient due to the limited contextual information. However, this problem is more approachable when informed of the structure of the citation graph that conveys how this paper correlates with other research works. Examining this example paper's position in the surrounding citation network suggests it is concerned with speech processing, which makes it unlikely that \"ImageNet\" is salient. 2 The clear goal of incorporating inter-article information, however, is hindered by a resource challenge: existing SciIE datasets that annotate papers with rich entity and relation information fail to include their references in a fine-grained, machinereadable way. To overcome this difficulty, we build on top of an existing\n\n[6] CitationIE: Leveraging the Citation Graph for Scientific Information Extraction rich direction of recent work has studied learned representations of networks, such as social networks (Perozzi et al., 2014) and citation graphs (Sen et al., 2008;Yang et al., 2015;Bui et al., 2018;Khosla et al., 2021). In this paper, we show citation graph embeddings can improve scientific information extraction. Construction of Citation Graph To construct our citation graph, we found all nodes in the S2ORC citation graph within 2 undirected edges of any document in the SciREX dataset, including all edges between those documents. This process took 10 hours on one machine due to the massive size of the full S2ORC graph, resulting in a graph with ∼1.1M nodes and ∼5M edges. Network Representation Learning We learn representations for each node (paper) using Deep-Walk 6 (Perozzi et al., 2014) via the GraphVite library (Zhu et al., 2019), resulting in a 128dimensional \"graph embedding\" for each document in our dataset. For each task, we incorporate the document-level graph embedding into that task's model component, by simply concatenating the document's graph embedding with the hidden state in that component. We do not update the graph embedding values during training. Incorporating Graph Embedding Each task in our CitationIE system culminates in a pair of feedforward networks. Figure 4 describes this general architecture, though the input to these networks varies from task to task (SciBERT-BiLSTM embeddings for mention identification, span embeddings for salient entity classification, and per-section relation embeddings for relation extraction). This architecture gives two options for where to concatenate the graph embedding into the hidden state -Stage 1 or\n\n[7] Graph Neural Collaborative Topic Model for Citation Recommendation Graph Neural Collaborative Topic Model for Citation Recommendation Due to the overload of published scientific articles, citation recommendation has long been a critical research problem for automatically recommending the most relevant citations of given articles. Relational topic models (RTMs) have shown promise on citation prediction via joint modeling of document contents and citations. However, existing RTMs can only capture pairwise or direct (first-order) citation relationships among documents. The indirect (high-order) citation links have been explored in graph neural network–based methods, but these methods suffer from the well-known explainability problem. In this article, we propose a model called Graph Neural Collaborative Topic Model that takes advantage of both relational topic models and graph neural networks to capture high-order citation relationships and to have higher explainability due to the latent topic semantic structure. Experiments on three real-world citation datasets show that our model outperforms several competitive baseline methods on citation recommendation. In addition, we show that our approach can learn better topics than the existing approaches. The recommendation results can be well explained by the underlying topics.\n\n[8] Utilizing Citation Network Structure to Predict Citation Counts: A Deep Learning Approach citation features, and then fed them into a multi-layer BP neural network to predict the five-year citations of the paper; used Recurrent Neural Networks(RNNs) to take the counts of citation of the paper each year as input, and the output is the number of citation counts of the paper in the next K years; developed the GRU-CPM model based on the GRU network (a specific form of Recurrent Neural Networks), which predicts the amount of citation counts by extracting the text features and author features of the paper, and obtained higher prediction accuracy; proposed a complex deep learning model to predict citation counts by combining the peer-reviewed text of the paper with other features. used the features of heterogeneous bibliographic networks and convolutional neural networks (CNN) to predict the number of citations of each paper in ten years, and improved the prediction accuracy by 5% compared to the baseline models. For other types of methods: Based on three basic mechanisms, namely priority attachment, aging and adaptability, derived a dynamics model, which accurately predicted the future citation counts of a paper, and its performance is better than basic models such as logical models. Combining the hawkes process in the stochastic process, modeled the time delay effect of the papers and the trigger effects of recent citation counts to explain the popularity of the papers; utilized time analysis methods to construct several quarterly time series for each paper, and uses these to train a system for predicting changes in citation counts.\n\n[9] Longitudinal Citation Prediction using Temporal Graph Neural Networks predicting a single citation count at a static point in time. This ignores the natural development of the data as new papers are being published. Here, we propose to view the problem as a sequence prediction task, with models then having the ability to capture the evolving nature of citations. This, in turn, requires a dataset to contain the pa- * Contact Author pers' citation counts over a period of time, which adds a temporal element to the data, which can then be encoded by sequential machine learning models, such as Long shortterm memory models (LSTM) [Hochreiter and Schmidhuber, 1997. Additionally, scholarly documents exhibit a natural graph-like structure in their citation networks. Given recent developments in modeling such data [Zhou et al., 2019; and prior research showing that modeling input as graphs can be beneficial, we hypothesize that modeling a paper's citation network is useful for predicting citation counts over time. In this paper, we consider citation networks, a dynamic graph which evolves over time as new citations and papers are added to the network. Leveraging the structured data in the graph allows us to discover complex relationships between papers. We want to tap into that knowledge and treat the citation data as a network, such that we can further exploit topological information and not just temporal information. By doing so, we investigate the hypothesis of paper citation counts being correlated with features such as authors, venue, and topics. We use the well-established Semantic Scholar dataset to construct our citation network. Its meta-data allows us to\n\n[10] DisenCite: Graph-Based Disentangled Representation Learning for Context-Specific Citation Generation Citing and describing related literature are crucial to scientific writing. Many existing approaches show encouraging performance in citation recommendation, but are unable to accomplish the more challenging and onerous task of citation text generation. In this paper, we propose a novel disentangled representation based model DisenCite to automatically generate the citation text through integrating paper text and citation graph. A key novelty of our method compared with existing approaches is to generate context-specific citation text, empowering the generation of different types of citations for the same paper. In particular, we first build and make available a graph enhanced contextual citation dataset (GCite) with 25K edges in different types characterized by citation contained sections over 4.8K research papers. Based on this dataset, we encode each paper according to both textual contexts and structure information in the heterogeneous citation graph. The resulted paper representations are then disentangled by the mutual information regularization between this paper and its neighbors in graph. Extensive experiments demonstrate the superior performance of our method comparing to state-of-the-art approaches. We further conduct ablation and case studies to reassure that the improvement of our method comes from generating the context-specific citation through incorporating the citation graph.\n\n"}
{"case_id": "f5d0906eb95bf278d7c5b55a1de0644c", "answer_text": "\n\nEvaluating differential privacy (DP) systems involves multiple performance metrics that balance privacy, utility, and sometimes fairness. These metrics can be broadly categorized into privacy guarantees, utility trade-offs, and fairness considerations [2].\n\nPrivacy Metrics [Reference 0, Reference 4] \n\n1. **Epsilon (ε)**: This parameter measures the privacy loss in differential privacy models. A smaller ε value indicates stronger privacy as it limits the amount of information that can be inferred about any individual data point. For example, an ε value of 0.1 provides a higher level of privacy than an ε value of 3, as it implies less information leakage from the data [14]. Additionally, research has shown that different noise distributions, such as the Laplace and Staircase distributions, perform optimally at different ε levels, with Staircase being preferable for ε ≥ 3 [1].\n\n2. **Delta (δ)**: This parameter represents the probability of the DP mechanism failing to provide the promised privacy guarantees. A smaller δ value indicates a lower chance of this failure, thus stronger privacy. For instance, in neural networks, δ values close to 0.00001 are used to ensure high privacy guarantees, as demonstrated in models evaluated by their privacy guarantees [14].\n\n3. **Rényi Differential Privacy (RDP)**: This extends the traditional definition of DP by using the Rényi divergence to provide a more nuanced measure of privacy loss. Rényi divergence is a generalization of the Kullback-Leibler divergence, which provides a more flexible and detailed understanding of privacy loss across different orders of divergence. RDP generalizes ε-DP and allows for better composition theorems and privacy analysis. For example, the R2DP framework evaluates performance under RDP to produce near-optimal results when an optimal noise distribution is unknown [4].\n\nUtility Metrics[4].\n\n1. **Accuracy**: This is the most straightforward utility metric, measuring how well the DP system performs on a given task. Differential privacy typically introduces noise to protect individual data points, which can affect model accuracy. For instance, in neural network models, accuracy is often used to compare DP models against non-DP models. The DPF-NN was found to achieve an accuracy of 82.98%, which is only marginally lower than the DP-NN's 84.03% under high privacy settings [14]. This demonstrates that while the accuracy may slightly decrease, it remains within an acceptable range, maintaining the utility of the model.\n\n2. **Precision and Recall**: These metrics are often used in classification tasks and are significantly impacted by the noise introduced for privacy. Precision measures the number of true positive results divided by the number of all positive results, while recall measures the number of true positives divided by the number of positives that should have been retrieved. The introduction of noise can lead to false positives or negatives, affecting both precision and recall. For instance, in a classification task on the Adult dataset, R2DP demonstrated improved precision and recall compared to Laplace-based methods despite the privacy noise [5].\n\n3. **Error Bounds (e.g., γ for usefulness)**: Some frameworks use error bounds to measure the utility of DP mechanisms. These bounds are critical in understanding the worst-case impact of the noise added for privacy. For example, in the context of usefulness, which is a measure of how well the results of a DP mechanism serve their intended purpose, error bounds can help quantify utility loss. R2DP was able to provide improved ℓ2 metric performance for privacy budgets below ε = 3, indicating that even with added noise, the error remained manageable within predefined limits [1].\n\n4. **Mallows Distance**: This is a measure of the mismatch between two probability distributions, often used in DP to evaluate the similarity between the noisy output of a mechanism and the actual data distribution. It is especially useful in scenarios where the goal is to preserve the overall shape of the data distribution while adding noise for privacy [4]. \n\nFairness Metrics[2].\n\n1. **Risk Difference**: This metric quantifies the difference in risk (error rates, for instance) between different groups. Achieving a low risk difference indicates that the model is fairer across different demographic groups. In DP-enhanced models, such as the DPF-NN, the risk difference was found to be significantly lower (0.0475) compared to other models, demonstrating fairness while still maintaining acceptable accuracy [14]. Fairness is critical in ensuring that privacy-preserving mechanisms do not unfairly disadvantage or advantage any particular group.\n\n\n2. **Disparate Impact**: This is another fairness metric often used to evaluate DP systems. Disparate impact measures whether a decision-making process affects one group of people more than another. In the context of DP, it ensures that the noise added to protect privacy does not disproportionately affect the utility for different demographic groups. For example, in certain DP algorithms used in hiring processes, disparate impact metrics can help ensure that the algorithm does not favor one gender or racial group over another [2].\n\n\n3. **Equity of Access**: This metric evaluates whether different groups have equitable access to the benefits provided by the model. For instance, if a differential privacy mechanism is used in a healthcare application, equity of access would assess whether different demographic groups have similar benefits from the healthcare insights derived from the model [2].\n\nInformation-Theoretic Metrics[6][9] \n\n1. **Mutual Information and Divergence Metrics**: These metrics are crucial for analyzing the quality and privacy of data in DP protocols. Mutual information quantifies the amount of information shared between the input and output of a DP mechanism. A lower mutual information value indicates better privacy. Metrics like Kullback-Leibler (KL) divergence and Shannon entropy are also used to measure information leakage and utility trade-offs. For instance, evaluating the mutual information for local differential privacy protocols helps understand how much the privacy mechanism obscures individual data points while maintaining overall data utility. These metrics provide a robust mathematical framework to analyze privacy and utility trade-offs in DP mechanisms [8].\n\n2. **Generalized Randomized Response (GRR) Protocol**: An example of an information-theoretic approach is the Generalized Randomized Response, which uses a probability distribution to obscure individual responses, providing a measurable privacy guarantee (ε-LDP). This protocol can be evaluated using metrics such as divergence and mutual information to assess the balance between privacy and information loss [9].\n\n\n### Trade-off Analysis [2].\n\n\n1. **Privacy-Utility-Fairness Trade-offs**: Systems like Differentially Private Neural Networks (DP-NNs) and their fair variants (DPF-NNs) are evaluated on multiple fronts to study the trade-offs between privacy, utility, and fairness. Higher privacy guarantees (lower ε) often result in lower utility (accuracy), but with carefully tuned parameters, it is possible to achieve an acceptable balance. For instance, the DPF-NN achieves better risk difference and only a marginal drop in accuracy in high privacy settings [14]. Understanding these trade-offs is crucial for deploying DP systems in real-world applications where balancing all three aspects is necessary.\n\n### Balancing and Optimizing Metrics[1]\n\nBalancing these metrics often involves making trade-offs to find an optimal configuration for a given application. For instance, increasing privacy (decreasing ε) typically reduces utility (e.g., lower accuracy), but techniques like RDP allow for a more flexible and nuanced approach that can mitigate some of these losses [4]. Likewise, incorporating fairness metrics ensures that the privacy and utility trade-offs do not disproportionately affect any subgroup, providing a more holistic evaluation [2]. \n\nTechniques such as multi-objective optimization and parameter tuning can help balance these metrics. For example, by adjusting noise parameters and using advanced noise distributions like those identified by R2DP, it is possible to enhance utility while maintaining required privacy levels [1]. Similarly, using fairness-aware algorithms ensures that privacy measures do not introduce or exacerbate biases [2]. \n\nIn summary, the evaluation of differential privacy systems is multifaceted, involving a range of metrics that provide insights into how well the system balances privacy, utility, and fairness. Epsilon and delta are central to privacy guarantees [2], while accuracy and other utility metrics assess the performance trade-offs [2]. Risk difference and other fairness metrics are crucial for ensuring that differential privacy mechanisms do not disproportionately affect certain groups [2]. Information-theoretic metrics further enrich the evaluation framework by providing deeper insights into data leakage and utility [8]. By understanding and optimizing these trade-offs, it is possible to achieve a balanced and effective privacy-preserving system [2]. \n\n\nReferences:\n[0] R2DP: A Universal and Automated Approach to Optimizing the Randomization Mechanisms of Differential Privacy for Utility Metrics with No Known Optimal Distributions More importantly, our evaluations show that, for medium regime ofprivacy budgets (which could be more desirable in practice), the classof optimal noise can be totally different. In fact, as shown in Genget al. (Geng andViswanath, 2016), the lower-bound of ϵitalic-ϵ\\epsilon at which theStaircase distribution performs better than the Laplace distributionis somewhere around ϵ=3italic-ϵ3\\epsilon=3 for both ℓ1subscriptℓ1\\ell_{1} and ℓ2subscriptℓ2\\ell_{2}metrics. As illustrated in Figure 4, in contrast toℓ1subscriptℓ1\\ell_{1} metric (for which the results of laplace and staircase arerelatively tight), R2DP can find a class of noises withsignificantly improved ℓ2subscriptℓ2\\ell_{2} metric for ϵ<3italic-ϵ3\\epsilon<3 (a logarithmicX axis is used to illustrate the performance in this region). The PDFof this class of noises is mostly two-fold distributions with Laplacedistribution as the first fold, and Gamma distribution as the secondfold. This finding is in line with the optimal class of noise proposedby Koufogiannis et al. (Koufogianniset al., 2015), i.e.,f​(v)=ϵn​Γ​(n2+1)πn2​Γ​(n+1)​e−ϵ​‖v‖2𝑓𝑣continued-fractionsuperscriptitalic-ϵ𝑛Γ𝑛21superscript𝜋𝑛2Γ𝑛1superscript𝑒italic-ϵsubscriptnorm𝑣2f(v)=\\cfrac{\\epsilon^{n}\\Gamma(\\frac{n}{2}+1)}{\\pi^{\\frac{n}{2}}\\Gamma(n+1)}e^{-\\epsilon||v||_{2}}. Furthermore, our results suggest different classes of optimalnoises (than those found in the literature) for different parameters,sensitivity, ϵitalic-ϵ\\epsilon and p𝑝p (index of ℓℓ\\ell norm). In particular,a larger p𝑝p tends to provide larger search spaces for R2DPoptimization, which results in further improved results forϵ<3italic-ϵ3\\epsilon<3 (Figure 4 (a,b,e,f) vs. (c,d,g,h)).\n\n[1] Investigating Trade-offs in Utility, Fairness and Differential Privacy in Neural Networks To enable an ethical and legal use of machine learning algorithms, they must both be fair and protect the privacy of those whose data are being used. However, implementing privacy and fairness constraints might come at the cost of utility (Jayaraman&Evans, 2019; Gong et al., 2020). This paper investigates the privacy-utility-fairness trade-off in neural networks by comparing a Simple (S-NN), a Fair (F-NN), a Differentially Private (DP-NN), and a Differentially Private and Fair Neural Network (DPF-NN) to evaluate differences in performance on metrics for privacy (epsilon, delta), fairness (risk difference), and utility (accuracy). In the scenario with the highest considered privacy guarantees (epsilon = 0.1, delta = 0.00001), the DPF-NN was found to achieve better risk difference than all the other neural networks with only a marginally lower accuracy than the S-NN and DP-NN. This model is considered fair as it achieved a risk difference below the strict (0.05) and lenient (0.1) thresholds. However, while the accuracy of the proposed model improved on previous work from Xu, Yuan and Wu (2019), the risk difference was found to be worse.\n\n[2] R2DP: A Universal and Automated Approach to Optimizing the Randomization Mechanisms of Differential Privacy for Utility Metrics with No Known Optimal Distributions In this section, we experimentally evaluate the performance of R2DP using six different utility metrics, i.e., ℓ1subscriptℓ1\\ell_{1},ℓ2subscriptℓ2\\ell_{2}, entropy, usefulness, Mallows and Rényidivergence. Furthermore, we investigate the tightness of R2DP under Rényi differential privacy (RDP in short) (Mironov, 2017)which provides a universal formulation of the privacy losses of variousDP mechanisms, as shown in Appendix F.2.2 (facilitating thecomparison between different mechanisms). Our objective is to verifythe following two properties about the performance of the R2DPframework w.r.t. all seven utility and privacy metrics: (1) R2DPproduces near-optimal results and (2) R2DP performs strictly betterthan well-known baseline mechanisms, e.g, Laplace and Staircasemechanisms, in settings where an optimal PDF is not known, e.g.,usefulness utility metric or Rényi differential privacy .\n\n[3] R2DP: A Universal and Automated Approach to Optimizing the Randomization Mechanisms of Differential Privacy for Utility Metrics with No Known Optimal Distributions We obtain our baseline results by applying the Naive Bayes classifier on the Adult dataset (45K training records and 5K testing records), the precision and recall results are derived as 0.814 and 0.825, respectively. Then, we evaluate the precision and recall of R2DP and Laplace-based naive classification (Vaidyaet al., 2013b) by varying the privacy budget for each PINQ query ϵ∈.1,10italic-ϵ0.110\\epsilon\\in.1,10 (sensitivity=1) where two different error bounds γ=0.05,0.1𝛾0.050.1\\gamma=0.05,0.1 are specified for R2DP. We have the following observations:\n\n[4] Information-theoretic metrics for Local Differential Privacy protocols Let 𝒜𝒜\\mathcal{A} be a finite set. A privacy protocol for 𝒜𝒜\\mathcal{A} is a pair Q=(Q~,ℬ)𝑄~𝑄ℬQ=(\\tilde{Q},\\mathcal{B}) where ℬℬ\\mathcal{B} is a finite set, and Q~=(Q~y|x)y∈ℬ,x∈𝒜∈ℝ≥0ℬ×𝒜~𝑄subscriptsubscript~𝑄conditional𝑦𝑥formulae-sequence𝑦ℬ𝑥𝒜superscriptsubscriptℝabsent0ℬ𝒜\\tilde{Q}=(\\tilde{Q}_{y|x})_{y\\in\\mathcal{B},x\\in\\mathcal{A}}\\in\\mathbb{R}_{\\geq 0}^{\\mathcal{B}\\times\\mathcal{A}} is a collection of nonnegative reals such that ∑yQ~y|x=1subscript𝑦subscript~𝑄conditional𝑦𝑥1\\sum_{y}\\tilde{Q}_{y|x}=1 for all x𝑥x. We will often identify 𝒜={1,⋯,a}𝒜1⋯𝑎\\mathcal{A}=\\{1,\\cdots,a\\} and ℬ={1,⋯,b}ℬ1⋯𝑏\\mathcal{B}=\\{1,\\cdots,b\\}, and consider Q~~𝑄\\tilde{Q} as a (b×a)𝑏𝑎(b\\times a)-matrix. We can consider Q𝑄Q as a random function: for x∈𝒜𝑥𝒜x\\in\\mathcal{A}, we let Q​(x)∈ℬ𝑄𝑥ℬQ(x)\\in\\mathcal{B} be the random variable with probability distribution Q~∙|x\\tilde{Q}_{\\bullet|x}.Furthermore, we let Q∗:𝒫𝒜→𝒫ℬ:subscript𝑄→subscript𝒫𝒜subscript𝒫ℬQ_{*}\\colon\\mathcal{P}_{\\mathcal{A}}\\rightarrow\\mathcal{P}_{\\mathcal{B}} be the map that sends p𝑝p to Q~⋅p⋅~𝑄𝑝\\tilde{Q}\\cdot p, where ⋅⋅\\cdot denotes matrix-vector multiplication. Note that if X∼psimilar-to𝑋𝑝X\\sim p, then Q​(X)∼Q∗​psimilar-to𝑄𝑋subscript𝑄𝑝Q(X)\\sim Q_{*}p.\n\n[5] Information-theoretic metrics for Local Differential Privacy protocols Typically one chooses the ℓ1subscriptℓ1\\ell_{1}- or ℓ2subscriptℓ2\\ell_{2}-norm for d𝑑d.The metric depends on d𝑑d, n𝑛n, and φ𝜑\\varphi, but this does not matter too much:we can get rid of the dependence on φ𝜑\\varphi by taking either the expected value over theprior distribution or the worst case,and the choice of d𝑑d can be tailored to the aggregator’s demands.As to the dependence on n𝑛n, for d=ℓ1,ℓ2𝑑subscriptℓ1subscriptℓ2d=\\ell_{1},\\ell_{2}it can be shown that the accuracy can be expressed as c⋅n−1/2⋅𝑐superscript𝑛12c\\cdot n^{-1/2}, 18, 55,hence maximising the accuracy comes down to maximising the constant c𝑐c.Nevertheless, there are several issues with this approach. First,the metric measures the utility of the pair (Q,φ^)𝑄^𝜑(Q,\\hat{\\varphi}),rather than just the utility of Q𝑄Q.This is an issue because in general there are many different choices for φ^^𝜑\\hat{\\varphi},without a clear a priori reason to prefer any one of them. Second, while for explicit linear φ^^𝜑\\hat{\\varphi} the accuracy can be approximated analytically,for many other choices of estimators this is more difficult.As such, this metric is hard to study from an analytical perspective, especially its dependence on φ𝜑\\varphi and n𝑛n. This makes it hard to compare privacy protocols.\n\n[6] Information-theoretic metrics for Local Differential Privacy protocols Let ε∈ℝ>0𝜀subscriptℝabsent0\\varepsilon\\in\\mathbb{R}_{>0}.The Generalised Randomised Response is the privacy protocol GRRa,ε=(Q~,ℬ)superscriptGRR𝑎𝜀~𝑄ℬ\\operatorname{GRR}^{a,\\varepsilon}=(\\tilde{Q},\\mathcal{B}) on 𝒜𝒜\\mathcal{A} satisfying ℬ=𝒜ℬ𝒜\\mathcal{B}=\\mathcal{A} andQ~y|x={eεeε+a−1, if y=x,1eε+a−1, if y≠x.subscript~𝑄conditional𝑦𝑥casessuperscripte𝜀superscripte𝜀𝑎1 if y=x,1superscripte𝜀𝑎1 if y≠x.\\tilde{Q}_{y|x}=\\left\\{\\begin{array}[{ll}\\frac{\\textrm{e}^{\\varepsilon}}{\\textrm{e}^{\\varepsilon}+a-1},&\\textrm{ if $y=x$,}\\\\\\frac{1}{\\textrm{e}^{\\varepsilon}+a-1},&\\textrm{ if $y\\neq x$.}\\end{array}\\right.(10.3)By design this privacy protocol satisfies LDP⁡(GRRa,ε)=εLDPsuperscriptGRR𝑎𝜀𝜀\\operatorname{LDP}(\\operatorname{GRR}^{a,\\varepsilon})=\\varepsilon. In the Propositions below, we use the abbreviated notation β:=eε−1assign𝛽superscripte𝜀1\\beta:=\\textrm{e}^{\\varepsilon}-1.\n\n[7] Investigating Trade-offs in Utility, Fairness and Differential Privacy in Neural Networks When ϵitalic-ϵ\\epsilon equals 0.1 and δ𝛿\\delta equals 0.00001, the average accuracy of the DPF-NN (M = 82.98%, SD = 0.19) is significantly lower by 1.05 (t(18) = -16.9, p << .001) compared to the DP-NN (M = 84.03%, SD = 0.05). With respect to average risk difference, the DPF-NN (M = 0.0475, SD = 0.0020) has a significantly lower average compared to the DP-NN (M = 0.1355, SD = 0.0024). This difference of 0.0880 is significant (t(18) = -89.1, p << .001).\n\n"}
{"case_id": "2bb40aa93ac3a6a673c839bd660718ac", "answer_text": "\n\nYes, specialized approaches have indeed been developed to leverage large language models (LLMs) for assisting users in authoring SQL queries. These methods primarily focus on enhancing the accuracy, efficiency, and usability of LLMs for natural language to SQL (NL2SQL) tasks [7]. The evolution of these specialized approaches can be categorized into three main strategies: in-context learning, decomposition techniques, and retrieval-augmented frameworks [1]. \n\n**In-Context Learning Approaches:** Recent advancements in in-context learning have significantly enhanced the effectiveness of large language models (LLMs) such as GPT-3 and Codex for a variety of tasks, including Text-to-SQL semantic parsing. This novel learning paradigm shows substantial promise for few-shot learning without the need for additional model finetuning [0]. Moreover, frameworks like DIN-SQL, which incorporate schema linking, classification, decomposition, SQL generation, and self-correction, represent a comprehensive approach to addressing complex queries [4].\nIn-context learning involves using examples within the input context to guide an LLM in generating SQL queries from natural language inputs. This method has proven effective in few-shot learning scenarios, especially with models like GPT-3 and Codex. These LLMs, when provided with examples of NL-to-SQL translations within the input context, can generalize well to new, unseen queries. For instance, the DESEM (Domain-specific Semantic Masking and Skeleton Retrieval) framework employs domain-specific word masking and schema-relevance filtering to improve SQL generation from LLMs [1]. Furthermore, the QDecomp (Question Decomposition) framework uses difficulty-based and similarity-diversity sampling to select few-shot examples, thus enhancing the prompt's effectiveness in generating accurate SQL queries [1]. These approaches highlight how leveraging context effectively can boost LLM performance without extensive fine-tuning.\n\nIn terms of performance, frameworks like DESEM and QDecomp have demonstrated significant improvements in benchmark tests. DESEM, for example, has shown improved accuracy in SQL generation tasks by effectively handling domain-specific terms and enhancing schema relevance [1]. QDecomp utilizes a strategic selection of few-shot examples, resulting in more reliable SQL output as evidenced by its performance metrics in various test scenarios [1]. However, these approaches also face challenges, such as the need for careful selection of examples to avoid overfitting and ensuring the diversity of examples to cover different query scenarios.\n\nTo design criteria for accurate SQL generation, whether a generated SQL can be successfully executed and elicit a correct answer for the user question is always the priority. As a complex programming task, generating the correct SQL in one go becomes challenging. Intuitively, considering the execution feedback/results in SQL generation assists the alignment to the corresponding database environment, which allows the LLMs to gather the potential executed errors and results to refine the generated SQL or hold a majority vote. The execution-aware methods in text-to-SQL incorporate the execution feedback in two main approaches: 1) Incorporating the feedback through second round prompting for regeneration, for every SQL query generated in the initial response, it will be executed in the corresponding database, thus obtaining feedback from the database. This feedback might be an error, or it might yield results that will be appended to the second round prompt. Through in-context learning of this feedback, LLMs are able to refine or regenerate the original SQL, thereby enhancing accuracy. 2) Utilize execution-based selection strategies for generated SQL, sample multiple generated SQL queries from LLM, and execute each in the database. Based on the results of each SQL query, use selection strategies (e.g., self-consistency, majority vote) to define a query from the SQL set that satisfies the criteria as the final predicted SQL. [3].\nAnother prominent strategy involves decomposing complex SQL queries into simpler sub-tasks or sub-questions. This approach can significantly reduce the complexity of the NL2SQL task, leading to better performance from LLMs. For example, the DIN-SQL (Decomposed In-Context Learning for SQL) framework decomposes the task into schema linking, classification, decomposition, SQL generation, and self-correction modules [4]. Similarly, the DEA-SQL (Decomposed and Enhanced Attention for SQL) approach breaks down queries into prerequisite and subsequent sub-tasks to streamline the SQL generation process [4]. These decomposition techniques allow LLMs to focus on manageable segments of the overall task, improving accuracy and reducing errors.\n\nPerformance evaluation of these decomposition methods, such as DIN-SQL, has shown enhanced accuracy and reduced error rates in SQL generation tasks. DIN-SQL’s modular approach has been particularly effective in handling complex queries, as evidenced by its improved performance metrics compared to traditional methods [4]. Nonetheless, a major limitation is the potential complexity added by the multiple modules, which could increase the computational overhead and latency in generating the final SQL query.\n\nRetrieval-Augmented Frameworks [5].\nRetrieval-augmented frameworks are another advanced strategy where external documents or examples are retrieved and used to inform the LLM's SQL generation process. This method provides additional context, which is particularly valuable for handling long-tail or less common queries. For instance, the QUILL (Query Intent with Large Language Models using Retrieval Augmentation and Multi-stage Distillation) framework demonstrates how retrieval augmentation can enhance query understanding by providing supplementary context, which helps in better SQL generation [5]. This approach mitigates some of the main challenges associated with NL2SQL tasks, such as the paucity of context in user queries and the need for extensive world knowledge [10]. By incorporating retrieval-augmented prompts, frameworks like QUILL achieve significant performance gains, making them practical for real-world applications.\n\nEmpirical results from the QUILL framework indicate substantial improvements in SQL generation accuracy due to the retrieval augmentation. By adding relevant external context, QUILL enhances the model’s understanding of queries, which is reflected in its superior performance metrics [5]. However, retrieval-augmented frameworks often suffer from increased latency due to the retrieval process and potential mismatch issues if the retrieved documents are not perfectly aligned with the query context.\n\n**Execution-Aware Methods:** To design criteria for accurate SQL generation, whether a generated SQL can be successfully executed and elicit a correct answer for the user question is always the priority. As a complex programming task, generating the correct SQL in one go becomes challenging. Intuitively, considering the execution feedback/results in SQL generation assists the alignment to the corresponding database environment, which allows the LLMs to gather the potential executed errors and results to refine the generated SQL or hold a majority vote. The execution-aware methods in text-to-SQL incorporate the execution feedback in two main approaches: 1) Incorporating the feedback through second round prompting for regeneration, for every SQL query generated in the initial response, it will be executed in the corresponding database, thus obtaining feedback from the database. This feedback might be an error, or it might yield results that will be appended to the second round prompt. Through in-context learning of this feedback, LLMs are able to refine or regenerate the original SQL, thereby enhancing accuracy. 2) Utilize execution-based selection strategies for generated SQL, sample multiple generated SQL queries from LLM, and execute each in the database. Based on the results of each SQL query, use selection strategies (e.g., self-consistency, majority vote) to define a query from the SQL set that satisfies the criteria as the final predicted SQL [3]. \nIn addition to the above strategies, execution-aware methods incorporate feedback from the database execution of generated SQL queries to refine and improve the initial outputs. This can involve a second round of prompting to regenerate SQL based on execution results or using selection strategies to identify the best-performing SQL from a set of generated queries [3]. For example, the Coder-Reviewer framework combines SQL generation with evaluation models that check the likelihood of the generated instructions being correct, thus iteratively improving the final SQL output [4].\n\nThe effectiveness of execution-aware methods is evident from their performance metrics, as these methods significantly reduce errors and increase the accuracy of generated SQL queries. By incorporating feedback and iterative refinement, frameworks like Coder-Reviewer have shown remarkable improvements in the precision of SQL outputs [4]. However, these methods can be resource-intensive, requiring multiple interactions with the database which could be computationally prohibitive for large-scale applications.\n\n**Comparison with Other Popular Methods:**\nCross-lingual Text-to-SQL semantic parsing aims to translate non-English utterances into SQL queries based on an English schema (assuming we have an internationalized database), enabling users to query databases in non-English languages [0]. For example, such a system could help people from around the world access the US government's open data with natural language questions in different languages [0]. State-of-the-art approaches for Text-to-SQL semantic parsing have been greatly improved by fine-tuning pre-trained language models as a sequence-to-sequence problem [0]. More recently, in-context learning with large language models (LLMs), such as GPT-3 and Codex, has emerged as a new learning paradigm [0]. This paradigm enables effective few-shot learning without model fine-tuning, showing its practical and scientific value [0]. Recent papers also have shown promising results applying in-context learning to the Text-to-SQL task [0]. Rajkumar et al. (2022) studied if LLMs are already competitive Text-to-SQL semantic parsers without further fine-tuning on task-specific training data [0]. Additionally, Poesia et al. (2022) and Rubin et al. (2022) investigated the exemplar retrieval problem for the semantic parsing task [0]. However, previous work mostly focused on English utterances, leaving other languages behind [0]. It is unclear if LLMs are competitive for cross-lingual Text-to-SQL with English exemplars using in-context learning [0]. Even in the mono-lingual setting (where the exemplars and the query are in the same language), many approaches are not practical [0]. DESEM is a prompt engineering framework with de-semanticization and skeleton retrieval [1]. The framework first employs a domain-specific words masking module to remove the semantic tokens in questions that preserve the question’s intentions and then utilizes an adjustable prompting module that retrieves the few-shot examples with identical question intentions and incorporates schema-relevance filtering to guide the LLM’s SQL generation [1]. The QDecomp framework introduces the InterCOL mechanism to incrementally incorporate the decomposed sub-questions with correlative table and column names [1]. With difficulty-based selection, the few-shot examples for QDecomp are difficult-level sampled [1]. Besides similarity-diversity sampling, the proposed SD+SA+Voting (Similarity-Diversity+Schema Augmentation+Voting) sampling strategy employs semantic similarity and k-Means cluster diversity for sampling few-shot examples and then enhances the prompt with schema knowledge (semantic or structure augmentation) [1]. C3 framework comprises a clear prompting component, which takes the question and schema as the LLMs input, generates a clear prompt that includes a schema that removes the redundant information irrelevant to the user question and a schema linking, and also a calibration component providing hints [1]. The LLMs take their composition as context-augmented prompts for SQL generation [1]. A retrieval-augmented framework is introduced with sample-aware prompting, which simplifies the original question and extracts the question skeleton from the simplified question, then finishes the sample retrieval in the repository according to skeleton similarities [1]. The retrieved samples are combined with the original question for few-shot prompting [1]. ODIS introduces the selection of a sample with out-of-domain demonstrations and in-domain synthetic data, which retrieves few-shot demonstrations from hybrid sources to augment the prompt representations [1]. DAIL-SQL proposed a novel approach to address the issues in few-shot sampling and organization, presenting a better balance between the quality and quantity of few-shot examples [1]. DAIL Selection first masks domain-specific words in user and few-shot example questions, then ranks the candidate examples based on the embedded Euclidean distance [1]. Meanwhile, the similarity between the pre-predicted SQL queries is calculated [1]. Finally, the selection mechanism obtains the similarity-sorted candidates according to the pre-set criteria [1]. The few-shot examples are guaranteed good similarity with both questions and SQL queries with this method [1]. ACT-SQL proposed dynamic examples in few-shot prompting, which is selected according to similarity score [1]. FUSED is presented to build a high-diversity demonstrations pool through human-free multiple-iteration synthesis to improve the diversity of the few-shot demonstrations [1]. The pipeline of FUSED samples the demonstrations to be fused by clustering, then fuses the sampled demonstrations to construct the pool to enhance few-shot learning [1]. Knowledge-to-SQL framework aims to build a Data Expert LLM (DELLM) to provide knowledge for SQL generation [1]. The DELLM is trained by supervised fine-tuning using human expert annotations and further refined by preference learning with the database’s feedback [1]. DELLM generates four categories of knowledge, the well-designed methods (e.g. DAIL-SQL, MAC-SQL) incorporating the generated knowledge to achieve better performance for LLM-based text-to-SQL with in-context learning [1]. \nWhile frameworks like DESEM, QDecomp, DIN-SQL, DEA-SQL, and QUILL show significant promise, they are part of a broader landscape of NL2SQL research. Other popular methods like fine-tuning pre-trained language models (e.g., T5 for SQL generation), or using techniques like constrained decoding as seen in PICARD, offer alternative benefits and trade-offs. Fine-tuning approaches, like those using T5, can achieve high accuracy on specific benchmarks (e.g., Spider) but often require extensive annotated data and computational resources [6]. Meanwhile, constrained decoding techniques such as PICARD improve syntactic correctness of the generated SQL by enforcing SQL grammar rules during decoding [6]. These methods can be highly effective but may not generalize as well to diverse, real-world query contexts as in-context learning or retrieval-augmented methods.\n\n**Real-World Applications:**  Cross-lingual Text-to-SQL semantic parsing (Sherborne and Lapata, 2022b;Min et al., 2019;Sherborne et al., 2020) aims to translate non-English utterances into SQL queries based on an English schema (assuming we have an internationalized database), enabling users to query databases in non-English languages [0]. For example, such a system could help people from around the world access the US government's open data with natural language questions in different languages. [0]  State-of-the-art approaches for Text-to-SQL semantic parsing have been greatly improved by finetuning pre-trained language models as a sequence to-sequence problem (Scholak et al., 2021;Yin et al., 2020;Herzig et al., 2020;Yu et al., 2021a,b;Shi et al., 2021a) [0]. More recently, in-context learning with large language models (LLMs), such as GPT-3 (Brown et al., 2020) and Codex (Chen et al., 2021), has emerged as a new learning paradigm [0]. This paradigm enables effective few-shot learning without model finetuning, showing its practical and scientific value [0]. Recent papers also have shown promising results applying in-context learning to the Text-to-SQL task [0]. Rajkumar et al. (2022) studied if LLMs are already competitive Text-to-SQL semantic parsers without further finetuning on task-specific training data [0]. Additionally, Poesia et al. (2022) and Rubin et al. (2022) investigated the exemplar retrieval problem for the semantic parsing task [0]. However, previous work mostly focused on English utterances, leaving other languages behind [0]. It is unclear if LLMs are competitive for crosslingual Text-to-SQL with English exemplars using in-context learning [0]. Even in the mono-lingual setting (where the exemplars and the query are in the same language), many approaches are not practical [0]. \n\n\nThese advanced approaches have been successfully implemented in various real-world applications, demonstrating their practical utility. For instance, the implementation of the QUILL framework in search engines has shown improved query understanding, leading to more accurate results and user satisfaction [5]. Additionally, the DESEM framework has been adopted in enterprise database management systems to assist non-expert users in extracting valuable insights without needing extensive SQL knowledge [1]. The DIN-SQL approach has been particularly useful in data analytics platforms where complex queries are common, providing a robust solution to break down and effectively manage the generation of SQL queries [4].\n\nIn conclusion, the development of specialized approaches for LLM-assisted SQL query authoring encompasses a range of techniques designed to enhance the effectiveness and applicability of these models in practical settings. By leveraging in-context learning, decomposition, retrieval augmentation, and execution-aware methods, researchers have significantly improved the accuracy and usability of LLMs for NL2SQL tasks [1][2][3]. These advancements not only make it easier for non-experts to interact with databases but also pave the way for more sophisticated and user-friendly database interfaces [9]. Performance metrics from these specialized frameworks demonstrate their efficacy and potential for real-world applications, marking a significant step forward in the field of natural language to SQL translation [7]. \n\n\nReferences:\n[0] XRICL: Cross-lingual Retrieval-Augmented In-Context Learning for Cross-lingual Text-to-SQL Semantic Parsing and practical applications. Cross-lingual Text-to-SQL semantic parsing (Sherborne and Lapata, 2022b;Min et al., 2019;Sherborne et al., 2020) aims to translate non-English utterances into SQL queries based on an English schema (assuming we have an internationalized database), enabling users to query databases in non-English languages. For example, such a system could help people from around the world access the US government's open data 1 with natural language questions in different languages. State-of-the-art approaches for Text-to-SQL semantic parsing have been greatly improved by finetuning pre-trained language models as a sequenceto-sequence problem (Scholak et al., 2021;Yin et al., 2020;Herzig et al., 2020;Yu et al., 2021a,b;Shi et al., 2021a). More recently, in-context learning with large language models (LLMs), such as GPT-3 (Brown et al., 2020) and Codex (Chen et al., 2021), has emerged as a new learning paradigm. This paradigm enables effective few-shot learning without model finetuning, showing its practical and scientific value (Beltagy et al., 2022). Recent papers also have shown promising results applying in-context learning to the Text-to-SQL task. Rajkumar et al. (2022) studied if LLMs are already competitive Text-to-SQL semantic parsers without further finetuning on task-specific training data. Additionally, Poesia et al. (2022) and Rubin et al. (2022) investigated the exemplar retrieval problem for the semantic parsing task. However, previous work mostly focused on English utterances, leaving other languages behind. It is unclear if LLMs are competitive for crosslingual Text-to-SQL with English exemplars using in-context learning. Even in the mono-lingual setting (where the exemplars and the query are in the same language), many approaches are not practical\n\n[1] Next-Generation Database Interfaces: A Survey of LLM-based Text-to-SQL DESEM  is a prompt engineering framework with de-semanticization and skeleton retrieval.The framework first employs domain-specific words masking module to remove the semantic tokens in questions that preserve the question’s intentions.And then utilizes an adjustable prompting module that retrieves the few-shot examples with identical question intentions and incorporates schema-relevance filtering to guide the LLM’s SQL generation.The QDecomp  framework introduces the InterCOL mechanism to incrementally incorporate the decomposed sub-questions with correlative table and column names.With difficulty-based selection, the few-shot examples for QDecomp are difficult-level sampled.Besides similarity-diversity sampling, proposed SD+SA+Voting (Similarity-Diversity+Schema Augmentation+Voting) sampling strategy.They first employ semantic similarity and k𝑘k-Means cluster diversity for sampling few-shot examples and then enhance the prompt with schema knowledge (semantic or structure augmentation).C3  framework comprises a clear prompting component, which takes the question and schema as the LLMs input, generates a clear prompt that includes a schema that removes the redundant information irrelevant to the user question and a schema linking, and also a calibration component providing hints.The LLMs take their composition as context-augmented prompts for SQL generation.A retrieval-augmented framework is introduced with sample-aware prompting , which simplifies the original question and extracts the question skeleton from the simplified question, then finishes the sample retrieval in the repository according to skeleton similarities.The retrieved samples are combined with the original question for few-shot prompting.ODIS  introduces the selection of a sample with out-of-domain demonstrations and in-domain synthetic data, which retrieves few-shot demonstrations from hybrid sources to augment the prompt representations.DAIL-SQL  proposed a novel approach to address the issues in few-shot sampling and organization, presenting a better balance between the quality and quantity of few-shot examples.DAIL Selection first masks domain-specific words in user and few-shot example questions, then ranks the candidate examples based on the embedded Euclidean distance.Meanwhile, the similarity between the pre-predicted SQL queries is calculated.Finally, the selection mechanism obtains the similarity-sorted candidates according to the pre-set criteria.The few-shot examples are guaranteed good similarity with both questions and SQL queries with this method.ACT-SQL  proposed dynamic examples in few-shot prompting, which is selected according to similarity score.FUSED  are presented to build a high-diversity demonstrations pool through human-free multiple-iteration synthesis to improve the diversity of the few-shot demonstrations.The pipeline of FUSED samples the demonstrations to be fused by clustering, then fuse the sampled demonstrations to construct the pool to enhance few-shot learning.Knowledge-to-SQL  framework aims to build a Data Expert LLM (DELLM) to provide knowledge for SQL generation.The DELLM is trained by supervised fine-tuning using human expert annotations  and further refined by preference learning with the database’s feedback.DELLM generates four categories of knowledge, the well-designed methods (e.g. DAIL-SQL , MAC-SQL ) incorporating the generated knowledge to achieve better performance for LLM-based text-to-SQL with in-context learning.\n\n[2] Next-Generation Database Interfaces: A Survey of LLM-based Text-to-SQL As an intuitive solution, decomposing a challenging user question into simpler sub-questions and using multi-step reasoning for implementation can reduce the complexity of the full text-to-SQL task , 8.Dealing with less complexity, LLMs have the potential to have better performance.The decomposition approaches for LLM-based text-to-SQL are categorized into two paradigms:(1) sub-task decomposing, provides additional parsing to assist the final SQL generation by decomposing the overall text-to-SQL task into smaller effective sub-tasks (e.g., schema linking, domain classification).(2) sub-question decomposing, divides the user question into sub-questions to reduce the question’s complexity and difficulty, then generates the sub-SQL by solving these questions to deduce the final SQL query.\n\n[3] Next-Generation Database Interfaces: A Survey of LLM-based Text-to-SQL To design criteria for accurate SQL generation, whether a generated SQL can be successfully executed and elicit a correct answer for the user question is always the priority .As a complex programming task, generating the correct SQL in one go becomes challenging.Intuitively, considering the execution feedback/results in SQL generation assists the alignment to the corresponding database environment, which allows the LLMs to gather the potential executed errors and results to refine the generated SQL or hold a majority vote .The execution-aware methods in text-to-SQL incorporate the execution feedback in two main approaches:1) Incorporating the feedback through second round prompting for regeneration, for every SQL query generated in the initial response, it will be executed in the corresponding database, thus obtaining feedback from the database.This feedback might be an error, or it might yield results that will be appended to the second round prompt.Through in-context learning of this feedback, LLMs are able to refine or regenerate the original SQL, thereby enhancing accuracy.2) Utilize execution-based selection strategies for generated SQL, sample multiple generated SQL queries from LLM), and execute each in the database.Based on the results of each SQL query, use selection strategies (e.g., self-consistency, majority vote ) to define a query from the SQL set that satisfies the criteria as the final predicted SQL.\n\n[4] Next-Generation Database Interfaces: A Survey of LLM-based Text-to-SQL DIN-SQL  proposed a decomposed in-context learning method consisting of four modules: schema linking, classification & decomposition, SQL generation, and self-correction.DIN-SQL first finishes the schema linking between the user question and the target database; the following module decomposes the user question into correlated sub-questions and does a difficulty classification.Based on the above information, the SQL generation module generates a corresponding SQL, and the self-correction module identifies and corrects the potential errors in the predicted SQL.This approach comprehensively considers the decomposition of both sub-tasks and sub-questions.Coder-Reviewer  framework proposed a re-ranking method, combining Coder models for the generation and Reviewer models to evaluate the likelihood of the instruction.Refer to the Chain-of-Thought  and Least-to-Most prompting , QDecomp  introduce question decomposition prompting, which follows the question reduction stage in least-to-most prompting and instruct the LLM to decompose the original complex question as the intermediate reasoning steps.C3  consists of three key components: clear prompting, calibration bias prompting, and consistency; these components are accomplished by assigning ChatGPT with different tasks.Firstly, the clear prompting component generates the schema linking and the distilled question-relevant schema as a clear prompt.Then, a multi-turn dialogue about text-to-SQL hints is utilized as a calibration bias prompt, which combines with the clear prompt to guide the SQL generation.The generated SQL queries are selected by consistency and execution-based voting to get the final SQL.MAC-SQL  presents a multi-agent collaborating framework; the text-to-SQL process is finished as the collaboration of the agents: Selector, Decomposer, and Refiner.The Selector preserves relevant tables for user questions; the Decomposer breaks down user questions into sub-questions and provides solutions; finally, the Refiner validates and refines the defective SQL.DEA-SQL  introduces a workflow paradigm aiming to enhance the attention and problem-solving scope of LLM-based text-to-SQL through decomposition.This method decomposes the overall task, enabling the SQL generation module to have the corresponding prerequisite (information determination, question classification) and subsequent (self-correction, active learning) sub-tasks.Through the workflow paradigm, an accurate SQL query is generated.SGU-SQL  is a structure-to-SQL framework, leveraging the inherent structure information to assist SQL generation.Specifically, the framework constructs a graph structure for the user question and the corresponding database respectively, then uses the encoded graphs to construct structure linking , 105.A meta operator decomposes the user question with a grammar tree and finally designs the input prompt with meta-operation in SQL.MetaSQL  introduces a three-stage approach for SQL generation, which consists of decomposition, generation, and rank.The decomposition stage uses semantic decomposition and metadata composition to process the user question.Taking the previously processed data as input, a text-to-SQL model using metadata-conditioned generation to generate some candidate SQL queries.Finally, a two-stage ranking pipeline is applied to get a global-optimal SQL query.PET-SQL  proposed a prompt-enhanced two-stage framework.Firstly, an elaborated prompt instructs the LLMs to generate preliminary SQL (PreSQL) where some few-shot demonstrations are selected based on similarity.Then, schema linking is found based on PreSQL and combined to prompt the LLMs to generate the Final SQL (FinSQL).Finally, multiple LLMs are utilized to generate a FinSQL, ensuring consistency based on the execution results.\n\n[5] QUILL: Query Intent with Large Language Models using Retrieval Augmentation and Multi-stage Distillation QUILL: Query Intent with Large Language Models using Retrieval Augmentation and Multi-stage Distillation Large Language Models (LLMs) have shown impressive results on a variety of text understanding tasks. Search queries though pose a unique challenge, given their short-length and lack of nuance or context. Complicated feature engineering efforts do not always lead to downstream improvements as their performance benefits may be offset by increased complexity of knowledge distillation. Thus, in this paper we make the following contributions: (1) We demonstrate that Retrieval Augmentation of queries provides LLMs with valuable additional context enabling improved understanding. While Retrieval Augmentation typically increases latency of LMs (thus hurting distillation efficacy), (2) we provide a practical and effective way of distilling Retrieval Augmentation LLMs. Specifically, we use a novel two-stage distillation approach that allows us to carry over the gains of retrieval augmentation, without suffering the increased compute typically associated with it. (3) We demonstrate the benefits of the proposed approach (QUILL) on a billion-scale, real-world query understanding system resulting in huge gains. Via extensive experiments, including on public benchmarks, we believe this work offers a recipe for practical use of retrieval-augmented query understanding. Queries -especially keyword search onespresent a unique challenge though. Their short * Corresponding Authors length, inherent ambiguity and lack of grammar mean query understanding tasks typically require more memorization and world knowledge than other NLP tasks (Broder et al., 2007). Consequently, despite LLMs leading performance on language and query understanding tasks -like intent classification, query parsing and relevance prediction -there is significant room for further improvement. In\n\n[6] NL2INTERFACE: Interactive Visualization Interface Generation from Natural Language Queries for visualizations. Given an NL query, NL4DV generates visualizations by parsing the NL query to extract attributes and tasks. In contrast, NL2INTERFACE creates interactive multivisualization interfaces which consider visualizations, interactions and layout. None of the existing works have explored this. Natural Language to SQL Translation Translation from natural language to SQL (Text-to-SQL) has been widely studied by the NLP community,23,24,27,31,34. Difficulties in text-to-SQL are mainly two-fold: encoding a variety of complex relationships between the user's query and multiple tables, and decoding the SQL with valid representations. Many successful solutions include question-table encoding and constrained decoding. Recently, Shaw et al. showed that fine-tuning a pre-trained T5 language model could yield competitive results on Spider text-to-SQL benchmark. PICARD constrains the autoregressive decoder during inference time to improve the quality of SQL generation. More recently, large language models (LLMs) like GPT-3 and Codex have been shown to perform incredibly well in many NLP tasks without any training. demonstrates Codex's near state-of-the-art performance on Spider in a zero-shot setting when prompted with in-context examples. In this work, NL2INTERFACE uses Codex as the back-end LLM. finds that Codex is able to generalize to unseen target programs with a few in-context demonstration examples. In our case, we prompt Codex to generate our designed SPS representations given natural language inputs by providing a few natural language-SPS pairs. Precision Interface The Precision Interface series of work,6,35 explores interface generation from queries. PI uses SQL queries as a proxy and generates interactive widgets from\n\n[7] Fine-Tuning Language Models for Context-Specific SQL Query Generation This paper contributes to the field of natural language processing and database querying by detailing the process of fine-tuning open-source large language models (LLMs) to accurately convert natural language questions into SQL queries in a retail context. Our approach, which leverages a synthetic dataset and the LoRa technique, has been shown to significantly enhance the performance of models like Starcoder Plus, Code-Llama, and Mistral, particularly when compared to the zero-shot performance of GPT-4. Code-Llama, in particular, demonstrated exceptional accuracy, surpassing 80% in both Snowflake SQL and GoogleSQL dialects.Through the course of our discussion, we have acknowledged the potential limitations of our study, including the possibility of overfitting and the lack of generalization beyond the specific dataset and SQL dialects used. These issues highlight the importance of careful dataset curation and rigorous model evaluation in future research.Moving forward, there is a clear opportunity to expand this research to encompass a wider range of domains and SQL dialects, and to explore the integration of these models into end-user applications. Striking a balance between accuracy through specialization and broader applicability through generalization remains a key challenge in deploying these models.By focusing on the intersection of machine learning, AI, and user accessibility, this study not only progresses the technical capabilities of Text-to-SQL models but also emphasizes the practical benefits of making complex data systems more accessible to non-experts. Continued refinement of these models brings us closer to a future with lower barriers to data insights, enabling a wider audience to harness the power of data analytics.\n\n[8] A Case for A Collaborative Query Management System query representation and modeling, efficient query-log processing (clustering, mining, etc.), maintenance in face of database updates, ranking (e.g., by similarity or popularity score), interface design, and query annotations. We discuss some of these challenges further in Section 4. Assisted Interaction Composing SQL queries can be a difficult task even for technically savvy users such as scientists or analysts. Furthermore, even if a user is fluent in composing queries, she can benefit from hints regarding what queries to ask. Thus, a key role of the CQMS is to assist users in query composition. For this, we define the Assisted Interaction Mode where the CQMS monitors the user as she types queries and provides suggestions for completing and correcting the queries. Figure 3 shows an example of assisted interaction where the system suggests several completions for the query being composed. It also shows similar queries at the bottom. In its simplest form, query completion consists in providing users with possible completions for relation and attribute names as the user types a query. Although this simple capability can already be helpful, query completion can be pushed much further. In particular, the system can make context-aware suggestions. For example, assume that the most popular table to include in the FROM clause is CityLocations. However, for queries that also include WaterSalinity, the most popular is WaterTemp. Thus, if the user has already included WaterSalinity, the system should suggest WaterTemp over CityLocations. could similarly suggest predicates in the WHERE clause of a query and even complete subclauses of the query. This capability\n\n[9] QUILL: Query Intent with Large Language Models using Retrieval Augmentation and Multi-stage Distillation Large language models (LLMs) such as mT5 (Xue et al., 2021) demonstrated significant performance improvements on a variety of natural language understanding (NLU) tasks. Specifically in the context of query understanding, researchers found that (a) model size significantly effects the quality of the resulting models (Nogueira et al., 2019;Han et al., 2020), and (b) using additional context in the form of query-associated documents is crucial to the model performance due to the paucity of context available in the query itself (Nogueira and Lin, 2019;Zhang et al., 2020). Retrieval augmentation of the query with the search results retrieved by it is a proven way to incorporate such context in LLM training for NLU tasks, as has been shown recently by models such as RAG (Lewis et al., 2020), REALM (Guu et al., 2020), and RETRO (Borgeaud et al., 2022). In this paper, we leverage this insight to improve performance of query intent prediction (Broder, 2002) -a crucial query understanding task that is at the heart of modern search engines -using LLMs. Prior work by Broder et al. (2007) found importance of retrieval augmentation using statistical methods for this task. Statistical retrieval augmentation has also been found critical for other query understanding tasks including query expansion (Broder et al., 2008;Diaz and Metzler, 2006) and query tagging (Wang, 2020). We demonstrate similar benefits when using retrieval-augmented LLMs as well. We also leverage Knowledge Distillation (Hinton et al., 2015;Mirzadeh et al., 2020;Gou et al., 2021) techniques to create a Student model that retains the LLMs gains. Query Intent Understanding While\n\n"}
{"case_id": "0919a8528cd20166163de3fdcb089efa", "answer_text": "\n\n\nThe advent of large language models (LLM) presents an opportunity to enhance compatibility between EHRs and clinical trial descriptions, promoting more accurate patient-trial matching [1]. By capitalizing on their sophisticated natural language processing abilities, these models can efficiently decipher, comprehend, and harmonize the diverse terminologies and ontologies present in both EHRs and clinical trial inclusion and exclusion criteria [0]. This enhanced interoperability not only streamlines the matching process but also ensures greater accuracy in identifying suitable trials for patients [0]. Consequently, large language models hold the potential to transform clinical trial matching, enabling better patient outcomes and contributing to more efficient medical research [0].\n\n\nLarge language models (LLMs) have indeed been applied to the schema matching problem in databases, and they have shown promising effectiveness [2]. Schema matching, which involves aligning the schemas of different databases to identify equivalent data fields, is a critical task in data integration and interoperability.\n\nEffectiveness[1]\n\nSeveral studies have explored the application of LLMs to schema matching. An experimental study on schema matching with LLMs demonstrated that these models could be effectively prompted to generate sets of match candidates based solely on schema documentation [2][7]. The study evaluated the LLM-generated matches against a real-world benchmark and found that the results were highly reasonable, with verification efforts being manageable.\n\nSpecifically, using models like GPT-3.5 and GPT-4, the number of retrieved matches was significantly smaller than the entire search space of attribute pairs, making manual verification feasible [7][8]. For instance, GPT-3.5 retrieved 136.2 matches in the N-to-1 configuration and 104.6 matches in the 1-to-N configuration, while GPT-4 retrieved even fewer matches, indicating a streamlined verification process [8]. This suggests that LLMs can significantly reduce the complexity and effort required for schema matching in practical applications.\n\n### Methodologies[0]\n\nThe adaptation of LLMs to various matching techniques has been beneficial. For instance, leveraging GPT-4 for schema matching resulted in reduced counts of attribute matches in certain configurations, implying improved precision and relevance of the matches [7]. The integration of LLMs in schema matching tasks has also been explored in combination with traditional similarity measures. One approach, for example, involved using N-gram string similarity combined with LLMs to enhance matching accuracy. This method calculates the N-gram string similarity \\( \\textit{sim}_{NG}(a,b) \\) between all possible attribute pairs in a dataset. For each attribute name, a set of its 3-grams is created, and then the Dice similarity is computed to evaluate potential matches [5]. These traditional measures, when integrated with LLM-generated matches, provide a robust hybrid approach, leveraging the strengths of both advanced machine learning models and proven similarity metrics.\n\nPractical Applications [0].\n\nPractical applications of LLMs in real-world scenarios further illustrate their effectiveness. For example, using GPT-4 in clinical trial matching involved structuring trial eligibility criteria and patient data into a format manageable by the LLM, which then produced cogent narratives explaining its matching decisions [9]. This practical implementation demonstrates how LLMs can handle complex, context-rich data and generate accurate matches, providing value in medical research and patient care.\n\nChallenges include data standardization, ethical considerations, and a lack of interoperability between Electronic Health Records (EHRs) and clinical trial criteria. [1] \n\nHowever, there are still challenges and considerations for the effective application of LLMs in schema matching. One notable issue is the context limit of LLMs, which restricts the amount of information that can be processed in a single prompt. This limitation necessitates the structuring of information beforehand, as seen in applications like clinical trial matching, where structured patient information was matched against structured trial data [9][10]. Additionally, the risk of generative hallucination, where LLMs may generate incorrect or misleading matches due to lack of awareness of the universal item pool, needs to be mitigated through post-processing operations and robust prompt designs [4].\n\nEfficiency and scalability are critical when applying LLMs to schema matching, especially across various scales and database sizes. Studies indicate that LLMs can handle large-scale schema matching tasks effectively. For instance, experimental data has included schemas ranging from 20 to 120 attributes, which are comparable to or larger than other published evaluations of relational schema matching [11]. The reasonable number of retrieved matches (<10% of the search space) further demonstrates the scalability of LLMs in managing extensive attribute pairs [8].\n\nThe process of matching patients with suitable clinical trials is essential for advancing medical research and providing optimal care [0]. However, current approaches face challenges such as data standardization, ethical considerations, and a lack of interoperability between Electronic Health Records (EHRs) and clinical trial criteria [0]. In this paper, we explore the potential of large language models (LLMs) to address these challenges by leveraging their advanced natural language generation capabilities to improve compatibility between EHRs and clinical trial descriptions [0]. We propose an innovative privacy-aware data augmentation approach for LLM-based patient-trial matching (LLM-PTM), which balances the benefits of LLMs while ensuring the security and confidentiality of sensitive patient data [1]. Our experiments demonstrate a 7.32% average improvement in performance using the proposed LLM-PTM method, and the generalizability to new data is improved by 12.12% [1]. Additionally, we present case studies to further illustrate the effectiveness of our approach and provide a deeper understanding of its underlying principles [1]. \n\nOverall, the evidence from various studies and practical applications suggests that while there are limitations, the application of LLMs to schema matching is not only feasible but also beneficial in terms of reducing manual effort and improving matching accuracy [2]. Future research and development could further optimize these models and address existing limitations, thereby enhancing their applicability and effectiveness in schema matching tasks [7]. \n\n\nReferences:\n[0] Large Language Models for Healthcare Data Augmentation: An Example on Patient-Trial Matching. The advent of large language models (LLM) 7 presents an opportunity to enhance the compatibility between EHRs and clinical trial descriptions, promoting more accurate patient-trial matching. By capitalizing on their sophisticated natural language processing abilities, these models can efficiently decipher, comprehend, and harmonize the diverse terminologies and ontologies present in both EHRs and clinical trial inclusion and exclusion criteria. This enhanced interoperability not only streamlines the matching process but also ensures greater accuracy in identifying suitable trials for patients. Consequently, large language models hold the potential to transform clinical trial matching, enabling better patient outcomes and contributing to more efficient medical research.\n\n[1] Large Language Models for Healthcare Data Augmentation: An Example on Patient-Trial Matching. The process of matching patients with suitable clinical trials is essential for advancing medical research and providing optimal care. However, current approaches face challenges such as data standardization, ethical considerations, and a lack of interoperability between Electronic Health Records (EHRs) and clinical trial criteria. In this paper, we explore the potential of large language models (LLMs) to address these challenges by leveraging their advanced natural language generation capabilities to improve compatibility between EHRs and clinical trial descriptions. We propose an innovative privacy-aware data augmentation approach for LLM-based patient-trial matching (LLM-PTM), which balances the benefits of LLMs while ensuring the security and confidentiality of sensitive patient data. Our experiments demonstrate a 7.32% average improvement in performance using the proposed LLM-PTM method, and the generalizability to new data is improved by 12.12%. Additionally, we present case studies to further illustrate the effectiveness of our approach and provide a deeper understanding of its underlying principles.\n\n[2] Schema Matching with Large Language Models: an Experimental Study In this paper, we aim to exploit this information and present an experimentalstudy on schema matching using an off-the-shelf generative Large LanguageModel (LLM). We investigate how LLMs can be prompted to generate a set of match candidates. We focus on the use of schema documentationas the sole signal and evaluate the performance against a newly definedreal-world benchmark.\n\n[3] How Can Recommender Systems Benefit from Large Language Models: A Survey In open-set item generation tasks (Bao et al., 2023a; Salinas et al., 2023; Di Palma et al., 2023; Jiang et al., 2023; Lin et al., 2023d; Mei and Zhang, 2023; Zheng et al., 2023a; Qiu et al., 2023; Liao et al., 2023; Harte et al., 2023; Li et al., 2023m; Zhai et al., 2023; Zhou et al., 2023a; Li et al., 2023n; Hua et al., 2023b; Geng et al., 2023; Li et al., 2023r; Yin et al., 2023; Jeong et al., 2023), LLM is required to directly generate the ranked item list that the user might prefer according to the user profile and behavior history without a given candidate item set.Since the candidate items are not provided in the input prompt, the large language model is actually not aware of the universal item pool ℐℐ\\mathcal{I}, thus bringing the generative hallucination problem (Mei and Zhang, 2023), where the generated items might fail to match the exact items in the item pool ℐℐ\\mathcal{I}.Therefore, apart from the design of input prompt templates (Hou et al., 2023b; Li et al., 2023p) and finetuning algorithms (Li et al., 2023m), the post-processing operations for item grounding and matching after the item generation are also required to overcome the generative hallucination problem (Mei and Zhang, 2023).We formulate the process as follows:(8)[i^kk=1Nsuperscriptsubscriptdelimited-[subscript^𝑖𝑘𝑘1𝑁\\displaystyle\\left[\\hat{i}_{k}\\right_{k=1}^{N}←LLM⁡(xu),←absentLLMsubscript𝑥𝑢\\displaystyle\\leftarrow\\operatorname{LLM}(x_{u}),[ikk=1Nsuperscriptsubscriptdelimited-[subscript𝑖𝑘𝑘1𝑁\\displaystyle[i_{k}_{k=1}^{N}←Match⁡([i^kk=1N,ℐ),←absentMatchsuperscriptsubscriptdelimited-[subscript^𝑖𝑘𝑘1𝑁ℐ\\displaystyle\\leftarrow\\operatorname{Match}\\left(\\left[\\hat{i}_{k}\\right_{k=1}^{N},\\mathcal{I}\\right),where Match⁡(⋅,⋅)Match⋅⋅\\operatorname{Match}(\\cdot,\\cdot) is the matching function, i^ksubscript^𝑖𝑘\\hat{i}_{k} is the LLM-generated items, and iksubscript𝑖𝑘i_{k} is the actual item matched from ℐℐ\\mathcal{I} according to i^ksubscript^𝑖𝑘\\hat{i}_{k}.LANCER (Jiang et al., 2023) employs knowledge-enhanced prefix tuning for generation ground and further applies cosine similarity to match the encoded representation of generated item text with the universal item pool ℐℐ\\mathcal{I}.Di Palma et al. (2023) leverage ChatGPT for user interest modeling and next item title generation with Damerau-Levenshtein distance (Miller et al., 2009) for item matching.\n\n[4] Schema Matching with Large Language Models: an Experimental Study Specifically, to obtain the baseline we calculate the N-gram stringsimilarity simN​G​(a,b)subscriptsim𝑁𝐺𝑎𝑏\\textit{sim}_{NG}(a,b) between all possible attribute pairs in adataset. For each attribute name a′superscript𝑎′a^{\\prime} we obtain the set of its 3-grams a𝑎aafter padding with special characters as described by Sun etal. . For example, the nameadmittime is transformed into the set {##a,#ad, adm, dmi, min, int,ntt, tti, tim, ime, me%,e%%}. For two sets a𝑎a and b𝑏b, we then calculate the Dicesimilarity:simN​G​(a,b):=2×|a∩b||a|+|b|assignsubscriptsim𝑁𝐺𝑎𝑏2𝑎𝑏𝑎𝑏\\textit{sim}_{NG}(a,b):=\\frac{2\\times|a\\cap b|}{|a|+|b|}Since each simN​G​(a,b)subscriptsim𝑁𝐺𝑎𝑏\\textit{sim}_{NG}(a,b) lies in the range;101;1, thisdefines an order on match candidates, with highest values appearing first. Onecan either set a threshold θ𝜃\\theta to decide which similarity value issufficient for a match or or limit the number of matches to the top k𝑘k rankedones. Using thresholding, all pairs simN​G​(a,b)≥θsubscriptsim𝑁𝐺𝑎𝑏𝜃\\textit{sim}_{NG}(a,b)\\geq\\thetawill then be output as a match, and using ranking, all pairs {(a0,b0),…,(an,bn)}:k\\{(a_{0},b_{0}),\\ldots,(a_{n},b_{n})\\}:k where simN​G​(ai,bi)≥simN​G​(aj,bj)subscriptsim𝑁𝐺subscript𝑎𝑖subscript𝑏𝑖subscriptsim𝑁𝐺subscript𝑎𝑗subscript𝑏𝑗\\textit{sim}_{NG}(a_{i},b_{i})\\geq\\textit{sim}_{NG}(a_{j},b_{j})for all i<j𝑖𝑗i<j will be output as a match. We choose the former and determine aseparate threshold per dataset as follows: we consider all calculatedsimilarity values as thresholds and pick the threshold that achieves the bestF1-score on the dataset. We then choose simN​Gsubscriptsim𝑁𝐺\\textit{sim}_{NG} with this threshold as thebaseline for the considered dataset. This approach favors the baseline, asit overestimates the capabilities of the N-gram string similarity for schemamatching. In practice, a data engineer cannot know which threshold to use.\n\n[5] Schema Matching with Large Language Models: an Experimental Study Overall, we deem most LLM-counts acceptable for manual verification as they aremuch smaller (<10%absentpercent10<10\\%) than the entire search space of all attribute pairs. Looking specifically at methods used in isolation (shown on the diagonal) weobserve that with GPT-3.5, N-to-1 retrieves the most matches(136.2136.2136.2), followed by 1-to-N (104.6104.6104.6) and N-to-M (30.030.030.0).Using GPT-4, the counts of 1-to-N and N-to-1 are further reduced while the count of N-to-M increases.\n\n[6] Schema Matching with Large Language Models: an Experimental Study Our experiments show that the number of retrieved matches is very reasonable(<10%absentpercent10<10\\% of the search space) and can be reduced further with the use ofGPT-4, rendering the verification effort for all task scopecombinations acceptable.\n\n[7] Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology We also explored applying GPT-4 directly to conduct end-to-end matching. Due to the context limit, we can’t fit the entire medical record for most patients into the prompt. Therefore, it is only feasible to consider direct matching against patient information that has already been structured, and currently it is only feasible for the zero-shot setting.Due to current restriction on applying Azure OpenAI services to PHI, we only test this using a de-id example adapted from the public TCGA dataset. The results can be found in Figures 10, 11 and 12. Remarkably, GPT-4 provides a cogent narrative of its matching decision, supported by detailed analysis of individual criteria. It is difficult to draw a generalizable conclusion from such an anecdotal example, but in combination with other experimental results, it certainly illustrates the potential in harnessing the general cognitive capabilities of large language models to scale clinical trial matching.\n\n[8] Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology Given the structured trial information, we can simulate an end-to-end evaluation by triangulating it with structured patient information via a common matching component to determine end-to-end match results. We can then compare these results against a gold match dataset.\n\n[9] Two-phase schema matching in real world relational databases We propose a new approach to the problem of schema matching in relational databases that merges the hybrid and composite approach of combining multiple individual matching techniques. In particular, we propose assigning individual matchers to two categories, \"strong\" matchers that provide a priori higher quality matches, and \"weak\" matchers that may be more sensitive to the inputs and are less reliable but can still help generate some matches. Matching is correspondingly done in two phases, with strong \"matches\" being produced by strong matchers being combined using a simple voting combiner, and weak matchers providing additional evidence for attributes left unmatched (again using a voting combiner). We observe that, while many recent advances in schema matching (Madhavan et al., 2005) use composite schema matching and rely on the existence of training schemas to train combiners, in many real-world situations it is not feasible to employ learning techniques because of the unavailability of training data (i.e., schemas or instance data.) We hypothesize that \"weak\" matchers can often hurt overall accuracy if used in a \"single-phase\" composite matcher that does not employ learning techniques. We implement our two-stage approach in the ASED system and evaluate it using real life schemas. The experiments validate our hypothesis regarding the negative effect of \"weak\" matchers and also show ASID performs comparably to state of the art systems while requiring no training schemas. We also demonstrate the benefits of a simple documentation-based matcher. Our experimental data included schemas ranging from 20 to 120 attributes. Note that schemas with 120 attributes are as large or larger than other published evaluations of relational schema matching.\n\n"}
{"case_id": "be5c0337461175e55f2a8fa9bcce5732", "answer_text": "\n\nLeading approaches for anomaly detection in process mining can be broadly categorized into several types, including feature-based, rule-based, clustering-based, classification-based, and online event stream-based methods [12]. These approaches leverage various methodologies to identify anomalous behaviors in process executions, addressing challenges unique to business processes [7].\n\n### 1. Feature-Based Approaches [0] \n\nFeature-based approaches typically involve transforming the process data into feature vectors and treating anomalies as point-cloud outliers in this feature space. For example, node/edge-level anomaly detection often flattens the graph into node vectors, effectively defining anomalous nodes based on their distance from other nodes in the feature space [0][4]. Similarly, subgraph-level anomaly detection might define anomalies as those subgraphs exhibiting significantly different structural or attribute patterns compared to the rest of the graph [5]. These methods rely heavily on appropriately designed features to capture the anomalous behavior. A practical application of this approach can be seen in intrusion detection systems, where unusual patterns in process logs are flagged as potential security breaches.\n\nRule-based anomaly detection techniques strive to learn and identify rules that represent normal process execution behavior. Unseen behavior can subsequently be compared to these rules and if the rules are not matching then the behavior is considered as anomalous. In addition, reversed approaches are also conceivable. Hence, known anomalous behavior is utilized to construct rules that match only to anomalous but not to non-anomalous process behavior [2]. \n\nComparable to process mining, rule mining also applies a two-pronged approach. Hence, first, existing rule mining techniques, such as, association rule mining or decision trees are applied. Subsequently, classic anomaly detection work typically assigns an anomaly score to each rule (e.g., to determine how important a rule is, i.e., not all rules must match) [6].\n\nMost process anomaly detection approaches were found to apply a stricter approach so that even minimal violations to the applied rules classify business process behavior as anomalous. Several techniques have been proposed for process anomaly detection rule mining. For example, Support Vector Machine techniques are applied in to mine rules which must be strictly met by a processes to not be [6].\n\nThe main advantage of the proposed rule mining approaches is that they generate a, probably small, set of rules that likely enables differentiating anomalous from non-anomalous process behavior [2]. However, the strictness enforced by the identified approaches likely increases the chance for incorrect analysis results in the case of flexible or evolving process definitions. Moreover, a substantial part of the identified rule-based approaches is heavily dependent on manual interventions and expert knowledge. Hence, experts must mark whole execution traces as normal or anomalous (i.e., to generate labeled training data) and review generated rules/combination of rules to achieve good anomaly detection performance [2]. \n\nRule-based approaches involve learning rules that represent the normal execution behavior of business processes. Any deviation from these rules is considered anomalous. This methodology often requires significant manual intervention and expert knowledge to generate and validate the rules [10]. Rule mining techniques such as association rule mining or decision trees are commonly used to extract these rules [6][12]. For instance, the use of Support Vector Machines (SVM) in rule mining has shown effectiveness in strictly defining rules for normal process behavior, thereby identifying anomalies efficiently [6]. The main advantage of rule-based approaches is their interpretability, as they can output the specific rules that motivated an anomaly decision, helping users understand the detected anomalies. However, the strictness enforced by these approaches can increase the chances of incorrect analysis results, especially in flexible or evolving process definitions [2]. Moreover, these approaches are often heavily dependent on manual interventions and expert knowledge to generate labeled training data and review generated rules [6].\n\n3. Clustering-Based Approaches\n\nClustering-based techniques, such as nearest neighbor, were found to be the most commonly used process anomaly detection work [2]. These work assumes that normal process behavior \"occurs\" in dense neighborhoods [2].\n\nClustering-based techniques group similar process behaviors together and identify anomalies as those behaviors that do not fit well into any cluster [2][7]. These methods are particularly useful for detecting collective anomalies, where an entire sequence or group of events is anomalous rather than individual points. For example, density-based clustering can be employed to detect outliers in streaming event data, providing real-time detection capabilities [7]. Clustering methods such as nearest neighbor or density-based clustering (e.g., DBSCAN) are common in this category [2]. A notable practical application is in real-time business process monitoring, where clustering approaches have been used to identify and segregate anomalous transaction sequences effectively, achieving high precision in anomaly detection [7]. However, the given support for perspectives other than control flow can be overly simplistic, and current clustering-based approaches mainly support point anomaly detection rather than collective anomalies [2].\n\n4. Classification-Based Approaches[^12].\n\n[^12]: Title: Anomaly Detection in Business Process Runtime Behavior - Challenges and Limitations\nText: publications. A. Classification based techniques Classification based techniques typically generate/learn a classifier (e.g., a reference process model) based on given logged process execution behavior. Hereby, given process behavior is utilized as labeled -normal -training data. Hence, a common assumption of these approaches is that the classifier generation data does not contain anomalies. By comparing executions to the generated classifier they can be classified as normal or anomalous (e.g., an anomaly could be process behavior that cannot be mapped on a generated reference process based classifier). Hence, classification based techniques are applied under the assumption that a classifier can be learned from given behavior that enables to distinguish normal and anomalous process executions. In the following we discuss a number of techniques that are applied to generate classifier for business process anomaly detection purposes. 1) Process Mining based: Process mining was found to be a common classifier generation approach. The typical process mining focused anomaly detection approach operates in two steps. First, one of many available process mining algorithms is applied (e.g., the mining approach presented in) to generate a reference process model that normal process behavior should conform to (i.e., training phase). Secondly, during the testing phase process behavior is compared with the just generated reference model, i.e., by mapping the behavior on the model. Several variants of this approach have been proposed which, for example, generate a model from a different amount of traces (e.g., the most frequent traces or 50% of all traces, cf.,,) while assuming that normal behavior can\n\n\nClassification-based techniques typically generate a classifier from logged process execution behavior, which is then used to categorize new behaviors as normal or anomalous. Process mining methods are often utilized to generate a reference process model that normal behaviors should conform to. Any behavior that deviates from this model is flagged as anomalous [9]. These methods can be supervised, relying on labeled training data to train the classifier, or unsupervised, generating the reference model directly from the event log [12]. For instance, in advanced manufacturing, classification-based methodologies have been pivotal in detecting deviations from standard operating procedures, thereby preventing defects and ensuring quality control [9]. However, the performance of these techniques can be limited by the availability of labeled training data, and they may struggle with the data imbalance issue, where data collected under abnormal conditions are significantly less compared to normal conditions [8].\n\nOnline Event Stream-Based Approaches focus on detecting event-level anomalies in real-time, which is necessary to identify potential anomalies promptly and take immediate countermeasures [1].\n\nTo address the need for real-time anomaly detection, online event stream-based approaches have been developed. These methods continuously monitor the stream of events and detect anomalies as they occur, enabling immediate countermeasures [1][11]. For example, next-activity prediction methods use machine learning models to predict the next event in a sequence and flag unlikely events as anomalies [1]. This approach can leverage various algorithms, including Random Forest, XGBoost, and LSTM, to improve detection accuracy in online settings [11]. A practical implementation of this approach is seen in financial fraud detection systems, where real-time anomaly detection models help identify and block fraudulent transactions as they happen [8]. \n\nTo handle challenges like incomplete cases and concept drift, these methods often incorporate advanced techniques. For instance, machine learning models used for next-activity prediction can be continuously updated as new data arrives, helping to maintain their accuracy over time. Additionally, strategies such as windowing and incremental learning can be employed to manage concept drift, ensuring the model adapts to changes in the underlying data distribution. Handling incomplete cases is another challenge, where methods like imputation, data augmentation, or using probabilistic models can help in making robust predictions despite missing information. By incorporating these techniques, online event stream-based approaches can provide timely and accurate anomaly detection in dynamic and evolving environments [11].\n\nEmerging Trends and Future Directions\n\nIn recent years, several emerging trends and future directions have gained attention in the field of anomaly detection in process mining [1]. \n\n1. **Integration of Advanced Machine Learning Techniques**: The use of advanced machine learning and deep learning models such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) is becoming more prevalent. These models are capable of capturing complex patterns in the data and can be particularly effective in detecting subtle and sophisticated anomalies that traditional methods might miss [8].\n\n2. **Multimodal Anomaly Detection**: There is a growing interest in incorporating multiple data sources and perspectives (e.g., temporal, resource, and control-flow perspectives) into the anomaly detection process. This multimodal approach can provide a more comprehensive view of the process and improve the robustness of anomaly detection methods [10].\n\n3. **Explainable AI**: As the complexity of anomaly detection models increases, so does the need for interpretability. Developing methods that not only detect anomalies but also provide clear explanations for their decisions is crucial for gaining user trust and facilitating the adoption of these technologies in practice [10].\n\n4. **Real-Time and Incremental Learning**: With the increasing availability of real-time process data, there is a growing emphasis on developing anomaly detection methods that can operate in online settings. Incremental learning techniques that continuously update the models as new data arrives are becoming more important to handle the dynamic nature of business processes [1][7].\n\n5. **Hybrid Approaches**: Combining multiple anomaly detection techniques (e.g., rule-based and machine learning-based methods) can leverage the strengths of different approaches and offer improved detection performance. Hybrid models can provide a balance between interpretability and accuracy, addressing the limitations of individual methods [12].\n\nComparison and Challenges\n\nEach of these approaches has its strengths and weaknesses. Feature-based methods are versatile and can be applied to a wide range of anomaly detection problems but may require thorough feature engineering and domain knowledge to design effective features [0]. Rule-based methods are interpretable and can provide clear explanations for detected anomalies. However, they often require extensive manual effort and expert knowledge and may struggle with flexible or evolving processes due to their rigid nature [2]. Clustering-based methods are effective at detecting collective anomalies but may not perform well with subtle, individual anomalies, and their performance can be highly dependent on the choice of clustering parameters [7]. Classification-based techniques benefit from the robustness of supervised learning but require labeled training data, which can be scarce or imbalanced, potentially leading to biased models [12]. Online event stream-based approaches excel in real-time detection and can handle dynamic changes in process behavior [11]. However, they face challenges such as handling incomplete cases, concept drift, and the computational overhead of continuously processing streaming data [1]. \n\n In conclusion, the choice of anomaly detection approach in process mining depends on the specific requirements of the task, including the type of anomalies being targeted (point, contextual, or collective) [0], the availability of labeled data [2], the need for real-time detection [1], and the importance of interpretability [12]. Practical implementations in various domains, such as security [14], finance, and manufacturing [8], highlight the effectiveness and applicability of these approaches in real-world scenarios. As the field continues to evolve, the integration of advanced machine learning techniques [7], multimodal data, explainable AI, real-time learning [1], and hybrid approaches will likely drive future advancements in anomaly detection for process mining. \n\n\nReferences:\n[0] Anomaly Mining: Past, Present and Future Finally, graph-level anomaly detection aims to identify unusual graphs within a (unordered) set or a (often temporal) sequence of graphs.Most dominant category of approaches is feature/similarity-based, which either“flatten” the entire graph into a single feature vector, or quantify pairwise similarity between graphs through an appropriately designed graph kernel Manzoor et al. (2016). These effectively treat anomalous graphs as point-cloud outliers in an explicit or implicit feature space.Subgraph-based approaches specifically define anomalous graphs as those with (the (dis)appearance of) dense subgraphs Eswaran et al. (2018)or with (the emergence of) “hotspots” Hooi et al. (2018), depending on whether anomalies are sought within a static set of graphs or time-evolving series of graphs.While the former is defined solely topologically based on edge density, the latter definition is driven by node-level signals (i.e. continuous attributes) where hotspots are defined as those groups of close-by nodes in the graph whose signal values are or have become abnormal.Another classic type of approaches is compression-based, which define anomalous graphs as those that cannot be compressed efficiently based on a data-driven graph encoding mechanism Noble and Cook (2003); Nguyen et al. (2020). One such mechanism is graph cover by motifs (or graphlets, substructures), wherein graphs that consist of frequent motifs can be encoded using relatively fewer bits, which is considered as the anomaly score.Note that motif-based encoding can be seen as a subgraph-based approach, discussed previously. The key distinction is that subgraph-based approaches define anomalies in terms of specific subgraphs (e.g. hotspots), whereas compression-based approaches quantify anomaliesimplicitly in a data-driven fashion; a graph is deemed more anomalous the less it exhibits frequent motifs (here, rare motifs can be any complement, rather than specifically defined).\n\n[1] The Analysis of Online Event Streams: Predicting the Next Activity for Anomaly Detection The Analysis of Online Event Streams: Predicting the Next Activity for Anomaly Detection Anomaly detection in process mining focuses on identifying anomalous cases or events in process executions. The resulting diagnostics are used to provide measures to prevent fraudulent behavior, as well as to derive recommendations for improving process compliance and security. Most existing techniques focus on detecting anomalous cases in an offline setting. However, to identify potential anomalies in a timely manner and take immediate countermeasures, it is necessary to detect event-level anomalies online, in real-time. In this paper, we propose to tackle the online event anomaly detection problem using next-activity prediction methods. More specifically, we investigate the use of both ML models (such as RF and XGBoost) and deep models (such as LSTM) to predict the probabilities of next-activities and consider the events predicted unlikely as anomalies. We compare these predictive anomaly detection methods to four classical unsupervised anomaly detection approaches (such as Isolation forest and LOF) in the online setting. Our evaluation shows that the proposed method using ML models tends to outperform the one using a deep model, while both methods outperform the classical unsupervised approaches in detecting anomalous events. Introduction Information systems, empowered by blockchain and IoT systems, allow an enormous amount of event data to be generated and logged in real-time. The data analytic techniques, such as process mining, are developed to manage the big volume of recorded real-time data. Process mining is a technique to identify and acknowledge the recorded events and gain insights to improve process\n\n[2] Anomaly Detection in Business Process Runtime Behavior - Challenges and Limitations considered as anomalous. A different approach is applied by the work in-. These work focuses more on detecting rules which, when met, identify anomalous behavior. For this, explorative process mining is applied by experts to generate a labeled training set. The approach in includes ontologies to gain additional knowledge about the processes under analysis. The main advantage of the proposed rule mining approaches is that they generate a, probably small, set of rules that likely enables to differentiate anomalous from non-anomalous process behavior. However, the strictness enforced by the identified approaches likely increases the change for incorrect analysis results in the case of flexible or evolving process definitions. Moreover, a substantial part of the identified rule based approaches is heavily dependent on manual interventions and expert knowledge. Hence, experts must mark whole execution traces as normal or anomalous (i.e., to generate labeled training data) and review generated rules/combination of rules to achieve a good anomaly detection performance. Some of identified rule mining approaches, e.g., the work in, were found to support contextual and point anomalies along with multiple perspectives. However, the given support for perspectives other than the control flow left the impression of being overly simplistic (e.g., temporal aspects were supported but only by a manually generated global rule that each activity duration must not exceed 15 minutes). 3) Clustering based: Clustering based techniques, such as, nearest neighbor, were found to be the most commonly used process anomaly detection work. These work assumes that normal process behavior \"occurs\" in dense neighborhoods.\n\n[3] Anomaly Mining: Past, Present and Future Node/edge-level anomaly detection aims to identify rare or unusual nodes in a single input graph.Feature-based approaches Akoglu et al. (2010) “flatten” the graph into individual node vectors, effectively defining anomalous nodes as point-cloud outliers in this feature space.Proximity-based approaches define anomalies in relation to others, where the assumption is that anomalous nodes are associated with/close-by to other anomalous nodes. This is typically applied to fraud detection problems, where fraudulent users/accounts/etc. can be found nearby in the graph Rayana and Akoglu (2015). Note that in this case, the definition is motivated by the application.Another category of approaches is community-based, which define anomalies as “misfits” with respect to the community structure. Those could be anomalous (bridge) edges between communities Tong and Lin (2011); Aggarwal et al. (2011), or community outliers that are within well-structured communities and yet, do not exhibit similar node attributes to others in the community Gao et al. (2010); Perozzi et al. (2014) (akin to contextual outliers in point-cloud OD).Note that in the latter case, the definition is informed by the availability of node attributes.\n\n[4] Anomaly Mining: Past, Present and Future Subgraph-level anomaly detection aims to identify unusual groups of nodes in a single input graph. These are akin to collective outliers in point-cloud OD.Density-based approaches define subgraph anomalies as those with significantly higher density than the overall graph Hooi et al. (2016). Provided edge timestamps, definition is also extended to those with high-density and also temporally-synchronized edges Beutel et al. (2013).Community-based approaches leverage node attributes to define anomalous subgraphs as those communities with poor structural connectivity and/or incoherent attribute values Perozzi andAkoglu (2016).\n\n[5] Anomaly Detection in Business Process Runtime Behavior - Challenges and Limitations affects the listed anomaly detection work, i.e., non control flow related process perspectives are currently not observed for anomalies by these approaches -which limits their anomaly detection capabilities. In addition the listed approaches only support point anomaly detection. We assume this as a frequently occurring substantial limitation which was not only observed for these work but also for most of the anomaly detection work discussed in the following. 2) Rule Mining based: Rule based anomaly detection techniques strive to learn and identify rules that represent normal process execution behavior. Unseen behavior can subsequently be compared to these rules and if the rules are not matching then the behavior is considered as anomalous. In addition reversed approaches are also conceivable. Hence, known anomalous behavior is utilized to construct rules that match only to anomalous but not to non-anomalous process behavior. Comparable to process mining, rule mining also applies a two pronged approach. Hence, first, existing rule mining techniques, such as, association rule mining or decision trees are applied. Subsequently, classic anomaly detection work typically assigns an anomaly score to each rule (e.g., to determine how important a rule is, i.e., not all rules must match). Most process anomaly detection approaches were found to apply a stricter approach so that even minimal violations to the applied rules classify business process behavior as anomalous. Several techniques have been proposed for process anomaly detection rule mining. For example, Support Vector Machine techniques are applied in to mine rules which must be strictly met by a processes to not be\n\n[6] Anomaly Detection in Business Process based on Data Stream Mining Identifying fraudulent or anomalous business procedures is today a key challenge for organisations of any dimension. Nevertheless, the continuous nature of business conveys to the continuous acquisition of data in support of business process monitoring. In light of this, we propose a method for online anomaly detection in business processes. From a stream of events, our approach extract cases descriptors and applies a density-based clustering technique to detect outliers. We applied our method to a real-life dataset, and we used streaming clustering measures for evaluating performances. In particular, we obtained Cluster Mapping Measure of 95.3% and Homogeneity of 98.1% discovering anomalous cases in real-time.\n\n[7] Augmented Time Regularized Generative Adversarial Network (ATR-GAN) for Data Augmentation in Online Process Anomaly Detection Supervised machine learning techniques, such as classification models, have been widely applied to online process anomaly detection in advanced manufacturing. However, since abnormal process states rarely occur in regular manufacturing settings, the data collected for model training may be highly imbalanced, which may result in significant training bias for supervised learning and, thus, further deteriorate the anomaly detection accuracy. To reduce the training bias, a natural idea is to incorporate data augmentation techniques to generate effective artificial sample data for the abnormal process states. However, most of the existing data augmentation methods do not effectively consider the temporal orders of the sensor signals, and they also usually require large amounts of actual samples to ensure satisfactory augmentation performance. To address these limitations, this article developed a novel data-driven methodology termed augmented time regularized generative adversarial network (ATR-GAN). By incorporating a proposed augmented generator, ATR-GAN is capable of generating more effective artificial samples for training supervised learning models. The novelty of this augmented generator in the proposed methodology can be summarized into three aspects: 1) an augmented filter layer is introduced in the augmented generator to identify the high-quality artificial samples; 2) in the augmented filter layer, a new distance metric termed time-regularized Hausdorff (TRH) distance is developed to accurately measure the similarity between actual samples and the generated artificial samples; and 3) batching techniques are also employed in the proposed augmented generator to further increase the diversity of the artificial data and fully utilize the relatively limited training data. In addition, the effectiveness of the proposed ATR-GAN is also validated by both numerical simulation and a real-world case study in additive manufacturing. Note to Practitioners—Online process anomaly detection currently plays a significant role in advanced manufacturing since unexpected anomalies may damage product quality and even result in catastrophic loss. In practice, processes are mostly under normal conditions, and anomalies rarely occur. Therefore, the data collected under abnormal conditions are very limited compared to normal conditions, which causes the data imbalanced issue, leading to deterioration in detection accuracy. Many existing data augmentation methods, such as generative adversarial network (GAN), cannot synthesize diversified high-quality artificial samples only using relatively limited actual samples. There is an urgent need in developing an effective data augmentation methodology to address the data imbalanced issue in process anomaly detection. This article developed a novel approach called augmented time regularized GAN (ATR-GAN) for online sensor data augmentation. With this new approach, the applications in additive manufacturing demonstrate that the performance of data augmentation can be improved effectively, and thereafter, the anomaly detection accuracy is also increased significantly. Moreover, the developed methodology is inherently integrated into a generic framework. Thus, it can be further transformed for applications in many other areas that need data augmentation.\n\n[8] Analyzing business process anomalies using autoencoders these systems are storing can be used to extract the events executed in the process, and thereby create so called event log files. Event logs are comprised of activities (and other miscellaneous information) that occurred during the execution of the process. These event logs enable process analysts to explore the underlying process. In other words, the event log consists of footprints of the process. Consequently, it is possible to recreate the process model by evaluating its event log. This is known as process model discovery and is one of the main ideas in the domain of process mining. Process mining provides methodologies to detect anomalies in the execution of a process; e.g., by discovering the as-is process model from the event log using discovery algorithms and then comparing the discovered model to a reference model. This is known as conformance checking. Another way of detecting anomalies is to compare the event log to the reference model. However, this approach requires the existence of such a reference model. If no reference model is available, process mining relies on discovering a reference model from the event log itself,3. These methods make use of a threshold to deal with infrequent behavior in the log, so that the discovered model is a good representation of the normal behavior of the process. Hence, this model can be used as a reference model for the conformance check. A key assumption in anomaly detection is that the anomalous executions occur less frequent than normal executions. This skewed distribution can\n\n[9] Anomaly Detection in Business Process Runtime Behavior - Challenges and Limitations promising approaches in this area are the identified rule mining techniques. This is because those can output the rules that motivated the normal or anomalous decision. c) Looking beyond point anomalies: It was found that most existing process anomaly detection work focuses on rather simple anomaly types, i.e., point anomalies. In addition a very small amount of all found publications propose approaches that take contextual aspects into account -these approaches were found to mainly focus on the temporal and resource perspective, cf. Fig. 4. Even publications that cover more than one perspective mostly only bundle two of the multiple available process perspectives in a rather \"closed\" way that can, based on our impression, hardly be extended to take additional perspectives into account (a majority of the found work even only supports the control flow perspective). This results in gaps and unmonitored areas which could be exploited by attackers to hide their intentions. To address this limitation approaches must be constructed that can incorporate multiple perspectives at once and that can be dynamically extended to take new perspectives and developments into account. As a quick solution it could be tried to aggregate the anomaly scores generated by multiple point anomaly detection approaches. Nevertheless this still leaves collective anomalies behind which are, based on our findings, currently hardly covered by almost all process anomaly detection work. d) Interoperability and Parallelism: Processes behavior takes place in complex parallel, inter-, and innerorganizational scenarios and environments. However, it was found that only non-parallelized inner-organizational behavior is taken into consideration by the\n\n[10] The Analysis of Online Event Streams: Predicting the Next Activity for Anomaly Detection execution. Recently, process mining has focused on process management and analysis on online settings including process discovery, conformance checking, and process monitoring techniques. Anomaly detection in process mining aims to detect anomalous behavior in event data,12. Such techniques have been used to identify potential fraudulent behavior to prevent compliance violations. In addition, they are also used to detect log quality issues to improve data quality. Most existing unsupervised anomaly detection techniques focus on the case-level or are situated in offline settings, i.e., they take as input of a batch event log that contains a set of completed cases. In practice, detecting anomalies in online streaming settings has many advantages, such as being able to take action and timely counter measures. Timely detection also helps dealing with concept drift. At the same time, online anomaly detection also faces many challenges. Unlike an offline setting, which only deals with completed cases, online detection should be able to continuously handle incomplete, ongoing cases. Moreover, online detection should pinpoint anomalies at event-level to allow timely, concrete reactions. For example, if credit card fraud is established, the techniques should immediately detect and pinpoint which purchase events are suspicious. In this paper, we propose to tackle unsupervised anomalous event detection by predicting which activity is next, assuming an ongoing case. More specifically, we first learn a predictive model to predict next activities by preprocessing the completed cases into feature vectors and using machine learning-based classification algorithms (such as Random Forest, XGBoost, LSTM). When a\n\n[11] Anomaly Detection in Business Process Runtime Behavior - Challenges and Limitations publications. A. Classification based techniques Classification based techniques typically generate/learn a classifier (e.g., a reference process model) based on given logged process execution behavior. Hereby, given process behavior is utilized as labeled -normal -training data. Hence, a common assumption of these approaches is that the classifier generation data does not contain anomalies. By comparing executions to the generated classifier they can be classified as normal or anomalous (e.g., an anomaly could be process behavior that cannot be mapped on a generated reference process based classifier). Hence, classification based techniques are applied under the assumption that a classifier can be learned from given behavior that enables to distinguish normal and anomalous process executions. In the following we discuss a number of techniques that are applied to generate classifier for business process anomaly detection purposes. 1) Process Mining based: Process mining was found to be a common classifier generation approach. The typical process mining focused anomaly detection approach operates in two steps. First, one of many available process mining algorithms is applied (e.g., the mining approach presented in) to generate a reference process model that normal process behavior should conform to (i.e., training phase). Secondly, during the testing phase process behavior is compared with the just generated reference model, i.e., by mapping the behavior on the model. Several variants of this approach have been proposed which, for example, generate a model from a different amount of traces (e.g., the most frequent traces or 50% of all traces, cf.,,) while assuming that normal behavior can\n\n[12] Anomaly intrusion detection based upon data mining techniques and fuzzy logic Intrusion detection systems (IDSs) attempt to identify attacks by comparing new data to predefined signatures known to be malicious (misuse IDSs) or to a model of normal behavior (anomaly-based IDSs). Anomaly intrusion detection approaches have the advantage of detecting previously unknown or new attacks, but suffer from the possible high false alarms due to the problem of behavior drifting and the difficulty of building an adaptive model. In this paper, we propose a model based on the data mining technique - naïve Bayes classification to classify an input event (system call sequences generated from privileged processes) as “normal” or “anomalous” to detect system anomalous behavior. The independent frequency of each system call from a process collected under the normal conditions is the basis for the classifier. The ratio of the probability of a sequence from a process and the probability NOT from the process serves as the input of a fuzzy system for the classification. Experimental results in a data set consisting of both normal and intrusion traces show that the model can successfully detect most of intrusion traces with a very low false alarm rate.\n\n"}
{"case_id": "15dec998cf77887f870ebf9a55bb7e89", "answer_text": "\n\nLanguage models (LMs) have demonstrated considerable effectiveness in automatically generating textual descriptions of scientific concepts [0], but their performance varies significantly based on the complexity of the task, the domain of the concepts, and the specific models and methods employed [10]. \n\nEffectiveness of Language Models[3]\n\nOne of the foremost applications of LMs in this context is the ACCoRD system, which produces distinct, diverse descriptions of scientific concepts by leveraging the myriad ways a concept is mentioned across scientific literature [0][1]. The ACCoRD system uses an elaborate extraction pipeline to gather relevant sentences from a large corpus of scientific papers. This step involves identifying all sentences that describe a concept in terms of another reference concept. Subsequently, the system employs GPT-3 in a few-shot setting to generate succinct and self-contained descriptions. A critical part of ACCoRD’s approach is its method of selecting a smaller, yet diverse subset of these generated descriptions to ensure that the variety of the concept's usages is captured. This allows the system to cover a wide range of relationship types and reference concepts, thereby supporting the understanding of concepts by providing multiple distinct perspectives [1]. \n\nTo validate the efficacy of this approach, a user study was conducted where participants were presented with descriptions produced by the ACCoRD system. The study demonstrated that users significantly preferred the diverse, generated descriptions over single, \"best\" descriptions. Specifically, users found that having access to multiple descriptions improved their understanding of complex scientific concepts, as it provided varied contexts and perspectives [0][1].\n\nAnother key area of effectiveness for LMs in generating scientific descriptions is in cross-modal generation tasks, such as generating molecular data based on text descriptions or vice versa [2]. For example, models like Text2Mol and ProteinDT can generate accurate molecular and protein-related information from textual descriptions and vice versa, demonstrating the bidirectional capability of cross-modal generation models [2]. These tasks benefit from LMs’ ability to capture and synthesize complex relationships between different data types, significantly enhancing the utility of scientific descriptions in practical applications.\n\nThe study of biological and chemical domains has also highlighted the efficacy of LMs in generating coherent and fluent descriptions [3]. Models such as BioGPT, which is pretrained extensively on biomedical literature, show remarkable performance in generating contextually accurate descriptions and performing tasks like relation extraction and question answering [3]. These models help bridge the gap between complex scientific data and natural language, facilitating better comprehension and utility of scientific information.\n\nApplications in Various Scientific Domains [6]. \n\nReal-world applications are abundant across various scientific domains. For instance, in drug discovery, models like ChatDrug leverage conversational LMs to perform drug editing tasks by modifying small molecules, peptides, and proteins to achieve desired properties [6]. Another example is BioTranslator, which integrates diverse modalities for sophisticated biological data analysis, showcasing its utility in accelerating scientific research [6]. Additionally, chemical and material sciences benefit from systems like DARWIN-MDP, which generates instructions for material and device prediction tasks, aiding in the development of new materials [6].\n\nWhen comparing these models, it is evident that domain-specific models like BioGPT excel in tasks requiring deep contextual understanding and fluency in the specific domain language [3]. Conversely, general models like GPT-3, utilized in the ACCoRD system, exhibit versatility by generating diverse descriptions across various scientific fields [1]. This indicates that while specialized models bring the advantage of domain expertise, more generalized models benefit from a broader applicability, albeit sometimes at the cost of depth in specific fields. \n\nAdditionally, LMs have been used to generate scientific definitions with controllable complexity, addressing the problem of adapting descriptions to readers with varying levels of background knowledge [10]. The use of sequence-to-sequence models for definition generation has proven successful, especially when combined with reranking techniques to control the complexity and fluency of the generated definitions [10]. This ability to tailor descriptions to the audience enhances the accessibility and understandability of scientific texts.\n\nMore broadly, numerous other systems have significantly impacted the field. For instance, SciGen provides a dataset for reasoning-aware data-to-text generation from scientific tables, which emphasizes the need for arithmetic reasoning capabilities [11]. Despite the challenges, this dataset has pushed the development of LMs to better handle complex reasoning tasks. Similarly, comprehensive multimodal models like Galactica and BioTranslator showcase how integrating various scientific modalities with natural language can accelerate scientific research and enhance data comprehension [6].\n\n### Challenges Faced by Language Models\n\nHowever, current systems assume a single \"best\" description per concept, which fails to account for the many potentially useful ways a concept can be described [0]. Readers of scientific papers often encounter unfamiliar concepts, which impedes their understanding [0]. While readers may turn to external encyclopedic resources like Wikipedia, these contain descriptions for only a small fraction of scientific concepts [0]. Natural language processing stands to help address these issues by automatically defining unfamiliar terms [10]. Most definition generation has focused on common, general-usage words in English [10]. In this paper, we introduce a self-supervised end-to-end system for automatically discovering scientific concepts from both semi-structured independent knowledge sources and unstructured academic documents [14]. \n\nHowever, while LMs are effective, they face challenges. Generating descriptions that require arithmetic reasoning, such as those derived from scientific tables, remains a difficult task [11]. Models often struggle with accurately interpreting and synthesizing numerical values and complex input structures. For example, although the dataset SciGen has prompted improvements, current state-of-the-art models still find it challenging to perform reasoning tasks effectively [11]. Additionally, models can generate descriptions that lack precision or include hallucinations, which compromise the reliability of the output [11]. Furthermore, domain-specific nuances and terminologies can be challenging for generalized models, leading to superficial understanding in highly specialized fields.\n\nFuture Developments and Improvements [0].\n\nLooking forward, there are several areas where LMs can be further developed and improved for scientific description generation [0][3][10]. \n\n**Enhanced Contextual Understanding**: Future models could incorporate more sophisticated contextual awareness mechanisms to provide even more accurate and relevant descriptions. This could include leveraging larger and more diverse datasets, as well as integrating more advanced techniques for context capture [0]. \n  \n2. **Improved Multimodal Integration**: Continued development in cross-modal and multimodal learning could enhance the ability of LMs to handle complex scientific data. This involves refining techniques to better synthesize and reconcile information across different modalities, such as text, images, molecular structures, and statistical data [2]. \n\n3. **Advanced Reasoning Capabilities**: Improving the reasoning abilities of LMs, particularly in handling and interpreting numerical and logical data, is crucial. This may involve developing new architectures or training methodologies that emphasize arithmetic reasoning and logical inference [11]. \n\n4. **User-Centric Adaptations**: Future systems could become more adept at tailoring output to specific user needs by integrating user feedback and interaction data. This could involve dynamic adjustment of description complexity and style based on user preferences and comprehension levels [10].\n\n5. **Reduction of Hallucinations**: Efforts to reduce hallucinations in generated content include refining loss functions during training [11], incorporating more robust validation mechanisms, and improving model interpretability. \n\nConclusion: [0]\n\nIn summary, LMs are quite effective at generating textual descriptions of scientific concepts, particularly when leveraging diverse datasets and advanced model architectures. Systems like ACCoRD demonstrate how LMs can provide multiple, varied descriptions that aid in understanding scientific concepts [0]. Meanwhile, cross-modal generation models and domain-specific LMs like BioGPT showcase the potential for accurately synthesizing complex scientific information [3]. Despite these successes, there remain challenges, particularly in tasks requiring intricate reasoning or highly specialized knowledge, suggesting ongoing areas for research and development. The real-world applications in various scientific domains underscore the utility and transformative potential of these LMs in practical scenarios. \n\n\nReferences:\n[0] ACCoRD: A Multi-Document Approach to Generating Diverse Descriptions of Scientific Concepts ACCoRD: A Multi-Document Approach to Generating Diverse Descriptions of Scientific Concepts Systems that can automatically define unfamiliar terms hold the promise of improving the accessibility of scientific texts, especially for readers who may lack prerequisite background knowledge. However, current systems assume a single\"best\"description per concept, which fails to account for the many potentially useful ways a concept can be described. We present ACCoRD, an end-to-end system tackling the novel task of generating sets of descriptions of scientific concepts. Our system takes advantage of the myriad ways a concept is mentioned across the scientific literature to produce distinct, diverse descriptions of target scientific concepts in terms of different reference concepts. To support research on the task, we release an expert-annotated resource, the ACCoRD corpus, which includes 1,275 labeled contexts and 1,787 hand-authored concept descriptions. We conduct a user study demonstrating that (1) users prefer descriptions produced by our end-to-end system, and (2) users prefer multiple descriptions to a single\"best\"description. Introduction Readers of scientific papers often encounter unfamiliar concepts, which impedes their understanding (Portenoy et al., 2022). This is because papers assume a priori knowledge, and often lack definitions for the scientific terms that they use. While readers may turn to external encyclopedic resources like Wikipedia, these contain descriptions for only a small fraction of scientific concepts (King et al., 2020), which has motivated the development of systems that automatically extract or generate descriptions for scientific concepts. Unfortunately, current systems only surface a single \"best\" result for all users, which is often extracted from a single input document\n\n[1] ACCoRD: A Multi-Document Approach to Generating Diverse Descriptions of Scientific Concepts as a tool in scientific discourse and education (Treagust et al., 1992;Heywood, 2002). (1) Users search for a target scientific concept from a pre-defined list and are shown cards for top reference concepts used to describe the target concept in terms of a particular relation. (2) Users click to expand the cards to see the extracted snippet (context) that produced the generated concept description, in addition to a link to the source paper. (3) Spans of text that are shared between the extracted context and generated description are highlighted to facilitate easy comparison. Our work expands upon the notion of a description in the context of description generation systems to include analogy-like descriptions that are currently not captured by either scientific definition (Kang et al., 2020) or relation extraction (Wadden et al., 2019) systems. In this work, we present Automatic Comparison of Concepts with Relational Descriptions (ACCoRD) -an end-to-end system that tackles the novel task of producing a set of distinct descriptions for a given target concept. 1 Given text from scientific papers, our system first extracts all sentences from across the corpus that contain a description of the concept in terms of any other concept. Then, conditioned on the extractions, ACCoRD generates succinct, self-contained descriptions of the concepts' relationship using GPT-3 (Brown et al., 2020) in the few-shot setting. The system finally selects a smaller, yet diverse subset of descriptions that captures the richness of a concept's usages by including multiple relation types and reference concepts. Our contributions are: 1. We introduce Description Set Generation\n\n[2] Scientific Large Language Models: A Survey on Biological & Chemical Domains Cross-modal Generation.Cross-modal generation tasks aim to generate data in one modality based on data from another modality, which involves establishing interconnections between the different modalities. MM-Sci-LLMs are designed to process data from multiple modalities including texts, molecules, proteins, and genomes.For example, Text2Molecule refers to generating molecular information (e.g., SMILES), based on input text descriptions. Many models can accomplish this task including Text2Mol (Edwards et al., 2021), MolT5 (Edwards et al., 2022), and MOL-Instruction (Fang et al., 2023a).The bidirectional conversion capability of cross-modal generation models allows them to not only generate data in one modality based on data from another modality but also to convert data from one modality to another. For example, Molecule2Text is to produce precise and pertinent textual descriptions derived from molecular data, namely molecule captioning.Similar to the mutual generation of molecule and text, Text2Protein aims to generate protein-related information including sequences and structures based on text descriptions. A representative model is ProteinDT (Liu et al., 2023f) which introduced a text-guided protein design method. Conversely, Protein2Text converts protein sequences into detailed text descriptions to express the complex biological characteristics of proteins. Prot2Text (Abdine et al., 2023), BioMedGPT-10B (Luo et al., 2023c) and MOL-Instruction (Fang et al., 2023a) can address this task effectively.Additionally, Protein2Molecule involves the process of generating relevant molecules from protein sequences, which can be achieved by ChemBERTaLM (Uludoğan et al., 2022) and DrugGPT (Li et al., 2023a). Overall, cross-modal generation in multi-modal models plays a crucial role in bridging the gap between different modalities and enabling the creation of new and cohesive multi-modal data.\n\n[3] Scientific Large Language Models: A Survey on Biological & Chemical Domains Biological Domain.Large language models trained on extensive biological corpora, such as BERT (Devlin et al., 2019) and its variations with the encoder-only LLM architecture, have demonstrated significant potential in Natural Language Processing (NLP) tasks within biology. BioBert (Lee et al., 2019), BioMegatron (Shin et al., 2020), PubMedBERT (Gu et al., 2021), BioM-BERT (Alrowili and Shanker, 2021) and LinkBERT (Yasunaga et al., 2022) initially trained on broad corpora like Wikipedia and textbooks and then fine-tuned on specific biological NLP tasks, showcase their significant improvements in various downstream tasks such as biological terminology understanding, named entity recognition, text similarity, and relation extraction. BlueBERT (Peng et al., 2020) presented a thorough empirical study on the efficacy of multi-task learning on BERT-based models for biomedical texts. Since the encoder-only models lack the capability of textual information generation, GPT and its variants (Radford et al., 2018, 2019a; Brown et al., 2020) with a decoder-only architecture have emerged as dominant players in the field of biological NLP. BioGPT (Luo et al., 2022b), an extension of GPT-2 (Radford et al., 2019b), has been extensively pretrained on biomedical literature, showcasing remarkable performance in relation extraction and question answering. Moreover, it demonstrates the ability to generate coherent and fluent descriptions within the biomedical context. BioMedGPT-LM (Luo et al., 2023c), which is incrementally pretrained on LLaMA2 (Touvron et al., 2023a), enabling the comprehensive understanding of various biological modalities and aligning them with natural language.\n\n[4] Scientific Large Language Models: A Survey on Biological & Chemical Domains In this survey, the comprehensive multimodal models include text, proteins, molecules, genomes and other scientific languages.Their potential impact spans diverse domains, particularly offering significant support in accelerating fundamental science research.Galactica (Taylor et al., 2022b), BioTranslator (Xu et al., 2023a), ChatDrug (Liu et al., 2023c), BioMedGPT0-10B (Luo et al., 2023c), DARWIN-MDP (Xie et al., 2023), BioT5 (Pei et al., 2023) and Mol-Instructions (Fang et al., 2023a) are representative comprehensive multimodal models.Galactica (Taylor et al., 2022b) is a scientific language model based on a decoder-only architecture, trained on a massive corpus of 106 billion tokens from various scientific texts and knowledge bases. Its strength lies in its ability to efficiently process extensive scientific datasets, enhancing its potential utility in specific scientific tasks.BioTranslator (Xu et al., 2023a) leverages PubMedBERT (Gu et al., 2021) for modeling textual descriptions and fine-tunes it using ontologies across various domains, showcasing its versatility in integrating diverse modalities for sophisticated biological data analysis.ChatDrug (Liu et al., 2023c) utilizes conversational language models to perform drug editing tasks. It addresses the problem of modifying small molecules, peptides, and proteins to achieve specific properties, such as solubility or secondary structure, by leveraging the knowledge extraction and summarization capabilities of the language model.BioMedGPT-10B (Luo et al., 2023c) is an open multimodal generative pre-trained transformer, to bridge the modality gap in biomedicine by aligning different biological modalities with natural language. It addresses the challenge of effectively communicating with diverse biological modalities through free text, including molecular structures, protein sequences, pathways, and cell transcriptomics.DARWIN-MDP (Xie et al., 2023) is a variant of the DARWIN series that involves 16 FAIR datasets to generate instructions for material and device prediction tasks, such as classification, regression, and design.BioT5 (Pei et al., 2023) is a pre-training framework that leverages structured and unstructured data sources to capture the underlying relations and properties of bio-entities. It aims to address the challenges in cross-modal generation and prediction tasks in the field of computational biology.Mol-Instructions (Fang et al., 2023a) aims to address the challenge of enhancing LLMs’ performance in tasks related to molecules, proteins, and biomolecular text by providing task-specific instructions.\n\n[5] Generating Scientific Definitions with Controllable Complexity Generating Scientific Definitions with Controllable Complexity Unfamiliar terminology and complex language can present barriers to understanding science. Natural language processing stands to help address these issues by automatically defining unfamiliar terms. We introduce a new task and dataset for defining scientific terms and controlling the complexity of generated definitions as a way of adapting to a specific reader’s background knowledge. We test four definition generation methods for this new task, finding that a sequence-to-sequence approach is most successful. We then explore the version of the task in which definitions are generated at a target complexity level. We introduce a novel reranking approach and find in human evaluations that it offers superior fluency while also controlling complexity, compared to several controllable generation baselines. Introduction Unfamiliar concepts and complex language can make understanding scientific information difficult for readers (Brossard and Shanahan, 2006;Shea, 2015;Martínez and Mammola, 2021), especially because understanding such terms is highly dependent on their domain knowledge. Given the wide variation in such knowledge, providing a onesize-fits-all definition may not be sufficiently understandable for all readers. We envision a software tool designed to aid readers with varying domain knowledge by automatically defining scientific terms. Such a tool would afford readers control over generated definitions, including their complexity. This hypothetical system motivates research on automated generation of scientific definitions and generation-time control of definition complexity. Prior work in generating definitions and personalizing generations to a reader falls short of these goals. Most definition generation has focused on common, general-usage words in English (Noraset A molecule that binds to\n\n[6] SciGen: a Dataset for Reasoning-Aware Text Generation from Scientific Tables We introduce SciGen, a new challenge dataset consisting of tables from scien-tiﬁc articles and their corresponding descriptions, for the task of reasoning-aware data-to-text generation. Describing scientiﬁc tables goes beyond the surface realization of the table content and requires reasoning over table values. The unique properties of SciGen are that (1) tables mostly contain numerical values, and (2) the corresponding descriptions require arithmetic reasoning. SciGen is the ﬁrst dataset that assesses the arithmetic reasoning capabilities of generation models on complex input structures, such as tables from scientiﬁc articles, and thus it opens new avenues for future research in reasoning-aware text generation and evaluation. The core part of SciGen, including the test data, is annotated by one of the authors of the corresponding articles. Such expert annotations do not scale to large training data sizes. To tackle this, we propose a pipeline for automatically extracting high-quality table-description pairs from the LaTeX sources of scientiﬁc articles. We study the effectiveness of state-of-the-art data-to-text generation models on SciGen and evaluate the results using common generation metrics and human evaluation. Our results and analyses show that adding high-quality unsupervised training data improves the correctness and reduces the hallucination in generated descriptions, however, the ability of state-of-the-art models is still severely limited on this task. 2\n\n[7] SciConceptMiner: A system for large-scale scientific concept discovery a comprehensive manually controlled vocabulary like Medical Subject Headings(MeSH) (Lowe and Barnett, 1994) contains tens of thousands of subjects (concepts) mostly in the bio-med domain; and an automated scientific knowledge exploration system such as Microsoft Academic Graph (MAG) (Shen et al., 2018) has hundreds of thousands of topics across all academic disciplines. A topic modeling system that is scalable not only to the size of documents but also to the number of topics is imperative. Second, the result of an LDA model is a list of frequency-based terms that form a topic. It requires manual efforts to annotate such lists to generate a human-readable theme or topic name. An automatic process of identifying topic themes with authoritative names and meaningful descriptions is desired to reduce costly human interventions. In this paper, we introduce a self-supervised end-to-end system, SciConceptMiner, for automatically discovering scientific concepts from both semi-structured independent knowledge sources and unstructured academic documents. It first obtains a list of concept candidates, either from external knowledge repositories such as Wikipedia (Völkel et al., 2006;Vrandečić and Krötzsch, 2014) and Unified Medical Language System (UMLS) (Bodenreider, 2004), or directly mining concepts from a collection of academic documents. Such concept lists are large and noisy. They are in the scale of millions and dominated by invalid or duplicate terms. We then send these candidates as queries to a search engine API and leverage rich Web content to identify legitimate concepts, cluster synonyms, and discard improper terms. The search API is also used to retrieve highquality concept descriptions. One example is\n\n"}
{"case_id": "948b6cb986a5d7732722975dbed9d420", "answer_text": "\n\nEvaluating complex scientific question answering (QA) systems in natural language processing (NLP) involves various methodologies, each addressing different aspects of performance. These methods broadly include holistic measures [0], human-centered evaluation schemes [1], automatic evaluation strategies [5], and hybrid approaches. \n\nHolistic Measures[0]\nHolistic measures involve end-to-end models based on neural networks, such as transformers, to compute scores from statistical or contextualized features within the responses or references [0]. These include:\n- **Referenced Scores**: These compare the generated response with a provided gold answer by learning an alignment score between the context and response, approximating human judgments [0].\n- **Hybrid Referenced-Unreferenced Evaluation Scores**: These scores are trained without human responses by smoothing the negative samples directly from the dataset [0].\n- **Unreferenced Scores (Unsupervised)**: These utilize large pre-trained language models to extract latent representations of text units, exploiting time shifts between them [0]. Models like BertScore, BLEURT, and MaUde are examples within this category [0].\n\n\n\nHuman-Centered Evaluation Schemes (HCES) rely on a group of people (e.g., experts or specialists) to evaluate the output of a QA system based on certain guidelines and criteria, e.g., regarding adequacy, fluency, and coherence of a text. However, it is also prone to human errors caused by subjectivity and bias in opinion. Frequently, a 5-point Likert-scale or numerical ranking criteria is used as a measurement tool for HCES evaluations [1]. Conceptually, HCES can be further classified into two sub-categories: Absolute evaluation and relative evaluation [3].\n\n\nHCES are often considered the gold standard due to their holistic nature and alignment with human judgment, despite being time-consuming and prone to subjectivity [1]. These are divided into:\n- **Absolute Evaluation**: Evaluators assess the naturalness, coherence, and fluency of the output using a predefined rubric, often rated on a scale such as a 5-point Likert scale [3]. This method, while popular, is susceptible to biases and discrepancies due to evaluator subjectivity [3].\n- **Relative Evaluation**: This involves comparing multiple outputs (from different systems or model versions) to determine which performs better based on specific criteria [1]. This approach attempts to mitigate individual biases by focusing on comparative rather than absolute ratings.\n\nAutomatic Evaluation Strategies [5].\nAutomatic methods offer consistency, speed, and scalability, making them invaluable for evaluating large-scale systems [5].\n- **Intrinsic Methods**: These evaluate the quality of outputs against a predetermined ground truth reference [5]. For instance, they might ask questions like, \"Does the generated text convey the same meaning as the source text?\"\n- **Extrinsic Methods**: These focus on how the outputs affect the performance of other systems, like evaluating whether generated paraphrases improve the accuracy of a QA model [5].\n\n## Hybrid Approaches\nHybrid approaches in evaluating QA systems combine different evaluation strategies to leverage the strengths of multiple methods while mitigating their individual weaknesses [5].\nCombining human judgments with automated metrics can balance the strengths and weaknesses of both approaches [5].\n- **Majority Voting and Ensemble Methods**: By integrating outputs from multiple systems, ensemble methods like majority voting can improve overall performance and reliability of QA systems [6]. Research has shown that such combinations often outperform individual systems by leveraging the diverse strengths of each [6].\n- **Contradiction-Based Systems**: Recent advancements have employed Natural Language Inference (NLI) techniques to improve system reliability by leveraging contradictions. Contradiction-based methods check for conflicting information to increase the robustness and trustworthiness of QA systems, which is particularly useful in scientific domains where accuracy is paramount [12].\n\nEvaluating Multi-Dimensional Aspects [4]. \nScientific QA systems are evaluated on multiple dimensions, including objective (information-based), subjective, and architectural evaluations [4].\n- **Objective Evaluation**: Involves checking the correctness and relevance of the provided information [4]. \n - **Subjective Evaluation**: Focuses on user satisfaction and the perceived quality of the responses [4]. \n- **Architectural Evaluation**: Examines the underlying design of the system, its robustness, and scalability [4].\n\n### Challenges and Proposals\n\nEvery category has its own strengths and weaknesses. In general, a HCES is considered the best evaluation that gives the most trustworthy score [1]. Typically, HCES relies on a group of people (e.g., experts or specialists) to evaluate the output of a QA system based on certain guidelines and criteria, e.g., regarding adequacy, fluency, and coherence of a text [1]. However, it is also prone to human errors caused by subjectivity and bias in opinion [1]. Frequently, a 5-point Likert-scale or numerical ranking criteria is used as a measurement tool for HCES evaluations [1]. Conceptually, HCES can be further classified into two sub-categories: Absolute evaluation and relative evaluation [3].\n\nAbsolute evaluation: In this type of HCES, individuals are asked to evaluate the output of a natural language generation/inference model using a 5-point scale [3]. The evaluator assesses the output texts of a task based on predefined criteria or rubric, rating each from 1-5 or on a nominal scale [3]. For example, a statement such as ”Rate the output based on the naturalness of the language” with options as very unnatural, unnatural, neutral, natural and very natural, or with 1-5 numerical options, each responding to a similar semantic meaning [3]. The results of which will be averaged over the rubric and used as the evaluation/annotation output [3]. For example, in machine translation, the task may require an evaluation from a person who is familiar with either the target language (i.e., monolingual) or both source and target languages (i.e., bilingual) to perform the required assessment (rating) on the generated text [3]. The absolute method is often subjected to the bias of the evaluator due to the discreteness of the scale and inter-annotator disagreements [3]. \n\nTherefore, individual evaluators may not always be preferred for some tasks, e.g., in order to obtain a more accurate assessment [3]. In turn, relative evaluation is used by comparing outputs from multiple systems to a gold standard or to each other to mitigate individual biases and increase reliability [1]. \n\nHolistic measures: This category of error scores utilizes end-to-end models based on neural networks, e.g., transformers, to perform the evaluation [0]. The end-to-end models can be directly applied to statistical or contextualized features that exist within the response or reference to compute the scores [0]. Such models may use different strategies to perform the evaluation process: (i) referenced scores, which compare the generated response with a provided gold answer, by learning an alignment score between context and response to approximate human judgments [0], (ii) hybrid referenced-unreferenced evaluation score, where the score is trained without needs for human responses by smoothing the negative samples directly from the dataset [0], or (iii) unreferenced scores (i.e., learned in an unsupervised manner) that use large pre-trained language models to extract latent representations of text units (i.e., utterances or words), and take advantage of the time shifts that exist between them [0]. The MTES based on end-to-end models include a wide range of error scores such as BertScore, RUSE, RUBER, BLEURT, ADEM, MaUde [0]. \n\nEvaluating QA Systems on Multiple Dimensions: QA systems are expanding beyond information retrieval and information extraction, to become full-fledged, complex NLP applications [4]. Evaluating QA systems as complex NLP systems suggests three different dimensions for evaluation: objective or information-based evaluation; subjective evaluation; and architectural evaluation [4]. The role of ambiguity resolution in QA systems is also significant, and methods to evaluate it might benefit future systems [4]. \nDespite these comprehensive methods, several challenges persist, such as bias, scalability, and the need for domain-specific evaluations. Innovative proposals like the QASA benchmark have emerged, necessitating full-stack reasoning and rationale generation for deeper understanding in scientific QA [7]. Such benchmarks align with recent studies asserting the importance of rationale generation for performance gains in complex QA tasks [7].\n\nEvaluating QA Systems on Multiple Dimensions Question-answering systems are expanding beyond information retrieval and information extraction, to become full-fledged, complex NLP applications [4]. In this paper we discuss the evaluation of question-answering systems as complex NLP systems, and suggest three different dimensions for evaluation: objective or information-based evaluation; subjective evaluation; and architectural evaluation [4]. We also discuss the role of ambiguity resolution in QA systems, and how ambiguity resolution might be evaluated [4]. \nBelow is a comparison summarizing the evaluation methods, their pros/cons, and examples [5].\n\n\n| **Method** | **Pros** | **Cons** | **Examples** |\n\nAnalyzing the question and detecting the answer type, or finding the embeddings of most similar vectors of question and context, is one of the core tasks of QA algorithms [2]. The QA algorithms also facilitate the query formulation during the query processing stage via parsing the passing tokens with precise descriptions in order to identify what kind of answer is supposed to be returned (e.g., paragraph, description, etc) [2]. Different strategies have been proposed to deal with such requirements, such as: Regular Expressions (regex), Part-of-Speech (POS), matched n-gram features, finding the first noun phrase after the question headword ‘wh*’, and Named Entity Recognition (NER) to identify the given tokens such as human, place, location, entity [2]. The question types classification with different answer types have been investigated in many studies such 125; 44; 19; 48; 49; 50; 51 [2]. The focus detection is meant to capture the question words that are most similar to the answer, while the purpose of the relation extraction is to find the relation that links the entities (i.e., object and subject) in the question to the entity that appears in the context [2]. \n | **Holistic Measures**             | Comprehensive, context-aware                | Computationally intensive                   | BertScore, BLEURT, MaUde [0]                     | \n| **Human-Centered Evaluation (HCES)** | Aligns with human judgment, detailed feedback [1] | Time-consuming, subjective biases [5] | Absolute and Relative Evaluation [1] [3] |\n\nAutomatic Evaluation Strategies are often fast, consistent, and scalable, but they may miss nuances and rely on pre-set metrics [5]. Evaluation of NLP systems can be classified into intrinsic and extrinsic methods [5]. Intrinsic methods evaluate the quality of the outputs against a predetermined ground truth, whereas extrinsic methods evaluate the impact of the outputs on the performance of other systems [5].\nHybrid Approaches Balances strengths of human and automated [3] Complexity in implementation Majority Voting, Contradiction-Based Systems [6][8].\n\nIn conclusion, a combination of holistic measures, human-centered evaluation, automatic strategies, and innovative hybrid approaches collectively provide a robust framework for evaluating complex scientific QA systems [0][1][5][7][12]. As these systems evolve, so too must their evaluation methodologies, ensuring they remain rigorous and reflective of real-world applications [10][13]. \n\n\nReferences:\n[0] Evaluation of Question Answering Systems: Complexity of judging a natural language Holistic measures: This category of error scores utilizes end-to-end models based on neural networks, e.g., transformers, to perform the evaluation. The end-to-end models can be directly applied to statistical or contextualized features that exist within the response or reference to compute the scores. Such models may use different strategies to perform the evaluation process: (i) referenced scores, which compare the generated response with a provided gold answer, by learning an alignment score between context and response to approximate human judgments 150, (ii) hybrid referenced-unreferenced evaluation score 149, where the score is trained without needs for human responses by smoothing the negative samples directly from the dataset, or (iii) unreferenced scores (i.e., learned in an unsupervised manner) that uses large pre-trained language models to extract latent representations of text units (i.e., utterances or words), and take advantage of the time shifts that exist between them 153. The MTES based on end-to-end models include a wide range of error scores such as BertScore142, RUSE 182, RUBER 149, BLEURT 151, ADEM 150, MaUde 153.\n\n[1] Evaluation of Question Answering Systems: Complexity of judging a natural language Every category has its own strengths and weaknesses. In general, a HCES is considered the best evaluation that gives the most trustworthy score. Typically, HCES relies on a group of people (e.g. experts or specialists) to evaluate the output of a QA system based on certain guidelines and criteria, e.g., regarding adequacy, fluency, and coherence of a text. However, it is also prone to human errors caused by subjectivity and bias in opinion. Frequently, a 5-point Likert-scale 199 or numerical ranking criteria is used as a measurement tool for HCES evaluations 202. Conceptually, HCES can be further classified into two sub-categories: Absolute evaluation and relative evaluation.\n\n[2] Evaluation of Question Answering Systems: Complexity of judging a natural language Analyzing the question and detecting the answer type, or finding the embeddings of most similar vectors of question and context, is one of the core tasks of QA algorithms. The QA algorithms also facilitate the query formulation during the query processing stage via parsing the passing tokens with precise descriptions in order to identify what kind of answer is supposed to be returned (e.g., paragraph, description, etc). Different strategies have been proposed to deal with such requirements, such as: Regular Expressions (regex), Part-of-Speech (POS), matched n-gram features, finding the first noun phrase after the question headword ‘wh*’, and Named Entity Recognition (NER) to identify the given tokens such as human, place, location, entity. The question types classification with different answer types have been investigated in many studies such 125; 44; 19; 48; 49; 50; 51. The focus detection is meant to capture the question words that are most similar to the answer, while the purpose of the relation extraction is to find the relation that links the entities (i.e., object and subject) in the question to the entity that appears in the context.\n\n[3] Evaluation of Question Answering Systems: Complexity of judging a natural language Absolute evaluation: In this type of HCES, individuals are asked to evaluate the output of a natural language generation/inference model using a 5-point scale 203. The evaluator assesses the output texts of a task based on predefined criteria or rubric, rating each from 1-5 or on a nominal scale, For example, a statement such as ”Rate the output based on the naturalness of the language” with options as very unnatural, unnatural, neutral, natural and very natural, or with 1-5 numerical options, each responding to a similar semantic meaning. The results of which will be averaged over the rubric and used as the evaluation/annotation output. For example, in machine translation, the task may require an evaluation from a person who is familiar with either the target language (i.e., monolingual) or both source and target languages (i.e., bilingual) to perform the required assessment (rating) on the generated text. The absolute method is often subjected to the bias of the evaluator due to the discreteness of the scale and inter-annotator disagreements. Therefore, individual evaluators may not always be preferred for some tasks, e.g., in order to obtain a more accurate assessment.\n\n[4] Evaluating QA Systems on Multiple Dimensions Evaluating QA Systems on Multiple Dimensions Question-answering systems are expanding beyond information retrieval and information extraction, to become fullfledged, complex NLP applications. In this paper we discuss the evaluation of question-answering systems as complex NLP systems, and suggest three different dimensions for evaluation: objective or information-based evaluation; subjective evaluation; and architectural evaluation. We also discuss the role of ambiguity resolution in QA systems, and how ambiguity resolution might be evaluated.\n\n[5] Intrinsic and Extrinsic Automatic Evaluation Strategies for Paraphrase Generation Systems to Alexander et al., perhaps automatic and manual evaluations are the most basic dichotomy for evaluating NLP systems. To assess performance of NLP systems, recruiting human experts is the most straightforward. However, it has two major limitations: first, humans often generate inconsistent results and the process is very slow. Secondly, manual evaluation is a time-consuming task and laborious. In other words, it is a costly evaluation approach. Thus, automatic evaluation is relatively more vital to evaluate and rank several NLP systems. Evaluation of NLP systems can also be classified into intrinsic and extrinsic methods, which can be performed either automatically or manually. In an intrinsic evaluation, quality of NLP systems outputs is evaluated against pre-determined ground truth (reference text) whereas an extrinsic evaluation is aimed at evaluating systems outputs based on their impact on the performance of other NLP systems. For instance, in intrinsic evaluation of paraphrase generation system, we would ask the following questions: Does the generated paraphrase convey meaning of the original text? On the other hand, extrinsic evaluation of generated paraphrases deals with their impact on the performance of other NLP systems. In this context, we might ask: Do the incorporated paraphrases significantly improve performance of question-answering model? Can the generated paraphrases be used as a surrogate of original text to train text classification models? If so, it can be concluded that, extrinsically, the considered paraphrases are useful. If the goal is to evaluate quality of generated paraphrases or performance of paraphrase generation systems, then it is usually easier to use\n\n[6] Improving Question Answering for Reading Comprehension Tests by Combining Multiple Systems Improving Question Answering for Reading Comprehension Tests by Combining Multiple Systems Most work on reading comprehension question answering systems has focused on improving performance by adding complex natural language processing (NLP) components to such systems rather than by combining the output of multiple systems. Our paper empirically evaluates whether combining the outputs of seven such systems submitted as the final projects for a graduate level class can improve over the performance of any individual system. We present several analyses of our combination experiments, including performance bounds, impact of both tie-breaking methods and ensemble size on performance, and an error analysis. Our results, replicated using two different publicly available reading test corpora, demonstrate the utility of system combination via majority voting in our restricted domain question answering task.\n\n[7] QASA: Advanced Question Answering on Scientific Articles Reasoning is the crux of intellectual thinking. While question answering (QA) tasks are prolific with various computational models and benchmark datasets, they mostly tackle factoid or shallow QA without asking deeper understanding. Dual process theory asserts that human reasoning consists of associative thinking to collect relevant pieces of knowledge and logical reasoning to consciously conclude grounding on evidential rationale. Based on our intensive think-aloud study that revealed the three types of questions: surface, testing, and deep questions, we first pro-pose the Q ASA benchmark that consists of 1798 novel question answering pairs that require full-stack reasoning on scientific articles in AI and ML fields. Then we propose the Q ASA approach that tackles the full-stack reasoning with large language models via associative selection, evidential rationale-generation, and systematic composition. Our experimental results show that Q ASA ’s full-stack inference outperforms the state-of-the-art I NSTRUCT GPT by a big margin. We also find that rationale-generation is critical for the performance gain, claiming how we should rethink advanced question answering. The dataset is available at https://github.com/lgresearch/QASA.\n\n[8] Using contradictions to improve QA systems approach is vulnerable to selecting supporting sources that might be wrong. Since supported or verified answers look more credible, a user might be mislead into uncritically accepting model outputs then they otherwise would be. Even though we could also find sources that wrongly contradict an answer, surfacing sources that contradict an answer might help a user engage critically and help a QA system select the least contradicted answers. Therefore we ask: under NLIbased setups how do contradictions contribute to the performance of question answering (QA) and how is this different from entailment-based systems? By exploring this question, we hope to show why researchers should be critical of the paradigm of verification in NLP systems and why future work utilizing critical and contradicted statements could provide unexplored opportunities for improving NLP systems. Our work makes the following contributions. We propose a method (Figure 1) that reformulates answers under QA as hypothesis statements which are then used with three-class NLI to rank and select answers. We demonstrate, across 9 multiple choice datasets and 7 extractive QA datasets models, that models which use QA confidence scores as well as both entailment and contradiction scores outperform all other setups. In addition, selecting the least contradicted answer provides a competitive approach to selecting answers that is often on par with or better than entailment-based systems. While this work is in a relatively limited setting, we suggest how leveraging contradictions could help improve QA inference in ways that are not possible with entailment-based systems. 1.1 Related work NLI for QA has been\n\n[9] Knowledge Graph Question Answering Leaderboard: A Community Resource to Prevent a Replication Crisis Data-driven systems need to be evaluated to establish trust in the scientific approach and its applicability. In particular, this is true for Knowledge Graph (KG) Question Answering (QA), where complex data structures are made accessible via natural-language interfaces. Evaluating the capabilities of these systems has been a driver for the community for more than ten years while establishing different KGQA benchmark datasets. However, comparing different approaches is cumbersome. The lack of existing and curated leaderboards leads to a missing global view over the research field and could inject mistrust into the results. In particular, the latest and most-used datasets in the KGQA community, LC-QuAD and QALD, miss providing central and up-to-date points of trust. In this paper, we survey and analyze a wide range of evaluation results with significant coverage of 100 publications and 98 systems from the last decade. We provide a new central and open leaderboard for any KGQA benchmark dataset as a focal point for the community - https://kgqa.github.io/leaderboard/. Our analysis highlights existing problems during the evaluation of KGQA systems. Thus, we will point to possible improvements for future evaluations.\n\n[10] Using contradictions to improve QA systems Using contradictions to improve QA systems Ensuring the safety of question answering (QA) systems is critical for deploying them in biomedical and scientific domains. One approach to improving these systems uses natural language inference (NLI) to determine whether answers are supported, or entailed, by some background context. However, these systems are vulnerable to supporting an answer with a source that is wrong or misleading. Our work proposes a critical approach by selecting answers based on whether they have been contradicted by some background context. We evaluate this system on multiple choice and extractive QA and find that while the contradiction-based systems are competitive with and often better than entailment-only systems, models that incorporate contradiction, entailment, and QA model confidence scores together are the best. Based on this result, we explore unique opportunities for leveraging contradiction-based approaches such for improving interpretability and selecting better answers. Introduction Safety in NLP systems is an unresolved issue, particularly in biomedical and scientific contexts where known issues such as hallucination and overconfidence provide obstacles for deploying them (Ji et al., 2022;Kell et al., 2021). Utilizing natural language inference (NLI) as a method for improving the safety and performance of NLP research is an active area of research (Li et al., 2022). However, these systems typically focus exclusively on entailment to verify answers. Similar research looks at building self-supporting NLP systems (Nakano et al., 2022;Menick et al., 2022) with the goal of improving safety by verifying model outputs with some external supporting source. These developments are troubling since a verification or self-supporting\n\n[11] Addressing a Question Answering Challenge by Combining Statistical Methods with Inductive Rule Learning and Reasoning Addressing a Question Answering Challenge by Combining Statistical Methods with Inductive Rule Learning and Reasoning A group of researchers from Facebook has recently proposed a set of 20 question-answering tasks (Facebook's bAbl dataset) as a challenge for the natural language understanding ability of an intelligent agent. These tasks are designed to measure various skills of an agent, such as: fact based question-answering, simple induction, the ability to find paths, co-reference resolution and many more. Their goal is to aid in the development of systems that can learn to solve such tasks and to allow a proper evaluation of such systems. They show existing systems cannot fully solve many of those toy tasks. In this work, we present a system that excels at all the tasks except one. The proposed model of the agent uses the Answer Set Programming (ASP) language as the primary knowledge representation and reasoning language along with the standard statistical Natural Language Processing (NLP) models. Given a training dataset containing a set of narrations, questions and their answers, the agent jointly uses a translation system, an Inductive Logic Programming algorithm and Statistical NLP methods to learn the knowledge needed to answer similar questions. Our results demonstrate that the introduction of a reasoning module significantly improves the performance of an intelligent agent.\n\n"}
{"case_id": "ef77a4f716eda4a6c980c9d7a56c5bb4", "answer_text": "\n\nA good ontology semantic similarity measure that considers multiple inheritance cases of concepts effectively integrates both the hierarchical structure of ontologies and the rich relational information provided by multiple super concepts [0].\n\nDisjunctive Shared Information (DiShIn) [1]. \nOne robust approach is the **Disjunctive Shared Information (DiShIn)** measure, which exploits the shared information content between ontology concepts [1]. DiShIn effectively manages the complexities introduced by multiple inheritance by considering disjunctive ancestors rather than just the nearest common ancestor (NCA). This approach calculates the shared information content by identifying all disjunctive common ancestors (DCA) first and then averaging their information content. This method improves correlation coefficients between semantic and sequence similarity, which has been shown to be particularly useful in managing large-scale comparative studies in domains such as the Gene Ontology (GO) [1]. However, a potential disadvantage is that DiShIn does not fully leverage more advanced ontology features and might miss some nuances inherent in more intricate ontological structures.\n\nGraph-based Shared Information (GraSM)[3].\nAnother notable measure is the **GraSM (Graph-based Shared Information)** method. This measure extends traditional information content methods by averaging the information content of all DCAs of two concepts [3]. It starts by identifying all disjunctive ancestors of each concept and then finds the common ones between the two sets. The information content of these common ancestors is averaged to provide the final similarity score. By factoring in all paths through the ontology graph rather than a single path or ancestor, GraSM provides a more nuanced and comprehensive measure of similarity, especially in complex biomedical ontologies like SNOMED-CT. GraSM's methodology has been empirically shown to improve correlation values and reduce computational time significantly, enhancing its practicality for large-scale ontologies [3]. However, the averaging process might dilute the significance of particularly informative paths, potentially reducing the sensitivity of the measure in some contexts.\n\n Extended Wu & Palmer Measure [0]. \n**Ontology-Based Semantic Similarity Measures** such as the one proposed by Wu & Palmer have shown good performance when extended to consider multiple inheritance through the inclusion of all subsumed concepts (ASC) rather than just the closest common parent (CCP) [0]. This extended measure considers the depth and distribution of all subsumed concepts to compute a more informed similarity score. This ensures that a greater amount of explicit knowledge is utilized, which enhances the measure's effectiveness in large and overlapping hierarchies [0]. Experimental comparisons on benchmark datasets have indicated that this extended measure outperforms the original Wu & Palmer measure, confirming its enhanced accuracy and reliability [0]. However, this method might be computationally intensive due to the need to consider all subsumed concepts, impacting its scalability.\n\nConSim[2].\n**ConSim** is another enhanced measure designed to capture relationships across different ontologies by leveraging existing hybrid measures, thereby improving the integration and retrieval of resources across domains [2]. ConSim builds on hybrid similarity measures and incorporates additional enhancements to better align with human judgment. It has shown to outperform other hybrid measures by 20% in terms of correlation coefficients, making it particularly useful in cross-ontology scenarios [2]. The main advantage of ConSim is its high accuracy in cross-domain applications, though its reliance on hybrid measures might introduce complexity in implementation and interpretation.\n\nCross-Cluster Approach[8]. \nFurthermore, the **Cross-Cluster Approach** integrates features such as cross-modified path length, common specificity, and local granularity to navigate the varying granularity degrees of clusters within ontologies [8]. This approach leverages three main features: (1) modified path length, which adjusts traditional path-based measures to account for cross-cluster paths; (2) common specificity, which measures the commonality of two concepts' specificity levels within their respective clusters; and (3) local granularity, which assesses the granularity of clusters to refine the similarity measure. This method has demonstrated a high correlation (0.873) with human similarity judgments, underscoring its accuracy and efficiency in handling semantic similarity in ontological structures with multiple inheritances [8]. While effective, the complexity of integrating multiple features might limit its usability in simpler ontological frameworks.\n\nMulti-Domain Measures [5].\nThe **Multi-Domain Measures**, including both the aggregative and integrative approaches, represent another effective strategy [12]. These measures lift single-ontology similarity measures into multi-domain contexts, allowing for more accurate comparisons of resources annotated with concepts from multiple domains. The aggregative approach calculates similarity scores independently for each domain and then averages these scores, while the integrative approach merges all relevant ontologies into a single one and computes similarity within this merged ontology. These approaches have shown a promising increase in performance when predicting new annotations based on existing ones, highlighting their utility in multidisciplinary datasets. The empirical results from multidisciplinary epidemiology datasets have shown that multi-domain measures outperform single-domain ones in the majority of cases [12]. However, the challenge of ensuring interoperability and equivalence between different ontologies could be a potential drawback.\n\nApplication in Diverse Fields\nBeyond the biomedical domain, these measures have broad applicability. In the field of **knowledge management**, ontology-based similarity measures help in organizing and retrieving large volumes of information by understanding the relationships between different concepts [2]. They are crucial in **web mining** for improving search results and recommendation systems by providing more contextually relevant information based on the semantics of search queries [6]. In **natural language processing (NLP)**, these measures enhance text analysis, machine translation, and sentiment analysis by better understanding the meaning and relationships among words and phrases [6]. In **e-commerce**, they improve product recommendation systems by accurately matching user preferences with product descriptions through semantic similarity [6].\n\nPath-Based and Information Content-Based Measures[13]\nAdditionally, **path-based measures** like *Leacock and Chodorow*’s measure and **information content-based measures** like *Resnik’s*, *Lin’s*, and *Jiang & Conrath*’s measures are other popular methods used in the field [6]. Although typically applied in simpler ontological structures or single-domain contexts, these measures can serve as benchmarks to evaluate the performance of more complex methods like DiShIn and GraSM. Compared to these simpler methods, both DiShIn and GraSM offer more nuanced assessments by addressing the intricacies of multiple inheritances and complex hierarchies, thus usually demonstrating improved performance in multi-inheritance scenarios.\n\nConclusion[2]\nIn conclusion, effective semantic similarity measures for ontologies with multiple inheritance include DiShIn, GraSM, and enhanced versions of traditional measures such as Wu & Palmer’s method extended to ASC. These measures incorporate both structural relationships and information content, offering robust and comprehensive similarity assessments across complex ontological structures, as validated by empirical results and statistical evaluations [1]. The applicability of these measures spans various fields, making them valuable tools in diverse contexts. It's important to consider the specific application requirements when choosing among these measures, as each has its unique advantages and potential drawbacks. \n\n\nReferences:\n[0] A New Ontology-Based Semantic Similarity Measure for Concepts Subsumed by Multiple Super Concepts A New Ontology-Based Semantic Similarity Measure for Concepts Subsumed by Multiple Super Concepts Semantic Similarity relates to computing the similarity between concepts of ontology. There exist four approaches to calculate the semantic similarity. The first approach is based on path length. Under this approach we studied and compared some of the measures on a bench mark dataset. Among the compared measures Wu & Palmer measure has the advantage of being simple to implement and has better performance compared to the other similarity measures. This measure considers only the depth of the LCS: (Least Common Subsumer) we call in our paper as Closest Common Parent for similarity computation. But there are complex and large taxonomies, covering thousands of interrelated concepts including several overlapping hierarchies, and extensive use of multiple inheritances (i.e. a concept is subsumed by several super concepts). For such taxonomies using only the LCS will ignore a great amount of explicit knowledge. To overcome this limitation we propose ontology based semantic similarity measure which extends Wu & Palmer measure by considering ASC :( All Subsumed Concepts). We compared both the measures on two benchmark datasets. The obtained results show that our measure gave improved similarity values compared to the Wu and Palmer measure.\n\n[1] Disjunctive shared information between ontology concepts: application to Gene Ontology accepted way to assess semantic similarity approaches in the biomedical field. However, semantic and structural correlation may not be the best way to assess semantic similarity, and better gold standards, not biased by structural features, are much required, especially in the case of DiShIn, where multiple inheritance is often associated with complex entities. DiShIn does not take advantage of the higher expressivity of more advanced ontology features than the straightforward subsumption relationships present in DAGs. For semantic similarity, subsumption relationships may be enough, but as we evolve to semantic relatedness, other relationships have to be considered and additional levels of distinction between asserted and inferred hierarchies may be required. Conclusions This paper presents DiShIn, a novel method for effectively exploiting multiple inheritance when calculating the shared information content between two ontology concepts. DiShIn can be easily integrated in any semantic similarity measure dependent on the information content shared by two concepts. DiShIn was applied to GO similarity measures, and its performance was evaluated against state-of-the-art measures using an existing platform for evaluation of protein similarity measures. In this setting, DiShIn was able to improve the correlation coefficients between semantic and sequence similarity, and also reduce the computational time of the common disjunctive ancestors identification, as previously proposed by GraSM. These results represent an important contribution towards effective management of multiple inheritance in large-scale comparative studies. As ontologies grow and interoperability between ontologies is required, multiple inheritance will become a prominent issue for semantic similarity measures. For example, the comparison of complex biomedical entities, such\n\n[2] ConSim: an enhanced semantic similarity measure to find the relationship between concepts in cross ontology ConSim: an enhanced semantic similarity measure to find the relationship between concepts in cross ontology Ontology has become the key concept for providing relevant and accurate information to the users for a particular domain or from multiple domains. To extract similar concepts from multiple ontologies, there is a need to develop a concept similarity measure between two independent ontologies. The intention of finding semantic similarity is to enhance the integration and retrieval of resources in a more meaningful and accurate way. Based on the existing hybrid similarity measures an enhanced similarity measure is proposed which brings out a more accurate relationship between the two concepts of different ontologies, the proposed measure is equivalent to human judgments. The proposed method has an improved technique over other methods. The experimental results establish the efficiency of our proposed work and reveals that similarity measure for cross ontology out performs the current hybrid similarity measures by 20% in terms of correlation coefficients.\n\n[3] Disjunctive shared information between ontology concepts: application to Gene Ontology methods (Results and Discussion Section). Methods This section presents the current approaches to define similarity between ontology concepts as a combination of their common and distinctive relationships in the ontology. Semantic similarity Resnik defined the similarity between two concepts c 1 and c 2 , represented as nodes in a DAG, as the amount of information content they share. Given the frequency freq(c) for each concept c in a corpus, the information content of a concept is inversely proportional to the frequency of that concept and its descendants: where maxFreq represents the maximum frequency of all concepts, i.e. the frequency of the root concept when it exists. Then, Resnik defined the amount of information content they share as: Jiang and Conrath defined distance between concepts as the difference between the ICs of both concepts and the IC of their MICA: Lin defined similarity as the IC of their MICA over the IC of both concepts: All of these measures defined similarity or distance based on the same Resnik definition of shared information that uses a single common ancestor. To deal with multiple inheritance, Couto et al. proposed GraSM, a new definition of shared information. GraSM defines it as the average of the information content of the disjunctive common ancestors of both concepts: where DCA grasm represents the disjunctive common ancestors of both concepts: where DA grasm represents the disjunctive ancestors of a concept: where Paths(a, c) gives the set of distinct paths from c to a in the DAG. For GraSM, a\n\n[4] Multi-domain semantic similarity in biomedical research ontologies are orthogonal, i.e. they represent different domains of reality, and thus rarely have concepts in common, and when they do, the overlapping concepts are very general. This type of measure is able to compare resources annotated with concepts from multiple domains of knowledge, as the biomedical entities mentioned above. Notice that we are considering the multidisciplinarity of biomedical resources from the point of view of knowledge representation (KR): we propose a means to explore the ontology-provided definition of concepts to compare multidisciplinary entities annotated with concepts from more than one ontology (non-KR measures exist that are agnostic to the issue of multiple domains; e.g. Pederson et al. compare concepts by comparing the textual neighbourhood of the concepts-the set of words that often appear near the concept in scientific literature). Instead of creating a multi-domain measure from scratch, our methodology is to leverage on existing singleontology measures, which have already been validated in a variety of scenarios, and lift them into multi-domain measures. As such, both the \"aggregative\" and \"integrative\" approaches take as input a single-ontology semantic similarity measure able to compare a set of concepts with another set of concepts (often called groupwise measures). The \"aggregative\" approach is depicted in Fig. 1. In this approach we independently compare each domain using a single-ontology measure, i.e. the concepts from one domain in the first resource are compared to the concepts from the same domain in the second resource. We do this Fig. 1 The aggregative approach. For each annotation domain in the entities being\n\n[5] Ontology-Based Measure of Semantic Similarity between Concepts Ontology-Based Measure of Semantic Similarity between Concepts Semantic similarity between concepts plays an important role in knowledge sharing, Web mining and semantic sense understanding. We proposed a new measure which combines the graph-based measure and information content-based measure. The measure take the condition into account that there is another ancestor concept whose information content is nearly the same with which of the Nearest Common Ancestor(NCA). The measure constructs the concept tree by Wordnet and computes the path length of the two concepts in the concept graph, local density and the connect power of the edge, and then integrates them with edge weight and information content. The result indicates the measure perform well in the experiment.\n\n[6] A Cross-Cluster Approach for Measuring Semantic Similarity between Concepts A Cross-Cluster Approach for Measuring Semantic Similarity between Concepts We present a cross-cluster approach for measuring the semantic similarity/distance between two concept nodes in ontology. The proposed approach helps overcome the differences of granularity degrees of clusters in ontology that most ontology-based measures do not concern. The approach is based on 3 features (1) cross-modified path length feature between the concept nodes, (2) a new features: the common specificity feature of two concept nodes in the ontology hierarchy, and (3) the local granularity of the clusters. The experimental evaluations using benchmark human similarity datasets confirm the correctness and the efficiency of the proposed approach, and show that our semantic measure outperforms the existing techniques. The proposed measure gives the highest correlation (0.873) with human ratings compared to the existing measures using the benchmark RG dataset and WordNet2.0\n\n[7] Multi-domain semantic similarity in biomedical research Multi-domain semantic similarity in biomedical research Background Given the increasing amount of biomedical resources that are being annotated with concepts from more than one ontology and covering multiple domains of knowledge, it is important to devise mechanisms to compare these resources that take into account the various domains of annotation. For example, metabolic pathways are annotated with their enzymes and their metabolites, and thus similarity measures should compare them with respect to both of those domains simultaneously. Results In this paper, we propose two approaches to lift existing single-ontology semantic similarity measures into multi-domain measures. The aggregative approach compares domains independently and averages the various similarity values into a final score. The integrative approach integrates all the relevant ontologies into a single one, calculating similarity in the resulting multi-domain ontology using the single-ontology measure. Conclusions We evaluated the two approaches in a multidisciplinary epidemiology dataset by evaluating the capacity of the similarity measures to predict new annotations based on the existing ones. The results show a promising increase in performance of the multi-domain measures over the single-ontology ones in the vast majority of the cases. These results show that multi-domain measures outperform single-domain ones, and should be considered by the community as a starting point to study more efficient multi-domain semantic similarity measures. Background Ontology-based semantic similarity uses the machinereadable definitions of concepts provided by ontologies to compare annotated entities based on their meaning. Contrast this with other similarity measures that use structural and/or physical properties of the entities: e.g. proteins have traditionally been compared based\n\n[8] Evaluating Semantic Similarity between Biomedical Concepts/Classes through Single Ontology Evaluating Semantic Similarity between Biomedical Concepts/Classes through Single Ontology Most of the existing semantic similarity measures that use ontology structure as their primary source can measure semantic similarity between concepts/classes using single ontology. The ontology-based semantic similarity techniques such as structure-based semantic similarity techniques (Path Length Measure, Wu and Palmer’s Measure, and Leacock and Chodorow’s measure), information content-based similarity techniques (Resnik’s measure, Lin’s measure), and biomedical domain ontology techniques (Al-Mubaid and Nguyen’s measure (SimDist)) were evaluated relative to human experts’ ratings, and compared on sets of concepts using the ICD-10 “V1.0” terminology within the UMLS. The experimental results validate the efficiency of the SemDist technique in single ontology, and demonstrate that SemDist semantic similarity techniques, compared with the existing techniques, gives the best overall results of correlation with experts’ ratings.\n\n"}
{"case_id": "e09a30179e0b587d57edb17317ae3288", "answer_text": "\n\nArtificial intelligence (AI) has significantly advanced the diagnosis and treatment of obstructive sleep apnea (OSA) by automating and refining various diagnostic processes and optimizing treatment approaches [1]. These advancements can be broadly categorized into automated diagnostic tools [0], prediction models [2], and treatment optimization systems [3]. \n\nAutomated Diagnostic Tools[3]. \n\nPolysomnography (PSG) is the gold standard for OSA diagnosis, capturing an exhaustive array of physiological signals over a night of sleep. However, PSG is labor-intensive, costly, and requires in-lab monitoring, which limits accessibility and increases the burden on healthcare systems [1][6]. AI techniques, particularly machine learning (ML) and deep learning (DL) models, have been developed to streamline this process.\n\nFor instance, artificial neural networks (ANNs) have demonstrated high accuracy in automating PSG interpretations. These models can classify respiratory events, predict the apnea-hypopnea index (AHI), and even stage sleep, thereby reducing the need for trained human experts and minimizing scorer variability [3]. A notable example is the adaptation of ANNs utilizing SpO2 and ECG data, achieving accurate OSA severity classification with minimal resource utilization [2][10]. Studies have shown that an ANN solely utilizing SpO2 can classify 90.4% of patients into the correct severity category based on estimated AHI and 94.4% based on oxygen desaturation index (ODI) [2]. Additionally, the use of AI in video monitoring systems has shown promise in objectively analyzing body movements during sleep to assist in diagnosing OSA [0].\n\nTraditional PSG, while effective, is often limited by accessibility issues and the labor-intensive nature of the process, which involves manual scoring by trained experts and inter-scorer variability [1][6]. In contrast, AI-based diagnostic tools offer a more robust, scalable solution that can automate much of this process, potentially reducing diagnostic wait times and increasing overall efficiency in sleep clinics [3].\n\nPrediction Models\n\nVarious ML models, including support vector machines (SVM), logistic regression, and random forests, have been applied to predict the severity of OSA based on clinical features and physiological data. For example, SVM models developed from clinical datasets have shown high sensitivity (0.93) and specificity (0.80), achieving an accuracy of 86% in distinguishing between mild, moderate, and severe OSA [8]. These models not only use complex physiological data but also integrate simpler, non-invasive measures like instantaneous heart rate (IHR) and blood oxygen saturation (SpO2) derived from wearable devices, making early diagnosis more accessible and less cumbersome [11][14].\n\nLong short-term memory (LSTM) recurrent neural networks (RNNs) have also been effective in leveraging temporal data from physiological signals to classify sleep apnea events. These models have shown remarkable accuracy, with some achieving an area under the receiver operating characteristic (ROC) curve of up to 0.98 [11][14]. Such efficiency in prediction models can enable quicker, remote diagnostics and continuous monitoring, thereby tackling the issue of under-diagnosis inherent in conventional methods [11].\n\nTraditionally, the diagnosis of OSA relied heavily on in-lab PSG, which, though thorough, is not always feasible for widespread screening due to its high costs and resource demands. Predictive models using AI can provide a non-invasive and cost-effective alternative for preliminary assessments, thus offering a significant advantage over traditional methods by potentially increasing the rate of early detection and intervention [9].\n\n \n### Real-World Applications\nAutomated computer-aided diagnosis systems have emerged as useful tools to deal with complex rules involving several biomedical recordings simultaneously, expediting diagnosis and treatment [1]. Among AI-based tools, Artificial Neural Networks (ANNs) have been applied in the context of sleep apnea-hypopnea syndrome (SAHS) for automated diagnosis, sleep staging, and treatment monitoring [1]. Furthermore, AI implementation in OSA has the potential to minimize diagnostic costs and improve CPAP compliance [2]. Various machine learning (ML) and deep learning (DL) techniques have been proposed to enhance the accuracy of OSA diagnosis, using data such as ECG signals, SpO2 levels, and heart rate variability [10][11][13].\n\n\nAI-powered tools are also making significant strides in real-world applications such as in-home monitoring and remote diagnostics. Wearable health trackers and remote monitoring systems equipped with AI algorithms can continuously collect and analyze data, providing real-time, actionable insights into sleep patterns and potential OSA events [14]. These devices can significantly enhance patient convenience and compliance, allowing for ongoing monitoring without the need for frequent in-lab visits.\n\nAI-driven telehealth applications enable healthcare providers to monitor patients remotely, offering timely interventions and adjustments to treatment plans as needed. This approach not only improves patient outcomes but also alleviates the burden on healthcare facilities by reducing the number of in-person consultations required [14]. Furthermore, AI integration into existing healthcare infrastructures promises to enhance the diagnostic value of ambulatory polygraphic recordings, making them more reliable and accessible for broader patient populations [12].\n\n ### Treatment Optimization Systems [2] \n\nAI has also played a pivotal role in optimizing therapeutic interventions for OSA. Continuous positive airway pressure (CPAP) therapy, a common treatment for OSA, can be precisely tailored using AI systems to enhance patient compliance and treatment efficacy. Machine learning algorithms have been deployed to adjust CPAP pressure dynamically, reducing titration failures and increasing usage compliance by over an additional hour per day on average [2]. Moreover, AI-driven systems utilize the \"Big Data\" gathered from CPAP devices and PSG to personalize treatment plans considering patient preferences and physiological responses [2].\n\nArtificial neural networks and other AI models have been employed to track and predict patient progression and response to treatment, ensuring that adjustments can be made in real time to optimize outcomes. For example, AI has been used to forecast and prevent sudden cardiac death (SCD) in severe OSA patients by analyzing ECG signals and other physiological markers [13].\n\nTraditional CPAP therapy often requires manual titration and frequent adjustments based on patient feedback and subsequent PSG sessions, which can be cumbersome and delay optimal treatment. AI-based systems streamline this process by providing real-time adjustments and personalized treatment plans, thereby improving adherence and outcomes more efficiently [2][13].\n\nChallenges and Future Directions\n\nWhile AI's potential in OSA management is immense, several challenges, such as the need for high-quality annotated datasets, potential biases in model training, and the necessity for external validation, persist. Many AI models, despite showing high performance in internal validations, lack rigorous external validations across diverse populations [9]. \n\n **Technical Hurdles**: Polysomnography is a generally adopted approach for diagnosing obstructive sleep apnoea (OSA), but it comes with critical drawbacks, including massive equipment cost, large expense on replacing damaged components, and the need for invasive devices to be worn while patients are struggling to sleep [0].\n  \n**Data Quality and Integration**: The success of AI heavily relies on the availability of high-quality, annotated datasets. Noisy data, missing values, and varying data formats can pose significant challenges for machine learning models [10].  \n\n\n\n- **Model Interpretability**: AI models, particularly deep learning models, are often criticized for their \"black-box\" nature, making it difficult for clinicians to understand and trust their decisions [6].\n\n\n\nPolysomnography (PSG) is the gold standard for diagnosing sleep obstructive apnea (OSA). It allows monitoring of breathing events throughout the night. The detection of these events is usually done by trained sleep experts. However, this task is tedious, highly time-consuming and subject to important inter-scorer variability [6]. In this study, we adapted our state-of-the-art deep learning method for sleep event detection, DOSED, to the detection of sleep breathing events in PSG for the diagnosis of OSA. We used a dataset of 52 PSG recordings with apnea-hypopnea event scoring from 5 trained sleep experts. We assessed the performance of the automatic approach and compared it to the inter-scorer performance for both the diagnosis of OSA severity and, at the microscale, for the detection of single breathing events. We observed that human sleep experts reached an average accuracy of 75% while the automatic approach reached 81% for sleep apnea severity diagnosis. The F1 score for individual event detection was 0.55 for experts and 0.57 for the automatic approach, on average. These results demonstrate that the automatic approach can perform at a sleep expert level for the diagnosis of OSA [6].\n - **Regulatory Approvals**: Medical AI tools must go through stringent regulatory processes to ensure their safety and efficacy. This can significantly delay their deployment in clinical settings [2].\n- **Clinical Workflow Integration**: Incorporating AI tools into existing clinical workflows without causing disruptions requires careful planning and training of healthcare personnel [2].\n\n**Ethical Hurdles**:\n\nAI and machine learning technologies are becoming increasingly prevalent in the diagnosis and treatment of obstructive sleep apnea (OSA). These technologies present significant advancements but also raise important ethical considerations. Implementing AI in OSA diagnosis can minimize costs associated with traditional methods and increase accessibility for patients [2]. However, it also poses challenges such as ensuring the privacy of patient data, maintaining trust in the automated systems, and addressing potential biases in the AI algorithms. Another concern is the regulation and standardization of AI tools to guarantee their safety and efficacy before they are widely adopted in clinical practice. Video monitoring and the use of photoplethysmography (PPG) with deep learning models are examples of non-invasive, AI-driven techniques that show promise but require careful ethical scrutiny [0][12]. \n- **Bias and Fairness**: AI models can inherit biases present in the training data, leading to unequal performance across different demographic groups. Ensuring fairness and equity in AI-driven healthcare solutions is crucial. [5]\n- **Patient Privacy**: The use of AI in healthcare involves handling sensitive patient data. Ensuring robust data protection and privacy measures is essential to gain patient trust and comply with regulations.\n\nFurther, the integration of AI tools into clinical practice must be streamlined to overcome technical and ethical hurdles and ensure seamless interoperability with existing healthcare systems [12].\n\nTraditional methods, though reliable, often fall short in terms of scalability and adaptability compared to AI-driven approaches [5]. However, the transition to AI-based systems will require addressing these challenges to ensure they can meet or exceed the robustness and reliability of existing methodologies [9].\n\nIn conclusion, AI has transformed the landscape of OSA diagnosis and treatment by providing automated, accurate, and accessible diagnostic tools [0, 2, 5], robust prediction models [3], and optimized treatment strategies [2]. These advancements herald a new era in sleep medicine, promising improved patient outcomes and more efficient healthcare delivery [1]. \n\n\nReferences:\n[0] Artificial intelligent vision analysis in obstructive sleep apnoea (OSA) Artificial intelligent vision analysis in obstructive sleep apnoea (OSA) Although polysomnography is a generally adopted approach for diagnosing obstructive sleep apnoea (OSA), there are several critical drawbacks with it, including massive equipment cost, large expense on replacing damaged components and more importantly invasive devices required to be worn while patients are struggling to sleep. Furthermore, there is no proof that polymonography obtains higher accuracy in detecting patients with OSA than more simple investigations. Video monitoring has been adopted to assist diagnosis on obstructive sleep apnoea. From practical researches, the best predictors of morbidity in individual patients, as assessed by improvements with CPAP therapy, are nocturnal oxygen saturation, 5 and movement during sleep. Hence, we purpose a robotic, objective and reliable video monitoring system with AI intelligence for analysis on human behavior during sleep, automatically generating a statistics report on body activity, including arm movement, limb movement, head movement and body rotation movement and arousal movement.\n\n[1] Usefulness of Artificial Neural Networks in the Diagnosis and Treatment of Sleep Apnea-Hypopnea Syndrome years due to the need for automating their complex diagnostic processes. Particularly challenging is the management of sleep apnea-hypopnea syndrome (SAHS). The gold standard technique for SAHS diagnosis is in-lab nocturnal polysomnography (PSG). During PSG, several neuromuscular and cardiorespiratory signals (up to 32 biomedical recordings) are monitored and stored for subsequent interpretation by trained personnel, which is a highly complex and time-consuming task. In addition, accessibility to diagnosis and treatment is limited due to insufficient resources, both human (trained specialists) and technical (specialized sleep units), which have led to large waiting lists. In this context, automated computer-aided diagnosis systems have emerged as very useful tools to deal with complex rules involving several biomedical recordings simultaneously, in order to expedite diagnosis and treatment. Among all the machine learning-based tools, ANNs have been widely applied in the context of SAHS and merit a thorough analysis. In order to analyze the usefulness of ANNs in the management of SAHS, an exhaustive review of the studies published during the past decade has been carried out. The review is structured as follows. First, the most relevant tasks regarding the ANNs learning process are outlined in Section 2. In this regard, some user-dependent decisions involving the ANN design and major issues concerning the training and testing processes are detailed. Second, in Section 3, the most relevant applications of ANNs are analyzed, including automated diagnosis, sleep staging, and treatment monitoring. Artificial neural networks ANNs are mathematical models inspired in the information processing capabilities of the nervous system designed to accomplish\n\n[2] Barriers of artificial intelligence implementation in the diagnosis of obstructive sleep apnea . Another ANN solely utilized the blood oxygen saturation signal (SpO2) to predict AHI and oxygen desaturation index (ODI), classifying 90.4% and 94.4% of patient into the correct severity category based on estimated AHI and ODI respectively. Physiological data from a singlelead ECG has also been utilized within an ANN to create an accessible, fast, portable method for diagnosis of sleep apnea. Implementation of AI into the diagnosis of OSA has the opportunity to minimize the cost associated with diagnosis. A cost analysis study demonstrated that the ANN (OSUNet) was the most cost effective as a prediction tool compared to HSAT and other screening questionnaires. Although the scope of this article is primarily focused on AI application of OSA diagnosis, there is also opportunity for this technology to be applied to optimizing treatment between surgery, use of OA, CPAP, or MT. An ANN has been utilized in achieving optimal CPAP pressure and decrease CPAP titration failure. A machine learning-based intelligent monitoring system has also been shown as a cost-efficient method to increase CPAP compliance, with an average increase of 1.14 h per day. There is a potential role for AI and utilizing \"Big Data\" gathered from CPAP devices and PSG to develop personalized effective treatment for individuals with OSA, while accounting for patient preference in terms of management. AI has the capacity to significantly benefit OSA diagnosis. Studies have shown the value of ANNs and the high specificity and sensitivity that is necessary for effective screening, notably much higher than\n\n[3] Usefulness of Artificial Neural Networks in the Diagnosis and Treatment of Sleep Apnea-Hypopnea Syndrome Usefulness of Artificial Neural Networks in the Diagnosis and Treatment of Sleep Apnea-Hypopnea Syndrome Usefulness of Artificial Neural Networks in the Diagnosis and Treatment of Sleep Apnea-Hypopnea Syndrome Sleep apnea-hypopnea syndrome (SAHS) is a chronic and highly prevalent disease con- sidered a major health problem in industrialized countries. The gold standard diagnostic methodology is in-laboratory nocturnal polysomnography (PSG), which is complex, costly, and time consuming. In order to overcome these limitations, novel and simplified diagnostic alternatives are demanded. Sleep scientists carried out an exhaustive research during the last decades focused on the design of automated expert systems derived from artificial intelligence able to help sleep specialists in their daily practice. Among auto mated pattern recognition techniques, artificial neural networks (ANNs) have demon - strated to be efficient and accurate algorithms in order to implement computer-aided diagnosis systems aimed at assisting physicians in the management of SAHS. In this regard, several applications of ANNs have been developed, such as classification of patients suspected of suffering from SAHS, apnea-hypopnea index (AHI) prediction, detection and quantification of respiratory events, apneic events classification, auto - mated sleep staging and arousal detection, alertness monitoring systems, and airflow pressure optimization in positive airway pressure (PAP) devices to fit patients’ needs. In the present research, current applications of ANNs in the framework of SAHS manage ment are thoroughly reviewed. Introduction In their daily practice, physicians must frequently decide a definitive diagnosis or the most suitable treatment using several variables from multiple clinical data sources, which is a highly complex task. A huge amount\n\n[4] Enabling Early Obstructive Sleep Apnea Diagnosis With Machine Learning: Systematic Review Enabling Early Obstructive Sleep Apnea Diagnosis With Machine Learning: Systematic Review Background American Academy of Sleep Medicine guidelines suggest that clinical prediction algorithms can be used to screen patients with obstructive sleep apnea (OSA) without replacing polysomnography, the gold standard. Objective We aimed to identify, gather, and analyze existing machine learning approaches that are being used for disease screening in adult patients with suspected OSA. Methods We searched the MEDLINE, Scopus, and ISI Web of Knowledge databases to evaluate the validity of different machine learning techniques, with polysomnography as the gold standard outcome measure and used the Prediction Model Risk of Bias Assessment Tool (Kleijnen Systematic Reviews Ltd) to assess risk of bias and applicability of each included study. Results Our search retrieved 5479 articles, of which 63 (1.15%) articles were included. We found 23 studies performing diagnostic model development alone, 26 with added internal validation, and 14 applying the clinical prediction algorithm to an independent sample (although not all reporting the most common discrimination metrics, sensitivity or specificity). Logistic regression was applied in 35 studies, linear regression in 16, support vector machine in 9, neural networks in 8, decision trees in 6, and Bayesian networks in 4. Random forest, discriminant analysis, classification and regression tree, and nomogram were each performed in 2 studies, whereas Pearson correlation, adaptive neuro-fuzzy inference system, artificial immune recognition system, genetic algorithm, supersparse linear integer models, and k-nearest neighbors algorithm were each performed in 1 study. The best area under the receiver operating curve was 0.98 (0.96-0.99) for age, waist circumference,\n\n[5] AI vs Humans for the diagnosis of sleep apnea Polysomnography (PSG) is the gold standard for diagnosing sleep obstructive apnea (OSA). It allows monitoring of breathing events throughout the night. The detection of these events is usually done by trained sleep experts. However, this task is tedious, highly time-consuming and subject to important inter-scorer variability. In this study, we adapted our state-of-the-art deep learning method for sleep event detection, DOSED, to the detection of sleep breathing events in PSG for the diagnosis of OSA. We used a dataset of 52 PSG recordings with apnea-hypopnea event scoring from 5 trained sleep experts. We assessed the performance of the automatic approach and compared it to the inter-scorer performance for both the diagnosis of OSA severity and, at the microscale, for the detection of single breathing events. We observed that human sleep experts reached an average accuracy of 75% while the automatic approach reached 81% for sleep apnea severity diagnosis. The F1 score for individual event detection was 0.55 for experts and 0.57 for the automatic approach, on average. These results demonstrate that the automatic approach can perform at a sleep expert level for the diagnosis of OSA.\n\n[6] Machine Learning Identification of Obstructive Sleep Apnea Severity through the Patient Clinical Features: A Retrospective Study Objectives: To evaluate the role of clinical scores assessing the risk of disease severity in patients with clinical suspicion of obstructive sleep apnea syndrome (OSA). The hypothesis was tested by applying artificial intelligence (AI) to demonstrate its effectiveness in distinguishing between mild–moderate OSA and severe OSA risk. Methods: A support vector machine model (SVM) was developed from the samples included in the analysis (N = 498), and they were split into 75% for training (N = 373) with the remaining for testing (N = 125). Two diagnostic thresholds were selected for OSA severity: mild to moderate (apnea–hypopnea index (AHI) ≥ 5 events/h and AHI < 30 events/h) and severe (AHI ≥ 30 events/h). The algorithms were trained and tested to predict OSA patient severity. Results: The sensitivity and specificity for the SVM model were 0.93 and 0.80 with an accuracy of 0.86; instead, the logistic regression full mode reported a value of 0.74 and 0.63, respectively, with an accuracy of 0.68. After backward stepwise elimination for features selection, the reduced logistic regression model demonstrated a sensitivity and specificity of 0.79 and 0.56, respectively, and an accuracy of 0.67. Conclusion: Artificial intelligence could be applied to patients with symptoms related to OSA to identify individuals with a severe OSA risk with clinical-based algorithms in the OSA framework.\n\n[7] Enabling Early Obstructive Sleep Apnea Diagnosis With Machine Learning: Systematic Review Background American Academy of Sleep Medicine guidelines suggest that clinical prediction algorithms can be used to screen patients with obstructive sleep apnea (OSA) without replacing polysomnography, the gold standard. Objective We aimed to identify, gather, and analyze existing machine learning approaches that are being used for disease screening in adult patients with suspected OSA. Methods We searched the MEDLINE, Scopus, and ISI Web of Knowledge databases to evaluate the validity of different machine learning techniques, with polysomnography as the gold standard outcome measure and used the Prediction Model Risk of Bias Assessment Tool (Kleijnen Systematic Reviews Ltd) to assess risk of bias and applicability of each included study. Results Our search retrieved 5479 articles, of which 63 (1.15%) articles were included. We found 23 studies performing diagnostic model development alone, 26 with added internal validation, and 14 applying the clinical prediction algorithm to an independent sample (although not all reporting the most common discrimination metrics, sensitivity or specificity). Logistic regression was applied in 35 studies, linear regression in 16, support vector machine in 9, neural networks in 8, decision trees in 6, and Bayesian networks in 4. Random forest, discriminant analysis, classification and regression tree, and nomogram were each performed in 2 studies, whereas Pearson correlation, adaptive neuro-fuzzy inference system, artificial immune recognition system, genetic algorithm, supersparse linear integer models, and k-nearest neighbors algorithm were each performed in 1 study. The best area under the receiver operating curve was 0.98 (0.96-0.99) for age, waist circumference, Epworth Somnolence Scale score, and oxygen saturation as predictors in a logistic regression. Conclusions Although high values were obtained, they still lacked external validation results in large cohorts and a standard OSA criteria definition. Trial Registration PROSPERO CRD42021221339; https://www.crd.york.ac.uk/prospero/display_record.php?RecordID=221339\n\n[8] Diagnosis of Obstructive Sleep Apnea from ECG Signals Using Machine Learning and Deep Learning Classifiers Obstructive sleep apnea (OSA) is a well-known sleep ailment. OSA mostly occurs due to the shortage of oxygen for the human body, which causes several symptoms (i.e., low concentration, daytime sleepiness, and irritability). Discovering the existence of OSA at an early stage can save lives and reduce the cost of treatment. The computer-aided diagnosis (CAD) system can quickly detect OSA by examining the electrocardiogram (ECG) signals. Over-serving ECG using a visual procedure is challenging for physicians, time-consuming, expensive, and subjective. In general, automated detection of the ECG signal’s arrhythmia is a complex task due to the complexity of the data quantity and clinical content. Moreover, ECG signals are usually affected by noise (i.e., patient movement and disturbances generated by electric devices or infrastructure), which reduces the quality of the collected data. Machine learning (ML) and Deep Learning (DL) gain a higher interest in health care systems due to its ability of achieving an excellent performance compared to traditional classifiers. We propose a CAD system to diagnose apnea events based on ECG in an automated way in this work. The proposed system follows the following steps: (1) remove noise from the ECG signal using a Notch filter. (2) extract nine features from the ECG signal (3) use thirteen ML and four types of DL models for the diagnosis of sleep apnea. The experimental results show that our proposed approach offers a good performance of DL classifiers to detect OSA. The proposed model achieves an accuracy of 86.25% in the validation stage.\n\n[9] Single Sensor Techniques for Sleep Apnea Diagnosis Using Deep Learning A large number of obstructive sleep apnea (OSA) cases are under-diagnosed due unavailability, inconvenience or expense of sleep labs. Hence, an automated detection by applying computational techniques to multivariate signals has already become a well-researched subject. However, the best-known techniques that use various features have not achieved the gold standard of polysomnography (PSG) tests. In this paper, we substantiate the medical conjecture that OSA directly impacts body parameters such as Instantaneous Heart Rate (IHR) and blood oxygen saturation (SpO2). We then use a deep learning technique called LSTM-RNN (long short-term memory recurrent neural networks) to experimentally prove that OSA severity detection can be solely based on either IHR or SpO2 signals, which can be easily, obtained using off-the-shelf non-intrusive wearable single sensors. The results obtained from LSTM-RNN model shows an area under curve (AUC) of 0.98 associated with very high accuracy on a dataset of more than 16,000 apnea non-apnea minutes. These results have encouraged our collaborating doctors to further come up with a diagnostic protocol that is based on LSTM-RNN, SpO2, and IHR, thereby increasing the chances of larger adoption among medical community.\n\n[10] Deep learning enables sleep staging from photoplethysmogram for patients with suspected sleep apnea Abstract Study Objectives Accurate identification of sleep stages is essential in the diagnosis of sleep disorders (e.g. obstructive sleep apnea [OSA) but relies on labor-intensive electroencephalogram (EEG)-based manual scoring. Furthermore, long-term assessment of sleep relies on actigraphy differentiating only between wake and sleep periods without identifying specific sleep stages and having low reliability in identifying wake periods after sleep onset. To address these issues, we aimed to develop an automatic method for identifying the sleep stages from the photoplethysmogram (PPG) signal obtained with a simple finger pulse oximeter. Methods PPG signals from the diagnostic polysomnographies of susptected OSA patients (n = 894) were utilized to develop a combined convolutional and recurrent neural network. The deep learning model was trained individually for three-stage (wake/NREM/REM), four-stage (wake/N1+N2/N3/REM), and five-stage (wake/N1/N2/N3/REM) classification of sleep. Results The three-stage model achieved an epoch-by-epoch accuracy of 80.1% with Cohen’s κ of 0.65. The four- and five-stage models achieved 68.5% (κ = 0.54), and 64.1% (κ = 0.51) accuracies, respectively. With the five-stage model, the total sleep time was underestimated with a mean bias error (SD) of of 7.5 (55.2) minutes. Conclusion The PPG-based deep learning model enabled accurate estimation of sleep time and differentiation between sleep stages with a moderate agreement to manual EEG-based scoring. As PPG is already included in ambulatory polygraphic recordings, applying the PPG-based sleep staging could improve their diagnostic value by enabling simple, low-cost, and reliable monitoring of sleep and help assess otherwise overlooked conditions such as REM-related OSA.\n\n[11] Single Channel ECG for Obstructive Sleep Apnea Severity Detection Using a Deep Learning Approach Obstructive sleep apnea (OSA) is a common sleep disorder caused by abnormal breathing. The severity of OSA can lead to many symptoms such as sudden cardiac death (SCD). Polysomnography (PSG) is a gold standard for OSA diagnosis. It records many signals from the patient’s body for at least one whole night and calculates the Apnea-Hypopnea Index (AHI) which is the number of apnea or hypopnea incidences per hour. This value is then used to classify patients into OSA severity levels. However, it has many disadvantages and limitations. Consequently, we proposed a novel methodology of OSA severity classification using a Deep Learning approach. We focused on the classification between normal subjects (AHI < 5) and severe OSA patients (AHI > 30). The 15-second raw ECG records with apnea or hypopnea events were used with a series of one-dimensional Convolutional Neural Networks (1-D CNN) for automatic feature extraction, deep recurrent neural networks with Long Short-Term Memory (LSTM) for temporal information extraction, and fully-connected neural networks (DNN) for feature encoding from a large number of features until it closed to two classes. The main advantages of our proposed method include easier data acquisition, instantaneous OSA severity detection, and effective feature extraction without domain knowledge from expertise. To evaluate our proposed method, 545 subjects of which 364 were normal and 181 were severe OSA patients obtained from the MrOS sleep study (Visit 1) database were used with the k-fold cross-validation technique. The accuracy of 79.45% for OSA severity classification with sensitivity, specificity, and F-score was achieved. This is significantly higher than the results from the SVM classifier with RR Intervals and ECG derived respiration (EDR) signal feature extraction. The promising result shows that this proposed method is a good start for the detection of OSA severity from a single channel ECG which can be obtained from wearable devices at home and can also be applied to near real-time alerting systems such as before SCD occurs.\n\n[12] Artificial Intelligence-based Remote Diagnosis of Sleep Apnea using Instantaneous Heart Rates Artificial Intelligence-based Remote Diagnosis of Sleep Apnea using Instantaneous Heart Rates Prolonged Sleep Apnea is a sleeping disorder that can cause arrhythmia, hypertension, and other serious health conditions leading to cardiovascular diseases and fatal strokes. Most widely used current clinical techniques for sleep apnea diagnosis are expensive, time-consuming, and cannot be performed remotely. Wearable watch-style health trackers continuously track sleep behavior, physiological data, and physical activity that can enable real-time remote diagnosis of sleep apnea. Recently, the application of Artificial Intelligence (AI) techniques within the field of medicine and remote diagnosis is gaining popularity. In this paper, several Artificial Intelligence (AI) models have been trained and tested to classify sleep apnea condition in real-time using sequential data of Instantaneous Heart Rates (IHR). Using the confusion matrix, the accuracy, precision, recall, specificity, FI Score, sensitivity, and area under the receiver operating characteristic curve of each model are computed and compared. The Bi-directional Long Short-Term Memory (LSTM) was found to be the best AI technique for classifying sleep apnea. The approach depicted in this study for diagnosing sleep apnea can allow the telemedicine, telehealth, and mHealth applications to detect several health risk factors in real-time using data streaming from the health trackers.\n\n"}
{"case_id": "99fd4c1b54e1367bcd3cb3b243c2cc19", "answer_text": "\n\nThe integration of Geographic Information System (GIS) spatial analysis techniques with economic modeling has shown to significantly improve the accuracy and effectiveness of environmental policy [1]. This synergistic approach leverages the strengths of both methodologies—GIS's spatial data capabilities and economic modeling's ability to analyze and predict economic behaviors and outcomes [13].\n\nFirstly, GIS provides powerful tools for the spatial representation of environmental data, which is essential for assessing the spatial dimensions of environmental policies. For instance, GIS's ability to integrate and visualize spatial and temporal data sets allows for a more comprehensive analysis of environmental impacts [1][2]. This integration facilitates the identification of spatial patterns and trends that might not be evident from non-spatial data alone. For example, in Victoria, Australia, a geocomputational system was developed to generate new representations of environmental policy outcomes, promoting a broader and more informed discussion during the decision-making process [1].\n\nEconomic modeling, when integrated with GIS, allows for the simulation and prediction of economic behaviors and their environmental impacts. For instance, the integration of the Soil and Water Assessment Tool (SWAT) with GIS enabled the spatial targeting of agri-environmental policies in the Calapooia watershed in Oregon, USA. This integration provided a detailed analysis of the environmental effects of policy measures, facilitating more effective policy designs [5]. Additionally, the integration of microeconomic models with spatially referenced geophysical data using GIS has been employed to assess the impact of agricultural policies on the environment. This methodology allows for the production and organization of data at a geographical level, which is crucial for agri-environmental analysis [12].\n\nMoreover, GIS-based spatial analysis can enhance the accuracy of environmental assessments by incorporating real-time data and enabling dynamic modeling. For instance, the use of Geographically-Weighted Regression (GWR) techniques in urban environmental quality assessments in Ontario, Canada, improved precision and accuracy by up to 20% compared to traditional GIS overlay and Principal Component Analysis (PCA) techniques [3]. Such improvements are vital for policies aimed at urban planning and environmental justice, as they provide more reliable data for decision-makers.\n\nAnother key advantage of integrating GIS with economic modeling is the ability to handle and analyze large-scale, complex datasets. For example, remote sensing combined with GIS can enhance the extraction of information from satellite imagery, leading to more accurate environmental models and assessments. This approach was demonstrated in the United Arab Emirates, where GIS and remote sensing were used to model various environmental issues, including urban growth and waste management [7]. The integration of hydrodynamic and water quality models with GIS in Chaohu City, China, also illustrated how such combined methodologies can provide more efficient and accurate decision support systems for environmental management [13].\n\nFurthermore, integrating GIS with economic modeling enables the consideration of spatial heterogeneity in policy analyses. This is crucial for accurately assessing the differential impacts of policies across different regions. For example, the evaluation of policies for drinking water protection using a spatial approach revealed that policy measures could have combined effects, influencing each other's cost-effectiveness [8]. Similarly, integrating GIS into travel cost analysis and benefit transfer in environmental economics allows for more consistent and spatially aware natural resource valuation [9].\n\nBeyond these examples, the integration extends to urban planning, where multi-methodological approaches like Integrated Spatial Assessment (ISA) help create sustainable land use patterns and predict future land use conflicts among urban, agricultural, and conservation areas [14]. Multi-criteria analysis integrated within GIS has also proven essential in territorial and urban planning, helping to reduce environmental and socio-economic impacts [6].\n\nHowever, several limitations and challenges remain. One significant challenge is the integration of different types of data, which can vary widely in terms of scale, resolution, and accuracy [11]. Inconsistencies and gaps in data can pose obstacles to creating reliable models. The need for high-quality, up-to-date data can also be resource-intensive, requiring significant time and financial investment. Additionally, there is the complexity of creating and maintaining integrated systems that are user-friendly and accessible to decision-makers who might not have technical expertise in both GIS and economic modeling [1]. \n\nAnother limitation is the computational intensity of processing large spatial datasets, which can require advanced computational resources and expertise [2]. Furthermore, differences between economic and environmental models in terms of their underlying assumptions and methodologies can create challenges in achieving seamless integration [11]. \n\nIn conclusion, while the integration of GIS spatial analysis techniques with economic modeling enhances the accuracy and effectiveness of environmental policy by providing detailed spatial representations [7], improving data accuracy and dynamic modeling [8], facilitating the handling of complex datasets [12], and recognizing spatial heterogeneity in policy impacts [13], it is essential to address the challenges and limitations to fully realize the potential of these combined methodologies. Effective integration supports more informed decision-making processes, ultimately leading to more effective and sustainable environmental policies. \n\n\nReferences:\n[0] Spatial and temporal representation of environmental policy outcomes using geocomputation: a case study in Victoria, Australia Spatial and temporal representation of environmental policy outcomes using geocomputation: a case study in Victoria, Australia For more than 30 years, cost-benefit analysis (CBA) has been used in many countries as an important tool for evaluating public policies. In most cases CBA uses KaldorHicks theory, which assumes that all the entities in the analysis should be considered equal when distributing benefits and costs. However, for environmental policy evaluations, and especially for cases where a policy is directly affecting a community or ecosystem with a high degree of vulnerability, the Kaldor-Hicks theory may not be in concordance with principals of equity and sustainability. In order to incorporate these two concepts in the Cost-Benefit Analysis, a new approach to presentation of environmental indicators for decision-making is proposed. The use of spatio-temporal environmental and social indicators provides the decision-maker with a broader picture of positive and negative impacts of the policy by considering entities in the analysis in a differential form, according to their location in the space-time. A geocomputational (GC) system, which incorporates a space-time model and a geographic information system (GIS), was created and designed to generate new representations of environmental policy outcomes. The system, which could be considered as a decision support system, has been designed to promote discussion in the decision process rather than as a technocratic tool. The indicators and computer-based system have been tested in a water resources case study with long-term impacts over a large area of south-eastern Australia.\n\n[1] GIS as a vital tool for Environmental Impact Assessment and Mitigation updated. In addition, the GIS and Environmental Data integration also need to visualize data, explore and report information, model outputs, and other records that change the response workflow. More importantly, the relationship of GIS and Environmental Data integration will provide for realtime analysis, and the complex permutation of spatial and temporal data sets. This unique GIS and Environmental Data Integration are exemplified in figure 2. As it can be noticed, the receptive method of GIS and Environmental Data Integration can be categorized based on:- Spatial Capabilities: including map's integration, and data amalgamation enabling spatial representation.  Environmental Monitoring and Data Management: monitor and regulate information to reduce the ecological risk, and better application of monitoring programs.  Accessibility with Security: access, validated and protect historical data and allow approved individuals to monitoring programs and admittance the stored data sites.  Auditing and Compliance: permitting the environmental data, including historical for the auditing purposes as required by the specific jurisdictions. A key aspect of the GIS and Environmental Data Integration is the ability to further navigate, modify, calculate data and store specific technical information, including precise schematics and photographs via customized system dashboards. This customized system dashboards will not only entail natural environment information such as Hydrography and soil geology but also Flora and Fauna facts, thus creating a responsive approach. Nevertheless, GIS will further assist with improving the EIA through careful and optimised amalgamation of ecosystem specific information such as any short and long-term deterioration evaluation and assessment. Accordingly, such evaluation requires effective environmental strategy as\n\n[2] Improving the Accuracy of Urban Environmental Quality Assessment Using Geographically-Weighted Regression Techniques Improving the Accuracy of Urban Environmental Quality Assessment Using Geographically-Weighted Regression Techniques Urban Environmental Quality (UEQ) can be treated as a generic indicator that objectively represents the physical and socio-economic condition of the urban and built environment. The value of UEQ illustrates a sense of satisfaction to its population through assessing different environmental, urban and socio-economic parameters. This paper elucidates the use of the Geographic Information System (GIS), Principal Component Analysis (PCA) and Geographically-Weighted Regression (GWR) techniques to integrate various parameters and estimate the UEQ of two major cities in Ontario, Canada. Remote sensing, GIS and census data were first obtained to derive various environmental, urban and socio-economic parameters. The aforementioned techniques were used to integrate all of these environmental, urban and socio-economic parameters. Three key indicators, including family income, higher level of education and land value, were used as a reference to validate the outcomes derived from the integration techniques. The results were evaluated by assessing the relationship between the extracted UEQ results and the reference layers. Initial findings showed that the GWR with the spatial lag model represents an improved precision and accuracy by up to 20% with respect to those derived by using GIS overlay and PCA techniques for the City of Toronto and the City of Ottawa. The findings of the research can help the authorities and decision makers to understand the empirical relationships among environmental factors, urban morphology and real estate and decide for more environmental justice. Introduction The terminology \"quality of life\" has been continuously discussed in the literature, so\n\n[3] Spatial Targeting of Agri-Environmental Policy Using Bilevel Evolutionary Optimization the Soil and Water Assessment Tool (SWAT) to provide the information about environmental effects required by the agency to specify their objectives. We applied the resulting integrated modeling system to the analysis of an incentive policy in the Calapooia watershed in Oregon, USA, using synthetic economic data generated from the Census of Agriculture. Through bilevel optimization, we were able to spatially target agri-environmental policy to find multiple objective Pareto frontiers that dominate those available from other methods.\n\n[4] Socio-Environmental Vulnerability Index: An Application to Rio de Janeiro-Brazil very low socio-environmental vulnerability ( Figure 5). DISCUSSION According to Malczewski and Ogryczak, since the 1990s the use of the multi-criteria analysis methodology has been growing in territorial and urban planning. Currently, multi-criteria analysis is increasingly integrated into GISs, creating a robust tool to aid spatial analysis processes through modelling and to support decision-making in issues with spatial distribution and consequences. Efforts to integrate GIS and multi-criteria analysis in the late 1980s and early 1990s may be associated with increasing GIS development. Sharifi et al. also claim that the integration of GIS and multi-criteria analysis provides an important methodology in the creation of options to reduce environmental and socio-economic impacts, as well as to assess and solve these impacts in the territory. The integration methodology between GIS and multi-criteria analysis used in the SEVI calculation corroborates other studies, where the author states that the integration between multicriteria decision-making techniques and SIGs represents a considerable advance in spatial analysis involving urban planning, compared to the conventional map overlay approaches. The importance of this methodology can be seen from the growing number of publications in the most diverse areas and regions of the world. Examples include the studies: where this methodology is used in land use planning in Switzerland; when identifying sites for the construction of sanitary landfills in the Lake Beys region of Turkey; in the construction of a flood risk map in Terengganu, Malaysia; in locating potential sites for ecotourism in Kenya; when identifying suitable regions for urban\n\n[5] Modeling the Environment with Remote Sensing and GIS: Applied Case Studies from Diverse Locations of the United Arab Emirates (UAE) where lack of accurate maps and the need for rapid and relatively accurate mapping techniques are urgent; this is becoming challenging if we know the dimension of large-scale engineering projects being implemented, particularly in the wealthy Gulf States,4. Remote sensing and GIS are promising new-time and cost-effective techniques to image remote arid and hyperarid lands. With the use of ancillary field data and the calibration of remote sensing inputs, data integration within a GIS can enhance the extraction of information from satellite imagery and has led to a synergistic approach in spatial data handling and modeling, hence improving the accuracy of a variety of outputs,9. In this chapter, we will expose the power and benefit of integrating remote sensing and geographic information systems to model our environment through various case studies applied to the arid/hyper arid environment of the United Arab Emirates. Four case studies are introduced and discussed in Section 3 namely: • Characterization of Al Ain city urban growth using multi-temporal remote sensing data and GIS. • Assessing landfill locations for waste management for the city of Abu Dhabi using GIS. Research approach Remote sensing and GIS are incorporated into environmental modeling for addressing environmental issues and problems. The core of this approach is to use the power embedded in these geospatial techniques to develop and implement a GIS project. Remote sensing here is treated as the science, technology, and techniques used to acquire the wanted data concerning the study area, processing those data, extracting relevant information about the\n\n[6] Cross-achievements between policies for drinking water protection. Cross-achievements between policies for drinking water protection. Environmental dynamics have important spatial dimensions, which calls for a spatial approach in policy analyses. Further to this, assessing agri-environmental policies involves analyses of individual measures as well as their combined effects on farmer behaviour and the environment. The integration of an economic behavioural model in a spatial framework has enabled analyses of a geographically targeted subsidy scheme for drinking water protection in combination with a uniform tax on commercial nitrogen fertilizer. Results show that policy measures for reducing nitrogen use can have combined effects (cross-achievements), thereby affecting each other's cost-effectiveness. Cross-achievements between a nitrogen fertilizer tax and a subsidy scheme based on elicitation are shown not to be additive, making partial analyses of policy measures more uncertain.\n\n[7] Integrating geographical information systems into travel cost analysis and benefit transfer Integrating geographical information systems into travel cost analysis and benefit transfer Environmental economists usually pay little attention to inherently spatial aspects of much of their work. Such analyses also tend to suffer from considerable inefficiencies in data handling. This paper demonstrates how use of GIS and appreciation of the nature of spatial datasets can greatly facilitate natural resource valuation using the technique of travel cost analysis. This work forms the basis for tackling an important research topic in environmental economics: the feasibility of transferring benefits between studies. Using GIS allowed us to implement such transfer with greater ease, consistency, and recognition of the spatial aspects of study design and variable handling than is normally seen in such research.\n\n[8] Using Integrated Economic and Ecological Information to Improve Government Policy Using Integrated Economic and Ecological Information to Improve Government Policy The potential of geographical information systems for strategic policy analysis is examined and demonstrated by example. One of the necessary conditions for this potential to be realised is the development of integrated data sets that combine economic, production and physical data sets at similar scales. The paper illustrates this potential using examples from a State-wide Resource Information and Accounting System developed by CSIRO.\n\n[9] Microeconomic and Geo-physical Data Integration for Agri-environmental Analysis, Georeferencing FADN Data: A Case Study in Italy Microeconomic and Geo-physical Data Integration for Agri-environmental Analysis, Georeferencing FADN Data: A Case Study in Italy The authors, inside a TAPAS action, have developed a methodology - based on the integration between FADN data with statistical, administrative and cartographic information - to improve the production of statistical data related to agricultural policies impact on land and environment. This methodology allows - using Geographic Information System (GIS) technology - to produce and organise data at geographical level. Data spatially referenced respond to the specific needs of agri-environmental analysis and problems, mostly related to specific areas (environmental vulnerability areas), inside defined boundaries (e.g. river basin). The GIS is implemented with several cartographic layers (Topographic and cadastral maps, land use, soils, water sources, climate, Digital Terrain Model etc.) and is related to the regional FADN database.\n\n[10] CBR-based integration of a hydrodynamic and water quality model and GIS—a case study of Chaohu City CBR-based integration of a hydrodynamic and water quality model and GIS—a case study of Chaohu City Monitoring on urban water environment and analysis of engineering improvement measures are intricate and time-consuming tasks. In previous studies, the integration of hydrodynamic and water quality models and geographical information system (GIS) usually takes three approaches: loose coupling, tight coupling, and full coupling. However, this paper adopted a special loose coupling approach—case-based reasoning (CBR) to develop an integrated decision support system. This was characterized by invoking the case base stored in the GIS platform as the output of the model. The fused capability of model’s water quality predication and strong spatial data processing analysis of GIS can be realized at the same time by integration. The functionality of the integrated system was illustrated through a case study of Chaohu, a medium-sized city in China, which includes case retrieval, result interpretation, and the visual display in the GIS platform. Results verified the feasibility and operability of the developed method. As a useful tool, the integrated decision support system makes it simpler and more convenient for decision makers to make decisions efficiently and quickly. Introduction With the accelerated urbanization progress, water environmental problems have become increasingly prominent. In order to understand the actual situation of water environment, a more suitable method is required to simulate and predict. Mathematical models of water environment have a strong ability to calculate and predict complex water environment (Bermúdez and García-García 2012). However, such complex models generally need a large number of geographically referenced information, and thus\n\n[11] Integrated Spatial Assessment (ISA): A Multi-Methodological Approach for Planning Choices way for developing more sustainable patterns of land use, producing a spatial representation of probable patterns of future land use for the following categories: existing conservation lands, existing urban lands, existing agricultural lands, areas for future conservation land use, areas for future urban land use, areas of probable future conflict between agricultural and conservation land uses, areas of probable future conflict between agricultural and urban land uses, areas of probably future conflict between conservation and urban land uses, areas of probable future conflict among agricultural, conservation and urban land. What if? In order to explore possible futures for a community What if? is an easy-to-use GIS-based Planning Support System (PSS) (Klosterman, 2001), that can be implemented to prepare long-term land use, population, housing and employment projections, political jurisdictions, and user-defined areas such as school districts, and traffic analysis zones ( fig. 7). The Fig. 7. Example of What if? application (source: Brail, 2008) www.intechopen.com package is easy to use, customized to the user's GIS data and policy issues, and provides outputs in easy-to-understand maps and tables. Indeed, What if? can be used to prepare long-term land use, population, and employment projections for census tracts and userdefined areas such as political jurisdictions and traffic analysis zones. It allows users to determine quickly and easily the impacts of alternative policies to control urban growth, preserving agricultural land, or expanding public infrastructure in easy-to-understand m a p s a n d t a b l e s . W h a t i f ? h a s b\n\n"}
{"case_id": "00bdd80debc8549198001289188c6bea", "answer_text": "\n\nHybrid approaches combine the strengths of both ontology-based and machine learning techniques to improve text summarization [0]. Ontologies provide a structured framework for understanding the relationships and semantics of terms within a text [4], while machine learning methods, particularly deep learning, offer powerful tools for pattern recognition and generation [6]. Here is a detailed breakdown of how these two approaches can be combined for effective text summarization:\n\nOntology-Based Approaches[1]\n\nOntology-driven methods involve the use of ontological structures to enhance the semantic understanding of a text [4]. Ontologies represent a domain in a hierarchical manner, capturing both the entities and their relationships [4]. \n\n1. **Ontology-Based Sentence Mapping**: One approach is to map sentences to nodes of a hierarchical ontology. This mapping can improve the semantic representation of a sentence by considering ontology attributes, thereby enhancing extractive summarization tasks [1][4]. For instance, a classifier can be trained to identify important sentences using ontology-based features, and such methods have shown to outperform baseline classifiers with higher Rouge scores for summary extracts [1][4].\n\n2. **Aspect Extraction and Sentiment Analysis**: Ontologies can also be used to identify aspects and features within a text, particularly useful in domain-specific summarizations like product reviews [12]. Ontology-based aspect extraction can identify implicit entities that lexicon-based methods might miss, thereby improving the accuracy of sentiment analysis and subsequent summarization [12].\n\nMachine Learning Approaches\n\nMachine learning, particularly deep learning, offers robust methods for handling large datasets and generating summaries [6].\n\n1. **Encoder-Decoder Architectures**: These architectures, widely used in neural machine translation, can also be applied to text summarization [2][3][7]. The encoder learns representations of the source text, while the decoder generates a summary. Models like GypSum use both graph-based neural networks and pre-trained models (e.g., CodeBERT) to learn hybrid representations that capture various aspects of the text [7].\n\n2. **Hybrid Deep Neural Networks**: Hybrid models often combine extractive and abstractive summarization methods. For example, a system might first use an extractive model to identify important sentences and then use an abstractive model to generate a coherent summary from these extracted sentences [10][11]. This combination leverages the precision of extractive methods and the flexibility of abstractive methods [10][11].\n\nIntegration of Ontology and Machine Learning [5]. \n\n1. **Pre-processing with Ontologies**: Before feeding text into a machine learning model, a pre-processing step can involve transforming the text using ontological resources. Word sense disambiguation and named entity recognition can be used to convert ordinary text into a more generalized form [0]. This helps in dealing with out-of-vocabulary words and improves the performance of neural models [0].\n\n2. **Hybrid Models**: Some approaches explicitly combine ontology-based methods with machine learning in a hybrid model. For instance, a hybrid ontology-based information extraction system can use extraction rules and machine learning extractors to identify both correct and incorrect statements within a text [5]. This method can be adapted to enhance text summarization by identifying key points and structuring them coherently [5].\n\n3. **Rule-based and Machine Learning Integration**: Combining rule-based systems with machine learning can also be effective. A preliminary knowledge base created from manual rules, supported by data, can specify the contexts where different summarization techniques are best applied. Such hybrid models have been shown to outperform competitive baselines in specific contexts like legal document summarization [13].\n\n### Case Studies and Models [0][2][14] \n\n1. **HTS-DL Framework**: The Hybrid Text Summarization System using Deep Learning (HTS-DL) combines extractive and abstractive summarization methods [6]. The encoder extracts relevant content, which the decoder then uses to generate a summary with acceptable semantic and syntactic structure. The hybrid nature of HTS-DL improves abstractive summarization performance and training efficiency [6].\n\n2. **GypSum Model**: This model uses Graph Attention Neural Networks (GAT) and pre-trained natural language models like CodeBERT to learn hybrid representations. GypSum introduces particular edges related to control flow into the abstract syntax tree and employs dual-copy mechanisms for effective summary generation. It has shown superior performance over existing code summarization models by effectively capturing the core functionality of code snippets [7].\n\nPerformance and Real-World Applications\n\n1. **Performance Improvements**: Hybrid models combining ontology and machine learning approaches have demonstrated superior performance metrics in several evaluation studies. For instance, in the experiment involving ontology-based extraction, the classifier trained with ontology features achieved higher Rouge scores compared to baseline models [1][4]. Similarly, the GypSum model outperformed existing code summarization models by effectively capturing the core functionality of code snippets [7].\n\n2. **Real-World Applications**: Hybrid summarization systems are being successfully applied in various domains. For example, the HTS-DL framework has been used to summarize multi-sentence documents without language dependency, making it versatile for multilingual applications [6]. Ontology-based aspect extraction has been particularly effective in summarizing product reviews by identifying implicit features and improving sentiment analysis accuracy [12].\n\n### Limitations and Challenges\n\nIn recent years, various methods for automatic text summarization have been proposed, combining both machine learning and ontology-based approaches to enhance summarization accuracy and efficiency [0]. Additionally, hybrid models that integrate extractive and abstractive summarization methods have shown promising results in creating coherent and informative summaries [6]. Despite advancements, challenges remain, particularly in handling out-of-vocabulary words and maintaining the semantic integrity of the summarized text [0]. Future work needs to address these issues by further refining hybrid methodologies and integrating more sophisticated ontological knowledge to improve the robustness and reliability of summarization systems [0].\n\n\nWhile hybrid approaches offer significant advantages, they also come with limitations and challenges [10].\n\n1. **Complexity in Integration**: The integration of ontology-based methods with machine learning models can be complex and requires significant computational resources. Ensuring seamless interoperability between the structured, rule-based ontology systems and flexible, learning-based machine learning models is a challenge [14].\n\n2. **Domain-Specific Ontologies**: The effectiveness of ontology-based methods is highly dependent on the quality and comprehensiveness of the ontology itself. Creating and maintaining domain-specific ontologies can be labor-intensive and may not be feasible for all domains [5].\n\n3. **Scalability**: Scalability can be an issue when dealing with very large datasets or real-time applications. The hybrid models need to be optimized for performance to handle large volumes of data efficiently [1].\n\n4. **Data Annotation and Pre-processing**: The pre-processing steps, including word sense disambiguation and named entity recognition, require high-quality annotated data and can be time-consuming [0]. Inaccurate pre-processing can adversely affect the performance of the subsequent summarization models [0]. \n\nIn summary, hybridizing ontology and machine learning for text summarization leverages the structured knowledge and semantic understanding provided by ontologies with the powerful pattern recognition and generation capabilities of machine learning models [0]. The combination can significantly enhance the summarization process, making it more accurate and semantically meaningful, but it also requires careful consideration of the associated challenges. [1] \n\n\nReferences:\n[0] Abstractive Text Summarization: Enhancing Sequence-to-Sequence Models Using Word Sense Disambiguation and Semantic Content Generalization Abstract Nowadays, most research conducted in the field of abstractive text summarization focuses on neural-based models alone, without considering their combination with knowledge-based approaches that could further enhance their efficiency. In this direction, this work presents a novel framework that combines sequence-to-sequence neural-based text summarization along with structure and semantic-based methodologies. The proposed framework is capable of dealing with the problem of out-of-vocabulary or rare words, improving the performance of the deep learning models. The overall methodology is based on a well-defined theoretical model of knowledge-based content generalization and deep learning predictions for generating abstractive summaries. The framework is composed of three key elements: (i) a pre-processing task, (ii) a machine learning methodology, and (iii) a post-processing task. The pre-processing task is a knowledge-based approach, based on ontological knowledge resources, word sense disambiguation, and named entity recognition, along with content generalization, that transforms ordinary text into a generalized form. A deep learning model of attentive encoder-decoder architecture, which is expanded to enable a coping and coverage mechanism, as well as reinforcement learning and transformer-based architectures, is trained on a generalized version of text-summary pairs, learning to predict summaries in a generalized form. The post-processing task utilizes knowledge resources, word embeddings, word sense disambiguation, and heuristic algorithms based on text similarity methods in order to transform the generalized version of a predicted summary to a final, human-readable form. An extensive experimental procedure on three popular data sets evaluates key aspects of the proposed framework, while the obtained results exhibit promising performance, validating the robustness of the proposed approach.\n\n[1] An Ontology-Based Approach to Text Summarization Extractive text summarization aims to create a condensed version of one or more source documents by selecting the most informative sentences. Research in text summarization has therefore often focused on measures of the usefulness of sentences for a summary. We present an approach to sentence extraction that maps sentences to nodes of a hierarchical ontology. By considering ontology attributes we are able to improve the semantic representation of a sentence's information content. The classifier that maps sentences to the taxonomy is trained using search engines and is therefore very flexible and not bound to a specific domain. In our experiments, we train an SVM classifier to identify summary sentences using ontology-based sentence features. Our experimental results show that the ontology-based extraction of sentences outperforms baseline classifiers, leading to higher Rouge scores of summary extracts.\n\n[2] GypSum: Learning Hybrid Representations for Code Summarization The Graph Attention Neural Networks (Veličković et al., 2018) (GAT) recurrently update the state of a node by aggregating its own state and the states of all its neighbours at each step, using the attention mechanism. Denote by eisubscripte𝑖\\textbf{{e}}_{i} and ei​jsubscripte𝑖𝑗\\textbf{{e}}_{ij}, the embedding of node i𝑖i and the embedding of the edge from node j𝑗j to node i𝑖i. GAT updates the node states as follows:(3)hi(0)=eisuperscriptsubscripth𝑖0subscripte𝑖\\textbf{{h}}_{i}^{(0)}=\\textbf{{e}}_{i}(4)si​j(l)=LeakyReLU​(𝑾(l)⋅(𝑾​hi(l)⊕𝑾​hj(l)⊕𝑾e​ei​j))superscriptsubscript𝑠𝑖𝑗𝑙LeakyReLU⋅superscript𝑾𝑙direct-sum𝑾superscriptsubscripth𝑖𝑙𝑾superscriptsubscripth𝑗𝑙subscript𝑾𝑒subscripte𝑖𝑗s_{ij}^{(l)}=\\textsc{LeakyReLU}(\\bm{W}^{(l)}\\cdot(\\bm{W}\\textbf{{h}}_{i}^{(l)}\\oplus\\bm{W}\\textbf{{h}}_{j}^{(l)}\\oplus\\bm{W}_{e}\\textbf{{e}}_{ij}))(5)ai​j(l)=e​x​p​(si​j(l))∑k∈𝒩​(i)e​x​p​(si​k(l))superscriptsubscript𝑎𝑖𝑗𝑙𝑒𝑥𝑝superscriptsubscript𝑠𝑖𝑗𝑙subscript𝑘𝒩𝑖𝑒𝑥𝑝superscriptsubscript𝑠𝑖𝑘𝑙a_{ij}^{(l)}=\\frac{exp(s_{ij}^{(l)})}{\\sum_{k\\in\\mathcal{N}(i)}exp(s_{ik}^{(l)})}(6)hi(l+1)=∑j∈𝒩​(i)σ​(ai​j(l)⋅hj(l)),superscriptsubscripth𝑖𝑙1subscript𝑗𝒩𝑖𝜎⋅superscriptsubscript𝑎𝑖𝑗𝑙superscriptsubscripth𝑗𝑙\\textbf{{h}}_{i}^{(l+1)}=\\sum_{j\\in\\mathcal{N}(i)}\\sigma(a_{ij}^{(l)}\\cdot\\textbf{{h}}_{j}^{(l)}),where hi(l)superscriptsubscripth𝑖𝑙\\textbf{{h}}_{i}^{(l)} denotes the state of node i𝑖i at layer l𝑙l, 𝑾𝑾\\bm{W} and 𝑾esubscript𝑾𝑒\\bm{W}_{e} are the shared weight matrices for nodes and edges, 𝑾(l)superscript𝑾𝑙\\bm{W}^{(l)} is the weight matrix at layer l𝑙l, ai​j(l)superscriptsubscript𝑎𝑖𝑗𝑙a_{ij}^{(l)} denotes the attention weight of contribution from node j𝑗j to i𝑖i at layer l𝑙l. 𝒩​(i)𝒩𝑖\\mathcal{N}(i) denotes the neighour nodes of node i𝑖i.\n\n[3] GypSum: Learning Hybrid Representations for Code Summarization Then we compute the sampling distribution for generation and copying. For sampling from the vocabulary, the probability distribution is directly transformed from dtsubscriptd𝑡\\textbf{{d}}_{t} as follows:(11)pv​o​c=Softmax​(Wv​o​c⋅dt),subscriptp𝑣𝑜𝑐Softmax⋅subscriptW𝑣𝑜𝑐subscriptd𝑡\\textbf{{p}}_{voc}=\\textsc{Softmax}(\\textbf{{W}}_{voc}\\cdot\\textbf{{d}}_{t}),where Wv​o​c∈ℝ|v​o​c​a​b|×desubscriptW𝑣𝑜𝑐superscriptℝ𝑣𝑜𝑐𝑎𝑏subscript𝑑𝑒\\textbf{{W}}_{voc}\\in\\mathbb{R}^{|vocab|\\times d_{e}} denotes the parameter matrix. For copying, we can naturally use the last layer’s attention weights of dtsubscriptd𝑡\\textbf{{d}}_{t} to all the encodings produced by each encoder, respectively, as the copy distribution, which can be formulated as follows:(12)ac=Attn_Scorect,ag=Attn_Scoregt,formulae-sequencesubscripta𝑐superscriptsubscriptAttn_Score𝑐𝑡subscripta𝑔superscriptsubscriptAttn_Score𝑔𝑡\\begin{split}\\textbf{{a}}_{c}&=\\textsc{Attn\\_Score}_{c}^{t},\\\\\\textbf{{a}}_{g}&=\\textsc{Attn\\_Score}_{g}^{t},\\end{split}where acsubscripta𝑐\\textbf{{a}}_{c} and agsubscripta𝑔\\textbf{{a}}_{g} are the probability distribution of copying from the input source code tokens and the leaf nodes, respectively. Attn_ ScorectsuperscriptsubscriptScore𝑐𝑡\\textsc{Score}_{c}^{t} and Attn_ScoregtsuperscriptsubscriptAttn_Score𝑔𝑡\\textsc{Attn\\_Score}_{g}^{t} denote the attention weights of dtsubscriptd𝑡\\textbf{{d}}_{t} to all the encodings of the c-encoder and the g-encoder, respectively, in the last decoder layer.\n\n[4] An Ontology-Based Approach to Text Summarization An Ontology-Based Approach to Text Summarization Extractive text summarization aims to create a condensed version of one or more source documents by selecting the most informative sentences. Research in text summarization has therefore often focused on measures of the usefulness of sentences for a summary. We present an approach to sentence extraction that maps sentences to nodes of a hierarchical ontology. By considering ontology attributes we are able to improve the semantic representation of a sentence's information content. The classifier that maps sentences to the taxonomy is trained using search engines and is therefore very flexible and not bound to a specific domain. In our experiments, we train an SVM classifier to identify summary sentences using ontology-based sentence features. Our experimental results show that the ontology-based extraction of sentences outperforms baseline classifiers, leading to higher Rouge scores of summary extracts.\n\n[5] Hybrid Ontology-Based Information Extraction for Automated Text Grading Hybrid Ontology-Based Information Extraction for Automated Text Grading Although automatic text grading systems have reached an accuracy level comparable to human grading, with successful commercial and research implementations (e.g., Latent Semantic Analysis), these systems can provide limited feedback about which statements of the text are incorrect and why they are incorrect. In the present work, we propose the use of a hybrid Ontology-based Information Extraction (OBIE) system to identify both correct and incorrect statements by combining extraction rules and machine learning based information extractors. Experiments show that given 77 student answers to a Cell Biology final exam question, our hybrid system can identify both correct and incorrect statements with high precision and recall measures.\n\n[6] HTS-DL: Hybrid Text Summarization System using Deep Learning HTS-DL: Hybrid Text Summarization System using Deep Learning Abstractive text summarization is the task of creating a summary from a document by merging facts from different sources and make a short description of them. In this procedure, the meaning and the content information should be kept. In this paper, a hybrid summarization system using deep recurrent neural network is proposed, which can create new sentences by information extracted from the text The proposed model is the combination of extractive and abstractive summarization and has the encoder-decoder structure. The encoder extracts information from the source document and encodes this information in a compressed representation. The decoder takes the encoder’s output as input and generates a summary, which has an acceptable semantic and syntactic structure. Experimental results show that the proposed model could improve both the performance of abstractive summarization and the time of training. This model does the single-document multi-sentence summarization and does not have any dependency on language. Therefore, it can be used for other languages without any modification in future.\n\n[7] GypSum: Learning Hybrid Representations for Code Summarization Code summarization with deep learning has been widely studied in recent years. Current deep learning models for code summarization generally follow the principle in neural machine translation and adopt the encoder-decoder framework, where the encoder learns the semantic representations from source code and the decoder transforms the learnt representations into human-readable text that describes the functionality of code snippets. Despite they achieve the new state-of-the-art performance, we notice that current models often either generate less fluent summaries, or fail to capture the core functionality, since they usually focus on a single type of code representations. As such we propose GypSum, a new deep learning model that learns hybrid representations using graph attention neural networks and a pre-trained programming and natural language model. We introduce particular edges related to the control flow of a code snippet into the abstract syntax tree for graph construction, and design two encoders to learn from the graph and the token sequence of source code, respectively. We modify the encoder-decoder sublayer in the Transformer's decoder to fuse the representations and propose a dual-copy mechanism to facilitate summary generation. Experimental results demonstrate the superior performance of GypSum over existing code summarization models.\n\n[8] Towards Hybrid Model for Automatic Text Summarization Towards Hybrid Model for Automatic Text Summarization The overflowing of textual data on the web needs an efficient tool that is able to manage and process data. In this context, automatic text summarization has shown a great importance in several application areas. It aims to create a coherent and fluent short version of a document while preserving of the main information. This method allows for a reduction in reading time by condensing relevant information from a large collection of documents. Several automatic text summarization approaches have been proposed in order to entail shorten parts of the document. These methods have good results, but they still need improvements related to the reliability of sentences extraction, redundancy, semantic relationships between sentences, etc. This paper introduces a new hybrid architecture, combining a 2-layer recurrent neural network (RNN) extractive model and a sequence-to-sequence attentional abstractive model. This method uses the advantages of both extractive and abstractive approaches. A given text is first fed into the extractive model to obtain its relevant content, then generalized using the abstractive model, resulting in the text’s summary. First experimental results on real-world data show that the proposed model can achieve competitive results for extractive, abstractive and hybrid models.\n\n[9] A Hybrid Approach of Text Summarization Using Latent Semantic Analysis and Deep Learning A Hybrid Approach of Text Summarization Using Latent Semantic Analysis and Deep Learning In the current scenario of Information Technology, excessive and vast information is available on online resources but it is not always easy to find relevant and useful information. Along this issue, the paper is presented a method on extractive single document text summarization using Deep Learning method - Self-Organizing Maps (SOM) which is an unsupervised method and Artificial Neural Networks (ANN) which is a supervised method. The work involves investigating the effect of adding mapped sentences from SOM visualization, and re-training the inputs on ANN for ranking the sentences. In individual experiment of the hybrid model, a different mapping of SOM is added to the ANN network as input vector. Hybrid model uses Stochastic Gradient Descent update set of parameters in an iterative manner to minimize the cost function. In addition, using back-propagation weight is being adjusted for the input vector. The empirical results show that the hybrid model using mapping clearly provides a comprehensive result and improves the F-score on average 5% on ROUGE-1, ROUGE-2, ROUGE-L and ROUGE-SU4. This novel method has been implemented on different documents, which are publicly available on Opinosis Dataset. The ROUGE toolkit has been used to evaluate summaries which are generated from the proposed model and other existing algorithms versus human generated summary.\n\n[10] Ontology-based Aspect Extraction for an Improved Sentiment Analysis in Summarization of Product Reviews Current approaches in aspect-based sentiment analysis ignore or neutralize unhandled issues emerging from the lexicon-based scoring (i.e., SentiWordNet), whereby lexical sentiment analysis only classifies text based on affect word presence and word count are limited to these surface features. This is coupled with considerably low detection rate among implicit concepts in the text. To address this issues, this paper proposed the use of ontology to i) enhance aspect extraction process by identifying features pertaining to implicit entities, and ii) eliminate lexicon-based sentiment scoring issues which, in turn, improve sentiment analysis and summarization accuracy. Concept-level sentiment analysis aims to go beyond word-level analysis by employing ontologies which act as a semantic knowledge base rather than the lexicon. The outcome is an Ontology-Based Product Sentiment Summarization (OBPSS) framework which outperformed other existing summarization systems in terms of aspect extraction and sentiment scoring. The improved performance is supported by the sentence-level linguistic rules applied by OBPSS in providing a more accurate sentiment analysis.\n\n[11] Combining Different Summarization Techniques for Legal Text Summarization, like other natural language processing tasks, is tackled with a range of different techniques - particularly machine learning approaches, where human intuition goes into attribute selection and the choice and tuning of the learning algorithm. Such techniques tend to apply differently in different contexts, so in this paper we describe a hybrid approach in which a number of different summarization techniques are combined in a rule-based system using manual knowledge acquisition, where human intuition, supported by data, specifies not only attributes and algorithms, but the contexts where these are best used. We apply this approach to automatic summarization of legal case reports. We show how a preliminary knowledge base, composed of only 23 rules, already outperforms competitive baselines.\n\n[12] A hybrid approach for automatic document summarization A hybrid approach for automatic document summarization Automatic text summarization come under the domains of natural language processing, machine learning and information retrieval. As the abundance of textual information grows so does the need for summarising it. Here we consider a method for single document summarization using natural language processing tools and an extractive approach based on sentence similarity and document context. The technique uses a weighted undirected graph based scoring on paragraphs and a word frequency based scoring system on the entire document to obtain summaries. The proposed method is validated through experiments and the results are promising.\n\n"}
{"case_id": "f0e4b81f93c36652a08a8160e12c9922", "answer_text": "\n\nAchieving precise positioning and context-aware networking in 6G mobile communication involves leveraging advanced technologies and methodologies that address current gaps and optimize performance [14]. Here are some key strategies and their underlying mechanisms, now with empirical results and specific case studies from recent literature to strengthen the response: \n\n1. **Advanced Localization Techniques** [1]:\n   - **Multi-Modal Localization**: By integrating different technologies such as radio, Visible Light Communication (VLC), and sensors, 6G systems can achieve cm- to mm-level precision in both indoor and outdoor environments [3]. For instance, a study demonstrated a hybrid VLC and RF-based positioning system achieving accuracy levels as fine as millimeters in controlled environments [0].\n   - **Reconfigurable Intelligent Surfaces (RISs)**: RISs can reflect electromagnetic waves towards desired directions, enhancing positioning accuracy, especially in Non-Line-of-Sight (NLoS) conditions. A case study highlighted how RIS-assisted localization achieved a marked improvement in positioning accuracy by providing additional geometric measurements and using passive beamforming [8]. The operational mechanism of RIS involves programmable passive reflecting unit cells that steer the incoming electromagnetic waves towards desired directions, thereby creating controllable NLoS links. However, challenges such as precise control over RIS phase shifts and the need for accurate environmental knowledge to optimize their deployment remain significant hurdles [8].\n   - **Ultra-Wideband and mmWave**: Utilizing ultra-wideband (UWB) and millimeter-wave (mmWave) frequencies enhances high-resolution spatial information collection. For example, experiments using mmWave systems with large MIMO arrays have shown sub-meter accuracy in dense urban environments, leveraging precise angle and time measurements [9]. Nevertheless, mmWave signals are susceptible to high path loss and blockage by obstacles, which poses significant challenges in maintaining consistent performance.\n\n**Context-Aware Networking**: Contextual understanding will be useful in several 6G applications. By communicating only when it is most efficient applications may significantly reduce their overall energy footprint. Insightful prediction of data transmission based on context paves the way for increased throughput on demand. To achieve hyper-personalization, context-awareness may be utilized to intelligently relocate personalization algorithms and sensor data to portions of the network, where storage and computing are quick and practical [0]. For instance, in healthcare contexts, it is crucial to fuse sensor data in order to discover patterns and deviations, and this is made possible by intelligent storage and distributed processing capabilities. Additionally, multi-modal localization is made possible by context-awareness, allowing mobile devices to transition between channels and communication technologies based on their present location. With multi-modality, devices may choose between national and private technologies, reducing power consumption and improving service quality. This diversity in communication styles necessitates the use of context-aware localization approaches.\n\n   - **Integration of AI and ML**: Machine Learning (ML) can be applied to process and interpret contextual data from multiple sensors and sources, enabling more accurate and adaptive network decisions. A recent study used ML algorithms to enhance RSS-based fingerprinting, which allowed the system to maintain localization accuracy despite environmental changes [5][13]. In another detailed example, AI-based methods dynamically adjusted network parameters based on real-time context data, improving network throughput by 30% in a simulated 6G network environment. However, the reliance on large datasets and the need for continuous learning pose challenges in maintaining the performance of ML models over time.\n   - **Physical Layer Security (PLS)**: Context awareness can be incorporated into PLS, leveraging environmental and situational data to adaptively secure communications. For example, AI-based methods that adjust security levels based on context-aware data such as node location and communication time were shown to significantly reduce breach attempts in a simulated 6G network environment [12]. However, ensuring the robustness and reliability of these AI-driven security measures in highly dynamic and heterogeneous environments is a challenge.\n\n3. **Joint Communication and Sensing (JC&S)** [2]. \n   - **Waveform Design**: Joint optimization of communication and sensing waveforms is crucial. For instance, Orthogonal Frequency-Division Multiplexing (OFDM) can be employed for both tasks due to its wide availability and efficient implementation. Experiments demonstrated that single-carrier waveforms, optimized for low peak-to-average power ratio (PAPR), achieved better performance in hardware-constrained scenarios [2]. The main challenge lies in balancing the trade-offs between communication data rates and sensing accuracy.\n   - **Distributed Sensing and Communication**: Implementing Distributed and Intelligent Sensing and Communications (DISAC) involves leveraging multi-modal sensor data integrated within the network infrastructure. A study showed that distributed sensing enhanced network efficiency by utilizing real-time context data, leading to a 25% increase in spectral efficiency in millimeter-wave networks [11]. However, managing and integrating data from diverse sensors while ensuring low latency and high accuracy is complex.\n   - **Specific Applications**: JC&S waveforms have been effectively employed in various real-world scenarios. For instance, in autonomous driving, JC&S is used to support high-speed communication and precise positioning, enabling real-time navigation and collision avoidance. In smart healthcare, JC&S provides timely patient monitoring by integrating body sensors and environmental data, ensuring accurate diagnostics and treatment delivery [2].\n\n4. **Energy-Efficient Context-Aware Mechanisms**: [0]\n   - **Dynamic Power Adaptation**: Context-aware green networking models, such as the Context-Aware Green Information-Centric Networking Model (CAGIM), can adapt the power consumption of network nodes based on link utilization and content popularity. Evaluation results indicated a significant reduction in network power consumption and improved efficiency [10]. Specifically, CAGIM dynamically adjusts the link rate in ICN nodes according to the popularity and traffic load of the content, reducing wasteful energy consumption. Additionally, it introduces a smart popularity-based caching strategy called the distinguished caching scheme (DCS), which partitions the cache storage into hot and cold-caching areas for popular and non-popular content objects, respectively. This approach enhances content diversity and cache hit rates, further decreasing power consumption by reducing network traffic. However, implementing these mechanisms in large-scale networks and ensuring their adaptability to varying traffic patterns is challenging.\n   - **Proactive Resource Allocation**: By using location information and long-term radio parameters to create radio environment maps, 6G systems can adopt proactive resource allocation strategies. A simulation study demonstrated reduced latency and improved network performance by minimizing the need for instantaneous CSI through such proactive measures [14].\n\n5. **Enhanced Beamforming Techniques**: Location-based beamforming schemes can offer substantially reduced reference symbol overhead compared to classic full channel state information (CSI)-based beamforming [1]. This allows for significant power savings in wideband 5G networks where acquiring full CSI requires wideband reference signals, while location estimation and tracking can, in turn, be accomplished with narrowband pilots [1]. Reconfigurable intelligent surfaces (RISs) can reflect incoming electromagnetic waves towards desired directions via programmable passive reflecting unit cells and a controller, implying lower deployment and operational costs than a base station or relay [8]. Under line-of-sight (LoS) blockage conditions, RISs can create controllable non-line-of-sight (NLoS) links to improve coverage and communication quality, enhancing positioning and sensing accuracy through additional geometric measurements [8]. \n   - **Geometric Beamforming**: High-precision positioning enables geometric beamforming, which does not require full-channel state information (CSI). Instead, it relies on the location of the user and scatterers to design optimal transmission beams. Case studies have shown that this method can substantially reduce overhead and enhance communication efficiency in urban environments [1][14]. The method involves using precise geometric information to focus the energy towards the intended receiver, thus minimizing interference and maximizing signal strength. The efficacy of this technique was highlighted in a practical deployment that demonstrated optimized spatial techniques to mitigate interference and improve signal quality.\n   - **Dynamic Beam Management**: By continuously estimating the user's location and adapting the beam direction, 6G systems can maintain high communication quality even in dynamic environments. This has been particularly useful in scenarios involving high mobility, such as vehicular networks and UAV communications [1]. Empirical data confirmed that dynamic beam management could maintain robust connectivity in high-speed train scenarios. The complexity of implementing real-time beam adjustments and the computational overhead involved are significant challenges. Advanced algorithms and high processing power are required to facilitate the real-time adjustments, ensuring continuity and minimizing latency, even as users move rapidly.\n \n- **Beam Squint Mitigation**: At mmWave and sub-THz frequencies, beam squint can become a significant issue, where beams shift in angle with frequency, leading to misalignment and loss of signal strength. Techniques such as frequency-selective beamforming and beam squint compensation algorithms are being developed to counteract this effect, ensuring precise beam alignment across the operating bandwidth. These methods are critical for maintaining effective communication in high-frequency bands, ensuring optimal performance and reliability in 6G networks [11].\n\n\nIn summary, achieving precise positioning and context-aware networking in 6G mobile communication necessitates a multifaceted approach that integrates advanced localization techniques [3], AI-driven context-aware networking [0], energy-efficient mechanisms [10], and innovative beamforming strategies [14]. These measures collectively enhance the accuracy, efficiency, and security of 6G networks, paving the way for a wide range of applications from autonomous vehicles to smart healthcare systems. Despite the promising advancements, addressing the limitations and challenges associated with these technologies will be crucial for their successful deployment and operation in real-world scenarios. \n\n\nReferences:\n[0] Localization as a Key Enabler of 6G Wireless Systems: A Comprehensive Survey and an Outlook Contextual understanding will be useful in several 6G applications. By communicating only when it is most efficient applications may significantly reduce their overall energy footprint. Insightful prediction of data transmission based on context paves the way for increased throughput on demand. To achieve hyper-personalization, context-awareness may be utilized to intelligently relocate personalization algorithms and sensor data to portions of the network, where storage and computing are quick and practical. For instance, in healthcare contexts, it is crucial to fuse sensor data in order to discover patterns and deviations, and this is made possible by intelligent storage and distributed processing capabilities , 73. Additionally, multi-modal localization is made possible by context-awareness, allowing mobile devices to transition between channels and communication technologies based on their present location. With multi-modality, devices may choose between national and private technologies, reducing power consumption and improving service quality. This diversity in communication styles necessitates the use of context-aware localization approaches .\n\n[1] High-Efficiency Device Positioning and Location-Aware Communications in Dense 5G Networks High-Efficiency Device Positioning and Location-Aware Communications in Dense 5G Networks In this article, the prospects and enabling technologies for high-efficiency device positioning and location-aware communications in emerging 5G networks are reviewed. We will first describe some key technical enablers and demonstrate by means of realistic ray-tracing and map based evaluations that positioning accuracies below one meter can be achieved by properly fusing direction and delay related measurements on the network side, even when tracking moving devices. We will then discuss the possibilities and opportunities that such high-efficiency positioning capabilities can offer, not only for location-based services in general, but also for the radio access network itself. In particular, we will demonstrate that geometric location-based beamforming schemes become technically feasible, which can offer substantially reduced reference symbol overhead compared to classic full channel state information (CSI)-based beamforming. At the same time, substantial power savings can be realized in future wideband 5G networks where acquiring full CSI calls for wideband reference signals while location estimation and tracking can, in turn, be accomplished with narrowband pilots.\n\n[2] Positioning and Sensing in 6G: Gaps, Challenges, and Opportunities Future positioning and sensing services shall rely on the ubiquitously available communication network and its hardware, thus avoiding the deployment of costly parallel infrastructure. Regarding joint waveform design, multi-carrier communication waveforms, such as  orthogonal frequency-division multiplexing (OFDM), are attractive for positioning and sensing thanks to wide availability and efficient implementation.On the other hand, single-carrier waveforms can offer a better solution in terms of hardware efficiency due to low peak-to-average power ratio (PAPR), but may lead to higher side-lobe levels than OFDM.To investigate their resolution, accuracy and clutter rejection characteristics, waveforms (single- or multi-carrier) can be evaluated through range-Doppler ambiguity function. Due to inherent trade-offs, the joint waveform optimization for positioning, communications and sensing requires careful consideration of conflicting requirements, such as data rate, accuracy, and main-lobe width and side-lobe levels of the ambiguity function.Moreover, joint communications-sensing waveforms should be robust to hardware imperfections at high frequencies, necessitating simultaneous design of multi-functional transceiver hardware and waveforms.\n\n[3] Localization as a Key Enabler of 6G Wireless Systems: A Comprehensive Survey and an Outlook Localization has been completely transformed by the advancement of multiple technologies, such as radars, sensors, AI, mmWave, THz, VLC, and RISs. As illustrated in Fig. 2. It is expected to play a dominant role in the next generation of communication networks through its incorporation to a variety of applications. Some indicative examples include pinpointing a device’s outdoor location with cm- to mm-level precision, accurate indoor localization with reliable interpretation of 3D data in addition to the 2D data that are presently available, or the incorporation of roll, pitch, and yaw into the localization process. To achieve these, 6G promises to use a combination of radio and VLC localization to achieve cm- to mm-level accuracy whether the user, object, or device is inside and/or outside.\n\n[4] Localization as a Key Enabler of 6G Wireless Systems: A Comprehensive Survey and an Outlook Zhou et al. in and investigated the correlation between the access point deployment and the localization precision based on the RSS and designed a simulated annealing algorithm for optimizing the access point positions. The authors of designed an approach that automatically updates the fingerprint database by employing a clustering algorithm to filter out the altered signals to achieve high localization accuracy. Using the observed signals and the calculated location of a node, the fingerprint database is constantly updated without manual intervention. In, the authors combined RSS-based fingerprinting with context-aware data regarding the node’s environment (i.e., building floor plan) and demonstrated that the proposed method can reduce the required access point number and mitigate the effects of wireless interference. The authors of developed a collaboration-based fingerprinting approach, in which several assistant nodes around an unknown node are selected, based on the RSS sequences similarity, and distances between them are used as auxiliary information to improve the positioning accuracy.\n\n[5] Positioning and Sensing in 6G: Gaps, Challenges, and Opportunities As one of the key enablers in 6G, a  reconfigurable intelligent surface (RIS) can reflect an incoming electromagnetic wave towards a desired direction via programmable passive reflecting unit cells and a controller, which implies lower deployment and operational costs than a BS or a relay.Under line-of-sight (LoS) blockage conditions, RISs can create controllable non-line-of-sight (NLoS) links to improve coverage and communication quality. In positioning and sensing applications, RISs with known locations can boost accuracy by providing additional geometric measurements. Through tailor-made design of RIS phase shifts (i.e., passive beamforming), positioning and sensing performance can be enhanced significantly under a-priori knowledge of UE/target locations.\n\n[6] Positioning and Sensing in 6G: Gaps, Challenges, and Opportunities Positioning is the process of estimating the location (and in some cases, orientation and velocity) of a device from radio measurements such as received signal power, time-of-flight between transmitter and receiver, direction of the signal, or any combination of those, as illustrated in Fig. 1 (left part). The position estimation performance depends on the resolution and accuracy of the underlying measurements, number of base stations involved, and relative positions of the BSs with respect to the  user equipment (UE).The cellular positioning reference signals employed in 5G include downlink positioning reference signal (DL-PRS) and uplink sounding reference signal (UL-SRS). Among the time and angle positioning methods, uplink/downlink time-difference-of-arrival (UL/DL-TDoA) use SRS/PRS, respectively, uplink angle-of-arrival (UL-AoA) uses SRS, and downlink angle-of-departure (DL-AoD) uses the beam index, whereas multi round trip time (multi-RTT) relies on both PRS and SRS.In a typical 5G network, signaling information that is intrinsic to the network, such as serving cell, serving beam, timing-advance and reference signal received power (RSRP), can be exploited to estimate location information. Methods using such information are called enhanced cell ID (e-CID) methods. The reference signals such as DL-PRS and UL-SRS can be employed to compute  time-difference-of-arrival (TDoA) between a pair of reference nodes (e.g., BSs), which can in turn yield a locus of points along a hyperbola in which UE may be present. Thus, TDoA measurements from multiple pairs of reference nodes can triangulate to a precise UE location. These are broadly termed as TDoA-based multilateration methods. In addition, 5G mmWave operation enables large dimension MIMO, which can provide high angular resolution, thereby enabling precise angle information between the reference node and the UE. The angle measurements between the UE and multiple reference nodes can be exploited to arrive at the position of the UE. These are usually termed as angle-based positioning methods. To avoid the need for tight synchronization in TDoA, round trip time (RTT) based methods from multiple BSs have also been introduced.\n\n[7] A Context-Aware Green Information-Centric Networking Model for Future Wireless Communications This research proposes a novel wireless information-centric networking (ICN) architecture, namely, Context-Aware Green ICN Model (CAGIM), which can adapt the power consumption of network nodes to optimized values according to the associated link utilization. The power adaption in ICN nodes is conducted through dynamically adjusting the link-rate corresponding to content popularity and traffic load to reduce wasteful energy consumption. Moreover, we propose a smart popularity-based caching strategy, called distinguished caching scheme (DCS), with the introduction of hot and cold-caching partitions of ICN node’s cache storage for popular and non-popular content objects, respectively. DCS improves the content diversity of the cache storage by adjusting, for each content, the number of chunks to be cached at ICN nodes based on its type and popularity level. DCS thus can further decrease the network system power consumption, thanks to its improved cache hit that reduces network traffic load. Toward the goal of realizing a context-aware green wireless network system with efficient content delivery, we also design a Wi-Fi Direct based scheme as an alternative approach to minimize power consumption and latency by sharing essential/important content objects via direct communications with power-saving mechanisms in the case that wireless local area network connections are not available. The evaluation results show that CAGIM can improve network efficiency by reducing both hop-count and power consumption considerably compared with existing wireless network systems with different well-known caching schemes. This proposal enables a flexible and efficient content delivery mechanism for future networks with various real-life scenarios, like Green building, Green company, and Green campus content accesses.\n\n[8] Towards Distributed and Intelligent Integrated Sensing and Communications for 6G Networks A final component of the physical layer, and the associated signal processing, is the exploitation of sensing and contextual information to improve communication operations.The optimization of the wireless communication system’s components requires, in general, the estimation of the parameters of the channel conditions in which it operates. Conventionally, the channel estimation is based on the exchange of pilot signals in time, frequency, and space, providing a snapshot of the channel.Such estimation processes need to be efficiently supported by the communication protocol with dedicated control phases and signaling, aiming for accurate, low latency, and scalable estimation.This classical approach becomes very challenging in dense smart wireless environments designed to use XL-MIMO nodes and multiple RISs , possibly realizing either hybrid analog and digital or fully digital beamforming with very low-resolution signal converters and passive reflective beamforming, especially in very high-frequency bands and under mobility scenarios. The DISAC paradigm envisions the optimization of large-scale and distributed communication provisioning with location and, more generally, context information becoming available from the distributed processing of data gathered from multi-modal sensory devices integrated within the wireless network infrastructure, paving the way for more efficient, context-aware, and environmentally sustainable communication networks.Even in simple scenarios, sensing-aided communication can offer significant performance gains, as shown in Fig. 4, for a full-duplex MIMO scenario at a millimeter-wave frequency. Leveraging direction estimation in the uplink leads to around 25% increase in the achievable spectral efficiency for the downlink (DL).\n\n[9] Context-Aware Security for 6G Wireless The Role of Physical Layer Security Meanwhile, the evolution towards 6G systems is expected to introduce new means of reaching situational awareness by harvesting and interpreting the “context” of the communication, including, network tomography, nodes’ constraints, the age of information, etc. Incorporating context awareness in QoSec is projected to allow handling more efficiently aspects related to identifying the risk or threat level and the required security level, particularly for applications with non-functional security requirements, such as autonomous vehicles, platooning, eHealth, etc. In this framework, incorporating security controls from the palette of physical layer security (PLS) can be particularly attractive due to their low computational complexity (relevant implementations are based on standard encoders) and their inherent ability to adapt to the transmission medium properties. The incorporation of PLS in 6G security requires indeed enhanced context awareness and can be particularly attractive for massive machine type communications (mMTC) and ultra-low latency use cases.\n\n[10] Localization as a Key Enabler of 6G Wireless Systems: A Comprehensive Survey and an Outlook On a different note, unsupervised techniques have been successfully applied for constructing the map of a radio-based system. An instance of such a system develops a logical floor plan using accelerometer and RSS measurements from users within the service area . This techniques is based on clustering techniques like k-means on RSS stacking difference, while each virtual room is assigned a representative localization fingerprint. Another proposed framework for unsupervised localization utilizes WiFi, gyroscope, compass, gyroscope, and accelerometer values of naturally moving users and tries to identify some fixed structures in the building (i.e., elevators, columns, stairs, and more) that greatly influence their movement. Afterwards, it employs k-means clustering to extract unique sensor signatures that can increase the localization accuracy for users starting from a known spot in the building . Localization can be also achieved by the combination of global-local optimization and a Markov model that fits the RSS traces into the structure of the environment based on unlabeled data and considers the solutions that do not violate the signal propagation .\n\n[11] Toward 6G Holographic Localization: Enabling Technologies and Perspectives previously highlighted, the advent of holographic radio will make available a larger number of degrees of freedom enhanced by the different operational EM propagation regimes (e.g., in the Fresnel region). Moreover, the possibility to fully control the EM field at both transmitter and receiver side, granted by the advent of new technologies, will help approaching the ultimate localization and communication performance limits. Indeed, this will open the possibility to enable new applications, in light of next 6G scenarios. In fact, accurate positioning of mobile users can enhance the performance of communication systems, also called as location-aware communication. Some examples are: (i) geometric beams at the transmitters can be designed for location-based beamforming without the need to estimate the full-band channel state information (CSI); (ii) the system can adopt sophisticated spatial techniques to mitigate the interference; (iii) location information and measured radio parameters over a long time period enable the construction of radio environment maps, opening many opportunities in terms of proactive resource allocation, without the need to know the instantaneous CSI; (iv) data can be communicated with low-latency by considering proactive location-based backhaul routing; (v) the CSI can be efficiently estimated from a minimal number of pilots by considering geometric channel models when the location of the user and the scatterers can be accurately computed; and (vi) the radiation pattern of the antennas can be optimized to We now present some potential applications that require high-accuracy localization and low-latency. In Fig. 1, an example of holographic localization scenario enabled by LISs is depicted. In this scenario,\n\n"}
{"case_id": "db4211ada9ca179cddb052bf63e732a1", "answer_text": "\n\nThe most accurate data analysis tool for the appraisal of mixed methodology research studies is the Mixed Methods Appraisal Tool (MMAT), particularly the version developed in 2018. This tool has been extensively validated and is widely used in systematic reviews that include both qualitative and quantitative studies [3][14]. \n\nOverview and Structure of MMAT [2]. \n\nThe MMAT is designed to concurrently appraise the methodological quality of qualitative, quantitative, and mixed-methods studies, addressing the complexities and heterogeneities inherent in mixed methodology research [2]. The MMAT includes two comprehensive screening questions applicable to all study types, ensuring that only studies with clear research questions and data that address those questions are considered. It then applies specific criteria tailored to the methodological quality of qualitative, quantitative, and mixed methods studies [0][14]. This structured approach makes it particularly suitable for mixed-methods studies, where it integrates aspects from both qualitative and quantitative paradigms to provide a holistic assessment. It employs sets of criteria for each study type, and an overall score is derived based on these individual appraisals [1][12][14].\n\nDevelopment and Validity[3]\n\nTo enhance content validity, the MMAT was developed through a comprehensive literature review and a modified e-Delphi study involving methodological experts. This process led to the identification and inclusion of core criteria, refining eight out of the 11 criteria from earlier versions and adding six new ones to better capture the quality dimensions of the studies [13][14]. These updates address limitations in earlier versions and incorporate user feedback to improve the tool's applicability and accuracy.\n\nReliability and Efficiency[2]\n\nThe reliability and efficiency of the MMAT have been tested, with studies showing moderate to perfect agreement between reviewers on MMAT criteria, and substantial agreement on overall quality scores [2]. This high level of inter-rater reliability underscores the tool's robustness and consistency in appraising mixed-method studies. In practice, the MMAT has been shown to be efficient, averaging around 14 minutes per study appraisal excluding the initial reading of articles. This efficiency, combined with high reliability, makes the MMAT an attractive choice for researchers conducting systematic reviews that require the appraisal of studies with heterogeneous methodologies [2].\n\nThe Mixed Methods Appraisal Tool (MMAT) version 2018 was developed to assess the quality of qualitative, quantitative, and mixed methods studies [14]. The MMAT includes five sets of criteria tailored for each type of study methodology, allowing for a comprehensive evaluation of different research designs [3]. It is a widely used tool in systematic reviews due to its flexibility and comprehensive approach [2]. The MMAT has been validated and is particularly useful for addressing complex research questions that require a mixed methods approach [2]. It has also been subject to reliability testing and has undergone several iterations to improve its content validity [14].\n\nTo illustrate the effective use of MMAT, consider a systematic review on the impact of participatory research on healthcare outcomes. In this review, MMAT was used to appraise 32 participatory research evaluation studies, with each study assessed by two independent reviewers [2]. The application of MMAT facilitated a structured and reliable appraisal process, enabling the synthesis of findings across diverse research designs effectively.\n\nSimilarly, a study on child and family outcomes following pandemics employed MMAT to assess the risk of bias and methodological quality of the included studies [3]. The use of MMAT ensured a comprehensive evaluation of both qualitative and quantitative aspects, contributing to the robustness of the systematic review's conclusions.\n\n\n### Comparison with Other Tools\nThe Mixed Methods Appraisal Tool (MMAT) version 2018 was used to evaluate the methodological quality of the included studies [0]. The MMAT contains 27 methodological quality criteria for appraising qualitative, quantitative, and mixed-methods studies [0]. Each criterion in the tool would be labeled with one asterisk if a requirement was met and would be labeled without an asterisk if a requirement was not met or \"cannot tell\" [0]. There are two comprehensive screening questions for all types of studies, namely \"Are there clear research questions?\" and \"Do the collected data allow to address the research questions?\" [12]. A further appraisal may not be feasible when the answer is \"no\" or \"cannot tell\" to one or both screening questions [12]. Of the remaining 25 questions, 5 of them are expressly set up for appraising the qualitative study, 15 of them for quantitative study (as the tool divides quantitative study into three types, including quantitative randomized controlled trials, quantitative non-randomized and quantitative descriptive), and 5 of them for mixed-methods study [0]. Accordingly, for each study, it will be labeled with 5 asterisks, or scored 100%, if it meets all criteria for the type of study [0]. By analogy, a study that meets 4 criteria will be labeled with 4 asterisks (or scored 80%) [0]. The MMAT does not have a specific standard cut-off value. However, two categories (low and high) or three categories (low, medium, and high) can be adopted. The crucial aspect is to carefully utilize the results of the appraisal in the review [0].\n\n\nWhile the MMAT is highly regarded, it is not the only tool available for the appraisal of mixed-methodology research studies. Other tools, such as the Critical Appraisal Skills Programme (CASP) and the Joanna Briggs Institute (JBI) Critical Appraisal Tools, can also be used to evaluate qualitative, quantitative, and mixed-methods studies, though they are typically more specialized for certain study designs [5][9][12].\n\nLimitations of systematic reviews include the potential for bias in the selection of studies and the quality of the included studies. To address these issues, various tools have been developed to assess the methodological quality of the studies being reviewed. One such tool is the Mixed Methods Appraisal Tool (MMAT), which is designed to evaluate qualitative, quantitative, and mixed-method studies [14]. The MMAT has been validated and is widely used in systematic reviews [3]. It includes five core quality criteria for each type of study design, allowing for a comprehensive assessment of methodological rigor [14]. Furthermore, the MMAT facilitates systematic mixed studies reviews by providing a structured approach to appraise studies that use different methods, addressing the challenge of evaluating the quality of heterogeneous study designs [2]. \n\nDespite its strengths, the MMAT has some potential limitations or criticisms. One criticism is related to its categorical scoring system which may oversimplify the nuanced quality differences between studies. Additionally, some users have found the criteria for mixed methods studies to be overly rigid, potentially overlooking the unique contributions of studies that do not fit neatly into predefined appraisal categories. Furthermore, although the MMAT focuses on core quality criteria, some researchers may prefer more detailed, context-specific tools that cater to the specificities of certain study designs or disciplines [6]. Users should be aware of these limitations and consider them when selecting a tool for their specific appraisal needs.\n\nConclusion [14]\n\nIn conclusion, the MMAT version 2018 stands out as the most accurate and reliable tool for appraising mixed methodology research studies. Its development process, integration of core quality criteria, and proven inter-rater reliability contribute to its suitability for this complex task [2][13][14]. However, researchers may consider alternative tools like CASP and JBI depending on their specific needs and contexts.\n\n\nReferences:\n[0] Restrictive Deterrence in Drug Offenses: A Systematic Review and Meta-Synthesis of Mixed Studies the data screening, the Mixed Methods Appraisal Tool (MMAT) version 2018 was used to evaluate the methodological quality of the included studies. The MMAT contains 27 methodological quality criteria for appraising qualitative, quantitative and mixed-methods studies. Each criterion in the tool would be labeled with one asterisk if a requirement was met and would be labeled without asterisk if a requirement was not met or \"cannot tell.\" There are two comprehensive screening questions for all types of studies, namely \"Are there clear research questions?\" and \"Do the collected data allow to address the research questions?\" A further appraisal may not be feasible when the answer is \"no\" or \"cannot tell\" to one or both screening questions. Of the remaining 25 questions, 5 of them are expressly set up for appraising the qualitative study, 15 of them for quantitative study (as the tool divides quantitative study into three types, including quantitative randomized controlled trials, quantitative non-randomized and quantitative descriptive), and 5 of them for mixed-methods study. Accordingly, for each study, it will be labeled with 5 asterisks, or scored 100%, if it meets all criteria for the type of study. By analogy, a study that meets 4 criteria will be labeled with 4 asterisks (or scored 80%). The MMAT does not have a specific standard cut-off value. However, two categories (low and high) or three categories (low, medium and high) can be adopted. The crucial aspect is to carefully utilize the results of the appraisal in the review. One author of the present study analyzed the methodological\n\n[1] MIXED METHODS APPRAISAL TOOL: STRENGTHENING THE METHODOLOGICAL RIGOR OF MIXED METHODS RESEARCH STUDIES IN NURSING arms need to meet completely the criteria of scientific rigor to be considered of good quality for the mixed methods study 15 . When both arms are rated different for the level of quality, the overall quality of the mixed methods study must be assigned to the lower rating 15 . For instance, if the quantitative arm is rated with \"high\" quality while the qualitative arm is rated as \"low\" quality, the overall quality of the mixed methods study must be \"low\". In sum, MMAT can be used to appraise a mixed methods study using two screening questions and additional questions based on the research questions and the types of the quantitative and qualitative arms. The integration in a mixed methods study must take a holistic approach, occurring at some point during the research process: data collection, data analysis, and when findings emerged 22 . Such an integration should result in meta-inference, generating a \"complete\" story of the phenomenon of interest . Instead of conducting two or more separate studies on one topic, applying mixed methods research to generate meta-inferences can be valuable, critical for nurses whose clients' health and life circumstance are usually complex which also demand complex answers 10,15 . APPLICATION OF MMAT TO APPRAISE METHODOLOGICAL RIGOR OF MIXED METHOD STUDIES IN NURSING Across the globe, even though researchers in different disciplines have increasingly acknowledged the benefits of mixed methods research, some nurse researchers still subscribe to either quantitative or qualitative research methodology because their methodology of choice is aligned with their perspective/\n\n[2] Testing the reliability and efficiency of the pilot Mixed Methods Appraisal Tool (MMAT) for systematic mixed studies review. Systematic literature reviews identify, select, appraise, and synthesize relevant literature on a particular topic. Typically, these reviews examine primary studies based on similar methods, e.g., experimental trials. In contrast, interest in a new form of review, known as mixed studies review (MSR), which includes qualitative, quantitative, and mixed methods studies, is growing. In MSRs, reviewers appraise studies that use different methods allowing them to obtain in-depth answers to complex research questions. However, appraising the quality of studies with different methods remains challenging. To facilitate systematic MSRs, a pilot Mixed Methods Appraisal Tool (MMAT) has been developed at McGill University (a checklist and a tutorial), which can be used to concurrently appraise the methodological quality of qualitative, quantitative, and mixed methods studies. The purpose of the present study is to test the reliability and efficiency of a pilot version of the MMAT. The Center for Participatory Research at McGill conducted a systematic MSR on the benefits of Participatory Research (PR). Thirty-two PR evaluation studies were appraised by two independent reviewers using the pilot MMAT. Among these, 11 (34%) involved nurses as researchers or research partners. Appraisal time was measured to assess efficiency. Inter-rater reliability was assessed by calculating a kappa statistic based on dichotomized responses for each criterion. An appraisal score was determined for each study, which allowed the calculation of an overall intra-class correlation. On average, it took 14 min to appraise a study (excluding the initial reading of articles). Agreement between reviewers was moderate to perfect with regards to MMAT criteria, and substantial with respect to the overall quality score of appraised studies. The MMAT is unique, thus the reliability of the pilot MMAT is promising, and encourages further development.\n\n[3] Child and Family Outcomes Following Pandemics: A Systematic Review and Recommendations on COVID-19 Policies Assessment and Risk of Bias The Mixed Methods Appraisal Tool, version 2018 (MMAT; Hong et al., 2018) was utilized to assess the quality of each study included in the review. This tool has been validated and is widely used in systematic reviews including both qualitative and quantitative studies. Two reviewers (V. Fong, A.V.) independently completed the quality assessment on each article meeting inclusion criteria. The MMAT was appropriate for the current study as it allows for the evaluation of study quality and risk of bias for all empirical literature (qualitative, quantitative, and mixed methods). There are 5 sets of criteria for each study methodology and each criterion are rated on a pass (1) or fail (0) basis. For example, the criteria for qualitative studies were as follows: (a) Is the qualitative approach appropriate to answer the research question? (b) Are the qualitative data collection methods adequate to address the research question? (c) Are the findings adequately derived from the data? (d) Is the interpretation of results sufficiently substantiated by data? (e) Is there coherence between qualitative data sources, collection, analysis and interpretation? For a full description of criteria for each study methodology refer to Hong et al. (2018). The quality of a particular study was determined by calculating the percentage of criteria met (e.g., if a study met 4/5 of the MMAT criteria it would receive a total score of 80%). The authors of this tool do not provide cutoff scores that differentiate high-versus low-quality studies but recommend that reviewers conducting quality assessments define their own\n\n[4] The methodological quality assessment tools for preclinical and clinical studies, systematic review and meta-analysis, and clinical practice guideline: a systematic review. To systematically review the methodological assessment tools for pre-clinical and clinical studies, systematic review and meta-analysis, and clinical practice guideline. We searched PubMed, the Cochrane Handbook for Systematic Reviews of Interventions, Joanna Briggs Institute (JBI) Reviewers Manual, Centre for Reviews and Dissemination, Critical Appraisal Skills Programme (CASP), Scottish Intercollegiate Guidelines Network (SIGN), and the National Institute for Clinical Excellence (NICE) up to May 20th, 2014. Two authors selected studies and extracted data; quantitative analysis was performed to summarize the characteristics of included tools. We included a total of 21 assessment tools for analysis. A number of tools were developed by academic organizations, and some were developed by only a small group of researchers. The JBI developed the highest number of methodological assessment tools, with CASP coming second. Tools for assessing the methodological quality of randomized controlled studies were most abundant. The Cochrane Collaboration's tool for assessing risk of bias is the best available tool for assessing RCTs. For cohort and case-control studies, we recommend the use of the Newcastle-Ottawa Scale. The Methodological Index for Non-Randomized Studies (MINORS) is an excellent tool for assessing non-randomized interventional studies, and the Agency for Healthcare Research and Quality (ARHQ) methodology checklist is applicable for cross-sectional studies. For diagnostic accuracy test studies, the Quality Assessment of Diagnostic Accuracy Studies-2 (QUADAS-2) tool is recommended; the SYstematic Review Centre for Laboratory animal Experimentation (SYRCLE) risk of bias tool is available for assessing animal studies; Assessment of Multiple Systematic Reviews (AMSTAR) is a measurement tool for systematic reviews/meta-analyses; an 18-item tool has been developed for appraising case series studies, and the Appraisal of Guidelines, Research and Evaluation (AGREE)-II instrument is widely used to evaluate clinical practice guidelines. We have successfully identified a variety of methodological assessment tools for different types of study design. However, further efforts in the development of critical appraisal tools are warranted since there is currently a lack of such tools for other fields, e.g. genetic studies, and some existing tools (nested case-control studies and case reports, for example) are in need of updating to be in line with current research practice and rigor. In addition, it is very important that all critical appraisal tools remain subjective and performance bias is effectively avoided.\n\n[5] Improving the usefulness of a tool for appraising the quality of qualitative, quantitative and mixed methods studies, the Mixed Methods Appraisal Tool (MMAT). Systematic reviews combining qualitative, quantitative, and/or mixed methods studies are increasingly popular because of their potential for addressing complex interventions and phenomena, specifically for assessing and improving clinical practice. A major challenge encountered with this type of review is the appraisal of the quality of individual studies given the heterogeneity of the study designs. The Mixed Methods Appraisal Tool (MMAT) was developed to help overcome this challenge. The aim of this study was to explore the usefulness of the MMAT by seeking the views and experiences of researchers who have used it. We conducted a qualitative descriptive study using semistructured interviews with MMAT users. A purposeful sample was drawn from the researchers who had previously contacted the developer of the MMAT, and those who have published a systematic review for which they had used the MMAT. All interviews were transcribed verbatim and analyzed by 2 coders using thematic analysis. Twenty participants from 8 countries were interviewed. Thirteen themes were identified and grouped into the 2 dimensions of usefulness, ie, utility and usability. The themes related to utility concerned the coverage, completeness, flexibility, and other utilities of the tool. Those regarding usability were related to the learnability, efficiency, satisfaction, and errors that could be made due to difficulties understanding or selecting the items to appraise. On the basis of the results of this study, we make several recommendations for improving the MMAT. This will contribute to greater usefulness of the MMAT.\n\n[6] PROTOCOL: Conflict of interest issues when engaging stakeholders in health and healthcare guideline development: a systematic review tool (CASP, 2018) for qualitative studies. We will use the Mixed Methods Appraisal Tool (MMAT) to assess the quality of mixed-methods studies (Hong et al., 2018). Critical appraisal will be conducted independently and in duplicate by two authors and any discrepancies will be resolved by consensus and consultation with a third author, when necessary. We will develop and pilot test a standardized data abstraction sheet with detailed instructions. Two reviewers will extract data independently and in duplicate using a structured Excel sheet. The sheet will be piloted on ten articles. Disagreements on extractions will be resolved by discussion and with a third member of the research team when necessary. Data synthesis We will analyze continuous variables using mean and 95% confidence intervals when data are normally distributed; otherwise, we will use the median and interquartile range (IQR). We will describe categorical variables using frequencies and percentages using Excel. We will summarize the findings in both narrative and tabular formats. We will aim to conduct stratified analysis by the level of engagement and by the step of the guideline development process when enough data is available. Treatment of qualitative research We will follow the guidance for thematic synthesis of qualitative research in systematic reviews as outlined by Thomas and Harden (2008), which includes coding the text and developing descriptive themes (Thomas & Harden, 2008). Abstracted data will include participant quotations from interviews or focus groups, narrative descriptive summaries, author hypotheses, explanations and recommendations, themes and sub-themes. We expect qualitative data to address perceptions, experiences or\n\n[7] How effective is undergraduate palliative care teaching for medical students? A systematic literature review was resolved by consensus and/ or with a third reviewer (either AD or MB). The mixed methods appraisal tool (MMAT) was used if the study was mixed methods 13 and Cochrane risk of bias tool was used if a study was quantitative. 14 The MMAT is a critical appraisal tool developed to evaluate studies using both qualitative and quantitative data. 15 MMAT was used in line with its original purpose, to appraise mixed methods research and to evaluate non-randomised quantitative research. Two screening questions are asked, before progression to more detailed analysis: 1. Are there clear research questions? 2. Do the collected data allow to address the research questions? In this review, the answer to both of these questions had to be 'yes' for a study to qualify for inclusion. Evaluation using MMAT subsequently focussed most heavily on appraising methodology, assessing five core criteria for each study type. These core criteria can be reviewed in detail, with additional usage guidance, using the 2018 iteration of the MMAT tool. 15 To aid interpretation of what was meant by the core quality criteria, the research team referred to this expanded guidance. A summary of the core criteria for mixed methods research and Open access non-randomised quantitative research, the ways in which the MMAT was used in this work, are listed in table 2. The Cochrane risk of bias tool was used to appraise any randomised trial studies; as it is the gold-standard for such evaluation. 14 The Cochrane risk of bias tool has more stringent appraisal criteria, focussing\n\n[8] Improving the content validity of the mixed methods appraisal tool: a modified e-Delphi study. The mixed methods appraisal tool (MMAT) was developed for critically appraising different study designs. This study aimed to improve the content validity of three of the five categories of studies in the MMAT by identifying relevant methodological criteria for appraising the quality of qualitative, survey, and mixed methods studies. First, we performed a literature review to identify critical appraisal tools and extract methodological criteria. Second, we conducted a two-round modified e-Delphi technique. We asked three method-specific panels of experts to rate the relevance of each criterion on a five-point Likert scale. A total of 383 criteria were extracted from 18 critical appraisal tools and a literature review on the quality of mixed methods studies, and 60 were retained. In the first and second rounds of the e-Delphi, 73 and 56 experts participated, respectively. Consensus was reached for six qualitative criteria, eight survey criteria, and seven mixed methods criteria. These results led to modifications of eight of the 11 MMAT (version 2011) criteria. Specifically, we reformulated two criteria, replaced four, and removed two. Moreover, we added six new criteria. Results of this study led to improve the content validity of this tool, revise it, and propose a new version (MMAT version 2018).\n\n[9] The Mixed Methods Appraisal Tool (MMAT) version 2018 for information professionals and researchers The Mixed Methods Appraisal Tool (MMAT) version 2018 for information professionals and researchers INTRODUCTION: Appraising the quality of studies included in systematic reviews combining qualitative and quantitative evidence is challenging. To address this challenge, a critical appraisal tool was developed: the Mixed Methods Appraisal Tool (MMAT). The aim of this paper is to present the enhancements made to the MMAT. DEVELOPMENT: The MMAT was initially developed in 2006 based on a literature review on systematic reviews combining qualitative and quantitative evidence. It was subject to pilot and interrater reliability testing. A revised version of the MMAT was developed in 2018 based on the results from usefulness testing, a literature review on critical appraisal tools and a modified e-Delphi study with methodological experts to identify core criteria. TOOL DESCRIPTION: The MMAT assesses the quality of qualitative, quantitative, and mixed methods studies. It focuses on methodological criteria and includes five core quality criteria for each of the following five categories of study designs: (a) qualitative, (b) randomized controlled, (c) nonrandomized, (d) quantitative descriptive, and (e) mixed methods. CONCLUSION: The MMAT is a unique tool that can be used to appraise the quality of different study designs. Also, by limiting to core criteria, the MMAT can provide a more efficient appraisal.\n\n"}
{"case_id": "5e6e212dc63d2064ba909a08576cb3ec", "answer_text": "\n\nFMRI (functional MRI) and MRI (structural MRI) datasets differ significantly in their data analysis methodologies due to the inherently different types of information they capture and their respective applications in neuroimaging [10].\n\nPreprocessing Requirements [5]. This preprocessing step ensures that all datasets have consistent resolutions and orientations, which is crucial for accurate comparisons [5]. Standard preprocessing steps often include motion correction, spatial normalization, and smoothing [5]. Using widely accepted software packages like FSL, SPM, and FreeSurfer, preprocessing can be standardized across different datasets [5]. Implementing these techniques minimizes variability and enhances the robustness of subsequent analyses [5]. Additionally, preprocessing helps in reducing noise and improving the signal-to-noise ratio in the imaging data [5]. This is particularly important when working with large, mixed-site fMRI datasets that may have been acquired using different scanners and protocols [5]. By addressing these preprocessing requirements, researchers can ensure that their analyses are more reliable and comparable across different studies [5]. These steps also facilitate the application of advanced statistical methods and machine learning algorithms to the preprocessed data [5]. Obtaining standardized preprocessing pipelines for specific datasets can further improve the reproducibility of neuroimaging studies [5]. \n\nFunctional MRI (fMRI) is a widely used neuroimaging technique that measures and maps brain activity by detecting changes associated with blood flow [6]. It has been applied in various research areas, such as the classification of autism spectrum disorder and gender from resting-state fMRI data [2]. Additionally, it is used to explore both static and dynamic functional connectivity in neurological conditions like migraine [7]. Preprocessing of fMRI data, including steps like brain extraction and motion correction, is crucial for accurate analysis and results [5]. Techniques like Independent Component Analysis (ICA) help in deriving functionally connected brain networks from fMRI data [10]. Moreover, public datasets such as the Adolescent Brain Cognitive Development (ABCD) Study and the Alzheimer’s Disease Neuroimaging Initiative (ADNI) provide extensive fMRI data for research purposes [4]. Advances in deep learning have further enhanced the classification accuracy of fMRI data, enabling better analysis of complex brain conditions [11]. \nfMRI datasets focus on measuring brain activity by detecting changes associated with blood flow. This technique uses the blood-oxygen-level-dependent (BOLD) contrast to infer neural activity indirectly. The analysis of fMRI data typically involves complex preprocessing steps, such as motion correction, spatial smoothing, and temporal filtering, to enhance the signal-to-noise ratio and remove artifacts [5]. Preprocessing is critical due to the dynamic nature of fMRI data, which necessitates steps like temporal filtering to manage time-series data from multiple voxels [7][10]. The high temporal resolution of fMRI, which can capture rapid changes in brain activity, necessitates meticulous preprocessing to ensure reliable data for subsequent analyses [12].\n\nStructural MRI [5].\nIn contrast, structural MRI datasets provide high-resolution images of brain anatomy, focusing on the structural aspects rather than functional activity. These datasets are primarily used to study brain morphology and structural abnormalities. Segmentation, which partitions the brain into different tissue types (e.g., gray matter, white matter), and cortical thickness measurements are common preprocessing steps. Software packages like Brain Extraction Tool (BET), FMRIB Software Library (FSL), Statistical Parametric Mapping (SPM), and FreeSurfer are often employed to handle the relatively static and high-resolution nature of structural MRI datasets [5]. The high spatial resolution of structural MRI allows for detailed and precise examination of brain structures, which is crucial for accurate anatomical studies [10].\n\n### Analysis Techniques [0].\n\nReferences:\n[0] Title: Association of Neuroimaging Data with Behavioral Variables: A Class of Multivariate Methods and Their Comparison Using Multi-Task FMRI Data Text: the significant difference in the nature and dimensionality of the datasets. Since the dimensionality of the behavioral datasets tends to be significantly smaller than the imaging datasets, for example, in the few hundreds compared to hundreds of thousands in imaging datasets, fusion results can be affected by this difference in dimensionality,38. Furthermore, assumptions about the imaging features may not hold for the behavioral features due to differences in the nature of the data. Therefore, most of the previous work exploring the relationship between neuroimaging and behavioral datasets focus primarily on performing individual analyses on each neuroimaging dataset and conducting univariate comparisons or using one to predict another. A few recently published methods use fusion-based methods on neuroimaging datasets and show improved performance compared with individual analysis methods,43,44. However, in both the individual and fusion analysis-based approaches, simple correlation-based methods are used to identify associations between imaging and behavioral features,39,41,43. Unfortunately, these pairwise analysis methods do not fully leverage the available information across datasets. One natural way to address this problem is to divide it into parts, fuse the imaging datasets first, obtain summary statistics, and then jointly analyze the behavioral variables with the summary statistics to identify multivariate associations across imaging and behavioral features, which we propose. In this work, we focus on identifying multivariate associations between imaging and behavioral datasets and present a novel two-step approach based on IVA. We first make use of the strength of IVA to allow full interaction among the datasets to estimate joint independent components and\n\n\nFunctional MRI (fMRI) is widely used in neuroimaging research due to its non-invasive nature and its ability to measure and map brain activity by detecting changes associated with blood flow. It has notably been employed to explore the functional characteristics of the migraine brain, revealing significant differences in brain networks across different headache phases [7]. Additionally, fMRI is used extensively to classify various conditions, such as autism spectrum disorder, by analyzing functional connectivity matrices [2]. The dimensionality and variability of fMRI datasets pose significant challenges, necessitating advanced data processing and analysis techniques [6]. The use of large, multi-site fMRI datasets has enabled the development of more robust deep learning models for neuroimaging studies [11]. \nfMRI data analysis often includes specialized methods like independent component analysis (ICA) to decompose data into functionally relevant components [6][10]. Additionally, fMRI data analysis often involves assessing functional connectivity between different brain regions, as demonstrated by dynamic functional connectivity analysis [7]. These techniques are designed to handle the high temporal resolution but lower spatial resolution of fMRI data compared to structural MRI. The ability to track temporal changes in neural activity allows researchers to study brain function in real-time, making fMRI particularly suited for exploring cognitive processes and brain dynamics [6].\n\nStructural MRI .[5]\nStructural MRI analysis mainly involves high-resolution 3D images to assess brain structure. Techniques focus on segmentation and normalization to align anatomical structures correctly across subjects [5]. These tasks are relatively more straightforward than fMRI analysis, as they do not involve time-series data. The exceptional spatial resolution of structural MRI provides a detailed map of brain anatomy, enabling precise morphological studies and the identification of structural abnormalities [7].\n\n### Machine Learning Applications\nMachine Learning has increasingly been applied to differentiate patients and normal controls using functional magnetic resonance imaging data across various brain conditions [1]. Researchers have leveraged large, publicly accessible datasets, such as those from the Alzheimer's Disease Neuroimaging Initiative (ADNI), the 1000 Functional Connectomes Project, and the Autism Brain Imaging Data Exchange (ABIDE), to train deep learning models and improve the classification accuracy of neuroimaging data [4]. One common approach involves reducing feature dimensionality through techniques like Independent Component Analysis (ICA) and employing Support Vector Machines (SVM) for classification tasks, yielding promising results in multiple studies [2]. Additionally, emerging ensemble deep learning methodologies have demonstrated enhanced performance in the diagnosis and classification of conditions such as autism spectrum disorder (ASD), by effectively managing the inherent variability and high dimensionality of neuroimaging datasets [11]. These advanced classification techniques aim to optimize the use of neuroimaging data, drawing on comprehensive evaluations and robust cross-validation strategies to ensure reliable and reproducible results [9]. The continuous evolution and adoption of these machine learning techniques underscore their potential in revolutionizing neuroimaging analyses and clinical diagnostics [14]. \n\n\n\nFunctional MRI (fMRI) is a powerful neuroimaging technique used to measure and map brain activity. By detecting changes in blood flow, fMRI can infer neural activity, thus providing insights into both normal brain function and various neurological conditions [1]. One notable advantage of fMRI is its non-invasive nature, making it a valuable tool in both clinical and research settings. Moreover, the advancements in data analysis techniques have significantly enhanced the utilization of fMRI datasets, allowing for better understanding of complex brain functions and disorders [6].\n\n\nDeep learning models, such as convolutional neural networks (CNNs), have been applied to fMRI data to classify conditions like autism spectrum disorder and to identify functional connectivity patterns [11]. This approach leverages the temporal and spatial characteristics of fMRI data to capture complex patterns of brain activity [11].\n\nStructural MRI [5].\nIn structural MRI analyses, more traditional statistical methods are often employed to identify structural differences between populations or within individuals over time [5][14]. Nevertheless, machine learning techniques are increasingly being adapted to handle high-resolution anatomical data for predictive modeling [11].\n\n\n### Variability and Impact of Preprocessing\n\nResearch highlights that preprocessing is a necessary step in fMRI, and if preprocessing is not carried out properly, it can significantly impact the results [5]. For instance, the scanner used and parameters of the scanning process can influence statistical analysis outcomes, as demonstrated by comparing different datasets [14]. Moreover, the dimensionality of datasets and the techniques employed, such as Independent Component Analysis (ICA) and Independent Vector Analysis (IVA), play crucial roles in preserving subject variability and ensuring accurate data interpretation [13].\n\n\nFunctional MRI (fMRI) is a neuroimaging technique that measures brain activity by detecting changes associated with blood flow. It relies on the fact that cerebral blood flow and neuronal activation are coupled. When an area of the brain is more active, it consumes more oxygen, and to meet this increased demand, blood flow to the active region also increases[0][1]. This technique is often used to understand the functional anatomy of the brain, assess brain function, and assist in the diagnosis and management of brain disorders [1][3]. In recent years, advanced data analysis techniques, such as independent component analysis (ICA) and machine learning models, have improved the ability to extract meaningful information from fMRI datasets [6][13]. Both task-based and resting-state fMRI (rs-fMRI) are widely used in research and clinical practice [7].\nThe preprocessing steps for fMRI, such as motion correction and temporal filtering, are crucial due to the dynamic nature of the data [5]. Differences in preprocessing pipelines can significantly impact the results, as highlighted by the impact of preprocessing on false positive rates in fMRI data [14].\n\nStructural MRI is a non-invasive imaging technique that provides detailed images of the brain's anatomical structure. This method is widely used in neuroimaging studies to investigate brain morphology, including the size and shape of different brain regions, and to detect abnormalities such as tumors or lesions. Structural MRI has become an essential tool in both clinical and research settings for diagnosing and understanding various neurological conditions and diseases.\nFor structural MRI, segmentation and normalization are essential preprocessing steps to ensure that anatomical structures are correctly aligned across subjects [5]. These steps are designed to handle the relatively static nature of high-resolution structural data [10].\n\nConclusion\n\nIn summary, while both fMRI and structural MRI datasets provide valuable insights into brain function and structure, respectively, their data analysis methodologies diverge significantly. fMRI analysis focuses on dynamic, time-series data requiring advanced preprocessing and statistical methods to identify functional connectivity and activity patterns. In contrast, structural MRI analysis emphasizes high-resolution anatomical data, necessitating different preprocessing steps and analysis techniques to study brain morphology. These differences underline the importance of tailoring analysis methods to the specific characteristics of each type of dataset to derive meaningful and accurate results [13].\n\n\nReferences:\n[0] Association of Neuroimaging Data with Behavioral Variables: A Class of Multivariate Methods and Their Comparison Using Multi-Task FMRI Data the significant difference in the nature and dimensionality of the datasets. Since the dimensionality of the behavioral datasets tends to be significantly smaller than the imaging datasets, for example, in the few hundreds compared to hundreds of thousands in imaging datasets, fusion results can be affected by this difference in dimensionality,38. Furthermore, assumptions about the imaging features may not hold for the behavioral features due to differences in the nature of the data. Therefore, most of the previous work exploring the relationship between neuroimaging and behavioral datasets focus primarily on performing individual analyses on each neuroimaging dataset and conducting univariate comparisons or using one to predict another. A few recently published methods use fusion-based methods on neuroimaging datasets and show improved performance compared with individual analysis methods,43,44. However, in both the individual and fusion analysis-based approaches, simple correlation-based methods are used to identify associations between imaging and behavioral features,39,41,43. Unfortunately, these pairwise analysis methods do not fully leverage the available information across datasets. One natural way to address this problem is to divide it into parts, fuse the imaging datasets first, obtain summary statistics, and then jointly analyze the behavioral variables with the summary statistics to identify multivariate associations across imaging and behavioral features, which we propose. In this work, we focus on identifying multivariate associations between imaging and behavioral datasets and present a novel two-step approach based on IVA. We first make use of the strength of IVA to allow full interaction among the datasets to estimate joint independent components and\n\n[1] PAIR Comparison between Two Within-Group Conditions of Resting-State fMRI Improves Classification Accuracy Classification approaches have been increasingly applied to differentiate patients and normal controls using resting-state functional magnetic resonance imaging data (RS-fMRI). Although most previous classification studies have reported promising accuracy within individual datasets, achieving high levels of accuracy with multiple datasets remains challenging for two main reasons: high dimensionality, and high variability across subjects. We used two independent RS-fMRI datasets (n = 31, 46, respectively) both with eyes closed (EC) and eyes open (EO) conditions. For each dataset, we first reduced the number of features to a small number of brain regions with paired t-tests, using the amplitude of low frequency fluctuation (ALFF) as a metric. Second, we employed a new method for feature extraction, named the PAIR method, examining EC and EO as paired conditions rather than independent conditions. Specifically, for each dataset, we obtained EC minus EO (EC—EO) maps of ALFF from half of subjects (n = 15 for dataset-1, n = 23 for dataset-2) and obtained EO—EC maps from the other half (n = 16 for dataset-1, n = 23 for dataset-2). A support vector machine (SVM) method was used for classification of EC RS-fMRI mapping and EO mapping. The mean classification accuracy of the PAIR method was 91.40% for dataset-1, and 92.75% for dataset-2 in the conventional frequency band of 0.01–0.08 Hz. For cross-dataset validation, we applied the classifier from dataset-1 directly to dataset-2, and vice versa. The mean accuracy of cross-dataset validation was 94.93% for dataset-1 to dataset-2 and 90.32% for dataset-2 to dataset-1 in the 0.01–0.08 Hz range. For the UNPAIR method, classification accuracy was substantially lower (mean 69.89% for dataset-1 and 82.97% for dataset-2), and was much lower for cross-dataset validation (64.69% for dataset-1 to dataset-2 and 64.98% for dataset-2 to dataset-1) in the 0.01–0.08 Hz range. In conclusion, for within-group design studies (e.g., paired conditions or follow-up studies), we recommend the PAIR method for feature extraction. In addition, dimensionality reduction with strong prior knowledge of specific brain regions should also be considered for feature selection in neuroimaging studies.\n\n[2] Ensemble Deep Learning on Large, Mixed-Site fMRI Datasets in Autism and Other Tasks In this article, we leverage publicly available datasets to amass and automatically pre-process a total of 43,838 functional MRIs from nine different collections. To test the application of CNNs to imaging data, we first classify autistic individuals from typically developing (TD) controls. To validate the proposed models, we then classify functional connectivity matrices based on gender and task vs resting state. All classifications were undertaken using a CNN that uniquely encodes multi-layered connectivity matrices, using an original deep learning architecture, partially inspired by Kawahara et al 2017. We opted to use these connectivity matrices as opposed to full fMRI datasets both for memory management purposes (the average fMRI dataset in our collection is 176 MB per file, while the connectivity matrix is just under 500 KB), and for interpretability, as connectivity matrices allow for the direct analysis of both localized areas and connections between areas. Due to the stochastic properties of NNs and set divisions, we use a standard stratified cross-validation strategy, performing each of our tests across 300 independent models using different subsamples and divisions of the total dataset. To incentivise the model to classify based on phenotypic differences rather than centre differences, class-balancing techniques across participant age and collection were used when building the training and test sets, and compared against the fully-inclusive samples.\n\n[3] Identification of Voxels Confounded by Venous Signals Using Resting-State fMRI Functional Connectivity Graph Community Identification Identifying venous voxels in fMRI datasets is important to increase the specificity of fMRI analyses to microvasculature in the vicinity of the neural processes triggering the BOLD response. This is, however, difficult to achieve in particular in typical studies where magnitude images of BOLD EPI are the only data available. In this study, voxelwise functional connectivity graphs were computed on minimally preprocessed low TR (333 ms) multiband resting-state fMRI data, using both high positive and negative correlations to define edges between nodes (voxels). A high correlation threshold for binarization ensures that most edges in the resulting sparse graph reflect the high coherence of signals in medium to large veins. Graph clustering based on the optimization of modularity was then employed to identify clusters of coherent voxels in this graph, and all clusters of 50 or more voxels were then interpreted as corresponding to medium to large veins. Indeed, a comparison with SWI reveals that 75.6±5.9% of voxels within these large clusters overlap with veins visible in the SWI image or lie outside the brain parenchyma. Some of the remaining differences between the two modalities can be explained by imperfect alignment or geometric distortions between the two images. Overall, the graph clustering based method for identifying venous voxels has a high specificity as well as the additional advantages of being computed in the same voxel grid as the fMRI dataset itself and not needing any additional data beyond what is usually acquired (and exported) in standard fMRI experiments.\n\n[4] Ensemble Deep Learning on Large, Mixed-Site fMRI Datasets in Autism and Other Tasks Datasets were acquired from OpenFMRI, 59; the Alzheimer’s Disease Neuroimaging Initiative (ADNI); ABIDE; ABIDE II; the Adolescent Brain Cognitive Development (ABCD) Study; the NIMH Data Archive, including the Research Domain Criteria Database (RDoCdb), the National Database for Clinical Trials (NDCT), and, predominantly, the National Database for Autism Research (NDAR); the 1000 Functional Connectomes Project; the International Consortium for Brain Mapping database (ICBM); and the UK Biobank; we refer to each of these sets as collections. OpenFMRI, NDAR, ICBM, and the 1000 Functional Connectomes Project are collections that comprise different datasets submitted from unrelated research groups. ADNI, ABIDE, ABIDE II, ABCD, and the UK Biobank are collections that were acquired as part of a larger research initiative.\n\n[5] Automatic autism spectrum disorder detection using artificial intelligence methods with MRI neuroimaging: A review research. ABIDE is recognized as the most complete and freely available database of MRI modalities for the automatic diagnosis of ASD (Khodatars et al., 2021). This dataset has two parts, ABIDE 1 and ABIDE-II, containing sMRI data, rs-fMRI data, and phenotypic data. 1112 datasets are involved in ABIDE I, and 1114 datasets are included in ABIDE II (Khodatars et al., 2021). ABIDE 1 also contains preprocessed data from MRI modalities for research, known as the preprocessed connectomes project (PCP) (Khodatars et al., 2021). Additionally, other available datasets, such as NDAR, UCI, and NRGR, have been used in ASD diagnostic. The results show that these datasets have been able to achieve satisfactory results. The datasets used for each study are summarized in Preprocessing techniques for functional and structural modalities Preprocessing techniques are needed to help CADS to achieve promising results. The sMRI and fMRI neuroimaging modalities have implemented fixed preprocessing steps using different software packages. The most common software packages are brain extraction tools (BET) (Soltaninejad et al., 2014), FMRIB software libraries (FSL), statistical parametric mapping (SPM), and FreeSurfer (Khodatars et al., 2021). The following is the standard preprocessing steps for fMRI and sMRI neuroimaging modalities. Some of them are common for both fMRI and sMRI modalities, so we will introduce them in the fMRI-related section. Figure 3 shows the standard fMRI and sMRI techniques. Also, the pipeline preprocessing techniques for ABIDE datasets will be introduced in another section. The standard Preprocessing is a necessary step in fMRI, and if preprocessing is not carried out properly, it\n\n[6] \"More Is Different\" in Functional Magnetic Resonance Imaging: A Review of Recent Data Analysis Techniques \"More Is Different\" in Functional Magnetic Resonance Imaging: A Review of Recent Data Analysis Techniques Two aspects play a key role in recently developed strategies for functional magnetic resonance imaging (fMRI) data analysis: first, it is now recognized that the human brain is a complex adaptive system and exhibits the hallmarks of complexity such as emergence of patterns arising out of a multitude of interactions between its many constituents. Second, the field of fMRI has evolved into a data-intensive, big data endeavor with large databases and masses of data being shared around the world. At the same time, ultra-high field MRI scanners are now available producing data at previously unobtainable quality and quantity. Both aspects have led to shifts in the way in which we view fMRI data. Here, we review recent developments in fMRI data analysis methodology that resulted from these shifts in paradigm.\n\n[7] Dynamic functional connectivity of migraine brain: a resting-state fMRI study. Migraine headache is an episodic phenomenon, and patients with episodic migraine have ictal (headache), peri-ictal (premonitory, aura, and postdrome), and interictal (asymptomatic) phases. We aimed to find the functional characteristics of migraine brain regardless of headache phase using dynamic functional connectivity analysis. We prospectively recruited 50 patients with migraine and 50 age- and sex-matched controls. All subjects underwent a resting-state functional MRI. Significant networks were defined in a data-driven fashion from the interictal (>48 hours apart from headache phases) patients and matched controls (interictal dataset) and tested to ictal or peri-ictal patients and controls (ictal/peri-ictal dataset). Both static and dynamic analyses were used for the between-group comparison. A false discovery rate correction was performed. As a result, the static analysis did not reveal a network which was significant in both interictal and ictal/peri-ictal datasets. Dynamic analysis revealed significant between-group differences in seven brain networks in the interictal dataset, among which a frontoparietal network (controls > patients, p=0.0467), two brainstem networks (patients > controls, p=0.0467 and <0.001), and a cerebellar network (controls > patients, p=0.0408 and <0.001 in two states) remained significant in the ictal/peri-ictal dataset. Using these networks, migraine was classified with a sensitivity of 0.70 and specificity of 0.76 in the ictal/peri-ictal dataset. In conclusion, the dynamic connectivity analysis revealed more functional networks related to migraine than the conventional static analysis, suggesting a substantial temporal fluctuation in functional characteristics. Our data also revealed migraine-related networks which show significant difference regardless of headache phases between patients and controls.\n\n[8] Ensemble Deep Learning on Large, Mixed-Site fMRI Datasets in Autism and Other Tasks Data were divided into three sets: a training set, comprising of two-thirds of the data and used to train the model; a validation set, comprising of one-sixth of the data and used to select the epoch at which training stopped; and a test set, used to assess the trained classifier performance, comprising of one-sixth of the data. The approximate total number of images used by each model was 10,000 for the gender and resting-state classification, and 4000 (limited by sample size) for the ASD classification.For all classifications, balancing was used such that each class comprised approximately half of the datasets. To account for covariates, classes were additionally balanced such that the distributions of different collections and ages were equal between classes. For collection balancing, equal numbers of datasets were used from each collections. For continuous age values, distributions of age between classes were made to fail a Mann-Whitney U-test, with p>0.05𝑝0.05p>0.05. We used standard stratified cross-validation rather than a holdout division across the 300 runs.\n\n[9] Temporal and Spatial Independent Component Analysis for fMRI Data Sets Embedded in the AnalyzeFMRI R Package Temporal and Spatial Independent Component Analysis for fMRI Data Sets Embedded in the AnalyzeFMRI R Package For statistical analysis of functional magnetic resonance imaging (fMRI) data sets, we propose a data-driven approach based on independent component analysis (ICA) implemented in a new version of the AnalyzeFMRI R package. For fMRI data sets, spatial dimension being much greater than temporal dimension, spatial ICA is the computationally tractable approach generally proposed. However, for some neuroscientific applications, temporal independence of source signals can be assumed and temporal ICA becomes then an attractive exploratory technique. In this work, we use a classical linear algebra result ensuring the tractability of temporal ICA. We report several experiments on synthetic data and real MRI data sets that demonstrate the potential interest of our R package. Introduction Magnetic resonance imaging (MRI) is now a prominent non-invasive neuroimaging technique largely used in clinical routine and advanced brain research. Its success is largely due to a combination of at least three factors: (1) sensitivity of MR signal to various physiological parameters that characterize normal or pathological living tissues (such as diffusion properties of water molecules, relaxation time of proton magnetization or blood oxygenation) leading to a vast set of MRI modalities (respectively restricted in our example to diffusion MRI, weighted structural and functional MRI); (2) constant hardware improvements (e.g., mastering high field homogeneous magnets and high linear magnetic field gradients respectively allow an increase of spatial resolution or a reduction of acquisition time); and (3) sustained efforts in various laboratories to develop methods: for image processing\n\n[10] Ensemble Deep Learning on Large, Mixed-Site fMRI Datasets in Autism and Other Tasks Deep learning models for MRI classification face two recurring problems: they are typically limited by low sample size, and are abstracted by their own complexity (the \"black box problem\"). In this paper, we train a convolutional neural network (CNN) with the largest multi-source, functional MRI (fMRI) connectomic dataset ever compiled, consisting of 43,858 datapoints. We apply this model to a cross-sectional comparison of autism spectrum disorder (ASD) versus typically developing (TD) controls that has proved difficult to characterize with inferential statistics. To contextualize these findings, we additionally perform classifications of gender and task versus rest. Employing class-balancing to build a training set, we trained 3 × 300 modified CNNs in an ensemble model to classify fMRI connectivity matrices with overall AUROCs of 0.6774, 0.7680, and 0.9222 for ASD versus TD, gender, and task versus rest, respectively. Additionally, we aim to address the black box problem in this context using two visualization methods. First, class activation maps show which functional connections of the brain our models focus on when performing classification. Second, by analyzing maximal activations of the hidden layers, we were also able to explore how the model organizes a large and mixed-center dataset, finding that it dedicates specific areas of its hidden layers to processing different covariates of data (depending on the independent variable analyzed), and other areas to mix data from different sources. Our study finds that deep learning models that distinguish ASD from TD controls focus broadly on temporal and cerebellar connections, with a particularly high focus on the right caudate nucleus and paracentral sulcus.\n\n[11] Comparison of simultaneous arterial spin labeling MRI and 15O-H2O PET measurements of regional cerebral blood flow in rest and altered perfusion states Arterial spin labelling (ASL) is a non-invasive magnetic resonance imaging (MRI) technique that may provide fully quantitative regional cerebral blood flow (rCBF) images. However, before its application in clinical routine, ASL needs to be validated against the clinical gold standard, 15O-H2O positron emission tomography (PET). We aimed to compare the two techniques by performing simultaneous quantitative ASL-MRI and 15O-H2O-PET examinations in a hybrid PET/MRI scanner. Duplicate rCBF measurements were performed in healthy young subjects (n = 14) in rest, during hyperventilation, and after acetazolamide (post-ACZ), yielding 63 combined PET/MRI datasets in total. Average global CBF by ASL-MRI and 15O-H2O-PET was not significantly different in any state (40.0 ± 6.5 and 40.6 ± 4.1 mL/100 g/min, respectively in rest, 24.5 ± 5.1 and 23.4 ± 4.8 mL/100 g/min, respectively, during hyperventilation, and 59.1 ± 10.4 and 64.7 ± 10.0 mL/100 g/min, respectively, post-ACZ). Overall, strong correlation between the two methods was found across all states (slope = 1.01, R2 = 0.82), while the correlations within individual states and of reactivity measures were weaker, in particular in rest (R2 = 0.05, p = 0.03). Regional distribution was similar, although ASL yielded higher perfusion and absolute reactivity in highly vascularized areas. In conclusion, ASL-MRI and 15O-H2O-PET measurements of rCBF are highly correlated across different perfusion states, but with variable correlation within and between hemodynamic states, and systematic differences in regional distribution.\n\n[12] Preserving subject variability in group fMRI analysis: performance evaluation of GICA vs. IVA Independent component analysis (ICA) is a widely applied technique to derive functionally connected brain networks from fMRI data. Group ICA (GICA) and Independent Vector Analysis (IVA) are extensions of ICA that enable users to perform group fMRI analyses; however a full comparison of the performance limits of GICA and IVA has not been investigated. Recent interest in resting state fMRI data with potentially higher degree of subject variability makes the evaluation of the above techniques important. In this paper we compare component estimation accuracies of GICA and an improved version of IVA using simulated fMRI datasets. We systematically change the degree of inter-subject spatial variability of components and evaluate estimation accuracy over all spatial maps (SMs) and time courses (TCs) of the decomposition. Our results indicate the following: (1) at low levels of SM variability or when just one SM is varied, both GICA and IVA perform well, (2) at higher levels of SM variability or when more than one SMs are varied, IVA continues to perform well but GICA yields SM estimates that are composites of other SMs with errors in TCs, (3) both GICA and IVA remove spatial correlations of overlapping SMs and introduce artificial correlations in their TCs, (4) if number of SMs is over estimated, IVA continues to perform well but GICA introduces artifacts in the varying and extra SMs with artificial correlations in the TCs of extra components, and (5) in the absence or presence of SMs unique to one subject, GICA produces errors in TCs and IVA estimates are accurate. In summary, our simulation experiments (both simplistic and realistic) and our holistic analyses approach indicate that IVA produces results that are closer to ground truth and thereby better preserves subject variability. The improved version of IVA is now packaged into the GIFT toolbox (http://mialab.mrn.org/software/gift).\n\n[13] Analysis of BOLD fMRI Signal Preprocessing Pipeline on Different Datasets while Reducing False Positive Rates interesting conclusions can be drawn. By comparing results of one patient dataset against the other patient dataset with corresponding size, say between Changgung CS and Changgung SCI, it is clear that the results have similar trend when adding preprocessing steps ( Figure 3). The data has zero false positive rates until covariates regression is performed. The following filtering step continues inflating false positives. It seems that although the neuro systems of patients show different level of malfunctions, there exist similarity in their rs-BOLD fMRI data time series. And after the same preprocessing steps, this similarity is shown by calculating the false positive rates. We can conclude that for rs-BOLD fMRI data analyzed using statistical method, the state of the subject (healthy or ill) does not affect false positive rates. On the other hand, how may the scanning machine used influence the results? By comparing Changgung normal and Beijing r38 dataset, it is clear that the results of two datasets are not the same, despite they all consist rs-BOLD fMRI data of healthy controls. In fact, Changgung normal dataset has similar trend as other Changgung patient datasets results, while Beijing r38 dataset and Beijing dataset are alike. This further confirms that the scanner used has significant influence on statistical analysis. As a result, we advocate all future researches take a closer look at the scanner, including parameters of the scanning process and reconstruction software used by the scanner. In the work of Eklund et al., 3 datasets (Beijing, Cambridge and Oulu) are analyzed under 4 different\n\n"}
{"case_id": "5480ba91951fc42e9beb989eea40360d", "answer_text": "\n\nThe integration of Explainable Artificial Intelligence (XAI) techniques such as SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations) significantly enhances the interpretability of complex machine learning models, addressing the \"black-box\" nature of these models [0]. Both SHAP and LIME provide methodologies to explain individual predictions by highlighting the contribution of each feature, thereby making complex models more transparent and trustworthy [2]. \n\n\n### SHAP's Contribution to Interpretability\n\nSHapley Additive exPlanations (SHAP) is an XAI method designed based on game theory. It aims to explain any model by considering each feature (or predictor) in the model as a player and the model outcome as the payoff [0].\n\n\nSHAP, inspired by game theory, assigns each feature an importance value that represents its contribution to the model's prediction. This method can provide both local and global explanations [0][2]. SHAP offers theoretical guarantees of consistency and local accuracy, which ensures that the explanations are both fair and reliable [11]. For instance, in the context of diagnosing COVID-19 based on gene expressions, SHAP identified key biomarkers contributing to the disease prediction, thus offering clinicians a clear understanding of the model's decision-making process [4]. Similarly, in finance, SHAP has been employed to explain loan decisions, demonstrating its versatility across domains [8].\n\nThe SHAP algorithm operates by calculating Shapley values, which represent the average marginal contribution of a feature to every possible coalition of features. This involves considering all possible subsets of features, which ensures that the contributions are fairly distributed [0][3]. While this provides robust explanations, it also makes SHAP computationally intensive, particularly when dealing with a large number of features, as calculating Shapley values has exponential complexity [12]. Moreover, SHAP assumes that features are independent, which may not always be the case in real-world datasets. This assumption can lead to misleading interpretations when feature dependencies exist [1]. Nevertheless, SHAP's robustness in communicating feature importance makes it a reliable tool for interpreting complex models, even in high-stakes domains such as healthcare, finance, and credit risk management [8][12].\n\nLIME's Contribution to Interpretability [0].\n\nLIME, on the other hand, approximates the complex model locally for a specific instance by fitting a simpler, interpretable model, such as a linear regression, in the vicinity of the instance [0][2]. This method is particularly useful for providing instance-specific explanations, which can help users understand why a model made a particular prediction for a specific case [1][6]. For example, in the diagnosis of retinoblastoma, LIME was able to identify critical regions in medical images that influenced the model's predictions, thereby offering valuable insights to medical professionals [6]. In the realm of cyber-fraud detection, LIME has been applied to generate explanations for predictions, enhancing trust in automated fraud detection systems [2].\n\nThe LIME algorithm works by perturbing the input data around a specific instance and observing how these perturbations affect the model’s predictions. It then fits an interpretable model to these perturbed samples to approximate the behavior of the complex model locally [0][7]. This approach of generating local explanations through perturbations and approximations makes it computationally more efficient compared to SHAP, particularly for large datasets [3]. However, LIME has its own limitations. It can be sensitive to the choice of perturbations, leading to variability in explanations [11]. Moreover, LIME is predominantly designed for local interpretability, making it less suitable for gaining a holistic understanding of the model's global behavior. Additionally, LIME does not handle feature dependencies well, similar to SHAP, which can lead to inaccurate explanations when features are correlated [1].\n\n\n\n### Comparison and Practical Implications\n\nSHapley Additive exPlanations (SHAP) is an XAI method designed based on game theory. It aims to explain any model by considering each feature (or predictor) in the model as a player and the model outcome as the payoff [0]. SHAP provides local and global explanation, meaning that it is able to explain the role of the features for all instances and for a specific instance [0]. Local Interpretable Model Agnostic Explanation (LIME) is another XAI method that aims to explain how the model works locally for a specific instance in the model [0]. It approximates any complex model and transfers it to a local interpretable model for a specific instance [0]. Many other approaches and methods were proposed with the similar aims to make the machine learning models more interpretable [0]. SHAP and LIME are the two most common XAI methods applied to tabular data [0].\n\n\n\nWhile both SHAP and LIME are powerful tools for model interpretability, they have distinct strengths and limitations. SHAP's global interpretability and theoretical guarantees make it well-suited for applications where understanding the overall behavior of the model is essential, such as in identifying biomarkers for diseases [4], credit risk management [8], and brain age prediction in neuroscience [9]. On the other hand, LIME's efficiency and instance-specific explanations are beneficial in scenarios where quick and localized understanding is required, such as in fraud detection [2] or medical imaging [6].\n\nFurthermore, combining SHAP and LIME can offer complementary insights, providing a more holistic view of the model’s behavior. For instance, in a study on COVID-19 diagnosis, using both SHAP and LIME allowed researchers to cross-verify the importance of specific features, thereby enhancing the reliability of the interpretations [4]. Similarly, in credit risk management, combining these methods ensured a more robust understanding of feature importance [8]. However, it is crucial to present the results of these XAI methods in a simple and understandable manner to end users, who may not have a technical background [1]. This can involve converting feature importance scores into more digestible forms, such as coefficients, and providing clear visualizations and explanations [1][2].\n\n\n\n### Other Contemporary XAI Methods [0]\n\n\n\nWhile SHAP and LIME are among the most well-known XAI techniques [0], there are several other methods that contribute to explainability.\n\n1. **iBreakdown**: This method extends the SHAP technique by considering local interactions between features, providing non-additive explanations that are particularly useful in the presence of feature interactions [5].\n   \n2. **DeepLIFT and Grad-CAM**: Specifically designed for deep learning models, these techniques offer explanations by attributing contributions back to the input features, making them suitable for interpreting complex neural networks [11].\n   \n3. **Global Surrogate Models**: These models approximate the predictions of complex AI models with simpler, interpretable ones, like linear models or decision trees, offering global explanations for how the complex model functions [7].\n\nSHapley Additive exPlanations (SHAP) is an XAI method designed based on game theory. It aims to explain any model by considering each feature (or predictor) in the model as a player and the model outcome as the payoff [0]. SHAP provides local and global explanation, meaning that it is able to explain the role of the features for all instances and for a specific instance [0]. Local Interpretable Model Agnostic Explanation (LIME) is another XAI method that aims to explain how the model works locally for a specific instance in the model [0]. It approximates any complex model and transfers it to a local interpretable model for a specific instance [6]. SHAP and LIME are the two most common XAI methods applied to tabular data [1]. Despite the limitations of SHAP and LIME in terms of uncertainty estimates, generalization, feature dependencies, and inability to infer causality, they have substantial value in explaining and interpreting complex machine models [12]. \n\nIntegrating XAI techniques like SHAP and LIME into machine learning workflows significantly enhances model interpretability by providing both local and global explanations [0]. SHAP is robust and theoretically sound, suitable for applications requiring a comprehensive understanding of model behavior, while LIME's efficiency and instance-specific insights are ideal for scenarios needing quick, localized explanations [1]. Practitioners should consider their specific needs—global versus local interpretability—and the computational resources available when choosing between SHAP and LIME [3]. Additionally, the combination of these methods or the use of other contemporary XAI techniques can offer more comprehensive and reliable insights into complex machine learning models [0]. \n\n\nReferences:\n[0] A Perspective on Explainable Artificial Intelligence Methods: SHAP and LIME Several approaches have been proposed as XAI methods dealing with a variety of data and model types, aiming to locally and globally explain the models outputs, from which SHAP  and LIME  form the most popular XAI methods. SHapley Additive exPlanations (SHAP)  is an XAI method designed based on game theory. It aims to explain any model by considering each feature (or predictor) in the model as a player and the model outcome as the payoff. SHAP provides local and global explanation, meaning that it is able to explain the role of the features for all instances and for a specific instance. Local Interpretable Model Agnostic Explanation (LIME)  is another XAI method that aims to explain how the model works locally for a specific instance in the model. It approximates any complex model and transfers it to a local interpretable model for a specific instance. Many other approaches and methods were proposed with the similar aims to make the machine learning models more interpretable. SHAP and LIME are the two most common XAI methods applied to tabular data. Despite the limitations of SHAP and LIME in terms of uncertainty estimates, generalization, feature dependencies, and inability to infer causality , they have substantial value in explaining and interpreting complex machine models.However, does the end user understands how these XAI methods work? And why they identify specific regions/features as more informative than others? Is it enough for the end user to know that these features/regions are more informative, because they increase the model output without knowing how the XAI method came up with such a result? For example, when SHAP assigns a high/low score for a feature, does the end user knows how this score is calculated? SHAP and LIME perform many analyses in the background and solve complex equations to come up with their explanations. In many settings, complex models will be interpreted by non-expert end-users, who may find understanding of the workings of XAI methods challenging. It is not expected that the end users from different domains understand every minutiae of XAI methods, but it is vital that they are aware of the general framework of the XAI method used. While XAI methods aimed to unveil the complexity of complex black box models, they themselves suffer from the same issue, in that their usefulness may be limited by the complexity of their outputs. In this commentary piece, we will discuss SHAP and LIME XAI methods, highlighting their underlying assumptions and whether the end users are possible to grasp their key concepts appropriately. We will also present some notions to increase the understanding XAI methods and promote their appropriate use by researchers.\n\n[1] A Perspective on Explainable Artificial Intelligence Methods: SHAP and LIME SHAP and LIME are two XAI methods that aid understanding of machine learning models in different research fields. They have been implemented in some sensitive domains , 19, 20 where misinterpreting the outcomes might be very expensive or critical. Data scientists who are working daily on machine learning and XAI, over-trust the explanations generated by XAI methods, misuse the interpretability tools and do not accurately understand the visualized output of the XAI methods .It is crucial that SHAP results are presented alongside the plots output with a simple language to explain the outcomes and the assumptions behind SHAP e.g. features are independent and model-dependent. Moreover, it would be better that the end user implements different machine learning models when the features are collinear in order to compare and contrast the outcome of SHAP from each model. Thereafter, the NMR  value would be useful to pick the model that presents the more stable list of informative features generated by SHAP if the aim is to explain the model globally. On the other hand, if the aim is to explain the model locally for a single instance or sub-group of individuals, then approximated SHAP value  would be better to be applied as this is a modified version of SHAP that takes into account the collinearity among the features. In addition, converting the scores of each feature of the model into a more digestible form e.g. coefficient value would inevitably increase the understanding of the score and ultimately the method itself. It is worthy that LIME provides explanation regarding the local model linearity with the model outcome as the users might not be familiar with the concept behind LIME. The users will be much aware and understand the outcome when a simple language accompanied the outcome. Moreover, the explanation of LIME might be different using the same model, but for other instance. In other words, the interpretation of LIME only applies for one subject and cannot be used or considered as a general interpretation for the whole model.\n\n[2] A Real-Time Prescriptive Solution for Explainable Cyber-Fraud Detection Within the iGaming Industry ability to explain a specific prediction is vital, especially when such a prediction motivates or directly invokes another action or reaction. XAI systems would not just help to instil trust in an AI but also act as a medium to understand better what the model believes to be the causal factors of a specific problem. XAI also provides a glimpse of how the model will behave in the future. A model can be either explained globally or locally. A global XAI system attempts to explain to the entire model while a local explanation works by generating the rationale behind every individual sample and subsequent prediction. One of the most recent advancements in local explanations is the local interpretable model-agnostic explanation (LIME) technique, introduced in Ribeiro et al.. Given an incoming sample for prediction, LIME approximates local linear artificial data points (in the neighbourhood of the incoming sample) and performs data perturbation to determine how every feature influences the model's prediction. This process results in prediction explanations consisting of the influential features represented as a mathematical equality. Lundberg and Lee also tackled the problem of prediction interpretability by presenting SHapley Additive exPlanations (SHAP). Similarly to LIME, SHAP also yields local explanations. Inspired by game theory, SHAP also determines the contribution of every feature to the model's prediction per incoming sample. One of the advantages of LIME and SHAP is their model agnosticism property, meaning that both techniques can work in conjunction with any model. Despite XAI still being in its infancy stage, researchers\n\n[3] State-of-the-Art Explainability Methods with Focus on Visual Analytics Showcased by Glioma Classification xAI method based on solid theory (Shapely value). Moreover, SHAP guarantees that the prediction is fairly distributed among all feature values. On the other hand, LIME for local explanation is faster than SHAP concerning computation time. In particular, if the aim is to analyze huge data sets, then LIME will provide a suitable alternative to the time-consuming computation of Shapely values. The SHAP approach considers this challenge by using approximation and optimization; however, not all model types are supported yet. In particular, LIME supports tabular data, text, and images. In other xAI methods, it is rare that all these types of data are supported. Discussion The output of any ML model should be comparable and interpretable. This is of particular interest to researchers in the medical domain as for cancer, where model performance may be compared with the one of clinicians. Some experts from the medical domain argue that transparency for black boxes is not of primary interest to AI applications in their domain, as doctors make diagnoses based on their experience, and complete information on the causality of medical issues are rare,49. However, xAI methods can help to gain new insights and forward biomedical knowledge to better understand interrelated characteristics and signaling components in pathologies. As a modeling approach, classifying glioma sub-types is exemplified: As the chosen dataset combining data from different brain tumor studies comprises sample data primarily from the glioma subtypes GBM, AASTR, and ODG, these three disease types were chosen to be classified to apply VA methods for interpreting\n\n[4] Explainable artificial intelligence model for identifying COVID-19 gene biomarkers. COVID-19 has revealed the need for fast and reliable methods to assist clinicians in diagnosing the disease. This article presents a model that applies explainable artificial intelligence (XAI) methods based on machine learning techniques on COVID-19 metagenomic next-generation sequencing (mNGS) samples. In the data set used in the study, there are 15,979 gene expressions of 234 patients with COVID-19 negative 141 (60.3%) and COVID-19 positive 93 (39.7%). The least absolute shrinkage and selection operator (LASSO) method was applied to select genes associated with COVID-19. Support Vector Machine - Synthetic Minority Oversampling Technique (SVM-SMOTE) method was used to handle the class imbalance problem. Logistics regression (LR), SVM, random forest (RF), and extreme gradient boosting (XGBoost) methods were constructed to predict COVID-19. An explainable approach based on local interpretable model-agnostic explanations (LIME) and SHAPley Additive exPlanations (SHAP) methods was applied to determine COVID-19- associated biomarker candidate genes and improve the final model's interpretability. For the diagnosis of COVID-19, the XGBoost (accuracy: 0.930) model outperformed the RF (accuracy: 0.912), SVM (accuracy: 0.877), and LR (accuracy: 0.912) models. As a result of the SHAP, the three most important genes associated with COVID-19 were IFI27, LGR6, and FAM83A. The results of LIME showed that especially the high level of IFI27 gene expression contributed to increasing the probability of positive class. The proposed model (XGBoost) was able to predict COVID-19 successfully. The results show that machine learning combined with LIME and SHAP can explain the biomarker prediction for COVID-19 and provide clinicians with an intuitive understanding and interpretability of the impact of risk factors in the model.\n\n[5] iBreakDown: Uncertainty of Model Explanations for Non-additive Predictive Models Explainable Artificial Intelligence (XAI) brings a lot of attention recently. Explainability is being presented as a remedy for lack of trust in model predictions. Model agnostic tools such as LIME, SHAP, or Break Down promise instance level interpretability for any complex machine learning model. But how certain are these explanations? Can we rely on additive explanations for non-additive models? In this paper, we examine the behavior of model explainers under the presence of interactions. We define two sources of uncertainty, model level uncertainty, and explanation level uncertainty. We show that adding interactions reduces explanation level uncertainty. We introduce a new method iBreakDown that generates non-additive explanations with local interaction.\n\n[6] Explainable AI for Retinoblastoma Diagnosis: Interpreting Deep Learning Models with LIME and SHAP Retinoblastoma is a rare and aggressive form of childhood eye cancer that requires prompt diagnosis and treatment to prevent vision loss and even death. Deep learning models have shown promising results in detecting retinoblastoma from fundus images, but their decision-making process is often considered a “black box” that lacks transparency and interpretability. In this project, we explore the use of LIME and SHAP, two popular explainable AI techniques, to generate local and global explanations for a deep learning model based on InceptionV3 architecture trained on retinoblastoma and non-retinoblastoma fundus images. We collected and labeled a dataset of 400 retinoblastoma and 400 non-retinoblastoma images, split it into training, validation, and test sets, and trained the model using transfer learning from the pre-trained InceptionV3 model. We then applied LIME and SHAP to generate explanations for the model’s predictions on the validation and test sets. Our results demonstrate that LIME and SHAP can effectively identify the regions and features in the input images that contribute the most to the model’s predictions, providing valuable insights into the decision-making process of the deep learning model. In addition, the use of InceptionV3 architecture with spatial attention mechanism achieved high accuracy of 97% on the test set, indicating the potential of combining deep learning and explainable AI for improving retinoblastoma diagnosis and treatment.\n\n[7] Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey Global surrogate models could be used as a way to approximate the predictions of highly non-linear AI models with an interpretable linear model or a decision tree. Global explanations answers the ’How’ in XAI, specifically “How generalized is my AI model?”, “How do variations of my AI model perform?”. A general use case of surrogate models in deep learning would be extraction of feature-rich layer embeddings for test inputs and training a linear classifier on the embeddings.The coefficients of the linear model could give insights to how the model behaves. In a high-level, SHAP and LIME can both be considered as surrogate models with different methodology to understand the local correlations than linear models. SpRAy technique we will see in Section IV-B5 also extract local features from a group of data to understand model behavior.\n\n[8] SHAP and LIME: An Evaluation of Discriminative Power in Credit Risk by (Lundberg and Lee, 2017). In finance, interpretability is especially important because the reliance of the model on the correct features must be guaranteed; yet, there aren't many studies focusing on the application of XAI in this specific context (Bussmann, 2020). propose a XAI model based on Shapley values applied in the context of loan decisions regarding SME seeking for financing through P2P platforms, whereas the research by (Ariza-Garzón et al., 2020) aim to assess the predictive capacity of several ML models in the context of P2P lending platforms' credit scoring, after that applying the Shapley method to provide explainability to the prediction. The most interesting precedent is perhaps the research of (HadjiMisheva et al., 2021), where the authors explore the utility of both SHAP and Lime frameworks in the context of credit risk management, outlining the practical hurdles in applying these techniques to several different kinds of ML algorithms as well as proposing solutions to the challenges faced. Our study aims to compare SHAP and LIME frameworks by evaluating their ability to define distinct groups of observations, employing the weights assigned to features through their local interpretability algorithm as input space for unsupervised approached and a supervised one. We do this building our approach on one of the best performing, yet complex, supervised learning algorithm, XGBoost (Chen and Guestrin, 2016), employed to predict the probability of default of italian Small and Medium Enterprises. LIME Locally Interpretable Model Agnostic Explanations is a post-hoc model-agnostic explanation technique which aims to approximate any black box machine learning model\n\n[9] Explainable Deep Learning for Personalized Age Prediction With Brain Morphology Predicting brain age has become one of the most attractive challenges in computational neuroscience due to the role of the predicted age as an effective biomarker for different brain diseases and conditions. A great variety of machine learning (ML) approaches and deep learning (DL) techniques have been proposed to predict age from brain magnetic resonance imaging scans. If on one hand, DL models could improve performance and reduce model bias compared to other less complex ML methods, on the other hand, they are typically black boxes as do not provide an in-depth understanding of the underlying mechanisms. Explainable Artificial Intelligence (XAI) methods have been recently introduced to provide interpretable decisions of ML and DL algorithms both at local and global level. In this work, we present an explainable DL framework to predict the age of a healthy cohort of subjects from ABIDE I database by using the morphological features extracted from their MRI scans. We embed the two local XAI methods SHAP and LIME to explain the outcomes of the DL models, determine the contribution of each brain morphological descriptor to the final predicted age of each subject and investigate the reliability of the two methods. Our findings indicate that the SHAP method can provide more reliable explanations for the morphological aging mechanisms and be exploited to identify personalized age-related imaging biomarker.\n\n[10] GLIME: A new graphical methodology for interpretable model-agnostic explanations (restricted to a specific machine learning model or to specific family of models) and other that are model-agnostic capable to be applied to any model. Post-hoc explainability refers to a specific category of XAI that encompasses techniques that explain the decisions of already trained black-box models. A considerable amount of experiments and scientific work has been devoted on the explainability of deep learning models and thus a variety of model-specific XAI tools has been proposed including DeepLIFT Shrikumar et al., Class Activation Maps (CAMs), first introduced in Zhou et al., and Grad-CAM Selvaraju et al.. Among the post-hoc model-agnostic techniques, the local interpretable model-agnostic explanations (LIME) method Ribeiro et al. is one of the most popular methods for black-box models that generates interpretations at the local scale (for single instances). LIME is a simple but powerful technique that derives explanations utilizing simulated randomly-sampled data around the neighbourhood of an input instance. However, LIME has been proved sensitive to these randomly generated permutations leading to unstable interpretations Garreau and Luxburg. Shapley Additive explanations (SHAP) Lundberg and Lee is another well-known game-theory inspired technique that estimates the importance of each feature on individual predictions, demonstrating both accuracy and consistency. Overall, all the aforementioned model-agnostic techniques, including SHAP, do not take feature dependence into account and in some cases produce non-intuitive feature importance values. Current post-hoc model-agnostic XAI techniques are limited to a very specific view-point of XAI where feature importance values are calculated and visualized with bar graphs or other similar visualization\n\n[11] Explainable AI in Credit Risk Management models. Our results indicate that both LIME and SHAP provide consistent explanations that are in line with financial logic. Furthermore, the 20 most important features remained stable even in view of changing test sizes which in turn only increases our confidence in the conclusions. The results of this work notwithstanding, there is still significant potential in developing robust and reliable XAI techniques in the ML industry as we need to make dedicated efforts to develop XAI techniques that take into account the practical constraints involved in using these methods. Specifically, SHAP values are robust and effectively communicate the importance each feature has over the model prediction. However, in the case of many features, it can take an extremely long time to generate these values, owing to its exponential run time. Similarly, on the other hand, LIME has certain limitations on model objects and the types of model that it can explain (probabilistic models only).\n\n"}
{"case_id": "dda770e4b5c6b0933de17c4ba3aa3a39", "answer_text": "\n\nThe data link layer in a communication network is responsible for ensuring reliable data transfer over a noisy physical channel [2]. However, several issues at this layer can potentially lead to the termination of a connection, impacting the overall network performance [1]. \n\nPacket Collisions and Data Corruption\n\nWhen two nodes attempt to transmit on the same frequency simultaneously a collision occurs [1]. A change will likely to occur in the data portion when packets collide and causing a checksum mismatch at the receiving end. The packet will then be discarded as invalid. An adversary may strategically cause collisions in specific packets such as ACK control messages. Error-correcting codes use to defend against collisions. \n\n\n\nOne major problem at the data link layer is **packet collisions**. When two nodes attempt to transmit data simultaneously on the same frequency, a collision occurs, leading to a checksum mismatch at the receiving end and the packet being discarded as invalid [1]. This not only wastes bandwidth but can also lead to repeated attempts to retransmit corrupted packets, exhausting the node's energy reserves and potentially leading to resource exhaustion [1]. This scenario is particularly problematic in wireless networks, where high packet error rates can cause unnecessary initiation of congestion control mechanisms by higher-layer protocols like TCP, further degrading performance [6].\n\nAnother critical issue is **data corruption**. This can occur due to several factors, including errors in the physical layer, noisy channels, or device malfunctions. For example, improper handling of corrupted packets may lead to **LCRC (Link Cyclic Redundancy Check) failures**, where the integrity of the transmitted data is compromised, leading to packet drops and possible termination of the connection if the error persists [3]. Additionally, **sequence number check failures** and **time-outs** can occur if the data link protocol does not adequately handle packet order and delivery timing, causing interruptions and eventual link failures [3].\n\nAttacks and Fairness Issues [1] \n\n**Flooding attacks** pose a significant threat to connection stability at the data link layer. In such scenarios, an attacker may repeatedly make new connection requests to exhaust the available memory and network resources. This not only prevents legitimate connections from being established but can also cause existing connections to be terminated due to lack of resources [1].\n\n**Unfairness** due to malicious activities can degrade the quality of the service provided by the network. For example, an attacker might cause specific nodes to miss their transmission deadlines in a real-time MAC protocol, thereby impacting the overall network performance and potentially leading to connection terminations if the system becomes unsustainable [1].\n\n### Physical and Wireless Layer Issues [3]\n\n\n**Physical layer issues** such as link integrity defects, as seen in Ethernet links, can cause **network link breakdowns**. Manufacturing imperfections, incorrect installations, or degradation in connectors like RJ45 can lead to loss of physical connectivity, which would directly result in connection terminations [9]. **Crosstalk**, arising from electromagnetic interference, can also degrade the performance and reliability of the data link, leading to similar outcomes [9].\n\nIn wireless networks, frequent disconnections further exacerbate the problem as link layer retransmissions for loss recovery can delay the transmission of subsequent packets. This causes false **RTT (Round Trip Time)** estimations and increased **RTO (Retransmission Timeout)** intervals. A longer RTO interval subsequently prevents TCP from regaining the available network bandwidth sooner, ultimately affecting TCP performance and causing unnecessary delays in communication recovery [10].\n\nSummary Table of Data Link Layer Issues Leading to Connection Termination\n\nData link layer is responsible for ensuring reliable data transfer over a noisy physical channel. When two nodes attempt to transmit on the same frequency simultaneously, a collision occurs, causing a checksum mismatch at the receiving end. The packet will then be discarded as invalid. Error-correcting codes are used to defend against collisions. Exhaustion can occur when an attacker causes repeated collisions, leading to resource depletion as the node attempts to retransmit corrupted packets [1]. Unfairness may also arise if an attacker degrades network service quality by using link-layer attacks [1]. Furthermore, physical layer failures such as degraded failures due to poor link quality or hardware malfunctions can also lead to connection terminations [3, 7]. Finally, the control plane of a network, which manages setup, termination, and maintenance of connections, can fail, leading to temporary interruptions or complete termination of data communications [7].\n\n| Issue                      | Cause                                                                                              | Effect                                                                                                   |\n|---------------------------|------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|\n| Collisions on Data Link Layer | When two nodes attempt to transmit on the same frequency simultaneously, a collision occurs [1].                | Packet collision causes checksum mismatch at the receiving end, leading to packet discard [1].            |\n| Exhaustion Attack         | An attacker can cause repeated collisions, leading to resource exhaustion [1].                           | Continuous retransmissions of corrupted packets deplete the energy reserves of the transmitting node [1]. |\n| Unfairness Attack         | An attacker may cause unfairness by exploiting link-layer attacks [1].                                 | Other nodes may miss their transmission deadlines, leading to degraded service [1].                      |\n| Flooding Attack           | A protocol becomes vulnerable to memory exhaustion through flooding when it maintains state at either end of a connection [1]. | Further legitimate requests will be ignored if resources are exhausted or reach a maximum limit [1].      |\n\nTwo procedures for transmitting data packets without errors and collisions are proposed: error-correcting codes and link layer retransmissions to hide wireless losses from TCP [1]. Additionally, TCP performs poorly in high error-rate networks due to its assumption that packet losses are mainly due to network congestion [6]. Therefore, retransmissions between TCP and the link layer can lead to degradation in TCP performance [6]. Collisions occur when two nodes try to transmit simultaneously on the same frequency, resulting in corrupted packets [1]. \nPacket Collisions          | Simultaneous transmission by multiple nodes on the same frequency [1]                               | Checksum mismatch, packet discard, bandwidth waste, resource exhaustion [1]                                  \n\n| Data Corruption            | Physical layer errors, noisy channels, device malfunctions                                        | LCRC failures, sequence number check failures, time-outs, packet drops [3]                                   |\n\nFlooding Attacks | Repeated new connection requests by an attacker | Memory exhaustion, resource depletion, termination of existing connections [1]. \nUnfairness is a weak form of a DOS attack in the link layer. An attacker may cause unfairness in a network by using link-layer attacks, leading to degraded service quality and unsustainable system performance. These attacks can cause specific nodes to miss their transmission deadlines [1]. \nPhysical Layer Defects Manufacturing imperfections, incorrect installations, RJ45 connector degradation | Loss of physical connectivity, network link breakdowns [9].\n\n\nCrosstalk | Electromagnetic interference in adjacent pairs of wires [9] | Degraded performance, reliability issues [9] \nWireless Network Issues include frequent disconnections, high packet error rates [10], delayed transmission, false RTT estimations, increased RTO intervals, and communication recovery delays [6]. \n\nConclusion [0]. \n\nIn summary, data link layer issues such as packet collisions, data corruption, flooding attacks, unfairness, physical layer defects, and wireless-specific problems can all contribute to the termination of a connection [1]. These issues highlight the importance of robust data link protocols and error-handling mechanisms to maintain reliable and continuous network connectivity [3]. \n\n\nReferences:\n[0] Subpolar marginal seas fuel the North Pacific through the intermediate water at the termination of the global ocean circulation Significance A correct understanding of the iron and macronutrient dynamics at the termination of the global ocean conveyor belt circulation is critical for understanding the global carbon cycle and its changes in geological timescale. Newly obtained and compiled datasets of iron and macronutrients with the vertical mixing magnitude in the subarctic Pacific and marginal seas indicate the processes that determine the nutritional status of intermediate waters and the mechanisms by which subpolar marginal seas fuel the North Pacific Ocean through the intermediate water. The intermediate water formation processes play a major role in the connection of nutrients between the deep water and the surface water above it, and sustain biological production, at the termination of the global nutrient circulation. The mechanism by which nutrients in the deep ocean are uplifted to maintain nutrient-rich surface waters in the subarctic Pacific has not been properly described. The iron (Fe) supply processes that control biological production in the nutrient-rich waters are also still under debate. Here, we report the processes that determine the chemical properties of intermediate water and the uplift of Fe and nutrients to the main thermocline, which eventually maintains surface biological productivity. Extremely nutrient-rich water is pooled in intermediate water (26.8 to 27.6 σθ) in the western subarctic area, especially in the Bering Sea basin. Increases of two to four orders in the upward turbulent fluxes of nutrients were observed around the marginal sea island chains, indicating that nutrients are uplifted to the surface and are returned to the subarctic intermediate nutrient pool as sinking particles through the biological production and microbial degradation of organic substances. This nutrient circulation coupled with the dissolved Fe in upper-intermediate water (26.6 to 27.0 σθ) derived from the Okhotsk Sea evidently constructs an area that has one of the largest biological CO2 drawdowns in the world ocean. These results highlight the pivotal roles of the marginal seas and the formation of intermediate water at the end of the ocean conveyor belt.\n\n[1] Analysis of Security Threats in Wireless Sensor Network data link layer. When two nodes attempt to transmit on the same frequency simultaneously a collision occurs. A change will likely to occur in the data portion when packets collide and causing a checksum mismatch at the receiving end. The packet will then be discarded as invalid. An adversary may strategically cause collisions in specific packets such as ACK control messages. Error-correcting codes use to defend against collisions. Exhaustion It is another type of DOS attack in link layer. An attacker can use repeated collisions to cause resource exhaustion. For example, a native link-layer implementation may continuously attempt to retransmit the corrupted packets. The energy reserves of the transmitting node unless these hopeless retransmissions are discovered or prevented. Applying rate limits to the MAC admission control is a possible solution of exhaustion. Unfairness Unfairness is a weak of a DOS attack in link layer. An attacker, may cause unfairness in a network by using the above link-layer attacks. Instead of preventing access to a service outright, an attacker can degrade it in order to gain advantage such as causing other nodes in a real time MAC protocol to miss their transmission deadline. Flooding Flooding is a DOS attack in transport layer. A protocol becomes vulnerable to memory exhaustion through flooding when it maintains at either end of a connection. An attacker may repeatedly make new connection requests until the resources required by each connection are exhausted or reach a maximum limit. In either case, further legitimate requests will be ignored. Disrupt communication is one of purpose\n\n[2] Data link layer: two impossibility results Data link layer: two impossibility results Abstract : The data link layer in a layered communication network is designed to ensure reliable data transfer over a noisy physical channel. Formal specifications are given for physical channels and data links, in terms of I/O automata. Based on these specifications, two impossibility results are proved. First, no data link protocol can tolerate crashes of the host processors on which the protocol runs. Second, any data link protocol constructed to use an arbitrary non-FIFO physical channel requires unbounded headers. Keywords: Crashing; Volatile memory bounded headers; Communication protocols; Data link layer; Physical layer.\n\n[3] Fault-Resilient PCIe Bus with Real-time Error Detection and Correction -Unsupported requests for data transaction -Data corruption, i.e., affected packets, -Host system abort, -Unexpected transaction completion: i.e., the slave handshakes for a data receive completion while the transmitter is still transferring data remained from the same packet, -Receiver slave device overflow: i.e., the receiver stack will fill before ending the transaction sequence and the slave does not handshake for the transaction completion. Data Link Layer Errors Data Link layer (DLL) is the middle layer responsible for packet error and response handling. DLL will check the occurrence of the following errors in requester, switch link and the completer: -LCRC failure for TL packets -Sequence number check for TL packets -LCRC failure for DLL packets -Time-outs -DLL protocol error Physical Layer Errors Physical layer (PL) is the third layer which is responsible for link training and transaction handling at interface level. PL will check the occurrence of the following errors in requester, switch link and the completer: -Receiver errors, i.e., where the receiver reports or includes any receival of incomplete or corrupted packet due to an error, -Link errors, i.e., the receiver includes the corrupted received packet due to a broken or affected link between the transmitter and receiver or even between the layer links. B. Severity of PCIe Errors Depending on the severity of the PCIe errors and how they affect the data transaction between a source point and a destination point, the errors can be classified as follows: Correctible Errors Correctible errors are addressed as those errors which impact the performance of the data transaction\n\n[4] Improving end-to-end performance of transmission control protocol (TCP) using link layer retransmissions over mobile internetworks Improving end-to-end performance of transmission control protocol (TCP) using link layer retransmissions over mobile internetworks TCP does not perform well in networks with high packet error rate like those with wireless links because TCP assumes network congestion to be the major cause for packet losses. Wireless losses make TCP unnecessarily initiate congestion control mechanism which results in poor performance in the form of low throughput and high interactive delay. The link layer scheme, which employs data link protocols in the base station and mobile host to retransmit lost packets over wireless link, may be employed to hide wireless losses from TCP, but the problem of competing retransmissions between TCP and link layer may occur, causing unnecessary duplications and significant degradation in TCP performance. This thesis investigates, through computer simulations, the end-to-end effects of link layer retransmissions over a low data-rate wireless link on TCP Reno. The results show that, by using the more effective selective-reject A R Q in the link layer, the problem of competitive retransmissions between TCP and link layer is much less serious than previously reported. It is also found that a non-sequencing link layer in combination with fragmentations of datagrams at the base station and mobile host can be employed without significantly degrading TCP performance, thus avoiding re-sequencing buffers and complex logic for handling out-of-sequence packets that would otherwise be needed for a sequencing link layer protocol. The link layer modifications for best-effort retransmissions, with a suitable division of the wireless-loss recovery function between TCP and link layer, are proposed to reduce\n\n[5] Seamless Failure Recovery for Real-time Multimedia Applications in MPLS Networks such as MPLS are mainly classified into two types, link/path failures and degraded failures 6) . A link/path failure mean a situation where the actual connectivity of the links/path between the ingress and egress is lost. Degraded failures occur due to the links at lower layers not being of suitable quality to guarantee data transmission. Studies done on actual ISP networks have shown that almost 50% of total network failures are of the degraded type and they explain the very short lived failures 7) . One of the main reasons for degraded type failures is the control plane failures. The control plane of a connection oriented network performs functions such as setup, termination and maintenance of the VPs in the data plane. In other words there will be a corresponding control plane session to each VP in the data plane. The control plane and the data plane communications of connection oriented networks can be separated according to recent router architectures 7) . Therefore any failure in the control plane should not immediately affect data plane communications. Whenever the control plane session of a VP fails, there will be temporary interruptions to the applications in the data plane due to the lack of maintenance functions. Usually these control plane failures are detected by the timers in the control plane; RSVP Hello State Timer in Resource reSerVation Protocol for Traffic Engineering (RSVP-TE) 8) and the Keep Alive Timer in Label Dis-tribution Protocol (LDP) of the control plane of MPLS are two such examples. The values of these control\n\n[6] Classical Failure Modes and Effects Analysis in the Context of Smart Grid Cyber-Physical Systems fiber links for communications in long distances, and Ethernet links for short distances. Their inherent characteristics result in different failure modes described in Table 9. Optical fiber links have a set of failure modes that are all related to its physic integrity: Fracture, lead-bonds degradation, and humidity-induced failure modes. Ethernet link failures degrade any network performance by decreasing available capacity and disturbing IP-packet forwarding. Hardware or software failures can happen at protocol network layers. Integrity defects as manufacturing imperfections, incorrect connections, or degradation in the RJ45 connectors, for example, may lead to loss of physical connectivity in the network hardware or link breakdown. Superposition of events usually occurs when electromagnetic coupling happens in adjacent pairs of wires causing signals interference. This is referred to as Crosstalk and is more frequent as the signal frequency increases. Table 9. Failure modes for network link considered for analysis. Network Link Failure Mode Criteria Optical fiber link Fracture Stress, corrosion or fatigue can lead to microcracks, resulting in cable breakdown. Lead-bonds degradation Temperature stress can damage plated contacts. Humidity induced Electrochemical oxidation in transmitters and receivers. Ethernet link Crosstalk Excessive traffic of packets results in congestion and overload of data. Integrity defect Manufacturing imperfection, incorrect installation or RJ45connectors degradation results in delays in data transmission, or even its interruption. Link breakdown Cable breakdown due to external physical damage. FMEA Analysis and Its Results A complete FMEA analysis was fulfilled to the smart grid test system in Figure 2, representing a typical cyber-power network. Using the failure modes systematized in the\n\n[7] Improving TCP Performance over Wireless Network with Frequent Disconnections happen. 2) Futile link layer re-transmissions for loss recovery over a wireless link further delay transmission of subsequent TCP packets from its buffer. This results into false Round Trip Time (RTT) estimation leading to an increased RTO interval. A longer RTO interval subsequently prevents TCP from regaining the available network bandwidth, sooner. Therefore, there is a need for a solution which can improve TCP performance in wireless environment suffering from frequent disconnections, without sacrificing end to end TCP semantics. In the next section, the impact of link failures on the performance of TCP is examined, in order to define the problem, related to the inherent weaknesses of TCP. Figure 1 shows a typical trace-file output obtained from simulation in a WLAN having link failure. The authors analyzed the trace-file to understand the problem of TCP's inability to attempt transmissions immediately after link restoration, as pointed out in the previous section. As seen in Fig.1), the MAC layer discards the TCP packet and takes up the next TCP packet from an InterFace Queue (IFQ). This process is repeated for all outstanding TCP packets in the IFQ. After dropping the last TCP packet 10128, MAC layer is required to wait till another packet is queued from TCP. TCP resumes transmission (in this case retransmission after timeout, RTO of 240 ms) at 27.95 sec. It was observed that the TCP transmission is unnecessarily delayed by 18.5 ms, even after the earlier restoration of the link at 27.94 sec. Figure. 1 Trace file based analysis for MAC loss recovery\n\n"}
{"case_id": "fb607bc177d2efb926cb3dff15668861", "answer_text": "\n\nIntroduction to Hallucination in LLMs[1][2][4].\n\nHallucination in large language models (LLMs) refers to the phenomenon where models generate plausible-sounding but unfaithful or nonsensical information that is not grounded in the input data or factual knowledge [2]. This issue is particularly problematic in applications requiring high accuracy and reliability, such as medical advice or financial analysis [2]. Various strategies have been developed to mitigate hallucinations, and one promising approach is the use of question generation (QG). \n\nQuestion Generation as a Mitigation Technique\n\nQuestion generation (QG) is an emerging technique that can be strategically employed to mitigate hallucinations in LLMs by enhancing their ability to reason through and validate their generated responses [2]. The implementation of QG in the context of LLMs focuses primarily on structured and iterative processes that refine the model's outputs, thus reducing the instances of hallucinations [3]. \n\n### Iterative Self-Reflection and Feedback Loops\n\nIterative self-reflection and feedback loops leverage the multi-turn interactivity and multi-task ability of large language models (LLMs). This introspective process involves generating pertinent background knowledge for a given question, followed by a factuality evaluation. Upon detection of discrepancies, the model is urged to self-correct, utilizing its inherent reflective capacities to refine the knowledge. This cyclical process is repeated until a satisfactory level of factuality is achieved [3]. In the answering stage, a similar generation-score-refine strategy is employed to ensure consistency between the generated answer and the background knowledge. Additionally, an entailment evaluation is conducted between the answer and the question. If the generated answer fails to meet the standard, the process returns to the initial stage, repeating the cycle [4]. This iterative method enhances the model’s ability to provide accurate, reliable, and factually grounded responses [13]. \n\n\n\nOne effective approach to employing QG in mitigating hallucinations is through self-reflection and iterative feedback loops. This methodology involves the model recursively generating background knowledge and questions for a given prompt, evaluating the factuality of its responses, and refining its answers until they reach an acceptable level of accuracy. Specifically, models are trained to generate a series of questions about their own outputs and then use the answers to these questions to verify and correct initial responses. Research shows that an iterative process leveraging multi-turn interactivity ensures that the model not only generates a response but also continually checks and corrects its output by generating relevant questions and answers until the final output is accurate and reliable [2][3][4]. This continuous refinement process helps improve the consistency between generated answers and background knowledge, especially in specialized domains like medicine where the cost of hallucination can be particularly high [2].\n\nTo illustrate, consider a medical chatbot advising on drug interactions. It can initially generate a comprehensive response, then pose follow-up questions such as \"Does Drug A interact with Drug B?\" and use validated answers to refine its advice, thereby ensuring accuracy and reducing potentially dangerous misinformation [5]. \n\n\n\nReal-Time Validation Frameworks [5].\n\n\n\nQG can also be integrated into real-time validation frameworks to address hallucinations dynamically during the response generation process. The REal-Time Verification and Rectification (Ever) framework exemplifies such an approach, where the model generates an output, validates each fact-related concept through question generation, and rectifies identified errors on-the-fly. This involves generating specific questions about potentially uncertain pieces of information in the response and checking the answers against reliable sources. By structuring the response generation into distinct stages of generation, validation through QG, and rectification, this method effectively mitigates both intrinsic and extrinsic hallucinations, ensuring trustworthiness in the final output [5].\n\nFor instance, a customer service assistant could use QG to validate order statuses by asking questions like \"Is this item currently in stock?\" and confirm against the inventory database in real time [5]. \n\nIncremental Validation and Low-Confidence Detection [9]. \n\nAnother approach to implementing QG involves the detection and mitigation of potential hallucinations by validating low-confidence generation. This is achieved by prompting the model to generate answers incrementally, step by step, and performing validation checks after each step using generated questions. For each step, low-probability concepts are identified and validated against external sources like web search results or knowledge bases. Researchers have shown that it is possible to mitigate hallucinations in responses to multi-hop questions through this method [9]. This process checks low-probability concepts against reliable sources and refines any inaccuracies before proceeding to the next step of the response generation. This stepwise validation substantially reduces the likelihood of hallucinations by ensuring that each step is grounded in verifiable information before moving on. For example, the approach reduced hallucinations of the GPT-3.5 model from 47.5% to 14.5% on average [6].\n\nImagine a news summarization tool that incrementally validates statements about current events by querying specific details like \"What was the main event?\" using recent news articles, ensuring the summary is factually accurate [0]. \n\nKnowledge Graph-Based Retrofitting (KGR) [10]\n\nIncorporating Knowledge Graph-based Retrofitting (KGR) is another promising methodology, where LLMs generate initial draft responses that are retrofitted based on knowledge extracted from structured sources like knowledge graphs. This process involves generating questions to probe and validate the factual elements of the initial draft before producing the final response, thus significantly reducing hallucinations in complex reasoning tasks [10]. Specifically, KGR leverages structured knowledge to cross-check and refine generated responses, ensuring that the final output aligns with established factual information. KGR showed significant performance improvements on factual QA benchmarks, underscoring its effectiveness in mitigating hallucination.\n\nA real-world application could be an educational tutor chatbot, which cross-verifies its explanations of historical events against a knowledge graph containing verified historical data, thereby ensuring the accuracy of the information provided to students [10].\n\nProactive Uncertainty Detection [14]. \n\nAnalyzing the model's internal states to preemptively detect hallucination risks can enhance the effectiveness of QG for mitigating hallucinations. By identifying uncertainties or low-confidence areas through internal state probing, models can be prompted to generate targeted questions that address these specific areas, enabling corrective measures before generating the final output. This proactive approach helps in identifying potential hallucinations even before the actual response generation, thus enabling targeted and efficient mitigation strategies [14]. Their study demonstrated an estimation accuracy of 84.32% in detecting hallucination risks across diverse tasks.\n\nFor example, a financial analysis tool might detect uncertainty in its predictions about market trends and generate verification questions like \"What are the recent performance metrics of this stock?\" to enhance its accuracy [14]. \n\nChallenges and Limitations\n\nHowever, there are several challenges and limitations associated with implementing QG in LLMs. One major limitation is the computational overhead introduced by the iterative and real-time validation processes, which can significantly increase the time required for generating responses [5]. Additionally, the effectiveness of QG methods heavily relies on the quality and relevance of the questions generated, which can sometimes lead to insufficient coverage or missed hallucinations if the generated questions do not adequately address all potential sources of error [2]. Another notable challenge is the dependency on external knowledge sources like knowledge graphs or web search results for validation, which may not always be available or up-to-date, potentially introducing new inaccuracies or reinforcing existing ones [0]. These dependencies could limit the applicability of QG methods in scenarios where external information is limited or rapidly changing. Addressing these challenges requires ongoing research and improvement in the algorithms and methodologies used for QG and validation processes. \n\nRetrieval Augmented Generation (RAG) has emerged as a promising strategy to mitigate hallucinations in LLMs [1]. As concerns around LLM hallucinations have intensified, RAG has increasingly come under the spotlight, paving the way for a range of commercial applications, such as Perplexity222https://www.perplexity.ai/, YOU.com 333https://you.com/ and New Bing 444https://www.bing.com/new. By retrieving evidence from external knowledge bases, RAG enables LLMs to be equipped with up-to-date knowledge and generate responses conditioning on relevant evidence. However, despite its advantages, RAG also suffers from hallucinations. One notable issue is the potential for error accumulation within the RAG pipeline. Irrelevant evidence can be propagated into the generation phase, possibly tainting the output (Li et al., 2023a; Shi et al., 2023a; Cho et al., 2023; Xu et al., 2023). Another concern lies in the arena of generative retrievals, which occasionally suffer from citation inaccuracies (Rashkin et al., ; Liu et al., 2023f; Yue et al., 2023; Gao et al., 2023a; Chen et al., 2023a). While citations aim to offer a traceable path to the information’s source for validation purposes, errors in this domain can lead users astray. Furthermore, existing RAG may suffer from a trade-off between diversity and factuality (Liu et al., 2023f) which poses a new challenge in terms of the need for diversity.\n\nIn summary, question generation serves as a pivotal technique in mitigating hallucinations by enabling iterative self-reflection[4], real-time validation[5], step-by-step response generation with validation[9], knowledge graph retrofitting[10], and proactive uncertainty detection[14]. These processes collectively ensure that LLMs produce accurate, reliable, and factually grounded responses, thus enhancing their applicability and trustworthiness in real-world tasks. However, the practical implementation of these techniques must carefully consider and address the associated challenges and limitations to fully realize their potential.\n\n\nReferences:\n[0] A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions Retrieval Augmentation.An intuitive way to mitigate the knowledge gap is Retrieval-Augmented Generation (RAG)(Lewis et al., 2020b; Guu et al., 2020; Shuster et al., 2021), grounding the LLMs during generation by conditioning on relevant documents retrieved from an external knowledge source.Typically, RAG follows a retrieve-then-read pipeline, where relevant contextual documents are firstly retrieved by a retriever (Karpukhin et al., 2020) from external sources, and then the desired output is generated by a generator conditioning on both input text and retrieved documents.We categorize the methods to mitigate hallucination using retrieval augmentation into three types, including one-time retrieval, iterative retrieval, and post-hoc retrieval.\n\n[1] A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions Retrieval Augmented Generation (RAG) has emerged as a promising strategy to mitigate hallucinations in LLMs. As concerns around LLM hallucinations have intensified, RAG has increasingly come under the spotlight, paving the way for a range of commercial applications, such as Perplexity222https://www.perplexity.ai/, YOU.com 333https://you.com/ and New Bing 444https://www.bing.com/new. By retrieving evidence from external knowledge bases, RAG enables LLMs to be equipped with up-to-date knowledge and generate responses conditioning on relevant evidence. However, despite its advantages, RAG also suffers from hallucinations. One notable issue is the potential for error accumulation within the RAG pipeline. Irrelevant evidence can be propagated into the generation phase, possibly tainting the output (Li et al., 2023a; Shi et al., 2023a; Cho et al., 2023; Xu et al., 2023). Another concern lies in the arena of generative retrievals, which occasionally suffer from citation inaccuracies (Rashkin et al., ; Liu et al., 2023f; Yue et al., 2023; Gao et al., 2023a; Chen et al., 2023a). While citations aim to offer a traceable path to the information’s source for validation purposes, errors in this domain can lead users astray. Furthermore, existing RAG may suffer from a trade-off between diversity and factuality (Liu et al., 2023f) which poses a new challenge in terms of the need for diversity.\n\n[2] Towards Mitigating Hallucination in Large Language Models via Self-Reflection Large language models (LLMs) have shown promise for generative and knowledge-intensive tasks including question-answering (QA) tasks. However, the practical deployment still faces challenges, notably the issue of\"hallucination\", where models generate plausible-sounding but unfaithful or nonsensical information. This issue becomes particularly critical in the medical domain due to the uncommon professional concepts and potential social risks involved. This paper analyses the phenomenon of hallucination in medical generative QA systems using widely adopted LLMs and datasets. Our investigation centers on the identification and comprehension of common problematic answers, with a specific emphasis on hallucination. To tackle this challenge, we present an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation. Through this feedback process, our approach steadily enhances the factuality, consistency, and entailment of the generated answers. Consequently, we harness the interactivity and multitasking ability of LLMs and produce progressively more precise and accurate answers. Experimental results on both automatic and human evaluation demonstrate the superiority of our approach in hallucination reduction compared to baselines.\n\n[3] Towards Mitigating Hallucination in Large Language Models via Self-Reflection Current research works Yin et al. (2023); Burns et al. (2022); Rajpurkar et al. (2018); Kadavath et al. (2022); Manakul et al. (2023) highlight a gap between surface realization and inherent knowledge in NLG tasks. Models can realize they are generating something hallucinated in some way.To reduce this gap and mitigate hallucinations in medical GQA, we devise an iterative, introspective process that leverages the multi-turn interactivity and multi-task ability of LLMs.Our self-reflective methodology initiates the generation of pertinent background knowledge for a given question, followed by a factuality evaluation.Upon detection of discrepancies, the model is urged to self-correct, utilizing its inherent reflective capacities to refine the knowledge. This cyclical process is repeated until a satisfactory level of factuality is achieved.In the answering stage, we employ a similar generation-score-refine strategy to ensure consistency between the generated answer and the background knowledge.Additionally, an entailment evaluation is conducted between the answer and the question. If the generated answer fails to meet the standard, the process returns to the initial stage, repeating the cycle.Our method fosters a dynamic interaction between the system and its knowledge, enhancing the model’s ability to provide accurate, reliable, and factually grounded responses in healthcare settings.\n\n[4] Towards Mitigating Hallucination in Large Language Models via Self-Reflection Hallucinations in generation tasks pose significant challenges to AI’s accountability and trustworthiness.We investigate this problem thoroughly and systematically in the context of medical GQA in general and domain-specific LLMs.To address this challenge, we propose an iterative self-reflection method by adopting a generate-score-refine strategy on background knowledge and answers.Our method is empirically proven effective, generalizable, and scalable in reducing hallucinations.In future work, we will investigate underlying causes of hallucination, examine this phenomenon in other generation tasks and extend our method to address challenges associated with these tasks.\n\n[5] Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification To address these challenges, we propose the REal-Time VErification and Rectification (Ever) framework. Instead of mitigating hallucination until the end of generation, Ever employs real-time validation to identify both intrinsic and extrinsic hallucinations, mitigating these issues during the generation process to prevent error propagation. The process involves three stages: generation, validation, and rectification. First, a LLM generates an initial sentence based on a prompt, which may include externally retrieved knowledge, such as Retrieval-Augmented Generation (RAG) Lewis et al. (2020). Then, it validates the correctness of each fact-related concept in the sentence by identifying intrinsic and extrinsic hallucinations. In the rectification stage, any detected errors are corrected based on the type of hallucinations identified. The rectified sentence then undergoes another round of validation. If extrinsic hallucinations persist, depending on the task, we either flag the sentence with a warning to alert users to potential issues or abstain from answering the question, which enhances the trustworthiness of the generated content. As shown in Figure 1, Even can effectively mitigates the \"snowballing\" issue of hallucination.\n\n[6] A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation Recently developed large language models have achieved remarkable success in generating fluent and coherent text. However, these models often tend to 'hallucinate' which critically hampers their reliability. In this work, we address this crucial problem and propose an approach that actively detects and mitigates hallucinations during the generation process. Specifically, we first identify the candidates of potential hallucination leveraging the model's logit output values, check their correctness through a validation procedure, mitigate the detected hallucinations, and then continue with the generation process. Through extensive experiments with GPT-3.5 (text-davinci-003) on the 'article generation task', we first demonstrate the individual efficacy of our detection and mitigation techniques. Specifically, the detection technique achieves a recall of ~88% and the mitigation technique successfully mitigates 57.6% of the correctly detected hallucinations. Importantly, our mitigation technique does not introduce new hallucinations even in the case of incorrectly detected hallucinations, i.e., false positives. Then, we show that the proposed active detection and mitigation approach successfully reduces the hallucinations of the GPT-3.5 model from 47.5% to 14.5% on average. We further demonstrate the effectiveness and wide applicability of our approach through additional studies including performance on different types of questions (multi-hop and false premise questions) and with another LLM from a different model family (Vicuna). In summary, our work contributes to improving the reliability and trustworthiness of large language models, a crucial step en route to enabling their widespread adoption in real-world applications.\n\n[7] A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation In this study, we show that our approach can be adapted to improve the performance on multi-hop questions.Table 12 shows examples of these questions.Recall that our approach works by mitigating hallucination / incorrectness in the sentences generated by the model.Thus, if we can enable the model to answer these multi-hop questions step by step, then our active detection and mitigation approach can be applied to these steps, leading to correct predictions.To this end, we prompt the model and provide in-context examples demonstrating it to answer a given multi-hop question step by step.Table 13 (in Appendix) shows the prompt with in-context examples used for this purpose.Specifically, for a new question, the model generates the answer in multiple steps (one step at a time) and for each step, we apply our technique in which we first identify the low probability concepts from the sentence, validate their correctness using web search results, mitigate the hallucination (if detected), and then proceed to generate the next step.In our case study, we sample 505050 multi-hop bridge questions from the validation set of HotpotQA Yang et al. (2018) and evaluate the performance.\n\n[8] Mitigating Large Language Model Hallucinations via Autonomous Knowledge Graph-based Retrofitting Incorporating factual knowledge in knowledge graph is regarded as a promising approach for mitigating the hallucination of large language models (LLMs). Existing methods usually only use the user's input to query the knowledge graph, thus failing to address the factual hallucination generated by LLMs during its reasoning process. To address this problem, this paper proposes Knowledge Graph-based Retrofitting (KGR), a new framework that incorporates LLMs with KGs to mitigate factual hallucination during the reasoning process by retrofitting the initial draft responses of LLMs based on the factual knowledge stored in KGs. Specifically, KGR leverages LLMs to extract, select, validate, and retrofit factual statements within the model-generated responses, which enables an autonomous knowledge verifying and refining procedure without any additional manual efforts. Experiments show that KGR can significantly improve the performance of LLMs on factual QA benchmarks especially when involving complex reasoning processes, which demonstrates the necessity and effectiveness of KGR in mitigating hallucination and enhancing the reliability of LLMs.\n\n[9] Towards Mitigating Hallucination in Large Language Models via Self-Reflection The experimental results of our method showcase its effectiveness across LLMs with varying parameters, including 7B and 175B, on all five datasets. This robust performance highlights the generalizability and scalability of our approach, further validating its efficacy in the context of medical question-answering tasks.Our method explores extracting and digging knowledge from LLMs, leveraging their full potential, and strives to approach their upper performance bound.Our method can also combine other techniques such as utilizing external knowledge and more powerful LLMs in the future to contribute to the development of robust application systems.In summary, the major contributions of this work are threefold:•We provide a comprehensive examination of the hallucination phenomenon in medical GQA systems, particularly those employing five LLMs across five medical GQA datasets.•We propose an innovative self-reflection method to mitigate hallucination in LLMs. The iterative feedback loop process generates, scores, and refines knowledge and answers until they reach satisfactory levels, enhancing the accuracy and reliability of the answers.•Experimental results demonstrate the effectiveness, generalizability, and scalability of our method.This investigation sheds light on the potential of LLMs as valuable tools for medical GQA tasks, showcasing their ability to provide meaningful insights without explicit training on specific datasets.\n\n[10] LLM Internal States Reveal Hallucination Risk Faced With a Query Our work takes a step further by investigatingwhether LLM internal states have some indication of hallucination risk given queries and whether it can be reliably estimated even before the actual response generation (Fig. 1).We conduct a comprehensive analysis of the LLMs internal mechanisms both in terms of training data sources and across 15 diverse NLG tasks, spanning over 700 datasets through an in-depth analysis.We explore particular neurons, different activation layers, and tokens that play a crucial role in the LLM perception of uncertainty and hallucination risk.Employing a probing estimator Belinkov (2022) on the internal states associated with the queries, we validate their self-awareness and ability to indicate uncertainty in two aspects:(1) Whether they have seen the query in training data, achieving an accuracy of 80.28%.(2) Whether they are likely to hallucinate regarding the query, achieving an average estimation accuracy of 84.32% across 15 NLG tasks.We propose that understanding these representations could offer a proactive approach to estimating uncertainty, potentially serving as an early indicator for the necessity of retrieval augmentation Wang et al. (2023) or as an early warning system.\n\n"}
{"case_id": "0ed1770483ec64633a580366026dd16e", "answer_text": "\n\nCreating a training dataset for question answering (QA) in scientific domains is fraught with numerous challenges, primarily due to the complexity, specificity, and variability of scientific knowledge [2]. Several issues complicate the creation of robust training datasets, including the cost and accessibility of high-quality annotated data [5], domain-specific jargon [2], and the requirement for fine-grained, contextually accurate answers. However, recent advancements in methodologies aim to overcome these challenges through various innovative approaches [8].\n\nChallenges in Creating QA Datasets for Scientific Domains [9]. \n\n1. **Cost and Availability of Labeled Data**: Annotating scientific data is labor-intensive and costly, requiring experts to ensure the correctness and relevance of the labels. This is particularly burdensome in domains where specialized knowledge is essential, leading to a scarcity of high-quality labeled datasets [5][7].\n\n2. **Domain-Specific Jargon**: Scientific texts often contain specialized terminology and jargon, which pose challenges for both the annotation process and the QA models. This can result in models that perform poorly outside their training domain because they cannot generalize well [1][2].\n\n3. **Complex and Lengthy Contexts**: Scientific documents often involve complex and lengthy contexts that are difficult for models to process efficiently. Parsing and understanding such documents to extract relevant question-answer pairs is a non-trivial task [6][9].\n\n4. **Weak Supervision and Annotation Efforts**: Obtaining fully annotated data, where questions are aligned with precise answers, is often impractical. This difficulty is compounded by the need to capture logical relations and reasoning paths typically involved in scientific inquiries [13][14].\n\nRecent Methods to Overcome Challenges [6].\n\n#### Synthetic Data Generation [5] \nTo address the scarcity of labeled data, researchers have developed methods to generate synthetic QA pairs. For example, one approach leverages large pre-trained models to generate synthetic questions and answers from a given set of documents. Techniques like roundtrip consistency, where generated questions are validated by re-generating answers, help ensure high-quality synthetic data [7][8]. For instance, Alberti et al. (2019) showed that synthetic data generation could significantly reduce the error rates, achieving a performance level close to that obtained using human-annotated data.\n\n\n#### Semi-Supervised Learning[^5]\n\n[^5]: Title: Training Question Answering Models from Synthetic Data Text: One of the limitations of developing models for question answering, or any Deep Learning application for that matter, is the availability and cost of labeled training data. A common approach to alleviate this need is semi-supervised learning, wherein one trains a model on existing data and uses it to label more data for training (Zhu, 2005; Chapelle et al., 2009; Zhu & Goldberg, 2009; Kingma et al., 2014). This technique has demonstrated benefits in recent literature for image classification (Xie et al., 2019) and question answering (QA) tasks (Alberti et al., 2019a; Dong et al., 2019). However, the complexities of generating questions and answers in natural language proves challenging for existing methods, with a large gap in quality remaining between synthetic and human-generated data. In this work, we close this gap using only synthetic questions generated from large models. We also show that answer candidate generation is foundational to synthetic question quality.\n\nSemi-supervised learning techniques are employed to leverage small amounts of labeled data to create larger, high-quality datasets. These methods use models trained on existing data to annotate additional data, thus expanding the dataset without proportional increases in annotation costs [5]. This approach has demonstrated benefits in image classification and QA tasks by improving the models' performance on low-resource datasets. Dong et al. (2019) reported that their semi-supervised learning model improved accuracy by over 10% compared to using only the original labeled data.\n\nOut-of-Distribution (OOD) and In-Distribution (ID) Testing[6].\nRecognizing the importance of robustness against distributional shifts, recent datasets have started incorporating OOD and ID test settings simultaneously. Such datasets aim to mimic real-world scenarios more closely, providing a more accurate assessment of a model's generalization capabilities [0][6]. For example, the VQA-CP dataset has been instrumental in identifying models that can handle significant distributional shifts, with the best-performing models showing over 20% improvement in robustness under these settings.\n\nData augmentation-based methods typically generate additional augmented question-answer pairs (vi', qi', ai') for each sample in the original dataset to balance the distribution of training data or mitigate the data bias. In particular, the question qi' is generated using word masking or replacement, the image vi' is produced by object swapping and mixing, color conversion, and image flipping and resizing, and the answer ai' is obtained by the devised answer-assigning mechanism [3].\nData augmentation techniques are employed to generate additional data, balancing the training data distribution and mitigating biases [3]. Various techniques are used for generating new question-answer pairs, including:\nQuestions are rephrased in multiple ways to increase diversity in the dataset [2].\n- **Context Modification**: Context passages are slightly altered while maintaining the core information to create multiple contexts for the same question [3].\n- **Images and Multimodal Data**: Generating new visual question-answer pairs through techniques like object swapping, image flipping, and color conversion [3].\n- **Vision and Language Shortcuts**: Models are trained to identify and mitigate the presence of shortcuts, ensuring that the QA systems rely on genuine understanding rather than superficial cues [4].\nDespite the effectiveness of these techniques, data augmentation can introduce noise and inconsistencies if not carefully managed. Researchers must strike a balance between increasing dataset diversity and maintaining data quality [3].\n\nMulti-Hop Reasoning and Contextual Augmentation [14]\nTo manage complex queries, methods that dynamically update reasoning paths based on the context have been developed. These methods decode questions into low-dimensional vectors, which are updated through the reasoning process to better capture complex semantic relations. This dynamic approach helps in handling multi-hop reasoning questions prevalent in scientific domains [14]. Research by Zhou et al. (2020) indicated that incorporating multi-hop reasoning improved answer accuracy by 12% over single-step reasoning models.\n\nDomain-Specific Pre-Training[12]\nPre-training language models on domain-specific corpora has shown significant improvements in QA performance. For instance, the PubMedQA dataset for biomedical research makes use of a corpus specifically tailored to the biomedical domain, enhancing the model's comprehension of domain-specific terminology and context. Techniques such as BioBERT and SciBERT have been specifically pre-trained on large-scale biomedical and scientific corpora, respectively, to better capture the nuances and specialized vocabulary inherent in these fields [9][12]. Pre-training on such domain-specific corpora results in models that exhibit a marked improvement in QA tasks in those domains—BioBERT, for example, outperforms BERT by 17% on several biomedical QA benchmarks.\n\n### Methodological Innovations and Applications [1] [2] [3] [5] [10]\n\n\n\nQuestion Generation (QG) and Answer Extraction [5]. \nTechniques that generate questions from existing answers and contexts help in creating richer datasets. By employing methods like reinforcement learning and latent-variable models, researchers aim to reduce the bias towards generating 'safe' questions with high lexical overlap, thereby increasing the diversity and quality of the dataset [8].\n\n#### Open-Domain QA and Reasoning[1]\nLeveraging large, web-scale collections of question-answer pairs for open-domain QA helps in building models that can handle a wide range of topics. This diversity ensures that models can generalize better across different domains, including scientific ones [10][12].\n\n#### Combining Visual and Textual Data [4]\nIn areas like visual question answering (VQA), combining multimodal data sources (text and images) addresses biases inherent in unimodal datasets. Methods have been developed to identify and mitigate shortcuts where textual or visual cues alone predict answers, thus requiring models to genuinely understand and integrate both modalities for correct answers. For instance, researchers proposed methods for generating VQA counterexamples, ensuring that the QA systems address multimodal bias effectively [4].\n\n#### Weak Supervision and Pseudo Annotations [14].\n\nTraining under Weak Supervision Signals.Similar to the SP-based methods, it is difficult for IR-based methods to reason the correct answers without any annotations at intermediate steps, since the model cannot receive any feedback until the end of reasoning.It is found that this case may lead to spurious reasoning He et al. (2021).To mitigate such issues, Qiu et al. a formulated the reasoning process over KBs as expanding the reasoning path on KB and adopted reward shaping strategy to provide intermediate rewards.To evaluate reasoning paths at intermediate steps, they utilized semantic similarity between the question and the reasoning path to provide feedback.Besides evaluating the reasoning path at intermediate steps, a more intuitive idea is to infer pseudo intermediate status and augment model training with such inferred signals.Inspired by bidirectional search algorithm on graph, He et al. proposed to learn the intermediate reasoning entity distributions by synchronizing bidirectional reasoning process.While most of existing work focused on enhancing the supervision signals at intermediate steps, few work paid attention to the entity linking step.Researchers utilized off-the-shelf tools to locate the topic entity in question, which may cause error propagation to subsequent reasoning.In order to accurately locate the topic entity without annotations, Zhang et al. proposed to train entity linking module through a variational learning algorithm which jointly modeled topic entity recognition and subsequent reasoning over KBs. \nSome approaches use weak supervision and pseudo annotations to supplement limited labeled data. By incorporating techniques such as back-training to adapt question generators to the target domain, researchers improve model performance while addressing the scarcity of labeled data [8]. This method helps in fine-tuning models to better handle domain-specific nuances without extensive manual annotation efforts. For example, semi-supervised learning was used to annotate biomedical QA datasets, leading to enhanced model accuracy through the introduction of pseudo-labeled data [8].\n\nIn conclusion, the creation of training datasets for QA in scientific domains is complex and resource-intensive[9]. However, innovations in synthetic data generation[7], semi-supervised learning[5], data augmentation[3], dynamic reasoning[14], and domain-specific pre-training[12] are paving the way for more robust and accurate QA systems. These methodologies not only address the current challenges but also lay the groundwork for future advancements in scientific QA. \n\n\nReferences:\n[0] Robust Visual Question Answering: Datasets, Methods, and Future Challenges What datasets should be developed? Although existing datasets, especially the OOD dataset, enable us to provide insight into the robustness of VQA methods, 203, it is essential to note that each dataset has its own unique limitations. Taking the most commonly used OOD dataset VQA-CP as an example, it has two shortcomings. First, its distribution between training and test splits is significantly different or even reversed, which may not align with the real-world scenario. Some methods, 34, 72 may be devised based on this prior, which may not reflect the robustness of these methods accurately. Second, VQA-CP lacks the validation split, which results in methods being tuned on the test split. Although GQA-OOD alleviates the above issues, its test split is too small, only containing 12,578 questions. Therefore, existing datasets may not be sufficient to evaluate robustness. Furthermore, the dataset does not involve fine-grained bias evaluations such as vision shortcut measurement. To address this issue, we should develop a dataset that satisfies the following properties in the future.•The dataset should be sufficiently large and complete, with adequate validation splits for fine-tuning hyper-parameters and large splits for training and testing.•The dataset should contain ID and OOD test settings simultaneously. In this way, we can conduct a comprehensive and fair evaluation of the robustness of VQA methods.•The distribution between training and test splits should be more natural, rather than artificially setting significantly different or even contradictory data distributions. The artificial distribution prior may be used to improve model performance, while the prior can not be applied to other situations, leading to poor generalization ability.•The OOD test setting should simultaneously include language, vision, and multimodality bias, to have a more refined assessment of robustness.•The question format should be various, particularly in the test split. The question in existing datasets is usually generated by the template. However, the question patterns generated by templates may be not enough, and be learned or memorized easily, resulting in inaccurate comparisons.\n\n[1] Instance-Based Question Answering: A Data-Driven Approach 2002). In recent years, learning components have started to permeate Question Answering (Clarke et al., 2003;Ravichandran et al., 2003;Echihabi and Marcu, 2003). Although the field is still dominated by knowledge-intensive approaches, components such as question classification, answer extraction, and answer verification are beginning to be addressed through statistical methods. At the same time, research efforts in data acquisition promise to deliver increasingly larger question-answer datasets (Girju et al., 2003;Fleischman et al., 2003). Moreover, Question Answering is expanding to different languages (Magnini et al., 2003) and domains other than news stories (Zweigenbaum, 2003). These trends suggest the need for principled, statistically based, easily re-trainable, language independent QA systems that take full advantage of large amounts of training data. We propose an instance-based, data-driven approach to Question Answering. Instead of classifying questions according to limited, predefined ontologies, we allow training data to shape the strategies for answering new questions. Answer models, query content models, and extraction models are also learned directly from training data. We present a basic implementation of these concepts and evaluate the performance. Motivation Most existing Question Answering systems classify new questions according to static ontologies. These ontologies incorporate human knowledge about the expected answer (e.g. date, location, person), answer type granularity (e.g. date, year, century), and very often semantic information about the question type (e.g. birth date, discovery date, death date). While effective to some degree, these ontologies are still very small, and inconsistent. Considerable manual effort is invested into building and maintaining accurate ontologies even though answer types are arguably not always disjoint\n\n[2] Question Answering on Linked Data: Challenges and Future Directions The straightforward way of training semantic parsers requires training data consisting of NL sentences annotated with the corresponding logical representation, which are very cumbersome to obtain.Recent works have explored different ways to reduce the annotation effort in order to bypass this challenge.One proposed way is to train on question-answer pairs instead . Another way is to automatically generate training data from the KB and/or from entity-linked corpora  (e.g. ClueWeb). Training with paraphrasing corpora  is another technique explored in several works to improve the range of expressions the system will be able to cover.\n\n[3] Robust Visual Question Answering: Datasets, Methods, and Future Challenges Data augmentation-based methods typically generate additional augmented question-answer pairs (vi′,qi′,ai′)superscriptsubscript𝑣𝑖′superscriptsubscript𝑞𝑖′superscriptsubscript𝑎𝑖′(v_{i}^{{}^{\\prime}},q_{i}^{{}^{\\prime}},a_{i}^{{}^{\\prime}}) for each sample in the original dataset 𝒟𝒟\\mathcal{D} to balance the distribution of training data or mitigate the data bias. In particular, the question qi′superscriptsubscript𝑞𝑖′q_{i}^{{}^{\\prime}} is generated using word masking or replacement, 120, the image vi′superscriptsubscript𝑣𝑖′v_{i}^{{}^{\\prime}} is produced by object swapping and mixing, color conversion, and image flipping and resizing, and the answer ai′superscriptsubscript𝑎𝑖′a_{i}^{{}^{\\prime}} is obtained by the devised answer-assigning mechanism. The answer prediction a^isubscript^𝑎𝑖\\hat{a}_{i} process is the same as the paradigm described in Equation (2):a^i=Ec​(Em​(Ev​(vi),Eq​(qi))),(vi,qi,ai)∈𝒟∪{(vi′,qi′,ai′)|i∈,n}.formulae-sequencesubscript^𝑎𝑖subscript𝐸csubscript𝐸msubscript𝐸vsubscript𝑣𝑖subscript𝐸qsubscript𝑞𝑖subscript𝑣𝑖subscript𝑞𝑖subscript𝑎𝑖𝒟conditional-setsuperscriptsubscript𝑣𝑖′superscriptsubscript𝑞𝑖′superscriptsubscript𝑎𝑖′𝑖1𝑛\\begin{split}&\\hat{a}_{i}=E_{\\mathrm{c}}(E_{\\mathrm{m}}(E_{\\mathrm{v}}(v_{i}),E_{\\mathrm{q}}(q_{i}))),\\\\&(v_{i},q_{i},a_{i})\\in\\mathcal{D}\\cup\\{(v_{i}^{{}^{\\prime}},q_{i}^{{}^{\\prime}},a_{i}^{{}^{\\prime}})|i\\in,n\\}.\\end{split}(9)\n\n[4] Robust Visual Question Answering: Datasets, Methods, and Future Challenges VQACE . The previously mentioned datasets commonly overlook multimodal bias, instead prioritizing the assessment of language bias learning.  Multimodal bias is such a phenomenon that the frequent co-occurrence of textual and visual elements within the training data predicts specific answers accurately. Similar to the language bias, multimodal bias often persists and transfers to the validation set, potentially impacting the generalizability of VQA models. In order to address this issue, Dancette et al. proposed a method for identifying shortcuts that predicts the correct answer based on the appearance of words in the question and visual elements in the image. Then, they built VQA counter examples where the shortcut rules result in inaccurate answers as the evaluation protocol.  As depicted in Fig. 10, VQA models may learn or memorize certain words such as “doing” and objects like “man”, “surfboard”, and “hand” to predict answers. Although this can result in accurate predictions for some examples (as seen in the left example), it can also lead to incorrect responses (as observed in the right example). There is also an easy subset in which the correct answers can be derived through at least one of the shortcuts. Statistics show that 90% of the bias in this dataset666https://github.com/cdancette/detect-shortcuts is multimodal, indicating that previous successful debiasing methods, 34, 30, 35 in addressing the shift in language distributions such as the shift in VQA-CP and VQA-Rephrasings but may not be as effective in reducing natural shortcuts from VQA.\n\n[5] Training Question Answering Models from Synthetic Data One of the limitations of developing models for question answering, or any Deep Learning application for that matter, is the availability and cost of labeled training data. A common approach to alleviate this need is semi-supervised learning, wherein one trains a model on existing data and uses it to label more data for training (Zhu, 2005; Chapelle et al., 2009; Zhu & Goldberg, 2009; Kingma et al., 2014). This technique has demonstrated benefits in recent literature for image classification (Xie et al., 2019) and question answering (QA) tasks (Alberti et al., 2019a; Dong et al., 2019). However, the complexities of generating questions and answers in natural language proves challenging for existing methods, with a large gap in quality remaining between synthetic and human-generated data. In this work, we close this gap using only synthetic questions generated from large models. We also show that answer candidate generation is foundational to synthetic question quality.\n\n[6] Robust Visual Question Answering: Datasets, Methods, and Future Challenges However, in parallel with the above works, several studies, 30, 31, 32, 33 found that the aforementioned generic methods tend to memorize statistical regularities or bias in the training data rather than ground images to predict answers. For example, in the middle bar chart of the fourth column of Fig. 1, we can see that “tennis” is the most frequent answer. These methods answer the questions of the second column mainly by exploiting the connections between critical words “what”, and “sports” of the questions and “tennis”. This will cause these methods to perform well in the In-Distribution (ID) test scenario that has similar answer distributions with the training split, such as the distribution in the middle bar chart of the fourth column, but poorly in the Out-Of-Distribution (OOD) test situation that has different or even reversed answer distributions, such as the distribution in the bottom bar chart. In order to address this issue, a significant body of literature on VQA has emerged in recent years, with a particular focus on eliminating bias, 34, 35 and evaluating robustness, 36, 37.\n\n[7] Training Question Answering Models from Synthetic Data In this work we seek to generate high quality training data for SQuAD style extractive question answering over a given set of documents D𝐷D. This requires us to sample (c,q,a)𝑐𝑞𝑎(c,q,a) triples for given paragraph contexts c∈D𝑐𝐷c\\in D according to probability p​(q,a|c)𝑝𝑞conditional𝑎𝑐p(q,a|c), where q𝑞q is a question resulting in answer a𝑎a, which exists as a contiguous span of text in c𝑐c. Leveraging the roundtrip consistency method (Alberti et al., 2019a), we achieve this by using a three step approach consisting of Answer Generation a^∼p​(a|c)similar-to^𝑎𝑝conditional𝑎𝑐\\hat{a}\\sim p(a|c), Question Generation q^∼p​(q|a^,c)similar-to^𝑞𝑝conditional𝑞^𝑎𝑐\\hat{q}\\sim p(q|\\hat{a},c), and Roundtrip Filtration a^=?a∗∼p​(a|c,q^)superscript?^𝑎superscript𝑎similar-to𝑝conditional𝑎𝑐^𝑞\\hat{a}\\stackrel{{\\scriptstyle?}}{{=}}a^{*}\\sim p(a|c,\\hat{q}). As illustrated by Algorithm 1 the synthesized dataset of triples is then used to finetune and train a BERT-based QA model similar to (Devlin et al., 2018).\n\n[8] Low-Resource Dense Retrieval for Open-Domain Question Answering: A Comprehensive Survey generator is properly trained, DR trained with QG can even match the fully-supervised performance while using zero in-domain annotations Ren et al., 2022). The biggest challenge lies in the training of the question generator. If the question distribution from our target domain is very different from available domains, the question generator might not be able to adapt well. A solution is to apply semi-supervised learning when target-domain questions are available. We can create pseudo training instances by back-training to adapt the question generator to the target domain (Kulshreshtha et al., 2021;Shen et al., 2022a). Another issue is the oneto-many mapping relations between questions and documents. Under this situation, seq2seq learning tends to produce safe questions with less diversity and high lexical overlap with the document. For example, Shinoda et al. (2021) found that QG reinforces the model bias towards high lexical overlap. We will need more sophisticated training techniques such as latent-variable models (Yao et al., 2018;Lee et al., 2020;Li et al., 2022a) and reinforcement learning (Yuan et al., 2017;Zhang and Bansal, 2019) to alleviate the model bias towards safe questions. Leveraging Documents + Question This section assumes additional access to unlabeled questions Q t in the target domain. In practice, annotating question-document relations requires a time-consuming process of reading long documents, with domain knowledge and careful sampling strategies to ensure enough positive samples. Unlabeled questions are much easier to obtain either through real user-generated content or simulated annotations, so it is common to have a predominance of unlabeled questions in low-resource scenario 4 . To\n\n[9] PubMedQA: A Dataset for Biomedical Research Question Answering Under reasoning-required setting, long answers are available in training but not inference phase. We use them as an additional signal for training: similar to Ma et al. (2018) regularizing neural machine translation models with binary bag-of-word (BoW) statistics, we fine-tune BioBERT with an auxiliary task of predicting the binary BoW statistics of the long answers, also using the special [CLS embedding. We minimize binary cross-entropy loss of this auxiliary task:ℒBoW=−1N​∑ibi​log​bi^+(1−bi)​log​(1−bi^)subscriptℒBoW1𝑁subscript𝑖subscript𝑏𝑖log^subscript𝑏𝑖1subscript𝑏𝑖log1^subscript𝑏𝑖\\mathcal{L}_{\\text{BoW}}=-\\frac{1}{N}\\sum_{i}b_{i}\\text{log}\\hat{b_{i}}+(1-b_{i})\\text{log}(1-\\hat{b_{i}})where bisubscript𝑏𝑖b_{i} and bi^^subscript𝑏𝑖\\hat{b_{i}} are ground-truth and predicted probability of whether token i𝑖i is in the long answers (i.e.: bi∈{0,1}subscript𝑏𝑖01b_{i}\\in\\{0,1\\} and bi^∈,1^subscript𝑏𝑖01\\hat{b_{i}}\\in,1), and N𝑁N is the BoW vocabulary size. The total loss is:ℒ=ℒQA+β​ℒBoWℒsubscriptℒQA𝛽subscriptℒBoW\\mathcal{L}=\\mathcal{L}_{\\text{QA}}+\\beta\\mathcal{L}_{\\text{BoW}}In reasoning-free setting which we use for bootstrapping, the regularization coefficient β𝛽\\beta is set to 0 because long answers are directly used as input.\n\n[10] CCQA: A New Web-Scale Question Answering Dataset for Model Pre-Training Selection (BFS), Wiki Link Prediction (WLP) (Chang et al., 2020) and Question Answering Infused Pre-training (QUIP) (Jia et al., 2021) show consistent improvements over baselines. However, most of these approaches still rely * Work done at Meta. 1 Our dataset generation script and CCQA pre-trained checkpoints can be found at https://github.com/ facebookresearch/CCQA on either unlabeled text, or synthetically generated question-answer (QA) pairs. In this paper, we explore a second, somewhat orthogonal dimension to these lines of work, examining if a web-scale collection of natural QA pairs can support ODQA through in-domain pre-training. Per definition, an ODQA system should be able to answer any question from an arbitrary domain. We believe that to approach this ability with indomain pre-training, a suitable dataset should address the following 5 challenges: (1) Size; ODQA requires knowledge of a wide variety of topics. The underlying dataset used for in-domain pretraining hence needs to cover this abundance of domains, requiring a web-scale dataset. (2) Naturalness; While synthetic corpora can potentially capture a wide variety of language phenomena, to understand and generate truly natural language in all facets, synthetic datasets are not sufficient. (3) Quality; Given the requirement for a diverse, large-scale dataset, high data quality in terms of cleanliness and sensibility becomes a major challenge. Given that web-scale data sources require highly automated approaches operating on noisy data, assuring data quality is non-trivial. (4) Diversity; Besides size, another challenge for any ODQA in-domain pre-training dataset is the generality of the corpus. The dataset needs to support answering many diverse questions to allow\n\n[11] CCQA: A New Web-Scale Question Answering Dataset for Model Pre-Training Here, we evaluate our new CCQA dataset as an in-domain pre-training corpus for this highly challenging task by converting the JSON representation into plain question-answer pairs, removing markup tags and additional metadata. Models & Training Using the question-answer pairs from the CCQA dataset, we in-domain pre-train large language models for question-answering. We start with vanilla BART and T5 transformer models, shown on the left side (green) in Figure 3. We then further in-domain pre-train the models using a denoising or sequence-to-sequence (seq2seq) setup (yellow box in Figure 3). For the denoising task, we follow the vanilla BART approach (Lewis et al., 2020a), using a concatenation of Q: <question> A: <answer> as the model input. For the seq2seq task, we train the model to predict the gold answer given a question as input. With the additional in-domain pretraining step, a variety of training-flows emerge, shown as numbered circles in Figure 3: (1) Using a vanilla pre-trained language model to fine-tune on the benchmark dataset. (2) Using the CCQA dataset for in-domain pretraining and subsequently fine-tune on the benchmark dataset. (3) Using a pre-trained language model to directly infer answers on the benchmark dataset (zero-shot). (4) Using the CCQA in-domain pre-trained model to directly infer answers on the benchmark dataset in zero-shot fashion. Datasets We evaluate the performance of our CCQA corpus as an in-domain pre-training dataset on 5 common benchmarks, based on 4 publicly available datasets in the closed-book setting: TriviaQA (TQA) is a short-form, factoid-style question-answering dataset (Joshi et al., 2017). For the closed-book task, we\n\n[12] A Survey on Complex Knowledge Base Question Answering: Methods, Challenges and Solutions •Parsers used in existing SP-based methods are difficult to cover diverse complex queries (e.g., multi-hop reasoning, constrained relations and numerical operations).Similarly, previous IR-based methods may fail to answer a complex query, as their ranking is performed over small-scope entities without traceable reasoning.•More relations and subjects in complex questions indicate a larger search space of potential logic forms for parsing, which will dramatically increase the computational cost.Meanwhile, more relations and subjects could prevent IR-based methods from retrieving all relevant entities for ranking.•Both approaches treat question understanding as a primary step. When questions become complicated in both semantic and syntactic aspects, models are required to have strong capabilities of natural language understanding and generalization.•It is expensive to label the ground truth paths to the answers (see the example in Figure 1) for complex questions.Generally, only question-answer pairs are provided.This indicates SP-based methods and IR-based methods have to be trained without the annotation of correct logic forms and reasoning paths, respectively.Such weak supervision signals bring difficulties to both approaches.\n\n[13] A Survey on Complex Knowledge Base Question Answering: Methods, Challenges and Solutions Overview.The overall procedure typically consists of the modules of retrieval source construction, question representation, graph based reasoning and answer ranking.These modules will encounter different challenges for complex KBQA.Firstly, the retrieval source construction module extracts a question-specific graph from KBs, which covers a wide range of relevant facts for each question. Due to unneglectable incompleteness of source KBs Min et al. (2013), the correct reasoning paths may be absent from the extracted graph. This issue is more likely to occur in the case of complex questions.Secondly, question representation module understands the question and generates instructions to guide the reasoning process.This step becomes challenging when the question is complicated.After that, reasoning on graph is conducted through semantic matching.When dealing with complex questions, such methods rank answers through semantic similarity without traceable reasoning in the graph, which hinders reasoning analysis and failure diagnosis.Eventually, this system encounters the same training challenge under weak supervision signals (i.e., question-answer pairs).The following parts illustrate how prior work deal with these challenges.Reasoning under Incomplete KB.IR-based methods first extract a question-specific graph from KBs, and conduct subsequent reasoning on it.Since simple questions only require 1-hop reasoning on the neighborhood of topic entity in KBs, IR-based methods are less likely to suffer from the inherent incompleteness of KBs Min et al. (2013).In comparison, it may be a severe problem for complex questions, where the correct reasoning path may be absent from the question-specific graph.Furthermore, this incompleteness reduces the neighborhood information used for encoding entities, which poses additional challenges for effective reasoning.To tackle this challenge, researchers utilize auxiliary information to enrich the knowledge source.Intuitively, large question-related text corpus retrieved from Wikipedia can provide a wide range of unstructured knowledge as supplementary evidence.Sun et al.  and Sun et al.  proposed to complement the subgraph extracted from incomplete KBs with extra question-related text sentences to form a heterogeneous graph and conduct reasoning on it.Instead of directly complementing sentences to question-specific graph as nodes, Xiong et al.  and Han et al. a proposed to fuse extra textual information into the entity representation to supplement knowledge.They first encoded sentences and entities conditioned on questions, and then supplemented the incomplete KB by aggregating representations of sentences to enhance corresponding entity representations.Besides extra text corpus, knowledge base embeddings have been adopted to alleviate the sparsity of KB by performing missing link prediction.Inspired by KB completion task, Saxena et al.  utilized pre-trained knowledge base embeddings to enrich the learned entity representations and address incomplete KB issue.Understanding Complex Semantics.In general, IR-based methods generate reasoning instructions by directly encoding questions as low-dimensional vectors through neural network (e.g., LSTM).Static reasoning instructions obtained through above approaches cannot effectively represent the compositional semantics of complex questions.In order to comprehensively understand questions, recent work dynamically updated the reasoning instructions during the reasoning process.To focus on the currently unanalyzed part of question, Miller et al. , Zhou et al.  and  Xu et al.  proposed to update the reasoning instruction with information retrieved along the reasoning process.Besides updating the instruction representation with the reasoned information, He et al.  proposed to focus on different parts of the question with dynamic attention mechanism.This dynamic attention mechanism can promote the model to attend to other information conveyed by the question and provide proper guidance for subsequent reasoning steps.Instead of decomposing the semantics of questions, Sun et al.  proposed to augment the representation of the question with contextual information from graph.They updated the reasoning instruction through aggregating information from the topic entity after every reasoning step.Uninterpretable Reasoning.Traditional IR-based methods rank answers by calculating a single semantic similarity between questions and entities in the graph, which is less interpretable at the intermediate steps.As the complex questions usually query multiple facts, the system is supposed to accurately predict answers over the graph based on a traceable and observable reasoning process.Even though some work repeated reasoning steps for multiple times, they cannot reason along a traceable path in the graph.To derive a more interpretable reasoning process, multi-hop reasoning is introduced. Specifically, Zhou et al.  and  Xu et al.  proposed to make the relation or entity predicted at each hop traceable and observable.They output intermediate predictions (i.e., matched relations or entities) from predefined memory as the interpretable reasoning path.Nevertheless, it can not fully utilize the semantic relation information to reason edge by edge.Thus, Han et al. b constructed a denser hypergraph by pinpointing a group of entities connected via same relation, which simulated human’s hopwise relational reasoning and output a sequential relation path to make the reasoning interpretable.Training under Weak Supervision Signals.Similar to the SP-based methods, it is difficult for IR-based methods to reason the correct answers without any annotations at intermediate steps, since the model cannot receive any feedback until the end of reasoning.It is found that this case may lead to spurious reasoning He et al. (2021).To mitigate such issues, Qiu et al. a formulated the reasoning process over KBs as expanding the reasoning path on KB and adopted reward shaping strategy to provide intermediate rewards.To evaluate reasoning paths at intermediate steps, they utilized semantic similarity between the question and the reasoning path to provide feedback.Besides evaluating the reasoning path at intermediate steps, a more intuitive idea is to infer pseudo intermediate status and augment model training with such inferred signals.Inspired by bidirectional search algorithm on graph, He et al.  proposed to learn the intermediate reasoning entity distributions by synchronizing bidirectional reasoning process.While most of existing work focused on enhancing the supervision signals at intermediate steps, few work paid attention to the entity linking step.Researchers utilized off-the-shelf tools to locate the topic entity in question, which may cause error propagation to subsequent reasoning.In order to accurately locate the topic entity without annotations, Zhang et al.  proposed to train entity linking module through a variational learning algorithm which jointly modeled topic entity recognition and subsequent reasoning over KBs.\n\n"}
{"case_id": "a5d23eb3a2555db0a82f6b64fed85baa", "answer_text": "\n\nFine-tuning smaller models can offer notable benefits over using larger language models (LLMs) in various tasks, particularly when considering factors such as computational efficiency, cost, customization, and specific task performance [1]. Here are some areas where fine-tuning smaller models proves advantageous:\n\n\n1. **Cost and Resource Efficiency**: Although usually fine-tuning is considered more expensive than prompt engineering, depending on the subscription models of the commercial LLMs, inference fees can quickly become a huge burden. For example, running GPT-4 for this study costs around 1,500 USD. Therefore, developers who routinely use LLMs for code might opt for a cheaper model or even in-house fine-tuned smaller models, even if that means lower accuracy [0].\n\n   Fine-tuning smaller models is often more resource-efficient than deploying large LLMs, especially in computationally constrained environments. Smaller models, like FLAN-T5 with only 780M parameters, can sometimes perform on par or even better than larger zero-shot LLMs (spanning from 7B to over 70B parameters) in tasks such as meeting summarization, making them a cost-efficient solution for real-world deployment [1]. This is critical for industries where operational costs and computational resources are significant constraints.\n\n2. **Task-Specific Accuracy**: [2]\n   In certain highly specialized tasks, fine-tuning smaller models can yield superior performance compared to the larger LLMs. For example, in automated software engineering tasks like code generation, fine-tuned smaller models have shown to outperform larger models such as GPT-4 with the best prompting strategies by significant margins—16.61% to 28.3% improvement in BLEU scores [2][4]. This illustrates that task-specific fine-tuning allows these models to tailor their capabilities closely to the requirements of a specific task, leading to higher accuracy.\n\n3. **Parameter-Efficient Fine-Tuning (PEFT)** [3]:\n   Techniques like PEFT (Parameter-Efficient Fine-Tuning) have been particularly effective for optimizing smaller models for specific tasks without the massive resource overhead typically required by full-scale fine-tuning of LLMs. For instance, methods like LoRA (Low-Rank Adaptation) and QLoRA (Quantized LoRA) enable efficient fine-tuning even for larger models with limited computational resources, demonstrating robust performance on tasks such as code generation [3][12]. PEFT techniques provide a middle ground where smaller models can be efficiently tuned to perform well on specific tasks while circumventing the need for large-scale computational resources.\n\n4. **Personalized and Domain-Specific Applications**: [9]\n   Smaller models can be fine-tuned to reflect the intricacies of domain-specific data. In agriculture, for example, fine-tuning models on domain-specific datasets led to a notable improvement in accuracy and the ability to generate location-specific insights that are critical for industry applications [9]. This kind of customization is often more manageable and effective with smaller models, which can be fine-tuned to include highly specialized knowledge without the inefficiency of querying extensive general-purpose knowledge bases.\n\n5. **Chain-of-Thought (CoT) Reasoning and Instruction Tuning** [5]. \n\n\n   Instruction tuning with rationales (CoT fine-tuning) has been shown to equip smaller models with enhanced reasoning capabilities. For example, Flan-T5 models (3B and 11B parameters) fine-tuned with CoT reasoning demonstrate significant improvements on benchmarks like BIG-Bench-Hard (BBH) [5][8]. This indicates that smaller models, when tuned appropriately, can achieve or even surpass the performance typically expected from larger models, particularly in tasks requiring logical reasoning and few-shot learning capability.\n\n6. **Multi-Stage Retrieval and Efficiency in Information Retrieval Tasks** [11]:\n   In information retrieval, smaller models fine-tuned to act as dense retrievers or pointwise rerankers can outmatch traditional segmenting and pooling strategies used by larger models. Fine-tuned models like RepLLaMA and RankLLaMA have been shown to outperform smaller models built on similar designs, providing efficient retrieval solutions while maintaining competitive performance [11][14]. This emphasizes the potential of smaller models in specialized roles where efficiency and effectiveness need to be balanced.\n\nIn summary, fine-tuning smaller models can be highly beneficial in scenarios where resource constraints, cost efficiency, task-specific performance, domain-specific customization, and parameter-efficient tuning are critical [1]. While larger LLMs offer broad generalization, smaller, fine-tuned models provide a tailored approach to solving specific problems effectively and efficiently [1]. \n\n\nReferences:\n[0] Prompt Engineering or Fine Tuning: An Empirical Assessment of Large Language Models in Automated Software Engineering Tasks To summarize the findings and put them in a bigger context, we conclude that like most other engineering problems the choice of prompt engineering vs. fine-tuning LLMs is a trade-off. The following are some of the main aspects that play a role in this trade-off:•Accuracy: The most obvious aspect is the accuracy of the models. As we saw in the RQs and discussed above, neither approach always dominates the other. However, conversational prompt engineering seems to be the winner in the long run.•Cost: Although usually fine-tuning is considered more expensive than prompt engineering, depending on the subscription models of the commercial LLMs, inference fees can quickly become a huge burden. For example, running GPT-4 for this study costs around 1,500 USD. Therefore, developers who routinely use LLMs for Code might opt for a cheaper model or even in-house fine-tuned smaller models, even if that means lower accuracy.•Ease of Use: Prompt Engineering is a big winner here, especially if one goes with the natural language-based interfaces. We realize this when we see even participants with little to no background in LLMs can do the ASE task with GPT-4; whereas they would not be able to work with fine-tuned models without a minimum knowledge of how to setup, configure, and call fine-tuned models APIs in the code.•Control: Finally, another aspect of the trade-off is the level of control on the internal configurations and outputs. For example, with GPT-4, although you can set the temperature to zero, you still get non-deterministic answers which makes the verification of the models harder and thus less interesting for some users.\n\n[1] Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight in the Real World for Meeting Summarization? Large Language Models (LLMs) have demonstrated impressive capabilities to solve a wide range of tasks without being explicitly fine-tuned on task-specific datasets. However, deploying LLMs in the real world is not trivial, as it requires substantial computing resources. In this paper, we investigate whether smaller, compact LLMs are a good alternative to the comparatively Larger LLMs2 to address significant costs associated with utilizing LLMs in the real world. In this regard, we study the meeting summarization task in a real-world industrial environment and conduct extensive experiments by comparing the performance of fine-tuned compact LLMs (e.g., FLAN-T5, TinyLLaMA, LiteLLaMA) with zero-shot larger LLMs (e.g., LLaMA-2, GPT-3.5, PaLM-2). We observe that most smaller LLMs, even after fine-tuning, fail to outperform larger zero-shot LLMs in meeting summarization datasets. However, a notable exception is FLAN-T5 (780M parameters), which performs on par or even better than many zero-shot Larger LLMs (from 7B to above 70B parameters), while being significantly smaller. This makes compact LLMs like FLAN-T5 a suitable cost-efficient solution for real-world industrial deployment.\n\n[2] Prompt Engineering or Fine Tuning: An Empirical Assessment of Large Language Models in Automated Software Engineering Tasks In this paper, we investigate the effectiveness of state-of-the-art LLM, i.e., GPT-4, with three different prompting engineering techniques (i.e., basic prompting, in-context learning, and task-specific prompting) against 18 fine-tuned LLMs on three typical ASE tasks, i.e., code generation, code summarization, and code translation. Our quantitative analysis of these prompting strategies suggests that prompt engineering GPT-4 cannot necessarily and significantly outperform fine-tuning smaller/older LLMs in all three tasks. For comment generation, GPT-4 with the best prompting strategy (i.e., task-specific prompt) had outperformed the first-ranked fine-tuned model by 8.33% points on average in BLEU. However, for code generation, the first-ranked fine-tuned model outperforms GPT-4 with best prompting by 16.61% and 28.3% points, on average in BLEU. For code translation, GPT-4 and fine-tuned baselines tie as they outperform each other on different translation tasks. To explore the impact of different prompting strategies, we conducted a user study with 27 graduate students and 10 industry practitioners. From our qualitative analysis, we find that the GPT-4 with conversational prompts (i.e., when a human provides feedback and instructions back and forth with a model to achieve best results) showed drastic improvement compared to GPT-4 with automatic prompting strategies. Moreover, we observe that participants tend to request improvements, add more context, or give specific instructions as conversational prompts, which goes beyond typical and generic prompting strategies. Our study suggests that, at its current state, GPT-4 with conversational prompting has great potential for ASE tasks, but fully automated prompt engineering with no human in the loop requires more study and improvement.\n\n[3] Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models In this study, we focus on the following research questions:–RQ1: How do LLMs tuned with PEFT compare to small language models tuned with full fine-tuning and PEFT on code generation?We study which group of models performs better within a maximum GPU memory of 24GB: 1) small models (¡ 1B parameters) employing full-parameter fine-tuning, as commonly used in previous SE studies (feng2020codebert; wang2021codet5; weyssow2022better; zhou2021assessing) and PEFT; or 2) newly emerging LLMs (≥\\geq 1B parameters) with PEFT.We study the effectiveness of a large spectrum of small language models and LLMs for code generation on two datasets covering codes of various lengths.We select a wide range of models of various sizes, pre-trained on diverse codebases and with different learning objectives to study how these factors impact their effectiveness.–RQ2: Are PEFT techniques more promising than ICL for LLMs of code?Given the wide use of ICL when applying LLMs for SE tasks (geng2024large; gao2023constructing; prenner2022can; xia2023automated), this RQ aims to investigate whether PEFT techniques consistently outperform ICL for LLMs.We compare the best-performing PEFT technique in RQ1 with ICL for the small language models and LLMs.We test all the models with zero to three randomly selected prompt examples and run each test using five different seeds to mitigate variance.–RQ3: How does LoRA perform when fine-tuning the LLMs jointly on both datasets?We investigate how LoRA performs in a joint training setting across all models by jointly fine-tuning on both training datasets using a single adapter.We compare the effectiveness of the joint fine-tuning approach with single-dataset fine-tunings to elucidate whether LoRA enables efficient tuning of LLMs on two diverse datasets, simultaneously.–RQ4: Can we further reduce resource utilization while tuning larger models through LoRA and quantization?We explore how quantization combined with LoRA allows further reduction of the required computational resources and the tuning of larger models up to 34B parameters under our resource-constrained scenario.Specifically, we assess QLoRA (dettmers2023qlora) that uses 4/8-bit integer precision instead of 16-bit floats for two larger variants of one of our best-performing LLM.\n\n[4] Prompt Engineering or Fine Tuning: An Empirical Assessment of Large Language Models in Automated Software Engineering Tasks Our quantitative analysis of these prompting strategies suggests that prompt engineering GPT-4 cannot necessarily and significantly outperform fine-tuning smaller/older LLMs in all three tasks.For comment generation, GPT-4 with the best prompting strategy (i.e., task-specific prompt) had outperformed the first-ranked fine-tuned model by 8.33% points average in BLEU.However, for code generation, the first ranked fine-tuned model outperforms GPT-4 with best prompting by 16.61% and 28.3% points in BLEU.For code translation, GPT-4 and fine-tuned baselines tie as they outperform each other on different translation tasks.From our qualitative analysis, we find that GPT-4 with the conversational prompts shows drastic performance improvement across the three examined ASE tasks compared to automated prompting strategies. Specifically, the improvements of conversational prompts against the best automated prompting strategies are 15.8%, 11.83%, and 16.1% for comment generation, code generation, and code translation respectively.Moreover, we observe that participants tend to request improvements (e.g. give more natural comments, make code readable, etc.), add more context, or give specific instructions (e.g. remove import statements, use certain types for parameters/variables, etc.) during the evolution of conversational prompts to help GPT-4 generate better responses. Our result suggests that, at its currentstate, GPT-4 with conversational prompting has great potential for ASE tasks, but fully automated promptengineering with no human in the loop requires more study and improvement.This paper makes the following contributions:•We present the first empirical study on systematically comparing automated prompting strategies for GPT-4 on three typical ASE tasks vs. fine-tuned LLMs from the literature.•We have conducted the first user study including 27 graduate students and 10 industry practitioners to explore the evolution of conversational prompts used in GPT-4 in solving AES tasks.•We identify gaps between conversational prompts and automated prompting strategies, as well as suggest future ideas for leveraging LLMs to solve ASE tasks.•We release the source code and the dataset of this work to help other researchers replicate and extend our study111https://anonymous.4open.science/r/gpt4_ase_tasks-6BF8.We organized the rest of this paper as follows.Section 2 presents the background and motivation of this study.Section 3 shows the setup of our empirical study.Section 4 shows the details of the methodology and protocols of our experiment.Section 5 presents the evaluation results.Section 6 presents the threats to the validity of this work.Section 7 concludes this paper.\n\n[5] The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning Language models (LMs) with less than 100B parameters are known to perform poorly on chain-of-thought (CoT) reasoning in contrast to large LMs when solving unseen tasks. In this work, we aim to equip smaller LMs with the step-by-step reasoning capability by instruction tuning with CoT rationales. In order to achieve this goal, we first introduce a new instruction-tuning dataset called the CoT Collection, which augments the existing Flan Collection (including only 9 CoT tasks) with additional 1.84 million rationales across 1,060 tasks. We show that CoT fine-tuning Flan-T5 (3B&11B) with CoT Collection enables smaller LMs to have better CoT capabilities on unseen tasks. On the BIG-Bench-Hard (BBH) benchmark, we report an average improvement of +4.34% (Flan-T5 3B) and +2.60% (Flan-T5 11B), in terms of zero-shot task accuracy. Furthermore, we show that instruction tuning with CoT Collection allows LMs to possess stronger few-shot learning capabilities on 4 domain-specific tasks, resulting in an improvement of +2.24% (Flan-T5 3B) and +2.37% (Flan-T5 11B), even outperforming ChatGPT utilizing demonstrations until the max length by a +13.98% margin. Our code, the CoT Collection data, and model checkpoints are publicly available.\n\n[6] The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning Recent work has focused on empowering relatively smaller LMs to effectively solve novel tasks as well, primarily through fine-tuning with rationales (denoted as CoT fine-tuning) and applying CoT prompting on a single target task (Shridhar et al., 2022; Ho et al., 2022; Fu et al., 2023). However, solving a single task does not adequately address the issue of generalization to a broad range of unseen tasks. While Chung et al. (2022) leverage 9 publicly available CoT tasks during instruction tuning to solve multiple unseen tasks, the imbalanced ratio compared to 1,827 tasks used for direct fine-tuning results in poor CoT results across smaller LMs (Longpre et al., 2023). In general, the community still lacks a comprehensive strategy to fully leverage CoT prompting to solve multiple unseen novel tasks in the context of smaller LMs.\n\n[7] RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture There are two common ways in which developers are incorporating proprietary and domain-specific data when building applications of Large Language Models (LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the prompt with the external data, while fine-Tuning incorporates the additional knowledge into the model itself. However, the pros and cons of both approaches are not well understood. In this paper, we propose a pipeline for fine-tuning and RAG, and present the tradeoffs of both for multiple popular LLMs, including Llama2-13B, GPT-3.5, and GPT-4. Our pipeline consists of multiple stages, including extracting information from PDFs, generating questions and answers, using them for fine-tuning, and leveraging GPT-4 for evaluating the results. We propose metrics to assess the performance of different stages of the RAG and fine-Tuning pipeline. We conduct an in-depth study on an agricultural dataset. Agriculture as an industry has not seen much penetration of AI, and we study a potentially disruptive application - what if we could provide location-specific insights to a farmer? Our results show the effectiveness of our dataset generation pipeline in capturing geographic-specific knowledge, and the quantitative and qualitative benefits of RAG and fine-tuning. We see an accuracy increase of over 6 p.p. when fine-tuning the model and this is cumulative with RAG, which increases accuracy by 5 p.p. further. In one particular experiment, we also demonstrate that the fine-tuned model leverages information from across geographies to answer specific questions, increasing answer similarity from 47% to 72%. Overall, the results point to how systems built using LLMs can be adapted to respond and incorporate knowledge across a dimension that is critical for a specific industry, paving the way for further applications of LLMs in other industrial domains.\n\n[8] Fine-Tuning LLaMA for Multi-Stage Text Retrieval The successful application of large language models in generative tasks has sparked interest in their potential to enhance retrieval.In this study, we demonstrate that it is possible to fine-tune a large model to act as a dense retriever (RepLLaMA) and a pointwise reranker (RankLLaMA), thereby establishing an effective, state-of-the-art multi-stage retrieval system that outperforms smaller models built on the same basic design.Moreover, our approach offers greater optimization and efficient inference potential than recent methods that prompt large language models for text reranking in a generative manner.This work underscores the potential of leveraging LLMs for retrieval tasks in the future, which we continue to explore.\n\n[9] Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey To further reduce memory usage during fine-tuning, some methods attempt to circumvent backpropagation within LLMs to address this issue. HyperTuning  employs a HyperModel to generate PEFT parameters using only fewshot examples. This approach demonstrates results comparable to those obtained through full model fine-tuning.PEFT Plug-in  first trains PEFT modules on small language models, which is more memory efficient compared to training on large ones. Subsequently, the research introduces a suite of techniques for seamlessly integrating these trained PEFT modules into LLMs during inference. This strategy effectively circumvents the necessity of gradient-based optimization directly on the larger models, resulting in substantial memory savings.However, it is important to note that both HyperModel and PEFT Plug-in still require additional model training, and this training cost cannot be entirely overlooked.MeZO  introduces a memory-efficient zeroth-order (ZO) optimizer for LLMs. Unlike conventional PEFT techniques, which rely on backpropagation to compute gradients for updating model parameters, MeZO fine-tunes LLMs through only forward passes. It accomplishes this by employing a ZO gradient estimator to calculate the gradient. Notably, MeZO implements an in-place solution for the classic ZO gradient estimator, effectively mitigating memory consumption during inference execution.This innovative approach allows for efficient fine-tuning of LLMs containing 30 billion parameters on a single GPU with 80GB of memory, all while maintaining performance that is comparable to fine-tuning using backpropagation. Furthermore, it can substantially decrease storage demands in comparison to the traditional PEFT methods such as LoRA and Adapter.\n\n[10] Fine-Tuning LLaMA for Multi-Stage Text Retrieval The effectiveness of multi-stage text retrieval has been solidly demonstrated since before the era of pre-trained language models. However, most existing studies utilize models that predate recent advances in large language models (LLMs). This study seeks to explore potential improvements that state-of-the-art LLMs can bring. We conduct a comprehensive study, fine-tuning the latest LLaMA model both as a dense retriever (RepLLaMA) and as a pointwise reranker (RankLLaMA) for both passage retrieval and document retrieval using the MS MARCO datasets. Our findings demonstrate that the effectiveness of large language models indeed surpasses that of smaller models. Additionally, since LLMs can inherently handle longer contexts, they can represent entire documents holistically, obviating the need for traditional segmenting and pooling strategies. Furthermore, evaluations on BEIR demonstrate that our RepLLaMA-RankLLaMA pipeline exhibits strong zero-shot effectiveness. Model checkpoints from this study are available on HuggingFace.\n\n"}
{"case_id": "e2d0eb391fbf674c070c74c387ee6248", "answer_text": "\n\nThe initialization of the transformer embedding layer is a crucial aspect that significantly influences the overall performance of the model [1]. Despite the availability of various pre-trained embeddings from existing models, there are several compelling reasons why transformer embedding layers are typically initialized randomly during pre-training [1]. \n\nVariance and Gradient Flow Considerations [2].\nDeep neural networks, including transformers, are highly sensitive to the variance of their parameters for proper gradient flow and effective training. The Xavier initialization, which sets the parameters to be zero-centered and within a narrow variance range, has been shown to be particularly effective in enabling better gradient flow and faster convergence during training [2]. Pre-trained embeddings, on the other hand, are not necessarily subject to these distributional constraints. For example, pre-trained token vectors like GloVe or T5 often have variances that are significantly different from those specified by the Xavier initialization, leading to suboptimal performance when directly used in transformers. This variance mismatch can hinder the gradient flow, making it difficult for the model to learn effectively during the early stages of training [2].\n\nInteraction with Positional Encodings [2].\nTransformers utilize positional encodings to capture the order and position of tokens in the input sequence. When pre-trained embeddings are added to positional encodings, a variance mismatch can occur, leading to a phenomenon known as \"absorption,\" where the embeddings with a higher variance overshadow the positional encodings [2]. This can diminish the impact of the positional encodings, reducing the model's ability to capture sequential information effectively. Conversely, positional encodings can also alter the word-word relations encoded by the embeddings, complicating the training process further [5].\n\n### Empirical Evidence on Pre-trained Embeddings\n\nEmpirical evidence on pre-trained embeddings showcases various benefits and limitations of using pre-trained embeddings in transformers. Initializing the embedding layer with pre-trained word vectors such as Glove, Word2Vec, and sub-word vectors from language models like BERT has shown to capture a range of useful knowledge, including lexical, syntactic, and factual relations [1]. Furthermore, such pre-trained parameters can help improve training effectiveness and convergence in downstream tasks, especially when training data is limited [1]. However, it is also noted that random initialization often performs better than pre-trained word embeddings when training transformer models from scratch, with inconsistent trends for sub-word embeddings [1].\n\nIn exploring the reasons for the inconsistent performance of pre-trained embeddings in transformers, two factors are highlighted. Firstly, the model's sensitivity to parameter distribution, where deep neural networks are sensitive to the variance of parameters for proper gradient flow, a standard ensured by Xavier initialization [2]. Secondly, interactions with position encodings, where the variance difference between pre-trained embeddings and positional encodings can lead to dominance of one type of information over the other, affecting performance [2]. Experimental findings show that pre-trained token vectors that do not fall within the Xavier initialization specifications tend to be less effective [5]. Moreover, pre-trained embeddings do contribute to model performance in terms of their semantic information, which is evident in the consistent performance decrease when elements of a pre-trained embedding layer are shuffled [5]. \n\nOverall, the empirical evidence suggests a nuanced view of pre-trained embeddings in transformers, indicating both their potential benefits and the need for careful consideration of initialization techniques to optimize performance across different tasks. \nEmpirical studies have shown mixed results when using pre-trained embeddings in transformers. For instance, while embeddings from models like BERT and mBERT, which have variances similar to Xavier, tend to perform well, other embeddings with significantly different variances like GloVe tend to perform poorly [4]. This suggests that the inherent semantic information in pre-trained embeddings doesn't always compensate for the variance mismatch issues, thus making random initialization a safer and more effective choice [4]. Moreover, standardizing pre-trained embeddings to match the Xavier distribution has sometimes led to performance improvements, but these are not consistently better than random initialization, highlighting the complexity of the issue [5].\n\nTask-Specific Impacts [7].\nThe effectiveness of random initialization versus pre-trained embeddings can vary depending on the task. For example, empirical results have shown that BERT and mBERT embeddings outperform other initialization methods in natural language inference (NLI) tasks like MNLI, which require capturing subtle semantic relationships [4]. However, for tasks like machine translation (e.g., de-en for Multi30k and IWSLT2017), pre-trained embeddings like GloVe significantly underperform due to their variance mismatch with the required initialization range [2]. This task-specific variance indicates that random initialization provides a more generalizable and robust starting point across a variety of tasks.\n\nPractical considerations in training efficiency have become increasingly important due to the large amount of resources required for training transformer models. Recent literature suggests that initializing transformer models with pre-trained embeddings or weights from larger models can significantly improve training convergence and efficiency. For instance, in one study, models initialized with weights subcloned from larger pre-trained models achieved faster convergence, demonstrating a higher accuracy within fewer training epochs compared to those with random initialization [11]. Additionally, proper initialization is essential to stabilize transformers during large-scale pre-training, which involves rescaling output matrices of self-attention modules and feed-forward networks [13].\nRandom initialization offers practical advantages in terms of computational efficiency and training speed. It allows for uniform control over the parameter distributions, leading to more consistent and predictable training dynamics. Techniques like weight subcloning, which involve initializing smaller models with scaled-down versions of larger pre-trained models, have shown significant improvements in training speed and convergence. These techniques indicate that carefully designed random initialization schemes can often outperform the use of pre-trained embeddings, especially when no initialization scheme perfectly matches the training requirements of the new model [7][11].\n\nLimitations and Scenarios Favoring Pre-trained Embeddings[1].\nDespite the multiple advantages of random initialization, there are scenarios where pre-trained embeddings might be preferable. When dealing with tasks that have very limited training data, the semantic information and pre-learned relationships in pre-trained embeddings can offer a substantial starting point, leading to better performance. Additionally, in tasks where domain-specific knowledge is crucial, pre-trained embeddings that encapsulate such knowledge can be highly beneficial. For example, in biological sequence analysis, embeddings pre-trained on nucleotide sequences have shown to capture domain-specific patterns and relations effectively [10].\n\n\n### Conclusion\n[1] [6] [7]\nThe decision to initialize transformer embedding layers randomly rather than with pre-trained embeddings is driven by multiple factors. Random initialization ensures proper variance and gradient flow, avoids complex interactions with positional encodings, and offers more consistent performance across various tasks. However, pre-trained embeddings can be beneficial in scenarios with limited training data or specific domain knowledge requirements. Thus, the choice of initialization strategy should be guided by the specific requirements and constraints of the task at hand [2][4][5][10].\n\n\nReferences:\n[0] On Initializing Transformers with Pre-trained Embeddings The transformer Vaswani et al. (2017) remains dominant as being the underlying architecture for state-of-the-art large language models Zhao et al. (2023). Because of the massive amount of resources and time required to train these models, any types of optimizations would be beneficial to save costs. Therefore, when training future transformer-based language models, initializing the right parameter values can be one of many methods that can help train the model more effectively. There are many good reasons for initializing the embedding layer with pre-trained word vectors such as Glove Pennington et al. (2014), Word2Vec Mikolov et al. (2013), and more recently sub-word vectors Bojanowski et al. (2017) from language models such as BERT Devlin et al. (2019). For one, they have been shown to capture a range of useful knowledge including lexical, syntactic, and even some types of factual relations Pennington et al. (2014). Also, from a transfer learning perspective, pre-trained parameters can help improve training effectiveness and convergence in downstream tasks, especially when training data is limited. Indeed, pre-trained token vectors111In this paper, we refer token embeddings to both sub-word and word embeddings have been shown to be useful for a wide-range of applications in other non-transformer based models Kocmi and Bojar (2017), Kim (2014), Collobert et al. (2011), Qi et al. (2018), Lample et al. (2016). However, when training transformer based models from scratch, it has become standard practice to prefer randomly initialized embeddings over pre-trained embeddings. Our experiments also indicate that random initialization performs better than pre-trained word embeddings, while for sub-word embeddings the trends are not consistent. What makes certain pre-trained embeddings ineffective in transformers?\n\n[1] On Initializing Transformers with Pre-trained Embeddings To answer this question, we investigate two possible factors.1.Model sensitivity to parameter distribution: Deep neural networks, including transformers, are sensitive to the variance of the parameters for proper gradient flow Glorot and Bengio (2010). Studies from Glorot and Bengio (2010) have shown a suitable variance range for parameter initialization to enable better gradient flow and faster convergence during training. The random initialization scheme utilizing such a range is known as Xavier initialization and is standard practice in training deep neural networks. In effect, it restricts the model parameter values, including those in token embeddings, to be zero centered and within a narrow variance range 222He et al. (2015) also introduced a similar initialization scheme, which accounts for non-linear activation in Rectified Linear Units. In this work, we focus only on Xavier initialization for settings without ReLU activation.. Pre-trained token vectors, however, are not necessarily subject to these distributional constraints and may perform poorly if the variance and mean does not lie within a preferred narrow range.2.Interactions with Position Encodings: Token embeddings in transformers are directly added to the positional encodings in order to track position information. If the variance of the word embeddings and positional encodings are widely different, then one type of information might dominate the other when added together. We refer to this phenomenon in the paper as \"absorption\", such as when the pre-trained embeddings with much higher variance \"absorb\" the positional encodings.To test the impact of these factors, we conduct an empirical study comparing various pre-trained/random embedding initializations on four tasks — machine translation (de-en for both Multi30k Elliott et al. (2016) and IWSLT2017 Cettolo et al. (2017), sentiment analysis (SST2) Socher et al. (2013), and natural language inference (MNLI) Williams et al. (2018). We tested GloVe for pre-trained word embeddings, and for sub-word embeddings, they include those directly obtained from the embedding layers of BERT, multilingual BERT (mBERT) Devlin et al. (2019)), T5 Raffel et al. (2020), and multilingual T5 (mT5) Xue et al. (2021). The experimental results support three key findings:\n\n[2] On Initializing Transformers with Pre-trained Embeddings On the other hand, BERT and mBERT embeddings consistently outperform the remaining embedding initializations in all experiments except for Multi30k, with MNLI showing the largest difference. This can be partially explained by BERT and mBERT embeddings having a similar variance to Xavier as shown in Table 2 (0.04<σB​E​R​T<0.050.04subscript𝜎𝐵𝐸𝑅𝑇0.050.04<\\sigma_{BERT}<0.05 vs 0.01<σX​a​v​i​e​r<0.020.01subscript𝜎𝑋𝑎𝑣𝑖𝑒𝑟0.020.01<\\sigma_{Xavier}<0.02 across all tasks). In addition, the increase in performance compared to random embeddings in general can also be explained by its semantic information. This is apparent when results show shuffled BERT embeddings to always perform worse than its original counterpart. The usefulness of this semantic information also varies for each task as shown by the variation in score differences between BERT and BERT_Shuffled across tasks. Regardless, the consistent performance decrease when shuffling any pre-trained embeddings show that the semantic information is being utilized. The same pattern follows when shuffling T5 and mT5 embeddings, supporting Finding 3.\n\n[3] On Initializing Transformers with Pre-trained Embeddings Finding 1: Pre-trained embeddings, such as GloVe, T5, and mT5, with variance many orders of magnitude higher than the variance of Xavier initialized embeddings tend to fare worse. Meanwhile, BERT and mBERT embeddings with a more similar range to Xavier performs on par or better than Xavier. Although the exact range for the optimal performance is not known, the Xavier initialization range have been widely used and tested to perform well. Therefore, pre-trained embeddings that do not fall within the Xavier specifications tend to be less effective. We know the embedding variance is important as experiments show that standardizing GloVe, T5, and mT5 embeddings to match the variance of the Xavier initialization scheme lead to substantial improvements. However, standardizing BERT and mBERT embeddings usually either made a neutral or negative effect to the performance.Finding 2: Interactions of pre-trained embeddings with the position embedding have a two-way effect. (a) Adding pre-trained embeddings to position embeddings can reduce the impact of position embeddings, since the range of pre-trained embeddings is much larger.(b) Adding position embeddings greatly alters the word-word relations encoded by the Xavier standardized pre-trained vectors. Despite this, the residual structure returns small but consistent benefits. Finding 3: Pre-trained embeddings do carry merit to the model performance in terms of semantic information. This is apparent in experiments where shuffling the elements of a pre-trained embedding layer degrades the transformer performance consistently even though the distribution remains untouched. In addition, standardizing certain pre-trained embeddings to match Xavier distribution sometimes yield small but consistent gains over Xavier initialized embeddings alone.\n\n[4] Not all parameters are born equal: Attention is mostly what you need have shown that models can retain competitive performance even when the FFN layer, which accounts for up to 48% of the parameters, is just a randomly initialised Glorot distribution. This opens the door for models where part of the parameters are generated on the fly during decoding, as opposed to stored statically on disk, making it particularly suitable to embedded devices with limited memory. A The value of training We measured the benefit of training individual components for a short time before fixing them, as opposed to having them untrainable from the beginning. With experiment we show whether training of certain components is more valuable at the beginning of the model training for the final model performance, or they continue to improve and contribute to the final model performance throughout training. We present our results on transformerbig model from Section 3 in Table 10 Contrary to our expectations, pretraining the model for a bit before freezing it yields worse overall convergence than having randomly initialised parameters without any pretraining. Pretraining and then freezing either the FFN layer or the attention layer yields similar behaviour. When pretraining is performed for 2 or 4 epochs, we noticed a that the model starts off a bit better than the randomly initialised one, but it quickly catches up in a few epochs. Furthermore convergence time is drastically increased, with small incremental improvements over many more epochs compared to the baseline. We note that when the component is pretrained for 7 epochs, the final model performance (and convergence time) starts to\n\n[5] Weight subcloning: direct initialization of transformers using larger pretrained ones Training large transformer models from scratch for a target task requires lots of data and is computationally demanding. The usual practice of transfer learning overcomes this challenge by initializing the model with weights of a pretrained model of the same size and specification to increase the convergence and training speed. However, what if no pretrained model of the required size is available? In this paper, we introduce a simple yet effective technique to transfer the knowledge of a pretrained model to smaller variants. Our approach called weight subcloning expedites the training of scaled-down transformers by initializing their weights from larger pretrained models. Weight subcloning involves an operation on the pretrained model to obtain the equivalent initialized scaled-down model. It consists of two key steps: first, we introduce neuron importance ranking to decrease the embedding dimension per layer in the pretrained model. Then, we remove blocks from the transformer model to match the number of layers in the scaled-down network. The result is a network ready to undergo training, which gains significant improvements in training speed compared to random initialization. For instance, we achieve 4x faster training for vision transformers in image classification and language models designed for next token prediction.\n\n[6] Investigation of the BERT model on nucleotide sequences with non-standard pre-training and evaluation of different k-mer embeddings. In recent years, pre-training with the transformer architecture has gained significant attention. While this approach has led to notable performance improvements across a variety of downstream tasks, the underlying mechanisms by which pre-training models influence these tasks, particularly in the context of biological data, are not yet fully elucidated. In this study, focusing on the pre-training on nucleotide sequences, we decompose a pre-training model of Bidirectional Encoder Representations from Transformers (BERT) into its embedding and encoding modules to analyze what a pre-trained model learns from nucleotide sequences. Through a comparative study of non-standard pre-training at both the data and model levels, we find that a typical BERT model learns to capture overlapping-consistent k-mer embeddings for its token representation within its embedding module. Interestingly, using the k-mer embeddings pre-trained on random data can yield similar performance in downstream tasks, when compared with those using the k-mer embeddings pre-trained on real biological sequences. We further compare the learned k-mer embeddings with other established k-mer representations in downstream tasks of sequence-based functional prediction. Our experimental results demonstrate that the dense representation of k-mers learned from pre-training can be used as a viable alternative to one-hot encoding for representing nucleotide sequences. Furthermore, integrating the pre-trained k-mer embeddings with simpler models can achieve competitive performance in two typical downstream tasks. The source code and associated data can be accessed at https://github.com/yaozhong/bert_investigation.\n\n[7] Weight subcloning: direct initialization of transformers using larger pretrained ones In Figure 9, we compare convergence curves between random initialization and weight subcloning for the ImageNet task. Additionally, for language modeling, we present the loss and perplexity in Figure 10. Our experiments demonstrate the significant advantages of weight subcloning in terms of training convergence. Models initialized with weight subcloning exhibit faster convergence, achieving higher accuracy within a limited number of training epochs.\n\n[8] BEiT: BERT Pre-Training of Image Transformers We find that proper initialization is important to stabilize Transformer, especially for large-scale pre-training.We first randomly initialize all the parameters within a small range, such as [−0.02,0.020.020.02[-0.02,0.02.Then, for the l𝑙l-th Transformer layer, we rescale the output matrices (i.e., the last linear projection within each sub-layer) of the self-attention module and the feed-forward network by 12​l12𝑙\\frac{1}{\\sqrt{2l}}.\n\n"}
{"case_id": "ba4b2561ecf64f1c1cc1e16e207312be", "answer_text": "\n\nMultiplexing is a critical technique for enhancing data transmission efficiency in communication networks by allowing multiple signals to share the same transmission medium, thereby increasing the overall capacity and efficiency of the system [1]. There are several multiplexing methods, each with its strengths and specific applications, which collectively contribute to significant improvements in data transmission [2]. Nevertheless, each method also has its own set of challenges and limitations [1]. \n\n\n### 1. Wavelength-Division Multiplexing (WDM) [4]\nWDM is extensively used in optical fiber communication, particularly to increase the bandwidth without the need for additional fibers. WDM divides the optical spectrum into channels, each carrying data independently. By using multiple wavelengths (colors of light) on the same fiber, WDM effectively multiplies the capacity of a single optical fiber [4][8]. This method is particularly advantageous in existing fiber infrastructure, as it allows for a substantial increase in data transmission rates without the need for significant physical upgrades. For instance, telecom companies exploit WDM to maximize the data throughput of undersea cables linking continents.\n- *Challenges*: WDM systems can be complex and expensive to deploy due to the need for precise wavelength control and management [4]. Additionally, they require sophisticated equipment to combine and separate wavelengths effectively [7].\n- *Comparison*: WDM is often preferred in scenarios where existing optical fiber infrastructure is in place, and there is a need to maximize bandwidth without laying new fiber, as seen in long-distance and undersea telecommunications [4]. \n\nSpace-Division Multiplexing (SDM) is considered a key communication technique to effectively combine multiple signals into one signal and transmit over a shared medium [1]. For instance, the space-division multiplexing accomplished through the multiple-input multiple-output (MIMO) scheme achieves significant capacity increase by realizing parallel channels, but it requires expensive hardware resources [1]. Combining SDM with other multiplexing schemes, such as WDM, can achieve up to 33 times higher link spatial efficiency and up to 300 times greater capacity compared to a WDM solution [3].\nSDM increases capacity by utilizing multiple spatial channels within the same medium. In the context of optical fibers, this can be achieved with multi-core fibers or multi-mode fibers where each core or mode acts as a separate channel [3]. SDM combined with other techniques such as WDM can achieve remarkably high spatial efficiency and capacity [3]. This is particularly useful in data centers where high traffic demands necessitate efficient fiber optic solutions.\n *Challenges*: Implementing SDM requires specialized fibers and equipment capable of handling multiple spatial channels [3]. The technology is still evolving, making it less accessible and more expensive than traditional single-mode fiber solutions [9]. \nComparison: SDM is more suited for high-capacity data centers and environments where new fiber installations are possible and economically feasible [3]. \n\nThe active introduction of Orthogonal Frequency-Division Multiplexing (OFDM) technology is now beginning to be widely used in wireless transmission systems, television, radio communication, and radio broadcasting. The efficiency of using a dedicated frequency band at a constant high transmission rate allows you to combat interference arising from the transmission of a useful information on the radio channel [2]. A method of improving quality of information transmission in communication systems with orthogonal frequency multiplexing is described. It is shown that with an increase in the number of receiving and transmitting antennas, the noise immunity increases significantly [2]. \nOFDM is a method particularly suited for wireless communication systems. It splits a high-rate data stream into multiple lower-rate streams that are transmitted simultaneously on different frequencies. This method enhances the efficiency of frequency usage and provides robustness against interference and signal fading [2][13]. The use of OFDM in conjunction with Multiple-Input Multiple-Output (MIMO) technology can further enhance data rates and reliability. For example, 4G and 5G cellular networks employ OFDM to deliver high-speed mobile internet access, improving user experience in dense urban environments.\nChallenges: OFDM systems can be susceptible to synchronization issues and peak-to-average power ratio (PAPR) problems, which require complex signal processing techniques to mitigate [2].\n *Comparison*: OFDM is ideal for environments with high mobility and multipath interference, such as mobile and wireless networks, where signal robustness is crucial [2]. \n\n4. Meta-Multiplexing[^1].\n\n[^1]: Reference [1] Title: Capacity Enhancement with Meta-Multiplexing. \nMeta-multiplexing is a novel methodology that leverages the overlapping of ordinary modulated signals to form artificial parallel channels. Unlike traditional multiplexing techniques that require substantial additional resources, meta-multiplexing enables these parallel channels while utilizing similar resources as traditional modulation schemes. By constructing a set of \"artificial\" parallel channels, meta-multiplexing breaks the conventional capacity limits of single channels under Additive White Gaussian Noise (AWGN) conditions. This method can achieve extraordinary spectral efficiencies and significantly enhance data transmission rates [1]. Meta-multiplexing is particularly impactful in scenarios demanding high data density over limited spectral bands, such as satellite communications and other high-frequency applications.\nChallenges: Meta-multiplexing is a relatively new concept and may face practical implementation challenges, such as the need for precise signal overlap management and potential interference issues [1].\n\n- *Comparison*: Meta-multiplexing might be preferred in cutting-edge applications like satellite communications where maximizing spectral efficiency with minimal additional resources is critical [1].\n\n\n\n5. Temporal and Protocol Multiplexing [1][4][6]. \nIn data networks, multiplexing can also occur in the temporal domain (time-division multiplexing) and at the protocol level. Time-Division Multiplexing (TDM) shares the same medium by assigning different time slots to different signals. Protocol multiplexing, on the other hand, involves sharing messages among several simultaneous communicating processes, thereby reducing the total number of messages transmitted [6][10]. Financial trading systems use TDM to ensure high-speed communications within milliseconds, enabling rapid buy and sell orders.\n- *Challenges*: TDM systems can suffer from latency issues if the time slots are not managed efficiently. Protocol multiplexing might complicate network protocols and require more sophisticated error handling mechanisms [6].\n*Comparison*: TDM is best suited for environments requiring precise timing and minimal latency, such as in financial trading or synchronous data transmission [10], whereas protocol multiplexing is useful for optimizing message handling and reducing redundancy in communication systems [6].\n\nQuantum multiplexing refers to the process of encoding quantum information on multiple modes to enhance transmission efficiency and capacity. It involves distributing quantum information across different channels to achieve higher rates of data transfer, improved loss resilience, and efficient use of energy [0].\nIn the emerging field of quantum communication, multiplexing can also drastically enhance transmission rates and loss resilience by encoding information on multiple optical modes. Research indicates that multiplexing continuous-variable quantum teleportation can exponentially increase the achievable quantum communication rates compared to single-mode systems. This is achieved by encoding quantum information across multiple modes of light, thereby improving energy efficiency and loss resilience [0]. This approach could lead to highly secure and efficient quantum networks, which are essential for future quantum internet infrastructures.\n\n- *Challenges*: Quantum multiplexing is currently in its nascent stages and faces significant technical challenges, including maintaining entanglement over long distances [0], and the need for highly sensitive detection equipment. Further research and technological advancements are necessary for practical implementation and widespread adoption.\n\nQuantum multiplexing is specifically advantageous for future quantum communication systems where security and loss resilience are imperative and where classical methods fall short [0].\n\nEach of these multiplexing techniques offers unique advantages and can be optimized for specific types of communications, whether it be for long-haul fiber optic networks [4], wireless networks [2], or emerging quantum communication systems [0]. By leveraging these multiplexing strategies, communication networks can significantly enhance their data transmission efficiency, capacity, and robustness against various forms of interference and signal degradation [1]. \n\n\nReferences:\n[0] Exponentially enhanced quantum communication rate by multiplexing continuous-variable teleportation Exponentially enhanced quantum communication rate by multiplexing continuous-variable teleportation A major challenge of today's quantum communication systems lies in the transmission of quantum information with high rates over long distances in the presence of unavoidable losses. Thereby the achievable quantum communication rate is fundamentally limited by the amount of energy that can be transmitted per use of the channel. It is hence vital to develop quantum communication protocols that encode quantum information as energy efficiently as possible. To this aim we investigate continuous-variable quantum teleportation as a method of distributing quantum information. We explore the possibility to encode information on multiple optical modes and derive upper and lower bounds on the achievable quantum channel capacities. This analysis enables us to benchmark single-mode versus multi-mode entanglement resources. Our research reveals that multiplexing does not only feature an enhanced energy efficiency, leading to an exponential increase in the achievable quantum communication rates in comparison to single-mode coding, but also yields an improved loss resilience. However, as reliable quantum information transfer is only achieved for entanglement values above a certain threshold, a careful optimization of the number of coding modes is needed to obtain the optimal quantum channel capacity. Introduction Quantum communication refers to the process of transferring quantum information between two parties commonly called Alice and Bob. This information transfer forms the cornerstone of many quantum information technologies, most importantly quantum cryptography,2, enabling secure communication, quantum dense coding, boosting the data rates with respect to classical transmission and quantum networking. A major challenge in all\n\n[1] Capacity Enhancement with Meta-Multiplexing Capacity Enhancement with Meta-Multiplexing Multiplexing services as a key communication technique to effectively combine multiple signals into one signal and transmit over a shared medium. Multiplexing can increase the channel capacity by requiring more resources on the transmission medium. For instance, the space-division multiplexing accomplished through the multiple-input multiple-output (MIMO) scheme achieves significant capacity increase by the realized parallel channel, but it requires expensive hardware resources. Here, we present a novel multiplexing methodology, named meta-multiplexing, which allows ordinary modulated signals overlap together to form a set of\"artificial\"parallel channels, meanwhile, it only requires similar resources as ordinary modulation schemes. We prove the capacity law for the meta-multiplexing system and disclose that under broad conditions, the capacity of a single channel increases linearly with the signal to noise ratio (SNR), which breaks the conventional logarithmic growth of the capacity over SNR. Numerous simulation studies verify the capacity law and demonstrate the high efficiency of meta-multiplexing. Through proof-of-concept hardware experiments, we tested the proposed method in communication practices and achieved a spectral efficiency of 81.7 bits/s/Hz over a single channel, which is significantly higher than the efficiency of any existing communication system. tion practices and achieved a spectral efficiency of 81.7 bits/s/Hz over a single channel, which is significantly higher than the efficiency of any existing communication system. In the celebrated 1948 paper (1), Shannon derived the seminal formula for the capacity of the additive linear white Gaussian noise (AWGN) channel: C = W log 2 (1 + SNR), where the capacity C is the tight upper bound of\n\n[2] Improved Efficiency of Data Transmission in Wireless Communication Systems with Orthogonal Frequency Multiplexing Improved Efficiency of Data Transmission in Wireless Communication Systems with Orthogonal Frequency Multiplexing The active introduction of Orthogonal Frequency Division Multiplexing (OFDM) technology is now beginning to be widely used in wireless transmission systems, television, radio communication, and radio broadcasting. The efficiency of using a dedicated frequency band at a constant high transmission rate allows you to combat interference arising from the transmission of a useful information on the radio channel. A method of improving quality of information transmission in communication systems with orthogonal frequency multiplexing is described. It is shown that with an increase in the number of receiving and transmitting antennas, the noise immunity increases significantly. An experiment was carried out to change the number of receiving-transmitting antennas and the signal-to-noise ratio, and the dependence of the appearance of an error on the number of transmitting antennas was obtained. The work investigated the software model of MIMO OFDM (modulators and demodulators). As a result of the analysis, the dependence of the appearance of the error frequency on the probability of the error was obtained. This showed that the use of OFDM and MIMO places increased demands on channel estimation.\n\n[3] Space-division multiplexing in data center networks: on multi-core fiber solutions and crosstalk-suppressed resource allocation multiplexing schemes, and three data center network topologies with two traffic scenarios. Extensive simulation results showthat combining bi-directional transmission in dense core fibers with tailored resource allocation schemes significantly increases the network capacity. Moreover, a multiplexing scheme that combines SDM and WDM can achieve up to 33 times higher link spatial efficiency and up to 300 times greater capacity compared to a WDM solution.\n\n[4] Towards Wavelength Division Multiplexing Towards Wavelength Division Multiplexing T here is a tremendous amount of optical fiber in use today for telecommunications and data communications. Data communication networks traditionally use multimode optical fiber operating in the near infrared wavelength over short spans of 1 km or less to link mainframe computers, servers, and clients within large buildings or a building campus. Terrestrial telecommunication networks use single mode optical fiber largely at 13 10 nm to link metropolitan areas. Undersea telecommunication networks use single mode optical fiber mainly at 1550 nm. Each o f these types of fiber links have practical limitations on the maximum transmission bandwidth. How can one increase transmission capacity once these expensive fiber links have been installed? One answer may be with wavelength division multiplexing (WDM). Multiple optical carriers, each operating at the original designed data rate, can increase the link bandwidth by an order of magnitude for a modest upgrade cost without replacing any of the basic fiber backbone.\n\n[5] On protocol multiplexing On protocol multiplexing The notion of protocol multiplexing usually means the ability of a protocol to accommodate several instances of higher level protocols at the same time. In this paper we propose to extend this notion to allow sharing of messages, and packets, among several simultaneous communicating processes, which will increase the communication efficiency by reducing the total number of messages transmitted through the media. This paper discusses several aspects of multiplexing and the interaction of multiplexing with other protocol issues.\n\n[6] Bidirectional wavelength-division multiplexing transmission over installed fibre using a simplified optical coherent access transceiver. High-speed broadband services require optical fibres in access networks, in which multiple subscribers are connected to service providers, to satisfy the continuously growing bandwidth demand. The primitive signaling scheme used in access networks enables the use of low-cost equipment but diminishes the bandwidth available to end-users. Thus, current technology will be unable to support future broadband demands. Coherent communication systems offer significantly improved power- and bandwidth-efficiency, but require fundamental simplifications to become economically viable for access networks. Here, we demonstrate a promising simplified coherent receiver exhibiting a robust performance against polarisation fluctuations over an installed fibre network. It enables the realisation of high-order modulation formats and offers high sensitivities, achieving a four-fold increase in the supported number of subscribers and approximately doubling the transmission distance compared to the recently standardized access technology. The proposed solution indicates that digital coherent technology can be feasible and transform the access networks, enabling ubiquitous new services and applications with uncontended, multi-gigabits/user broadband connections.\n\n[7] Multi-dimensional data transmission using inverse-designed silicon photonics and microcombs. The use of optical interconnects has burgeoned as a promising technology that can address the limits of data transfer for future high-performance silicon chips. Recent pushes to enhance optical communication have focused on developing wavelength-division multiplexing technology, and new dimensions of data transfer will be paramount to fulfill the ever-growing need for speed. Here we demonstrate an integrated multi-dimensional communication scheme that combines wavelength- and mode- multiplexing on a silicon photonic circuit. Using foundry-compatible photonic inverse design and spectrally flattened microcombs, we demonstrate a 1.12-Tb/s natively error-free data transmission throughout a silicon nanophotonic waveguide. Furthermore, we implement inverse-designed surface-normal couplers to enable multimode optical transmission between separate silicon chips throughout a multimode-matched fibre. All the inverse-designed devices comply with the process design rules for standard silicon photonic foundries. Our approach is inherently scalable to a multiplicative enhancement over the state of the art silicon photonic transmitters.\n\n[8] Dispersive multiplexing in multimode optical fiber. Recently developed ideas in the field of wireless communications suggest that the presence of scattering can be used to enhance, rather than degrade, the total information capacity of a transmission system. This concept is applied to data transmission over multimode optical fiber, and the result is an optical multiplexing that can increase the capacity of such fiber. Experimental results demonstrate the feasibility of this approach. The technique may play an important role in future high-bandwidth local area networking applications.\n\n[9] Exploiting multiple heterogeneous networks to reduce communication costs in parallel programs Exploiting multiple heterogeneous networks to reduce communication costs in parallel programs The different types of messages used by a parallel application program executing in a distributed system can each have unique characteristics so that no single communication network can produce the lowest latency for all messages. For instance, short control messages may be sent with the lowest overhead on one type of network, such as Ethernet, while bulk data transfers may be better suited to a different type of network, such as Fibre Channel or HiPPI. In this paper, we investigate how to exploit multiple heterogeneous communication networks that interconnect the same set of processing nodes by dynamically selecting the best (lowest latency) network for each message based on the message size. We also show how to aggregate these multiple parallel networks into a single virtual network to further reduce the latency and increase the available bandwidth. We test this multiplexing and aggregation on a cluster of SGI multiprocessors interconnected with both Fibre Channel and Ethernet. We find that multiplexing between Ethernet and Fibre Channel can substantially reduce communication overhead in a synthetic benchmark compared to using either network alone. Aggregating these two networks into a single virtual network can further reduce communication delays for applications with many large messages. The best choice of either multiplexing or aggregation depends on the mix of message sizes in application program and the relative overheads of the two networks.\n\n[10] Spectral Efficiency of Distributed Antenna Network Using MIMO Spatial Multiplexing Spectral Efficiency of Distributed Antenna Network Using MIMO Spatial Multiplexing Multiple-input multiple-output (MIMO) spatial multiplexing is known to increase the transmission rate without bandwidth expansion. However, in cellular networks (CNs), the transmission rate of a user close to the cell edge significantly degrades because the received signal-to-interference plus noise power ratio (SINR) degrades due to the presence of strong co-channel interference (CCI) from neighboring cells. Distributed antenna network (DAN), in which many antennas are spatially distributed over the cell, is suitable for MIMO spatial multiplexing because the received SINR improves over the entire cell. In this paper, assuming block transmission with cyclic prefix (CP) insertion, we theoretically derive an expression for the downlink spectral efficiency of DAN-MIMO spatial multiplexing in a multi-cell environment. Then, we propose the optimal and suboptimal transmit power allocation schemes for DAN-MIMO spatial multiplexing. We evaluate the spectral efficiency distribution by Monte Carlo numerical computation method to show that DAN allows single frequency reuse and achieves higher spectral efficiency compared to CN.\n\n"}
{"case_id": "3b46ec4e348c2a317d20f9922ee004b1", "answer_text": "\n\nThe tree covering technique is a powerful method used to represent trees succinctly, and it has significant applications in both ordinal trees and binary trees [5]. However, the approach and implications differ based on the type of tree being considered. \n\nApplication to Ordinal Trees[1]\n\nTree covering for ordinal trees primarily aims to support various navigational operations efficiently while requiring minimal space. The method involves partitioning the nodes into subsets called mini trees and micro trees, which are then compressed into a more compact representation. Succinct representations of ordinal trees, such as those developed by Geary et al., have been improved by further techniques to support additional operations such as level-order traversal and multiple traversal orderings [2][3]. These representations typically occupy 2n + o(n) bits of space, achieving near information-theoretic lower bounds while still supporting constant time operations [2][3].\n\nOne notable aspect of tree covering for ordinal trees is its adaptability to different coding schemes, such as the balanced parenthesis sequence (BP) and depth-first unary degree sequence (DFUDS). These coding schemes allow ordinal trees to be represented in a way that supports various tree operations efficiently [2]. The BP encoding, for instance, uses a depth-first traversal method and represents the tree with a sequence of opening and closing parentheses, providing a direct way to navigate the tree structure [5]. Additionally, these methods enable the efficient extraction of specific words of length O(log n) bits from these sequences in O(f(n)) time using minimal additional space, making them useful for various applications [3].\n\n### Application to Binary Trees [0]\n\nFor binary trees, the tree covering technique often focuses on optimizing both space and time complexity for specific operations, such as range-minimum queries (RMQ) and navigation [5]. Binary tree representations leverage transformations into ordinal trees to utilize existing succinct representations. This transformation preserves the navigational properties of the original binary tree while achieving a more compact form [0][6].\n\nOne of the critical distinctions in the binary tree application is the use of tree covering to enhance specific data structures like the Cartesian tree. By representing a Cartesian tree succinctly, the technique enables efficient preprocessing and querying for RMQ problems with linear time construction and near-optimal space complexity [6]. For example, using these techniques, a 2n + O(1)-bit encoding of the tree structure of the Cartesian tree can be created in linear time using only O(√n log n) bits of working space, significantly improving preprocessing times and memory efficiency [6]. The resulting binary tree representations support operations like inorder, preorder, and postorder traversals, adding flexibility and efficiency to their usage [1].\n\nAdditionally, while the ordinal tree representations are generally static, meaning they do not often change post-construction, the binary tree covering techniques can be adapted to support updates. This allows for dynamic operations, making the representations suitable for scenarios where the tree structure may evolve over time [5].\n\nCommonalities and Differences\n\nBoth applications of tree covering share the goal of minimizing space while maintaining efficient access and manipulation capabilities [2][3]. However, the main differences lie in: \n- **Traversal Support**: Ordinal trees benefit from multiple traversal ordering supports (e.g., preorder, postorder, level-order), whereas binary trees leverage specific orders like inorder in addition to pre/post-order traversals [2][3].\n- **Functional Focus**: Binary trees often employ tree covering to optimize range-minimum queries and dynamic updates, while ordinal trees use it to enhance navigational operations and encoding schemes [1][6].\n- **Space Efficiency**: Both methods aim for succinctness, with ordinal trees achieving 2n + o(n) bits and binary trees leveraging ordinal transformations to reach similar compactness while supporting additional operations [1][5], including significant improvements in construction space and time as shown by efficient RMQ preprocessing algorithms [6].\n\nComparison with Other Tree Representation Methods[0][1][1][3][4][5][6][7][8][11][12][13][14]\n\nWhile tree covering techniques offer many benefits [5], it is useful to compare them with other popular methods for tree representations, such as: \n\nPointer-based representations have been extensively used in representing binary trees efficiently [1]. Various methods have been developed to optimize the space usage while maintaining the ability to perform navigational operations in constant time. For example, a standard representation of a binary tree node would use three pointers-to its left and right children, and to its parent [1]. However, this pointer-based approach can be space-intensive, using Θ(n log n) bits to store an n-node tree [1]. To mitigate this, several succinct representations have been proposed, which transform binary trees into ordinal trees and then represent these ordinal trees in a space-efficient manner [0]. These techniques significantly reduce the space requirement, making them suitable for large-scale applications where minimizing space usage is critical [2]. By leveraging the relationship between binary trees and ordinal trees, it is possible to support various operations such as basic navigation, subtree size calculation, and lowest common ancestor (LCA) queries efficiently [2]. These succinct binary tree representations ensure that even with minimal space usage, the necessary tree operations can still be performed in O(1) time [2]. Additionally, some transformations have been shown to support efficient level-order traversal, offering a useful ordering previously supported only with a limited set of operations [3]. \n   - **Space Usage**: Typically use Θ(n log n) bits due to three pointers (left, right, parent) per node [1].\n - **Operations**: Direct access to parent and children nodes, which is efficient but consumes more space compared to succinct representations [1]. \n   \n**2. Balanced Parenthesis (BP) and Depth-First Unary Degree Sequence (DFUDS):** [0]\n   - **Space Usage**: Achieves a compact form of 2n + o(n) bits [5].\n - **Operations**: Supports efficient navigation operations but may require more complex encoding and decoding processes [3].\n   \n**3. Huffman Tree Representations:** [4]\n   - **Space vs. Time Trade-off**: Huffman trees, especially quaternary tree representations, can offer faster processing speeds at a slight increase in space [4]. This is particularly useful when decoding speed is crucial.\n\n**4. Cover Trees and Metric Trees:** [8]\n   - **Space Usage**: O(n) for cover trees [8].\n- **Operations**: Optimized for nearest neighbor and range search queries, with good balance between storage and query efficiency [12]. \n\n### Limitations\n\nSuccinct binary tree representations have been shown to use minimal space while supporting a range of operations efficiently in constant time [1]. Various methods have been developed to represent trees with minimal space, and some representations even support different traversal orderings [2]. Additionally, transformations between binary trees and ordinal trees allow for efficient and succinct representations [0].\n\n\n\nWhile tree covering techniques offer impressive space and time optimizations [5], they also come with limitations:\n- **Static Nature**: For many ordinal tree representations, the static nature means they do not adapt well to updates post-construction[5]. This can be a limitation in applications requiring frequent tree modifications.\n- **Complex Transformations**: Transforming binary trees into ordinal trees to leverage succinct representations can introduce complexity. Depending on the specific operations required, the overhead of these transformations might offset the benefits gained from succinctness. [0]\n- **Space-Time Trade-off**: Although succinct representations minimize space usage, they may come at the cost of increased preprocessing time or more complex algorithms to perform certain operations [6]. \n\n In summary, while tree covering techniques are employed to optimize space and operations for both ordinal and binary trees [3], the specific applications and efficiency objectives do differ, reflecting the unique characteristics and requirements of each tree type [0]. Moreover, understanding their limitations is crucial for effectively leveraging these techniques in various applications. \n\nWe present a method that transforms a binary tree to an ordinal tree (indeed we study several and similar transformations) with properties used to simulate various operations on the original binary tree [0]. We show that using known succinct ordinal tree representations from the literature, we can build succinct binary tree representations that have not yet been discovered [1]. We also introduce a relationship between two standard ordinal tree encodings (BP and DFUDS) and study how this relationship can be utilized in our binary tree representations [6]. Transformations We define four transformations τ1, τ2, τ3, and τ4 that transform a binary tree into an ordinal tree [9]. We describe the i-th transformation by stating how we generate the output ordinal tree Ti given an input binary tree Tb [9]. Let n be the number of nodes in Tb. The number of nodes in each of T1, T2, T3, and T4 is n + 1, where each node corresponds to a node in Tb except the root (think of the root as a dummy node) [0]. In the following, we show the correspondence between the nodes in Tb and the nodes in Ti by using the notation τi(u) [0]. \n\nTo ensure clarity, it is important to explain several terms and concepts that were mentioned:\n\n - **Balanced Parenthesis (BP) Sequence**: This is a method for encoding the structure of a tree using a sequence of parentheses [6]. An opening parenthesis is used when a node is first visited, and a closing parenthesis is used after all of its descendants have been visited. This encoding provides a simple way to navigate the tree structure. \n- **Depth-First Unary Degree Sequence (DFUDS)**: This is another method for encoding a tree structure. It uses a sequence that records the degree of each node in a unary format during a depth-first traversal. This encoding helps in navigating the tree and performing various operations efficiently [3].\n - **Range-Minimum Query (RMQ)**: An RMQ problem involves preprocessing an array or sequence so that queries about the minimum value in any given subrange can be answered efficiently. Cartesian trees are often used in RMQ problems to achieve this efficiently [6]. \n- **Cartesian Tree**: A Cartesian tree is a binary tree derived from a sequence of numbers. It is used to represent the sequence in a way that supports efficient range queries [6].\n\nUnderstanding these concepts is essential to fully grasp how tree covering techniques are applied differently to ordinal and binary trees [2].\n\n\nReferences:\n[0] On Succinct Representations of Binary Trees + n/(log n) O(1) bits of space, where n denotes the number of nodes in the represented ordinal tree. Succinct Binary Trees Via Ordinal Trees We describe succinct binary tree representations which are based on ordinal tree representations. In other words, we present a method that transforms a binary tree to an ordinal tree (indeed we study several and similar transformations) with properties used to simulate various operations on the original binary tree. We show that using known succinct ordinal tree representations from the literature, we can build succinct binary tree representations that have not yet been discovered. We also introduce a relationship between two standard ordinal tree encodings (BP and DFUDS) and study how this relationship can be utilized in our binary tree representations. Transformations We define four transformations τ 1 , τ 2 , τ 3 , and τ 4 that transform a binary tree into an ordinal tree. We describe the i-th transformation by stating how we generate the output ordinal tree T i given an input binary tree T b . Let n be the number of nodes in T b . The number of nodes in each of T 1 , T 2 , T 3 , and T 4 is n + 1, where each node corresponds to a node in T b except the root (think of the root as a dummy node). In the following, we show the correspondence between the nodes in T b and the nodes in T i by using the notation τ i (u)\n\n[1] On Succinct Representations of Binary Trees indexing very large text collections. In such applications, the space usage of the binary tree is an important consideration. While a standard representation of a binary tree node would use three pointers-to its left and right children, and to its parent-the space usage of this representation, which is Θ(n log n) bits to store an n-node tree, is unacceptable in large-scale applications. A simple counting argument shows that the minimum space required to represent an n-node binary tree is 2n − O(log n) bits in the worst case. A number of succinct representations of static binary trees evaluated. Finally, the results of,5 do not focus on the construction space or time, while this is considered in. Our Results. We recall that there is a well-known transformation between binary trees and ordinal trees (in fact, there are four symmetric versions of this transformation). This allows us to represent a binary tree succinctly by transforming it into an ordinal tree, and then representing the ordinal tree succinctly. We note a few interesting properties of the resulting binary tree representations: • The resulting binary tree representations support inorder numbering in addition to either postorder or preorder. • The resulting binary tree representations support a number of operations including basic navigation, subtree size and LCA in O(1) time; the latter implies in particular that they are suitable for the RMQ problem. • The resulting binary tree representations use 2n + n/(log n) O(1) bits of space, and so have low redundancy. • Since there are implementations of ordinal\n\n[2] Succinct ordinal trees based on tree covering Succinct ordinal trees based on tree covering Various methods have been used to represent a tree on <i>n</i> nodes in essentially the information-theoretic minimum space while supporting various navigational operations in constant time, but different representations usually support different operations. Our main contribution is a succinct representation of ordinal trees, based on that of Geary et al., that supports all the navigational operations supported by various succinct tree representations while requiring only 2<i>n</i> + <i>o</i>(<i>n</i>) bits. It also supports efficient level-order traversal, a useful ordering previously supported only with a very limited set of operations. Our second contribution expands on the notion of a single succinct representation supporting more than one traversal ordering, by showing that our method supports two other encoding schemes as abstract data types. In particular, it supports extracting a word (<i>O</i>(lg <i>n</i>) bits) of the balanced parenthesis sequence or depth first unary degree sequence in <i>O</i>(<i>f</i>(<i>n</i>)) time, using at most <i>n</i>/<i>f</i>(<i>n</i>)+<i>o</i>(<i>n</i>) additional bits, for any <i>f</i>(<i>n</i>) in <i>O</i>(lg <i>n</i>) and Ω(1).\n\n[3] Succinct ordinal trees based on tree covering Various methods have been used to represent a tree on <i>n</i> nodes in essentially the information-theoretic minimum space while supporting various navigational operations in constant time, but different representations usually support different operations. Our main contribution is a succinct representation of ordinal trees, based on that of Geary et al., that supports all the navigational operations supported by various succinct tree representations while requiring only 2<i>n</i> + <i>o</i>(<i>n</i>) bits. It also supports efficient level-order traversal, a useful ordering previously supported only with a very limited set of operations.\n Our second contribution expands on the notion of a single succinct representation supporting more than one traversal ordering, by showing that our method supports two other encoding schemes as abstract data types. In particular, it supports extracting a word (<i>O</i>(lg <i>n</i>) bits) of the balanced parenthesis sequence or depth first unary degree sequence in <i>O</i>(<i>f</i>(<i>n</i>)) time, using at most <i>n</i>/<i>f</i>(<i>n</i>)+<i>o</i>(<i>n</i>) additional bits, for any <i>f</i>(<i>n</i>) in <i>O</i>(lg <i>n</i>) and Ω(1).\n\n[4] Balancing decoding speed and memory usage for Huffman codes using quaternary tree is implemented in this paper. We introduced quaternary tree instead of binary tree in Huffman principle. We have shown that representation of Huffman code using quaternary tree is more beneficial than Huffman code using binary tree in terms of processing speed with an insignificant increase in required space. When speed is the main factor, then the quaternary tree based technique performs better than the binary tree based technique. Thus, the proposed technique provides a way to balance between the decoding time and memory usage.\n\n[5] Entropy Trees and Range-Minimum Queries In Optimal Average-Case Space by Farzan and Munro; He, Munro and Rao added further operations for ordinal trees and Davoodi et al. designed inorder support. Tree covering has predominantly been used for ordinal trees, and it can be extended to support updates, but our presentation is geared towards succinctly storing static binary trees. Trees, mini trees, micro trees The main idea of TC is to partition the nodes of a tree into subsets that form contiguous subtrees. These subtrees are then (conceptually) contracted into a single node, and a second such grouping step is applied. We then represent the overall (binary) tree as a (binary) tree of mini trees, each of which is a (binary) tree of micro trees, each of which is a (binary) tree of actual nodes: nodes micro tree mini tree overall tree The number of nodes in micro trees is kept small enough so that an encoding of the local topology of each micro tree fits into less than one word, say 1 2 lg n bits. The number of (original) nodes in one mini tree is polylogarithmic (usually lg 2 n), so that the overall tree (the green level) has few enough nodes to fit a non-succinct (e.g., pointer-based) representation into o(n) bits of space. The O(n/ log 2 n) mini trees can similarly be stored with pointers by restricting them to the polylogarithmic range inside a given mini tree. More generally, we can afford to store the overall tree (of mini trees, a.k.a. tier-1 macro tree) and the mini trees (of\n\n[6] On Succinct Representations of Binary Trees trees that are very fast in practice,13,12, we believe the resulting binary tree representations will perform well in practice. • One of the binary tree representations, when applied to represent the Cartesian tree, gives the same data structure as the 2d-Min-Heap of Fischer and Heun. • If one represents the ordinal tree using the BP encoding then the resulting binary tree encoding is Zaks' sequence; we believe this to be the first succinct binary tree representation based on Zaks' sequence sequence. Finally, we also show in Section 3 that using these representations, we can make some improvements to the preprocessing phase of the RMQ problem. Specifically, we show that given an array A, a 2n + O(1)-bit encoding of the tree structure of the Cartesian tree of A can be created in linear time using only O( √ n log n) bits of working space. This encoding can be augmented with additional data structures of size o(n) bits, using only o(n) bits of working space, thereby \"improving\" the result of where n + o(n) working space is used (the accounting of space is slightly different). We now discuss two encodings of ordinal trees. The first encoding is the natural balanced parenthesis (BP) encoding,16. The BP sequence of an ordinal tree is obtained by performing a depth-first traversal, and writing an opening parenthesis each time a node is visited, and a closing parenthesis immediately after all its descendants are visited. This gives a 2n-bit encoding of an n-node ordinal tree as\n\n[7] Mapping Canopy Cover in African Dry Forests from the Combined Use of Sentinel-1 and Sentinel-2 Data: Application to Tanzania for the Year 2018 used in order to predict the binary presence or absence of tree cover, while Level 1 was used to predict the different levels of canopy cover. For closed canopy cover, we distinguished between tree cover from evergreen and deciduous forests (classes 101 and 102 from Level 3). The classification was done based on the 3 × 3 pixels central plot from the 5 × 5 pixels that were interpreted for each plot. The 9 annotated pixels from the central plot were used into two different thematic mapping approaches: the first approach, called \"pixel\" in Figure 7, classified the pixels into classes according to their intrinsic spectral values, while the second approach, called \"window\" in Figure 7, used a contextual spectral approach to classify the pixels while also considering their neighboring pixels. The binary tree cover-no tree cover discrimination was trained with the \"pixel\" approach only; that is, the 9 pixels were considered as 9 independent training samples. For the classification of the canopy cover classes, both approaches were tested. The \"window\" approach, where the 9 pixels were considered as a single training sample, was specifically The binary tree cover-no tree cover discrimination was trained with the \"pixel\" approach only; that is, the 9 pixels were considered as 9 independent training samples. For the classification of the canopy cover classes, both approaches were tested. The \"window\" approach, where the 9 pixels were considered as a single training sample, was specifically used to include the class Tree Cover 1-10%, expressing the presence of one or several single trees\n\n[8] Fast Exact Max-Kernel Search indexing scheme used for MKS: • Explicit representation of the objects is not required. • It should have sub-quadratic construction time. The kd-tree and the metric tree exhibit good characteristics and are widely used in NNS. However, the kd-tree requires the explicit representation of the points ϕ(x) in H for its axis-aligned splits. For similar reasons, random-projection trees and PCA-trees cannot be used for MKS. Metric trees can be constructed without the explicit representations since they can work with only the ability to evaluate the induced metric d K 4 . In this paper, we consider the recently formulated cover tree for MKS. It provides a systematic way to build a tree without explicit object representations and with a subquadratic construction time. A key difference between cover trees and the aforementioned trees is that the kdtree and the metric tree are binary trees while the cover trees can have multiple number of children. Moreover, the time complexities of building and querying a cover trees have been analyzed extensively,25, whereas kdtrees and metric trees have been analyzed only under very limited settings. Indexing in kernel space with cover trees A cover tree stores a dataset S of size n in the form of a levelled tree. The structure has O(n) space requirement (Theorem 1 in). Each level is a \"cover\" for the level beneath it and is indexed by an integer scale i which decreases as the tree is descended. Let C i denote the set of nodes\n\n[9] On Succinct Representations of Binary Trees we show all of these mappings (−1 means that the mapping is off by 1, due to the presence of the dummy root): inorder → preorder − 1 postorder → postorder 2 inorder is only defined on binary trees Effect of Transformations on Ordinal Tree Encodings The four transformation methods define four ordinal trees T 1 , T 2 , T 3 , T 4 (from a given binary tree) which may look completely different, however they have relationships. In the following, we show a pairwise connection between T 1 and T 3 and between T 2 and T 4 . From paths to siblings and vice versa. Recall the following transformation functions on the binary tree nodes: τ 3 (right-child(u)) = first-child(v) If we combine the above functions, we can conclude the following: The above relation yields a connection between paths in T 1 and the siblings in T 3 . Consider any downward path (u 1 , u 2 , . . . , u k ) in T 1 , where u 1 is the i-th child of a node for any i > 1 and all u 2 , . . . , u k are the first child of their parents. All the nodes u 1 , u 2 , . . . , u k become siblings in T 3 and they are all the children of a node which corresponds to the previous-sibling of u 1 in T 1 . Also, if u 1 is the root of T\n\n[10] A sharp estimate for cover times on binary trees A sharp estimate for cover times on binary trees We compute the second order correction for the cover time of the binary tree of depth $n$ by (continuous-time) random walk, and show that with probability approaching 1 as $n$ increases, $\\sqrt{\\tau_{\\mathrm{cov}}}=\\sqrt{|E|}[\\sqrt{2\\log 2}\\cdot n - {\\log n}/{\\sqrt{2\\log 2}} + O((\\log\\logn)^8$, thus showing that the second order correction differs from the corresponding one for the maximum of the Gaussian free field on the tree. Introduction The cover time of a random walk on a graph, which is the time it takes the walk to visit every vertex in the graph, is a basic parameter and has been researched intensively over the last several decades (see,13,14 for background). One often studied aspect concerns precise estimates for cover times on specific graphs including 2D lattices and regular trees. For the 2D discrete torus, the asymptotics of the cover time were established by Dembo, Peres, Rosen and Zeitouni. For regular trees, the asymtotics of the cover time were evaluated by Aldous and a tightness result for the cover time after suitable normalization was demonstrated by Bramson and Zeitouni. It was conjectured in that the cover time of 2D discrete tori exhibits a similar tightness behavior. Meanwhile, the supremum of the Gaussian free field (GFF) was also heavily studied. For squares in the 2D lattice, the first order asymptotics were evaluated by Bolthausen, Deuschel and Giacomin. Interestingly, both and are based on the study of similar tree structures for the 2D lattice; in fact,\n\n[11] Plug-and-play dual-tree algorithm runtime analysis nodes of both the query tree and reference tree. Some of these algorithms are derived using the cover tree (Beygelzimer et al., 2006), a data structure with compelling theoretical qualities. When cover trees are used, Dual-tree all-nearest-neighbor search and approximate kernel density estimation have O(N ) runtime guarantees for O(N ) queries (Ram et al., 2009a); minimum spanning tree calculation scales as O(N log N ) (March et al., 2010). Other problems have similar worst-case guarantees (Curtin and Ram, 2014;March, 2013). In this work we combine the generalization of Curtin et al. (2013b) with the theoretical results of Beygelzimer et al. (2006) and others in order to develop a worst-case runtime bound for any dual-tree algorithm when the cover tree is used. Section 2 lays out the required background, notation, and introduces the cover tree and its associated theoretical properties. Readers familiar with the cover tree literature and dual-tree algorithms (especially Curtin et al., 2013b) may find that section to be review. Following that, we introduce an intuitive measure of cover tree imbalance, an important property for understanding the runtime of dual-tree algorithms, in Section 3. This measure of imbalance is then used to prove the main result of the paper in Section 4, which is a worst-case runtime bound for generalized dual-tree algorithms. We apply this result to three specific problems: nearest neighbor search (Section 5), approximate kernel density estimation (Section 6), and range search / range count (Section 7), showing linear runtime bounds for each of those algorithms. Each of these bounds is an\n\n[12] Meta Ordinal Regression Forest for Medical Image Classification With Ordinal Labels As mentioned above, all trees in ORF and CORF are assigned the same weights, which can increase the inevitable tree-wise prediction variance. To cope with this drawback, we propose multiplying the tree-wise losses with specific weights. Therefore, the gradients of 𝜽𝜽\\bm{\\theta}, backpropagated from the losses of all the trees, can be affected by the weights ωtsubscript𝜔𝑡\\omega_{t}. Therefore, our training objective function is defined as:𝜽∗=arg​min𝜽⁡Ltr​(𝜽)=1N​∑i=1N1T​∑t=1Tωti​Rti​(𝜽),superscript𝜽subscriptargmin𝜽superscript𝐿tr𝜽1𝑁superscriptsubscript𝑖1𝑁1𝑇superscriptsubscript𝑡1𝑇superscriptsubscript𝜔𝑡𝑖superscriptsubscript𝑅𝑡𝑖𝜽\\displaystyle\\bm{\\theta}^{*}=\\operatorname*{arg\\,min}_{\\bm{\\theta}}L^{\\mathrm{tr}}(\\bm{\\theta})=\\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{T}\\sum_{t=1}^{T}\\omega_{t}^{i}R_{t}^{i}(\\bm{\\theta}),(4)where N𝑁N is the number of training images, Rtisuperscriptsubscript𝑅𝑡𝑖R_{t}^{i} denotes the classification loss generated by (2), and ωtsubscript𝜔𝑡\\omega_{t} represents the specific weight for the t𝑡t-th tree that is learned by the TWW-Net, which will be subsequently introduced. Different from , equation (4) imposes the weights on the different trees w.r.t. one training sample i𝑖i, rather than on the different samples.\n\n[13] Trees and -subsets of ω1ω1 Abstract We study descriptive set theory in the space by letting trees with no uncountable branches play a similar role as countable ordinals in traditional descriptive set theory. By using such trees, we get, for example, a covering property for the class of -sets of . We call a family of trees universal for a class of trees if ⊆ and every tree in can be order-preservingly mapped into a tree in . It is well known that the class of countable trees with no infinite branches has a universal family of size ℵ1. We shall study the smallest cardinality of a universal family for the class of trees of cardinality ≤ ℵ1 with no uncountable branches. We prove that this cardinality can be 1 (under ¬CH) and any regular cardinal κ which satisfies (under CH). This bears immediately on the covering property of the -subsets of the space . We also study the possible cardinalities of definable subsets of . We show that the statement that every definable subset of has cardinality <ωn or cardinality is equiconsistent with ZFC (if n ≥ 3) and with ZFC plus an inaccessible (if n = 2). Finally, we define an analogue of the notion of a Borel set for the space and prove a Souslin-Kleene type theorem for this notion.\n\n"}
{"case_id": "aab38dd1282ff3387cc8bf9bdf13b3aa", "answer_text": "\n\nLeading approaches to automatic scientific paper review generation generally involve a combination of bibliometric analysis, machine learning (ML), and natural language processing (NLP) techniques [0]. These techniques vary in complexity and methodology, each with its own set of strengths and weaknesses. \n\n### 1. Bibliometric Analysis with Deep Learning [1]\n\n- **Approach**: This method combines bibliometric analysis to identify key papers in a research area with a BERT-based neural network to evaluate and extract review-worthy sentences from these key papers [0][2]. The BERT-based neural network is pre-trained on a large corpus of text and fine-tuned on scientific literature, enabling it to understand and extract the most pertinent sentences from key papers.\n- **Strengths**: This approach leverages a well-structured citation network to identify influential papers and uses deep learning for sentence extraction, ensuring that the reviews are both contextually relevant and focused on seminal works [1][2]. It has been shown to produce relevant one-sentence descriptions that succinctly capture the essence of the key papers [2]. Furthermore, by focusing on key sentences, it maintains a high level of precision in what it extracts.\n- **Weaknesses**: The primary limitation lies in its dependency on high-quality citation data. If the citation network is sparse or inaccurate, the identification of key papers might be flawed, leading to subpar review generation. For example, in emerging fields or interdisciplinary research areas, where citation networks are still developing, this approach might struggle [3]. Additionally, according to evaluations on datasets like PubMed, the ROUGE score indicates high relevance but sometimes lacks depth [0].\n\nHybrid Approaches Using Citation and Textual Features [3]. \n\n- **Approach**: These methods build citation graphs and use a combination of citation metrics, co-citation analysis, bibliographic coupling, and text-based similarities (like TF-IDF) to select relevant papers [3]. Specifically, the system constructs a citation graph and uses co-citation and bibliographic coupling to find papers that frequently appear together, while TF-IDF helps in identifying text-based similarities.\n- **Strengths**: Such approaches are versatile and work well across both established and emerging research areas, as they do not rely solely on established citation networks [3]. They also allow for comprehensive coverage by combining multiple features. The hybrid nature ensures that even if one metric fails to find relevant papers, others might still succeed.\n- **Weaknesses**: The integration of various metrics can be computationally intensive and may require significant preprocessing. Moreover, balancing the weight of different metrics to achieve optimal performance can be challenging [3]. Additionally, the multi-step process might introduce additional complexity in tuning and maintaining the system. Benchmarks from automatic evaluations using datasets such as PubMed have shown mixed results, where the hybrid approach provides high recall but at the cost of precision [1].\n\nMachine Learning and NLP Models [8].\n\n- **Approach**: State-of-the-art NLP models such as BERT and transformer-based architectures are employed to generate reviews. Some models use a combination of supervised learning, active learning, and clustering to classify papers and generate review summaries [5][9][12]. For instance, supervised learning is used to train the model on labeled datasets, while active learning helps in iteratively improving the model by querying human experts for the most informative samples.\n- **Strengths**: These models can handle large datasets and generate reviews that cover multiple aspects of a paper. They are particularly effective in producing summaries that encapsulate the core ideas and salient points of a paper [5][12]. Advanced models like KID-Review incorporate oracle pre-training to improve the coverage of review aspects and performance [8].\n- **Weaknesses**: NLP models often face difficulties in maintaining fluency and coherence in generated text. They also require extensive annotated datasets for training, which might not always be available [9][12]. Additionally, evaluations show that while factual correctness is generally maintained, the generated content might lack the depth and constructiveness found in human reviews. For instance, automatic reviews generated for NeurIPS papers were found to be less constructive but correctly captured core ideas [12]. Furthermore, handling interdisciplinary research or less-documented fields can be challenging for models heavily reliant on domain-specific training data [11].\n\n### 4. Template-Based Generation and Sentence Selection [14]. \n\n- **Approach**: These methods utilize predefined templates for comment generation and sentence ranking algorithms like LexRank, coupled with integer linear programming for optimal sentence selection [8][14]. LexRank ranks sentences based on their similarity to a query and their importance within a document, while integer linear programming ensures the best combination of sentences is selected to form a coherent summary.\n- **Strengths**: Template-based approaches provide a structured and systematic way to generate reviews, ensuring that all important aspects of a paper are covered systematically [8]. Sentence selection methods such as LexRank help in maintaining relevance and comparability across different reviews [14].\n- **Weaknesses**: The reliance on predefined templates can limit flexibility and adaptability, especially in rapidly evolving research fields. Also, the performance of sentence selection algorithms heavily depends on the quality of the initial ranking, which might not always capture nuanced insights effectively [14]. Evaluations through frameworks like manual expert reviews indicate that while structured summaries are consistent, they may lack the breadth of more flexible, human-generated content [6].\n\nUnsupervised Learning and Clustering [4].\n\n- **Approach**: Unsupervised methods like clustering are used to group similar papers and map them to research themes, forming the basis for generating review summaries [4][10]. These methods do not require labeled data and can identify inherent structures within the data by grouping papers with similar characteristics.\n- **Strengths**: These methods are particularly useful for initial scoping reviews and can handle large volumes of unlabeled data, making them suitable for emerging research areas with limited predefined topics [4]. They also offer flexibility and reduce the reliance on extensive annotated datasets.\n- **Weaknesses**: The clusters generated might need further refinement and manual intervention to ensure relevance and coherence, particularly when the unsupervised algorithms fail to capture the nuanced relationships between papers [4][10]. For example, after clustering similar papers, researchers might manually map these clusters to specific concepts or research themes, thus enhancing the relevance of the generated summaries [4]. The lack of labeled data might also lead to less accurate grouping, requiring human expertise to refine the clusters. Additionally, handling interdisciplinary fields may result in clusters that do not fully capture the complexity of the interconnected domains, necessitating expert intervention for accurate mapping and summarization [11].\n\nEmerging Trends and Recent Developments\n\n- **Approach**: Recent advancements include the integration of external knowledge bases and cognitive psychology principles to improve the depth and quality of automated reviews. Knowledge-guided frameworks and the use of oracle pre-training are among the notable innovations [8].\n- **Strengths**: Incorporating external knowledge allows review systems to understand the broader context of the research, leading to more comprehensive and insightful reviews. Oracle pre-training has been shown to enhance the coverage of various aspects of the paper, providing a richer and more nuanced review [8].\n\n- **Weaknesses**: These advanced approaches often require more sophisticated infrastructure and computational resources [6]. Implementing external knowledge bases and pre-training strategies can be complex and time-consuming. Additionally, such systems may still struggle with the balance between coherence and detailed analysis, especially in interdisciplinary or newly emerging research fields [12].\n\n\nIn conclusion, while each approach has distinct advantages in terms of relevance, coverage, and computational efficiency, they also come with inherent limitations that need to be addressed [2]. Combining elements from different approaches and ongoing advancements in NLP and ML techniques are likely to yield more sophisticated and reliable systems for automatic scientific paper review generation [0]. \n\n\nReferences:\n[0] Automatic generation of reviews of scientific papers In this work, we present a method of automatic review generation by combining bibliometric analysis for key papers identification and deep learning natural language processing by a BERT-based network that evaluates how review-worthy are sentences in identified key papers. The resulting tool generates a list of sentences, each one best describing the result of key papers in response to a query from the user. We evaluate our tool automatically on the PubMed dataset by ROUGE score and manually by asking experts in the field to evaluate specific queries. Both evaluations show that our method can produce relevant one-sentence descriptions of papers.\n\n[1] Automatic generation of reviews of scientific papers papers in popular research areas makes it hard to explore the scientific achievements and to discover possible future research directions. Therefore, automatic tools for scientific papers analysis are demanded by researchers and hence are being developed in various areas. In this paper, we present a novel method for automatic review generation that combines bibliometric analysis of a specific research area to identify key papers together with a BERT-based deep neural network trained to extract the most relevant sentence from these key papers. The result is a tool able to automatically generate a review based on a query from the user. We also asked experts in various biological areas to evaluate our tool applied to the PubMed database, and the results demonstrate that generated reviews are indeed relevant to posted queries. The rest of the paper is organized as follows. First, we describe relevant work in the areas of bibliometry and automatic summarization. Then we detail our method, along with the preprocessing process we applied to the PubMed database. Lastly, we present the results of automatic and expert evaluation and conclude the paper. Historically, bibliometrics has arisen from the statistical studies of bibliographies. Nowadays, it can be applied to all sorts of publications, from scientific papers and books to newspapers and patents. Statistics, such as the identification of authors with the highest number of publications and countries with the highest contribution to the research field, can be obtained from such analysis. Most of the bibliometric methods are based on paper similarity, which can\n\n[2] Automatic generation of reviews of scientific papers In this paper, we present a novel method for automatic review generation that combines bibliometric analysis of a specific research area to identify key papers together with a BERT-based deep neural network trained to extract the most relevant sentence from these key papers. The result is a tool able to automatically generate a review based on a query from the user.\n\n[3] Automatic generation of reviews of scientific papers The automatic review pipeline starts with a search query by keywords or phrases, where users can choose the most recent, the most cited, or the most relevant papers and limit the size of search results. A citation graph is built on-the-fly and shows overall publication dynamics in time and is used to detect the most popular articles, authors, and journals. We use a hybrid approach for papers similarity determination - a combination of citation, co-citation graph features, bibliographic coupling, and text-based similarity based on the TF-IDF metric. This approach yields excellent results in the case of well-established research areas where citations and co-citations graphs contain a lot of information, as well as in arising topics such as Covid-19, where citations graph is almost empty due to the recency of the papers.\n\n[4] Artificial intelligence to automate the systematic review of scientific literature At the beginning of the planning phase, it is recommended to perform a preliminary analysis of the scope and magnitude of the SLR Stansfield2013 . In the context of health research, “scoping” reviews are a way to quickly identify research themes, for which papers need to be catalogued in order to obtain a “map” of the research topic. Due to its descriptive nature, unsupervised learning is suitable because it does not need data labels, i.e. predefined research topics in this case. In particular, clustering becomes a relevant approach here, as it is able to identify groups of entities like papers sharing characteristics. Lingo3G555https://carrotsearch.com/lingo3g/ (Last accessed: February 14, 2023) is a document clustering algorithm that has been used to group similar papers based on their title and abstract Stansfield2013 . It allows papers to be associated to more than one cluster, and can also generate hierarchical clusters, thus providing a more refined topic classification. After clustering, the reviewer can map clusters to concepts. The method was evaluated using the results of previous “scoping” reviews from a health institution, comparing the topics automatically generated by clustering with those assigned by manual review.\n\n[5] Artificial intelligence to automate the systematic review of scientific literature In response to RQ2, ML is the most frequent AI area, with contributions exploring different learning paradigms: supervised and active learning for classification and, less often, unsupervised learning for clustering. Active learning has become the reference approach for paper selection Yu2019 ; Kontonatsios2017 ; Yu2018 ; Ros2017 . With this approach, the cost of labelling is explicitly modelled, not assuming endlessly availability of previously labelled training data. Another recurrent characteristic is imbalance during the paper selection task, for which authors have selected algorithms specifically designed for problems with imbalanced class distribution Frunza2010 , or have incorporated some data balancing technique Thomas2021 ; Yu2018 .\n\n[6] Can We Automate Scientific Reviewing? Can We Automate Scientific Reviewing? The rapid development of science and technology has been accompanied by an exponential growth in peer-reviewed scientific publications. At the same time, the review of each paper is a laborious process that must be carried out by subject matter experts. Thus, providing high-quality reviews of this growing number of papers is a significant challenge. In this work, we ask the question\"can we automate scientific reviewing?\", discussing the possibility of using state-of-the-art natural language processing (NLP) models to generate first-pass peer reviews for scientific papers. Arguably the most difficult part of this is defining what a\"good\"review is in the first place, so we first discuss possible evaluation measures for such reviews. We then collect a dataset of papers in the machine learning domain, annotate them with different aspects of content covered in each review, and train targeted summarization models that take in papers to generate reviews. Comprehensive experimental results show that system-generated reviews tend to touch upon more aspects of the paper than human-written reviews, but the generated text can suffer from lower constructiveness for all aspects except the explanation of the core ideas of the papers, which are largely factually correct. We finally summarize eight challenges in the pursuit of a good review generation system together with potential solutions, which, hopefully, will inspire more future research on this subject. We make all code, and the dataset publicly available: https://github.com/neulab/ReviewAdvisor, as well as a ReviewAdvisor system: http://review.nlpedia.ai/. TL;QR This paper proposes to use NLP models to generate reviews for scientific papers .\n\n[7] KID-Review: Knowledge-Guided Scientific Review Generation with Oracle Pre-training KID-Review: Knowledge-Guided Scientific Review Generation with Oracle Pre-training The surge in the number of scientific submissions has brought challenges to the work of peer review. In this paper, as a first step, we explore the possibility of designing an auto- mated system, which is not meant to replace humans, but rather provide a first-pass draft for a machine-assisted hu- man review process. Specifically, we present an end-to-end knowledge-guided review generation framework for scientific papers grounded in cognitive psychology research that a better understanding of text requires different types of knowledge. In practice, we found that this seemingly intuitive idea suffered from training difficulties. In order to solve this problem, we put forward an oracle pre-training strategy, which can not only make the K ID -R EVIEW better educated but also make the generated review cover more aspects. Experimentally, we perform a comprehensive evaluation (human and automatic) from different perspectives. Empirical results have shown the effectiveness of different types of knowledge as well as oracle pre-training . We make all code, relevant datasets available: https://github.com/yyy-Apple/KIDReview as well as the K ID - R EVIEW system: http://nlpeer.reviews. Introduction The rapid growth of research publication not only requires scientists to devote more time to the literature review (Luu et al. 2020;Jha, Abu-Jbara, and Radev 2013;Mohammad et al. 2009;Xing, Fan, and Wan 2020), but brings difficulties to peer review (Yuan, Liu, and Neubig 2021). To address this problem, a small handful of works make a preliminary exploration towards automatic scientific review generation. Wang et al. (2020) perform template-based comment generation\n\n[8] The Concept of System for Automated Scientific Literature Reviews Generation The Concept of System for Automated Scientific Literature Reviews Generation We present a concept of system which is aimed to create a literature review of scientific articles having a small sketch of statements as the input. Key elements of the system include transformer-based BERT encoder, deep LSTM decoder and a loss function which combines auto-encoder loss and forces generated summaries to be in the input text domain. We propose to use PMC open access subset for model learning. Introduction Recent advances in machine learning methods demonstrate impressive results in a wide range of areas including generation of a new content. State of the art methods techniques based on generative adversarial networks (GANs), variational auto-encoders (VAE) and autoregressive models allow to generate images, videos, voice, texts which are very close or even indistinguishable from those create by humans. The success of such algorithms is determined by the availability of large structured datasets which allow to train complex models. Among promising sources of structured data which can be used for development of generative models one can distinguish scientific papers. A number of large paper collections are available including arXiv.org and Pubmed Central. Paper texts are well structured, labeled with keywords, summarized by abstract and title and are organized in a citation network. Such data seems to be very attractive source of information to train sophisticated algorithms for text processing and generation. In this paper we present a design of the algorithm which will help to create a literature review based on a draft sketch. The idea\n\n[9] Artificial intelligence to automate the systematic review of scientific literature All the above methods work under a supervised strategy. Note that in these cases, the corpus of candidate papers could be comprised of thousands of irrelevant papers retrieved by automatic search if the search string is too generic or not sufficiently refined. Active learning has appeared as a relevant paradigm for paper selection, since it is founded on the idea that labelling is a costly process that can be only partially done by querying an external oracle during the learning process. The classification can be performed by usual techniques for supervised learning, SVM being the preferred one for paper selection. Based on this idea, Abstrackr applies SVM under an active learning approach where the oracle is the human reviewer Wallace2012 . Implemented as a web tool, Abstrackr shows the title, keywords and abstract of a paper to be labelled as relevant, irrelevant or borderline. Reviewers are asked to highlight those terms that support their decision, which will be exploited then for learning by the SVM classifier.\n\n[10] Generating Summaries for Scientific Paper Review generation models, it is promising to consider new applications in fields where they have not been leveraged in practice before. We propose that one such task is scientific review summary generation. We evaluate in this paper the feasibility of automatically generating review summaries for scientific papers. We use state-of-the-art models for text summarization, and apply them to our problem. We release a dataset of articles and reviews from NeurIPS, which we use to assess the performance of automatic summarization models for the problem of review summary generation. Dataset We build a dataset of articles and associated reviews by scraping NeurIPS's conference website, 1 and collecting all articles published in NeurIPS between 2013 and 2020, along with their reviews. To obtain the full text of the papers, we downloaded the PDFs from the website and extracted the text using Grobid. 2 Reviews were extracted directly from the HTML content of the web pages, and, where needed, heuristics were used in order to exclude the texts of the author's responses. Each article can have several reviews. Table 1 summarizes statistics about the dataset. Summarization Experiments Reviews of scientific articles are usually comprised of a short summary, followed by the comments comprising the reviewer's evaluation of the article, mentioning its strengths and its weaknesses. The initial summary of the paper is usually a short objective description of its contents, so in theory it could be inferred solely based on the article's content. Based on this premise, we formulate the problem of automatic review generation as a text summarization problem.\n\n[11] Can We Automate Scientific Reviewing? for Scientific Domain: Although previous works, as exemplified by (Beltagy et al., 2019) have pre-trained BERT on scientific domain, we observe that using these models with transformer decoders perform much worse than BART on sequence generation tasks in terms of fluency and coherence, which calls for general sequence to sequence models pre-trained on scientific domain for higher-quality review generation. (3) Structure Information: Review generation systems could get a deeper understanding of a given research paper if structural information can be provided. To this end, outputs from scientific paperbased information extraction tasks (Hou et al., 2019;Jain et al., 2020) can be utilized to guide review generation. (4) External Knowledge: Besides the paper itself, review systems can also rely on external knowledge, such as a citation graphs constructed based on more scientific papers or a knowledge graph connecting concepts across different papers (Luan et al., 2018;Lo et al., 2020). Also, recently, August et al. (2020) compile a set of writing strategies drawn from a wide range of prescriptive sources, it would be also valuable to transfer this knowledge into the auto-review system. Datasets (5) More Open, Fine-grained Review Data: In this work, we annotate fine-grained information (aspects) of each review manually. However, this information could potentially be obtained directly from the peer review system. How to access this information appropriately would be an important and valuable step in the future. (6) More Accurate and Powerful Scientific Paper Parsers: Existing parsing tools (e.g. science-parse, grobid) for scientific papers are commonly designed for certain specific paper templates, and also still\n\n[12] Automatic Generation of Review Matrices as Multi-document Summarization of Scientific Papers Automatic Generation of Review Matrices as Multi-document Summarization of Scientific Papers A synthesis matrix is a table that summarizes various aspects of multiple documents. In our work, we specifically examine a problem of automatically generating a synthesis matrix for scientific literature review. As described in this paper, we first formulate the task as multidocument summarization and question-answering tasks given a set of aspects of the review based on an investigation of system summary tables of NLP tasks. Next, we present a method to address the former type of task. Our system consists of two steps: sentence ranking and sentence selection. In the sentence ranking step, the system ranks sentences in the input papers by regarding aspects as queries. We use LexRank and also incorporate query expansion and word embedding to compensate for tersely expressed queries. In the sentence selection step, the system selects sentences that remain in the final output. Specifically emphasizing the summarization type aspects, we regard this step as an integer linear programming problem with a special type of constraint imposed to make summaries comparable. We evaluated our system using a dataset we created from the ACL Anthology. The results of manual evaluation demonstrated that our selection method using comparability improved\n\n"}
{"case_id": "983e73defc06e6794a856330905dc787", "answer_text": "\n\n**Active Learning for Fine-Tuning Large Language Models [0][1][2][7]**\n\n\nActive learning (AL) is a promising and effective strategy for fine-tuning large language models (LLMs), particularly in scenarios involving low-resource settings and specialized domains [0]. Multiple studies have demonstrated the advantages of AL in improving data efficiency and model performance while managing the costs associated with data labeling [1]. \n\nData Efficiency and Model Performance[6]\n\nOne major advantage of AL is its ability to optimize data selection, which reduces the amount of labeled data needed to achieve high performance. This efficiency is crucial in low-resource scenarios where annotated data is scarce or expensive to obtain. For instance, by leveraging Bayesian approximations to measure model uncertainty, AL can prioritize the most informative and uncertain data points for manual labeling, thereby accelerating the training process and improving model performance with fewer data points [0][6][12]. This approach has been particularly effective in fine-tuning models like BERT for natural language understanding tasks in environments with limited labeled data [1][6]. \n\n### Domain-Specific Applications[2]  \n\n\nMoreover, AL has shown its efficacy across various domains, including those with specialized vocabularies and requirements, such as the legal domain. Despite the challenges posed by domain-specific terminology and the need for expert annotation, AL strategies have proven effective in fine-tuning pre-trained language models. For instance, active learning strategies have been successfully applied to RoBERTa for efficient legal text classification, demonstrating improvements in model performance with reduced annotation costs [2].\n\nEnhancing Model Generalizability [2]\n\nIn addition to improving data efficiency, AL can enhance the performance of LLMs by focusing on diverse and representative samples. Techniques such as Mixed Aspect Sampling (MAS), which encompasses selecting instances that capture different semantic aspects of the dataset (e.g., topical, linguistic, semantic relations), ensure that the selected data addresses potential biases and improves the model's generalizability [5]. Furthermore, studies have shown that combining uncertainty and diversity sampling strategies results in better performance compared to random sampling, as these methods select data points that are both informative and representative of the dataset's diversity [4][7][10].\n\nFew-Shot Learning Scenarios [3]. \n\nInterestingly, AL's effectiveness extends beyond traditional supervised learning to few-shot learning scenarios. Few-shot learning, also known as in-context learning, enables models to perform tasks or adapt to new domains based on a small number of examples given as context. In these contexts, selecting demonstrations that are semantically similar to the test input examples can significantly improve performance. This approach has been shown to consistently outperform other methods such as uncertainty or diversity sampling in various tasks and across different model families and sizes [3][14].\n\nChallenges and Considerations[3]. \n\nHowever, AL is not without its challenges. In some cases, traditional methods like uncertainty sampling may underperform in settings such as in-context learning, highlighting the need for tailored strategies depending on the specific learning paradigm [14]. Additionally, while AL can reduce the number of labeled examples needed, it may still exhibit higher latency and lack generalizability, necessitating further research and development of hybrid strategies to minimize these drawbacks [7].\n\n## Conclusion\nRecently, active learning (AL) methods have been used to effectively fine-tune pre-trained language models for various NLP tasks such as sentiment analysis and document classification [7]. However, given the task of fine-tuning language models, understanding the impact of different aspects on AL methods such as labeling cost, sample acquisition latency, and the diversity of the datasets necessitates a deeper investigation [7]. This paper examines the performance of existing AL methods within a low-resource, interactive labeling setting [7]. We observe that existing methods often underperform in such a setting while exhibiting higher latency and a lack of generalizability [7]. To overcome these challenges, we propose a novel active learning method TYROGUE that employs a hybrid sampling strategy to minimize labeling cost and acquisition latency while providing a framework for adapting to dataset diversity via user guidance [7]. Through our experiments, we observe that compared to SOTA methods, TYROGUE reduces the labeling cost by up to 43% and the acquisition latency by as much as 11X, while achieving comparable accuracy [7]. Finally, we discuss the strengths and weaknesses of TYROGUE by exploring the impact of dataset characteristics [7]. \n\nIn summary, active learning works well when fine-tuning large language models, especially by maximizing data efficiency, reducing annotation costs, and improving model performance across various domains and learning paradigms [7]. By carefully selecting the most informative and representative samples, AL can significantly enhance the capabilities of LLMs in low-resource and specialized settings [0]. \n\n\nReferences:\n[0] Fine-tuning BERT for Low-Resource Natural Language Understanding via Active Learning parameter space, impact model training convergence with fewer data points? To answer these questions, we explore the use of recently introduced Bayesian approximations of model uncertainty (Gal and Ghahramani, 2016) for data selection that potentially leads to faster convergence during fine-tuning by only introducing new data points that maximize the knowledge gain of the model. To the best of our knowledge, the work presented in this paper is the first demonstration of combining modern transfer learning using pre-trained Transformer-based language model such as the BERT model with active learning to improve performance in low-resource scenarios. Furthermore, we explore the effect of trainable parameters reduction on model performance and training stability by analyzing the layer-wise change of model parameters to reason about the selection of layers excluded from training. The main findings of our work are summarized as follows: a) we found that the model's classification uncertainty on unseen data can be approximated by using Bayesian approximations and therefore, used to efficiently select data for manual labeling in an active learning setting; b) by analyzing layer-wise change of model parameters, we found that the active learning strategy specifically selects data points that train the first and thus more general natural language understanding layers of the BERT model rather than the later and thus more task-specific layers. Base Model In (Devlin et al., 2019) a simple classification architecture subsequent to the output of the Transformer is used to calculate the cross-entropy of the classifier for a text classification task with C classes. Specifically, first, a dropout operation (Srivastava\n\n[1] Fine-tuning BERT for Low-Resource Natural Language Understanding via Active Learning Fine-tuning BERT for Low-Resource Natural Language Understanding via Active Learning Recently, leveraging pre-trained Transformer based language models in down stream, task specific models has advanced state of the art results in natural language understanding tasks. However, only a little research has explored the suitability of this approach in low resource settings with less than 1,000 training data points. In this work, we explore fine-tuning methods of BERT - a pre-trained Transformer based language model - by utilizing pool-based active learning to speed up training while keeping the cost of labeling new data constant. Our experimental results on the GLUE data set show an advantage in model performance by maximizing the approximate knowledge gain of the model when querying from the pool of unlabeled data. Finally, we demonstrate and analyze the benefits of freezing layers of the language model during fine-tuning to reduce the number of trainable parameters, making it more suitable for low-resource settings. Introduction Pre-trained language models have received great interest in the natural language processing (NLP) community in the last recent years (Dai and Le, 2015;Radford, 2018;Howard and Ruder, 2018;Baevski et al., 2019;. These models are trained in a semi-supervised fashion to learn a general language model, for example, by predicting the next word of a sentence (Radford, 2018). Then, transfer learning (Pan and Yang, 2010;Moeed et al., 2020) can be used to leverage the learned knowledge for a down-stream task, such as text-classification (Do and Ng, 2006;Aggarwal and Zhai, 2012;Reimers et al., 2019;Sun et al., 2019b). Devlin et al. (2019) introduced the \"Bidirectional\n\n[2] An Efficient Active Learning Pipeline for Legal Text Classification has been outperformed by fine-tuning pre-trained language models for several tasks in natural language processing, including text classification (Howard and Ruder, 2018). However, fine-tuning these models still needs large labeled datasets to perform well on the downstream task (Dodge et al., 2020;Zhang et al., 2021;Mosbach et al., 2021). Collecting a large annotated dataset is a highly expensive and time-consuming process in specialized domains, where annotation can only be performed by the domain experts, such as the legal domain (Hendrycks et al., 2021). Active Learning (AL) has been proved effective for data-efficient fine-tuning of pre-trained language models in non-specialized domains like news, emotions, and movies (Ein-Dor et al., 2020;Margatina et al., 2022). In addition, Margatina et al. (2022) have shown that the unlabeled data can be used to adapt the pre-trained language model to the downstream task, thereby improving the active learning performance with no extra annotation cost. On the specialized domains, Chhatwal et al. (2017) have evaluated multiple AL strategies in the legal domain before the emergence of pre-trained language models. Nevertheless, to the best of our knowledge, the effectiveness of active learning in fine-tuning pre-trained language models in the legal domain has been poorly studied. In this work, we focus on efficient legal text classification with RoBERTa (Liu et al., 2019) by leveraging existing AL strategies. We identify two challenges in deploying AL strategies in the legal domain; First, legal texts contain a specialized vocabulary that is not common in other domains, including the ones on which pre-trained language models are trained. Second, the annotation\n\n[3] Active Learning Principles for In-Context Learning with Large Language Models The field of Natural Language Processing (NLP) has recently witnessed a remarkable paradigm shift with the emergence of in-context learning with large language models (LLMs), also referred to as few-shot learning Brown et al. (2020). Traditionally, NLP systems heavily relied on supervised learning approaches, where large amounts of labeled training data were necessary to achieve high predictive performance. However, in-context learning has changed this status-quo by enabling LLMs to learn from limited, context-specific examples and adapt to new tasks and domains with remarkable proficiency Zhao et al. (2021); Chowdhery et al. (2022); García et al. (2023); Wei et al. (2023b); Touvron et al. (2023); Bubeck et al. (2023). Unlike more traditional approaches, which require extensive retraining or fine-tuning for every new task, in-context learning empowers LLMs to generalize from a few examples that are fed to the model through prompting to learn a new task at hand, without any weight updates.\n\n[4] Active Learning Principles for In-Context Learning with Large Language Models Our key contributions are as follows:•We formulate the selection of in-context examples as a single iteration AL problem and explore the effectiveness of four standard approaches: uncertainty, diversity, similarity and random sampling.•We evaluate 151515 models, between 125125125M and 303030B parameters, from the GPT Radford et al. (2019); Brown et al. (2020); Black et al. (2022) and OPT Zhang et al. (2022a) families in 151515 classification and 999 multi-choice tasks, using different AL sampling techniques to select demonstrations for few-shot learning.•We demonstrate that while diversity and uncertainty sampling perform slightly better than random sampling, choosing in-context examples that are semantically similar to the input test examples outperforms consistently all other methods by a large margin across model families and sizes in all tasks.•We show that while uncertainty sampling is one of the strongest AL approaches in supervised learning, this does not generalize to in-context learning, where interestingly it underperforms. Our analysis, however, shows that larger models might perform better with uncertain demonstrations, hinting that uncertainty might be an emerging LLM ability.\n\n[5] Active Learning for Effectively Fine-Tuning Transfer Learning to Downstream Task Language model (LM) has become a common method of transfer learning in Natural Language Processing (NLP) tasks when working with small labeled datasets. An LM is pretrained using an easily available large unlabelled text corpus and is fine-tuned with the labelled data to apply to the target (i.e., downstream) task. As an LM is designed to capture the linguistic aspects of semantics, it can be biased to linguistic features. We argue that exposing an LM model during fine-tuning to instances that capture diverse semantic aspects (e.g., topical, linguistic, semantic relations) present in the dataset will improve its performance on the underlying task. We propose a Mixed Aspect Sampling (MAS) framework to sample instances that capture different semantic aspects of the dataset and use the ensemble classifier to improve the classification performance. Experimental results show that MAS performs better than random sampling as well as the state-of-the-art active learning models to abuse detection tasks where it is hard to collect the labelled data for building an accurate classifier.\n\n[6] Fine-tuning BERT for Low-Resource Natural Language Understanding via Active Learning Recently, leveraging pre-trained Transformer based language models in down stream, task specific models has advanced state of the art results in natural language understanding tasks. However, only a little research has explored the suitability of this approach in low resource settings with less than 1,000 training data points. In this work, we explore fine-tuning methods of BERT - a pre-trained Transformer based language model - by utilizing pool-based active learning to speed up training while keeping the cost of labeling new data constant. Our experimental results on the GLUE data set show an advantage in model performance by maximizing the approximate knowledge gain of the model when querying from the pool of unlabeled data. Finally, we demonstrate and analyze the benefits of freezing layers of the language model during fine-tuning to reduce the number of trainable parameters, making it more suitable for low-resource settings.\n\n[7] Low-resource Interactive Active Labeling for Fine-tuning Language Models Recently, active learning (AL) methods have been used to effectively fine-tune pre-trained language models for various NLP tasks such as sentiment analysis and document classification. However, given the task of fine-tuning language models, understanding the impact of different aspects on AL methods such as labeling cost, sample acquisition latency, and the diversity of the datasets necessitates a deeper investigation. This paper examines the performance of existing AL methods within a low-resource, interactive labeling setting. We observe that existing methods often underperform in such a setting while exhibiting higher latency and a lack of generalizability. To overcome these challenges, we propose a novel active learning method T YROGUE that employs a hybrid sampling strategy to minimize labeling cost and acquisition latency while providing a framework for adapting to dataset diversity via user guidance. Through our experiments, we observe that compared to SOTA methods, T Y - ROGUE reduces the labeling cost by up to 43% and the acquisition latency by as much as 11 X , while achieving comparable accuracy. Finally, we discuss the strengths and weaknesses of T YROGUE by exploring the impact of dataset characteristics.\n\n[8] Active Learning Principles for In-Context Learning with Large Language Models Still, its importance and potential value is on the rise Zhang et al. (2022c); Rauch et al. (2023), as the current language model pretraining paradigm continues to advance the state-of-the-art Tamkin et al. (2022).Given the fundamental premise that“not all data is equal” it is reasonable to expect researchers to actively seek the “most informative” data for pretraining or adapting their large language models (LLMs), as well as identifying the most valuable in-context examples for few-shot learning scenarios. Previous work has explored AL for prompt-based finetuning (Köksal et al., 2022), proposing a method based in inter-prompt uncertainty sampling withdiversity coupled with the PET architecture Schick andSchütze (2021a, b) that outperforms all AL baselines.\n\n[9] Fine-tuning BERT for Low-Resource Natural Language Understanding via Active Learning The main findings of our work are summarized as follows: a) we found that the model’s classification uncertainty on unseen data can be approximated by using Bayesian approximations and therefore, used to efficiently select data for manual labeling in an active learning setting; b) by analyzing layer-wise change of model parameters, we found that the active learning strategy specifically selects data points that train the first and thus more general natural language understanding layers of the BERT model rather than the later and thus more task-specific layers.\n\n[10] Active Learning Principles for In-Context Learning with Large Language Models In this study, we have examined the selection of demonstrations, i.e., labeled data that provide examples of solving a task, for in-context learning with LLMs. We formulated the selection process as a single iteration active learning problem and evaluated four standard approaches: uncertainty, diversity, similarity, and random sampling. Our evaluation involved 151515 models of varying size from the GPT and OPT families, encompassing 151515 classification tasks and 999 multi-choice tasks. Through extensive experimentation, we have demonstrated that selecting demonstrations that are semantically similar to the test input examples consistently outperforms all other methods by a significant margin across all model families, sizes, and tasks.This corroborates findings of several previous and concurrent studies that explore the properties of “good” in-context examples Liu et al. (2022); Shi et al. (2022). Interestingly, our findings reveal that uncertainty sampling, although effective in supervised learning, underperforms in the in-context learning paradigm. This highlights the importance of our work in exploring the principles of active learning in the context of few-shot learning.\n\n"}
{"case_id": "afff22e411e34366146c45ec4c7ff599", "answer_text": "\nRecent research on the Sherali-Adams (SA) hierarchy has unveiled significant results across various domains, including computational complexity, integer programming, and combinatorial optimization. These findings have expanded our understanding of the efficacy and limitations of the Sherali-Adams hierarchy in solving complex optimization problems, offering both theoretical insights and practical implications [1]. \n\n\n### Integrality Gaps and Limitations\n\nThe Sherali-Adams hierarchy provides a standardized way to tighten LP relaxations of Boolean integer programs by adding variables and constraints in a parameterized number of rounds [2]. Even Ω(log n/log log n) levels of the Sherali-Adams relaxation have an integrality gap of Ω(n^(1/4)) for the Densest k-subgraph problem [0]. Additionally, polynomial levels of the Lasserre relaxation exhibit an integrality gap ratio that can be as large as n^(2/53 - O(ϵ)) [0]. For the ATSP problem, significant integrality ratios have also been identified within the Sherali-Adams hierarchy [7].\n\n\nOne of the focal points of recent research has been the integrality gaps of the Sherali-Adams hierarchy. These gaps measure how close the solutions obtained by the hierarchy are to the optimal integer solutions. In the context of the Densest \\( k \\)-subgraph problem, it was shown that even \\(\\Omega\\left(\\frac{\\log n}{\\log \\log n}\\right)\\) levels of the Sherali-Adams relaxation have an integrality gap of \\(\\tilde{\\Omega}(n^{1/4})\\) [0]. This highlights the inherent limitations in approximating certain problems using the hierarchy, even when multiple levels are employed.\n\nSimilarly, for the Max-Cut problem, a significant result is that for any constant \\(\\epsilon > 0\\), there exist graphs for which the \\( n^{\\Omega(1)} \\)-round Sherali-Adams LP has a max-cut integrality gap of \\(2 - \\epsilon\\). This implies that any LP relaxation of size up to \\(2^{n^{\\Omega(1)}}\\) exhibits such a gap, underscoring the challenges in closing the gap through the SA hierarchy alone [2].\n\n Improved Algorithms and Structural Properties [1]. \n\nDespite these limitations, advancements have been made in developing more refined versions of the SA hierarchy. For instance, researchers have proposed the use of border bases, a concept from computational algebraic geometry, to generate hierarchies of linear programming relaxations that are tighter than those of Sherali and Adams while still allowing polynomial-time optimization for a fixed number of rounds [1]. This approach does not generate primal solutions but offers an innovative dual-bounds methodology. This refinement allows for better handling of certain integer programming problems by leveraging algebraic structures to tighten the relaxation.\n\nIn graph theory, work on the asymmetric Traveling Salesman Problem (ATSP) has identified digraph properties that allow constructing fractional feasible solutions for higher levels of the SA hierarchy. Specifically, it was proven that for the standard LP relaxation of ATSP, there are digraphs with specific structural properties that lead to integrality ratios of \\( \\frac{4}{3}, \\frac{6}{5}, \\frac{8}{7}, \\dots \\) for higher levels of the hierarchy [3]. This presents a more nuanced understanding of how certain graph properties can influence the performance of SA relaxations.\n\nApplications to Constraint Satisfaction Problems (CSPs) [4]. \n\nThe Sherali-Adams hierarchy has also shown promise in the realm of Constraint Satisfaction Problems (CSPs). It was demonstrated that the \\( k \\)-th level of the SA hierarchy can establish \\( k \\)-consistency for CSPs, effectively solving optimization problems and establishing consistencies in one go [5]. This is particularly relevant for valued CSPs, where the hierarchy was shown to solve all instances to optimum under specific conditions.\n\nFor example, valued constraint languages that have a tournament pair multimorphism were previously known to be tractable using a combination of consistency techniques, modular decompositions, and submodular function minimization. The SA hierarchy, however, can handle these cases more efficiently by establishing \\( k \\)-consistency directly [4]. Another practical instance is in finite-valued CSPs, where the basic linear programming (BLP) relaxation, representing the first level of the SA hierarchy, was proven to solve all tractable cases optimally. If BLP fails to solve any instance of a finite-valued constraint language, it indicates that the language is NP-hard [5]. This direct approach simplifies the process compared to traditional methods, making it easier to handle complex CSPs.\n\n### Case Study: Combinatorial Optimization\n\nA notable example of the SA hierarchy's application in combinatorial optimization is its use in the Traveling Salesman Problem (TSP). Researchers constructed instances where even the higher levels of the SA hierarchy could not close the integrality gap for the standard LP relaxation. Despite these gaps, innovative methodological approaches, such as the integration of structural properties of digraphs, allowed for finding near-optimal solutions more efficiently. For instance, the work on the ATSP mentioned previously not only identified the limitations but also leveraged digraph properties to achieve better integrality ratios [3]. This dual perspective of understanding both the limitations and potential improvements illustrates the practical implications of theoretical insights gained from studying the SA hierarchy.\n\nNovel Methodological Approaches[1].\n\nA groundbreaking approach to studying the SA hierarchy involves the use of tensors and multilinear algebra. By interpreting SA acceptance as a homomorphism problem involving a tensorized version of the original structures, researchers have developed a new framework to understand and analyze the hierarchy's effectiveness. This methodology not only applies to the Sherali-Adams hierarchy but can also be extended to other algorithmic relaxations, providing a versatile tool for theoretical explorations [14]. This tensor-based approach enables a more geometric and algebraic analysis, offering deeper insight into the structure of solutions by describing the geometry of specific tensor spaces associated with the problems. This technique has been particularly useful in showing non-solvability results for certain problems like approximate graph coloring, thereby enriching the overall theoretical landscape of the hierarchy's capabilities.\n\n### Real-World Applications\n\nReal-world applications of the Sherali-Adams hierarchy span various optimization tasks. In the context of CSPs (Constraint Satisfaction Problems), it has been shown that the hierarchy establishes \\( k \\)-consistency for CSPs, meaning that the hierarchy can be used to solve optimization problems efficiently without needing nested applications of consistency methods [4]. Additionally, specific levels of the hierarchy have demonstrated their ability to provide tighter linear programming relaxations for problems like the Densest \\( k \\)-subgraph, Max-Cut, and TSP (Traveling Salesman Problem), as well as their various asymmetric versions [0][2][3][6][9]. This versatility illustrates the hierarchy's potential in offering robust solutions across a spectrum of challenging combinatorial optimization problems [5]. \n\n\n\nRecent results have also shown that the Sherali-Adams hierarchy can be applied effectively in real-world scenarios. For example, researchers have applied SA relaxations to problems in logistics and network design, where the hierarchy was able to provide near-optimal solutions more efficiently than other methods. In particular, the use of the SA hierarchy in optimizing supply chain logistics has shown promising results in reducing transportation costs and improving delivery times. Additionally, applications in machine learning, such as training large-scale neural networks, have benefited from the hierarchy's ability to handle complex constraint systems and provide tighter bounds on solution spaces [1].\n\n### Conclusion [2]\n\nIn summary, recent research on the Sherali-Adams hierarchy has provided a deeper understanding of its integrality gaps, limitations, and applications across various domains. Important advancements include tighter relaxations using border bases [1] and structural analyses in graph theory, as well as novel tensor-based methodologies [14]. These approaches not only address some of the hierarchy’s limitations but also extend its applications to real-world scenarios, such as logistics and machine learning. The Sherali-Adams hierarchy continues to be a powerful tool in optimization, with ongoing research pushing its theoretical and practical boundaries. \n\nReferences:\n[0] Polynomial integrality gaps for strong SDP relaxations of Densest k-subgraph In this paper, we first study lift-and-project relaxations for Densest k𝑘k-subgraph  obtained from the Sherali-Adams hierarchy. We show that even Ω​(log⁡nlog⁡log⁡n)Ω𝑛𝑛\\Omega\\big{(}\\frac{\\log n}{\\log\\log n}\\big{)} levels of the Sherali-Adams relaxation have an integrality gap of Ω~​(n1/4)~Ωsuperscript𝑛14\\tilde{\\Omega}(n^{1/4}). Then, we turn to the Lasserre hierarchy for Densest k𝑘k-subgraph. We show an integrality gap of polynomial ratio (nϵsuperscript𝑛italic-ϵn^{\\epsilon}, for small enough constant ϵitalic-ϵ\\epsilon) for almost linear (n1−O​(ϵ)superscript𝑛1𝑂italic-ϵn^{1-O(\\epsilon)}) levels of the Lasserre relaxation. If we only aim at an integrality gap for polynomial (nϵsuperscript𝑛italic-ϵn^{\\epsilon}) levels of the Lasserre relaxation, the ratio of the gap can be as large as n2/53−O​(ϵ)superscript𝑛253𝑂italic-ϵn^{2/53-O(\\epsilon)}.\n\n[1] ON THE CONNECTION OF THE SHERALI-ADAMS CLOSURE AND BORDER BASES ON THE CONNECTION OF THE SHERALI-ADAMS CLOSURE AND BORDER BASES The Sherali-Adams lift-and-project hierarchy is a fundamental construct in integer programming, which provides successively tighter linear programming relaxations of the integer hull of a polytope. We initiate a new approach to understanding the Sherali-Adams procedure by relating it to methods from computational algebraic geometry. Our two main results are the equivalence of the Sherali-Adams procedure to the computation of a border basis, and a refinement of the Sherali-Adams procedure that arises from this new connection. We present a modified version of the border basis algorithm to generate a hierarchy of linear programming relaxations that are tighter than those of Sherali and Adams, and over which one can still optimize in polynomial time (for a fixed number of rounds in the hierarchy). In contrast to the well-known Gröbner bases approach to integer programming, our procedure does not create primal solutions, but constitutes a novel approach of using computer-algebraic methods to produce dual bounds.\n\n[2] Sherali-adams strikes back It is now known that the most principled and general form of this strategy is the Sherali–Adams LP hierarchy [SA90, reviewed in Section 2.4.At a high level, the Sherali–Adams LP hierarchy gives a standardized way to tighten LP relaxations of Boolean integer programs, by adding variables and constraints.The number of new variables/constraints is parameterized by a positive integer R𝑅R, called the number of “rounds”. Given a Boolean optimization problem with n𝑛n variables, the R𝑅R-round Sherali–Adams LP has variables and constraints corresponding to monomials of degree up to R𝑅R, and thus has size O​(n)R𝑂superscript𝑛𝑅O(n)^{R}.A remarkable recent line of work [CLRS16, KMR17 has shown that for any CSP (such as max-cut), the R𝑅R-round Sherali–Adams LP relaxation achieves essentially the tightest integrality ratio among all LPs of its size.Nevertheless, even this most powerful of LPs arguably struggles to certify good bounds for max-cut.In a line of work [dlVM07, STT07 concluding in a result of Charikar–Makarychev–Makarychev [CMM09, it was demonstrated that for any constant ϵ>0italic-ϵ0\\epsilon>0, there are graphs (random ΔΔ\\Delta-regular ones, Δ=O​(1)Δ𝑂1\\Delta=O(1)) for which the nΩ​(1)superscript𝑛Ω1n^{\\Omega(1)}-round Sherali–Adams LP has a max-cut integrality gap of 2−ϵ2italic-ϵ2-\\epsilon. As a consequence, every max-cut LP relaxation of size up to 2nΩ​(1)superscript2superscript𝑛Ω12^{n^{\\Omega(1)}} has such an integrality gap.\n\n[3] On integrality ratios for asymmetric TSP in the Sherali–Adams hierarchy We have the following results pertaining tothe balanced LP relaxation of ATSP:We formulate a property of digraphs that we callthe good decomposition property, andgiven any digraph with this property,we construct a vector y𝑦y on the edges such thaty𝑦y is a fractional feasible solution tothe level t𝑡t tightening of the balanced LP by the Sherali-Adams system.Charikar, Goemans, and Karloff (CGK)constructed a family of digraphs for whichthe balanced LP has an integrality ratio of 2.We show that the digraphs in the CGK familyhave the good decomposition property,hence, we obtain an integrality ratio for level t𝑡t of SA.In more detail,we prove that for any integer t≥0𝑡0t\\geq 0 and small enough ϵ>0italic-ϵ0\\epsilon>0,there is a digraph G𝐺G from the CGK familyon ν=ν​(t,ϵ)=O​((t/ϵ)t/ϵ)𝜈𝜈𝑡italic-ϵ𝑂superscript𝑡italic-ϵ𝑡italic-ϵ\\nu=\\nu(t,\\epsilon)=O((t/\\epsilon)^{t/\\epsilon})vertices such that the integrality ratio of the level-t𝑡t tightening ofBal LP is at least1+1−ϵt+1≈2,32,43,54,…11italic-ϵ𝑡12324354…1+\\frac{1-\\epsilon}{t+1}\\approx 2,\\frac{3}{2},\\frac{4}{3},\\frac{5}{4},\\dots (where t=0𝑡0t=0 identifies the original relaxation).\n\n[4] Sherali-Adams Relaxations for Valued CSPs It isfolklore that the k𝑘kth level of Sherali-Adams hierarchy establishesk𝑘k-consistency for CSPs. We demonstrate that one linearprogramming relaxation is powerful enough to establish consistency as well assolving an optimisation problem in one go without the need of nestedapplications of consistency methods.For example, valued constraint languages having a tournament pair multimorphismwere previously known to be tractable using ingenious application of variousconsistency techniques, advanced analysis of constraint networks using modulardecompositions, and submodular functionminimisation . Here, we show that an even lessrestrictive condition (having a binary conservative commutative operation insome fractional polymorphism) ensures that the Sherali-Adams relaxation solvesall instances to optimum.\n\n[5] The Power of Sherali-Adams Relaxations for General-Valued CSPs The first level of the Sherali-Adams hierarchy is known as the basiclinear programming (BLP) relaxation .In , the authors gave a precise algebraiccharacterisation of ΓΓ\\Gamma for which any instance of VCSP⁡(Γ)VCSPΓ\\operatorname{VCSP}(\\Gamma) is solvedto optimality by BLP, see also .The characterisation proved important not only in the study ofVCSPs  and other classes of problems , butalso in the design of fixed-parameter algorithms .In , it was then shown that for finite-valued CSPs, the BLPsolves all tractable cases; i.e. if BLP fails to solve any instance ofsome finite-valued constraint language then this language is NP-hard.The BLP has been considered in the context of CSPs for robustapproximability , 27 and constant-factorapproximation , 26.Higher levels of Sherali-Adams hierarchy have been considered for(in)approximability of CSPs , 14, 70.Semidefinite programming relaxations have also been considered in the context ofCSPs for approximability  and robustapproximability .Concrete lower bounds on Sherali-Adams and other relaxationsinclude , 15, 32, 1.Whilst the complexity of valued constraint languages is open, it has been shownthat a dichotomy for constraint languages, conjecturedin , implies a dichotomy for valued constraintlanguages . Our results give a complete complexityclassification for a large class of VCSPs without any dependence on thedichotomy conjecture . Since the announcement of ourresults , the tractability results obtained in this paper wereshown using different methods (preprocessing combined with an LPrelaxation) .\n\n[6] Sherali-adams relaxations of the matching polytope Sherali-adams relaxations of the matching polytope We study the Sherali-Adams lift-and-project hierarchy of linear programming relaxations of the matching polytope. Our main result is an asymptotically tight expression 1+1/k for the integrality gap after k rounds of this hierarchy. The result is derived by a detailed analysis of the LP after k rounds applied to the complete graph K_{2d+1}. We give an explicit recurrence for the value of this LP, and hence show that its gap exhibits a \"phase transition,\" dropping from close to its maximum value 1+1/2d to close to 1 around the threshold k=2d-Θ(√d). We also show that the rank of the matching polytope (i.e., the number of Sherali-Adams rounds until the integer polytope is reached) is exactly 2d-1.\n\n[7] On integrality ratios for asymmetric TSP in the Sherali–Adams hierarchy Our main result pertains to the standard LP relaxation of ATSP.Our key contribution is to identifya structural property of digraphs thatallows us to construct fractional feasible solutionsfor the level t𝑡t tightening of the standard LP by the Sherali-Adams system.This construction is much more difficult than theconstruction for the balanced LP.We present a simple family of digraphs thatsatisfy the structural property, and this immediatelygives our results on integrality ratios.We prove that for any integer t≥0𝑡0t\\geq 0 and small enough ϵ>0italic-ϵ0\\epsilon>0,there are digraphs G𝐺G on ν=ν​(t,ϵ)=O​(t/ϵ)𝜈𝜈𝑡italic-ϵ𝑂𝑡italic-ϵ\\nu=\\nu(t,\\epsilon)=O(t/\\epsilon) verticessuch that the integrality ratio ofthe level t𝑡t tightening of the standard LP on G𝐺Gis at least 1+1−ϵ2​t+3≈43,65,87,109,…11italic-ϵ2𝑡3436587109…1+\\frac{1-\\epsilon}{2t+3}\\approx\\frac{4}{3},\\frac{6}{5},\\frac{8}{7},\\frac{10}{9},\\dots.The rank of a starting relaxation (or polytope)is defined to be the minimum number of tighteningsrequired to find the integral hull (in the worst case).An immediate corollary is thatthe SA-rank of the standard LP relaxation on a digraph G=(V,E)𝐺𝑉𝐸G=(V,E)is at least linear inV|𝑉|V|, whereas,the rank in terms of the number of edges is Ω​(|E|)Ω𝐸\\Omega(\\sqrt{|E|})(since the LP is on a complete digraph, namely, the metric completion).\n\n[8] On integrality ratios for asymmetric TSP in the Sherali–Adams hierarchy By Lemma 4.8 andLemma 4.9, we have for each e∈En​e​w=E∪D𝑒superscript𝐸𝑛𝑒𝑤𝐸𝐷e\\in E^{new}=E\\cup{D} andany S∈𝒫t+1𝑆subscript𝒫𝑡1S\\in\\mathcal{P}_{t+1},(yℐ,t+1−e∗yℐ,t+1)S={0if ​e∈E​(Cj)​ where ​j∈ℐ∪ℱ¯(e is a solid, integral edge)ySℐ,t+1if ​e∈D​(Cj)​ where ​j∈ℐ∪ℱ¯(e is a dashed, integral edge)1t+3​1Stour​(e),t+1if ​e∈E​(Cj)​ where ​j∈ℱ−ℐ(e is a solid, fractional edge)t+2t+3​ySℐ+index​(e),tif ​e∈D​(Cj)​ where ​j∈ℱ−ℐ(e is a dashed, fractional edge)subscriptsuperscript𝑦ℐ𝑡1𝑒superscript𝑦ℐ𝑡1𝑆cases0if 𝑒𝐸subscript𝐶𝑗 where 𝑗ℐ¯ℱotherwise(e is a solid, integral edge)subscriptsuperscript𝑦ℐ𝑡1𝑆if 𝑒𝐷subscript𝐶𝑗 where 𝑗ℐ¯ℱotherwise(e is a dashed, integral edge)1𝑡3subscriptsuperscript1tour𝑒𝑡1𝑆if 𝑒𝐸subscript𝐶𝑗 where 𝑗ℱℐotherwise(e is a solid, fractional edge)𝑡2𝑡3subscriptsuperscript𝑦ℐindex𝑒𝑡𝑆if 𝑒𝐷subscript𝐶𝑗 where 𝑗ℱℐotherwise(e is a dashed, fractional edge)\\displaystyle(y^{\\mathcal{I},\\,t+1}-e*y^{\\mathcal{I},\\,t+1})_{S}~{}=~{}\\begin{cases}0&\\textup{if~{}}e\\in E(C_{j})\\textup{~{}where~{}}j\\in\\mathcal{I}\\cup\\overline{\\mathcal{F}}\\\\&\\qquad\\textup{{($e$ is a solid, integral edge)}}\\\\y^{\\mathcal{I},\\,t+1}_{S}&\\textup{if~{}}e\\in D(C_{j})\\textup{~{}where~{}}j\\in\\mathcal{I}\\cup\\overline{\\mathcal{F}}\\\\&\\qquad\\textup{{($e$ is a dashed, integral edge)}}\\\\\\frac{1}{t+3}\\textbf{1}^{\\textit{tour\\/}(e),\\,t+1}_{S}&\\textup{if~{}}e\\in E(C_{j})\\textup{~{}where~{}}j\\in\\mathcal{F}-\\mathcal{I}\\\\&\\qquad\\textup{{($e$ is a solid, fractional edge)}}\\\\\\frac{t+2}{t+3}y^{\\mathcal{I}+\\textit{index}(e),\\,t}_{S}&\\textup{if~{}}e\\in D(C_{j})\\textup{~{}where~{}}j\\in\\mathcal{F}-\\mathcal{I}\\\\&\\qquad\\textup{{($e$ is a dashed, fractional edge)}}\\\\\\end{cases}(16)Hence, in every case, yℐ,t+1−e∗yℐ,t+1∈𝒮​𝒜t​(ATSPDFJ​(Gn​e​w))superscript𝑦ℐ𝑡1𝑒superscript𝑦ℐ𝑡1𝒮superscript𝒜𝑡ATSPDFJsuperscript𝐺𝑛𝑒𝑤y^{\\mathcal{I},\\,t+1}-e*y^{\\mathcal{I},\\,t+1}\\in\\mathcal{SA}^{t}({\\textsf{ATSP${}_{\\mathit{DFJ}}$}}(G^{new}))∎\n\n[9] The Sherali-Adams Hierarchy for Promise CSPs through Tensors and give three main contributions. (1) Tensorisation We propose a new approach to the study of the Sherali-Adams hierarchy inspired by multilinear algebra. We interpret Sherali-Adams acceptance as a homomorphism problem involving a tensorised version of the original structures. Essentially, the problem of distinguishing the cases when the k-th level of the hierarchy accepts or rejects a given instance is cast as the problem of checking the existence of a homomorphism between the k-th tensor power of the instance and a space of tensors built from the k-th tensor power of the constraint language and the specific minion Q conv capturing the power of BLP. Equivalently, the k-th level of Sherali-Adams is interpreted as BLP applied to the k-th tensor powers of the instance and the constraint language. This allows us to describe the functioning of the algorithms in the hierarchy by describing the geometry of a space of tensors -which can be accomplished by using multilinear algebra. As far as we know, this approach has not appeared in the literature on Sherali-Adams (and related algorithmic techniques such as local consistency and the high-dimensional Weisfeiler-Leman algorithm,36). The scope of this idea is not limited to the Sherali-Adams hierarchy and to the PCSP framework. By replacing the minion Q conv with different minions, one can use the tensorisation construction to analogously characterise acceptance for other hierarchies of relaxation algorithms. For example, one can cast acceptance for the k-th level of local consistency (which is less powerful than Sherali-Adams) as the same homomorphism problem as above,\n\n"}
{"case_id": "fc927b39177cd2aad8a8cbcef75ab62c", "answer_text": "\n\nThe advent of generative AI (GAI) is transforming how computer science is taught to college students, with educators and researchers exploring various strategies to effectively integrate these cutting-edge tools into the curriculum [4]. The leading thoughts on this subject encompass adapting curricula to focus on higher-level concepts [3], developing critical thinking skills [1], and addressing ethical considerations [13], while keeping student engagement and learning outcomes at the forefront. \n\n**1. Shifting Focus to Higher-Level Concepts:** GAI tools shift the focus to higher levels of abstraction. Students expressed (6/12) that GAI tools could help reduce the amount of time spent working at lower levels of abstraction. Students often gave examples of how GAI tools could shift the learner’s focus toward higher-level skills (e.g., design, the analysis and evaluation of code, or advanced software engineering concepts) which students are more likely to encounter in later computing courses [3].  \nOne prominent perspective is that GAI tools should be used to shift the educational focus from lower-level implementation details to higher-level concepts and design patterns. This approach allows students to engage with complex topics such as software design and advanced software engineering concepts earlier in their academic careers. For instance, a study found that students using ChatGPT could focus more on understanding design patterns and the overall structure of software, rather than getting bogged down by routine coding tasks [3]. This method not only helps students understand the underlying principles of software architecture but also prepares them for real-world applications, where higher-level problem-solving skills are crucial.\n\n **2. Enhancing Critical Thinking and Analytical Skills:** Generative AI tools can facilitate critical thinking. Instructors were excited about the potential to use these models in their courses to help students further develop their critical thinking and analysis skills. One instructor commented on the benefit of using ChatGPT to produce a first draft, after which students would decide or criticize the validity of the report, enhancing their analytical skills [1]. Additionally, another instructor mentioned the shift towards analysis and troubleshooting over mere production, emphasizing the importance of higher-level cognitive skills in understanding and building systems [1]. \nEducators are also leveraging GAI to foster critical thinking and analytical skills. By using generative AI to produce initial drafts or code snippets, students are tasked with the responsibility of assessing, critiquing, and improving upon the AI-generated content. This process deepens their understanding and encourages them to think critically about the correctness and efficiency of code [1]. For example, in a recent implementation, students were asked to evaluate AI-generated reports and identify any errors or areas for improvement, thus promoting an analytical mindset that is essential for their professional development. Effectiveness in enhancing critical thinking is measured through various assessment methods, such as requiring students to articulate their reasoning for identifying errors and solutions, as well as through performance in exams and project work that demand analytical rigor.\n\n**3. Incorporating Ethical and Societal Implications:** AI applications can impact society in both positive and negative ways. Students should be able to identify ways that AI is contributing to their lives as well as that the ethical construction of AI systems requires attention to the issues of transparency and fairness [0]. As AI becomes more widely utilized, there is a need for non-computer scientists to understand how the technology works, and how it can impact their lives [2]. Students must be equipped with the skills to build and work with AI to meet the needs of a shifting workforce. This means not only teaching students about what AI is, but also enabling them to create with it, think critically about its impacts, and advocate for responsible use [9]. The centrality of ethics is critical as AI systems have historically been biased against marginalized groups such as women, people of African, Asian, and Latin descent, Indigenous peoples, and low-income individuals and communities [13]. \n\nAnother significant consideration is the integration of ethical and societal implications of AI within the computer science curriculum. Given the pervasive impact of AI technologies, it is essential for students to grasp both the potential benefits and harms associated with their use. Recent curricula have included modules specifically focused on AI ethics, addressing issues such as privacy, fairness, and the responsible design of AI systems [9][14]. One notable approach is embedding ethics throughout the curriculum rather than isolating it in a single module or course. This approach helps students think more holistically about the implications of the technologies they are building. Furthermore, some courses have adopted project-based learning that includes ethical implications as a central element of students' work, requiring them to consider how their solutions impact various stakeholders and societal norms [13]. By embedding these topics into the curriculum, students are made aware of the broader implications of their work and are better prepared to handle the ethical challenges they may encounter in their careers.\n\nUpdating Assessment Methods: Instructors need to update assessment methods. In terms of how to update curriculum to adapt courses to account for the existence of these models, instructors did not have a very clear idea of what exactly needed to be done, with many expressing uncertainty about the best way to assess student progress, prevent plagiarism, and ensure that students do not rely too heavily on the models, hindering their education. In their interviews, instructors did mention a few considerations they had for updating their assessment methods: Mitigate plagiarism by developing engaging assignments and reducing busy work, change course grade weights such that easily plagiarized assignments can’t carry students’ grades, and continue to give proctored exams since they can’t be plagiarized like other out-of-class assignments [1]. \nThe rapid adoption of GAI tools necessitates a rethinking of traditional assessment methods. Educators are exploring ways to update curricula to mitigate plagiarism and ensure genuine learning. Specific strategies include developing engaging assignments that require critical analysis, adjusting grade weights to minimize the impact of easily plagiarizable tasks, and continuing to use proctored exams [1]. In one instance, educators found success by creating assignments that demanded higher-order thinking and deep engagement with the material, which significantly reduced opportunities for plagiarism and enhanced learning outcomes. Additionally, methods such as personalized project work, peer reviews, and open-ended questions in assessments provide opportunities for students to demonstrate their understanding beyond rote memorization and surface-level engagement.\n\n**5. Embracing Generative AI in the Classroom:** Generative AI tools can facilitate critical thinking. A couple of instructors were excited about the potential to use these models in their courses to help students further develop their critical thinking and analysis skills. Participant I6 said, “Let them actually use ChatGPT to come back with a report as a first draft. But then the job of the student is not simply to return the report, but instead, look at the report ChatGPT wrote, and then decide or criticize whether it’s right or wrong, and then, if it’s wrong, then what is wrong with it, or try to improve…So in that sense, it is more useful for the student to develop their critical thinking…instead of just like the old way of just writing a report.” Additionally, participant I5 said, “Just the production of something is not going to be as important, perhaps leaning more into analysis, perhaps leaning more into your ability to troubleshoot. Versus creation which is one of which is one of the steps that you know of many steps in any sort of understanding or building a system or an algorithm or a data structure or anything else. …It’s not, give me code, it’s, here is some code, give me some sort of analysis that matches or fits into something that is not easily template-tized, something that is not easily described or describable.” Instructors need to update assessment methods. In terms of how to update curriculum to adapt courses to account for the existence of these models, instructors did not have a very clear idea of what exactly needed to be done, with many expressing uncertainty about the best way to assess student progress, prevent plagiarism, and ensure that students do not rely too heavily on the models, hindering their education. In their interviews, instructors did mention a few considerations they had for updating their assessment methods (1). GAI tools shift the focus to higher levels of abstraction. Students stated that GAI tools could help reduce the amount of time spent working at lower levels of abstraction and shift the learner’s focus toward higher-level skills such as design, analysis, and evaluation of code or advanced software engineering concepts (3). GAI tools should be incorporated into classrooms. Instructors emphasized that they believe GAI tools should not be banned and should embrace their existence and include them in the classroom (5). Generative models are now capable of producing natural language text that is comparable in quality to the text produced by people, generating code, code explanations, and programming exercises (4). The results suggest that GAI tools will play an increasingly significant role in computing education, but students and instructors also raised numerous concerns about how these models should be integrated to support the needs and learning goals of students (4).\nThere is a consensus among educators that GAI tools should be embraced rather than banned. Instructors advocate for their use as educational aids, provided that students can explain and justify the AI-generated outputs. For example, an initiative allowed students to use ChatGPT for generating initial code but required them to explain why the code was correct or identify and fix errors if it was not [5]. This inclusive approach not only incorporates technological advancements into the learning process but also aligns teaching methods with contemporary practices in the tech industry.\n\n**6. Project-Based Learning and Interdisciplinary Integration: **[6][9][13][14][9][14] \nProject-based learning and interdisciplinary integration are also important strategies. Many educators emphasize the value of hands-on projects where students can apply AI techniques to solve real-world problems, thereby reinforcing their learning and fostering creativity [13]. For instance, a project involved students creating AI models to detect environmental hazards, which helped them understand both the technical and societal impacts of their work [8]. Additionally, integrating AI with other disciplines, such as creative arts or social sciences, can provide a more rounded educational experience, making the subject matter more accessible and engaging for students from diverse academic backgrounds [9].\n\nIn summary, the leading thoughts on teaching computer science in the age of generative AI revolve around leveraging these tools to shift focus to higher-level concepts [3], enhance critical thinking [1], address ethical considerations [13], update assessment methods [1], embrace GAI in educational practices [5], and promote project-based and interdisciplinary learning [13]. These strategies aim to equip students with both the technical skills and ethical awareness necessary for the evolving landscape of AI and computing.\n\n\nReferences:\n[0] Teaching Machine Learning in School: A Systematic Mapping of the State of the Art for K-12 Working Group (AI4K12), a joint initiative of the Association for the Advancement of Artificial Intelligence (AAAI) and the Computer Science Teachers Association (CSTA) aims at developing guidelines for teaching K-12 students about artificial intelligence. To frame these guidelines, \"big ideas\" in AI that every student should know are defined (Touretsky et al., 2019a): 1. Perception: Computers perceive the world using sensors. Students should understand that machine perception of spoken language or visual imagery requires extensive domain knowledge. 2. Representation and Reasoning: Agents maintain models/representations of the world and use them for reasoning. Students should understand the concept of representation and understand that computers construct representations using data, and these representations can be manipulated by applying reasoning algorithms that derive new information from what is already known. 3. Learning: Computers can learn from data. Students should understand that machine learning is a kind of statistical inference that finds patterns in data. 4. Natural Interaction: Making agents interact naturally with humans is a substantial challenge for AI developers. Students should understand that while computers can understand natural language to a limited extent, at present they lack the general reasoning and conversational capabilities of even a child. 5. Societal Impact: AI applications can impact society in both positive and negative ways. Students should be able to identify ways that AI is contributing to their lives as well as that the ethical construction of AI systems requires attention to the issues of transparency and fairness. Thus, while AI is \"the science and engineering of making intelligent machines that\n\n[1] Generative AI in Computing Education: Perspectives of Students and Instructors GAI tools can facilitate critical thinking A couple of instructors (2/6) were excited about the potential to use these models in their courses to help students further develop their critical thinking and analysis skills. Participant I6 said,“Let them actually use ChatGPT to come back with a report as a first draft. But then the job of the student is not simply to return the report, but instead, look at the report ChatGPT wrote, and then decide or criticize whether it’s right or wrong, and then, if it’s wrong, then what is wrong with it, or try to improve…So in that sense, it is more useful for the student to develop their critical thinking…instead of just like the old way of just writing a report.”Additionally, participant I5 said,“Just the production of something is not going to be as important, perhaps leaning more into analysis, perhaps leaning more into your ability to troubleshoot. Versus creation which is one of which is one of the steps that you know of many steps in any sort of understanding or building a system or an algorithm or a data structure or anything else. …It’s not, give me code, it’s, here is some code, give me some sort of analysis that matches or fits into something that is not easily template-tized, something that is not easily described or describable.”Instructors need to update assessment methods. In terms of how to update curriculum to adapt courses to account for the existence of these models, instructors did not have a very clear idea of what exactly needed to be done, with many expressing uncertainty about the best way to assess student progress, prevent plagiarism, and ensure that students do not rely too heavily on the models, hindering their education. In their interviews, instructors did mention a few considerations they had for updating their assessment methods:•Mitigate plagiarism by developing engaging assignments and reducing busy work (I3)•Change course grade weights such that easily plagiarized assignments can’t carry students’ grades (I4, I5)•Continue to give proctored exams since they can’t be plagiarized like other out out-of-class assignments (I4)\n\n[2] I'm Going to Learn What?!?: Teaching Artificial Intelligence to Freshmen in an Introductory Computer Science Course I'm Going to Learn What?!?: Teaching Artificial Intelligence to Freshmen in an Introductory Computer Science Course As artificial intelligence (AI) becomes more widely utilized, there is a need for non-computer scientists to understand 1) how the technology works, and 2) how it can impact their lives. Currently, however, computer science educators have been reluctant to teach AI to non-majors out of concern that the topic is too advanced. To fill this gap, we propose an AI and machine learning (ML) curriculum that is specifically designed for first-year students. In this paper, we describe our curriculum and show how it covers four key content areas: core concepts, implementation details, limitations, and ethical considerations. We then share our experiences teaching our new curriculum to 174 randomly-selected Freshman students. Our results show that non-computer scientists can comprehend AI/ML concepts without being overwhelmed by the subject material. Specifically, we show that students can design, code, and deploy their own intelligent agents to solve problems, and that they understand the importance and value of learning about AI in a general-education course.\n\n[3] Generative AI in Computing Education: Perspectives of Students and Instructors GAI tools shift the focus to higher levels of abstraction. Students expressed (6/12) that GAI tools could help reduce the amount of time spent working at lower levels of abstraction. Students often gave examples of how GAI tools could shift the learner’s focus toward higher-level skills (e.g., design, the analysis and evaluation of code, or advanced software engineering concepts) which students are more likely to encounter in later computing courses.For example, participant S9 described how students might focus on design patterns and software design rather than implementation details. They said,“You don’t usually get design patterns until you are in graduate school, right? Unless you’d specifically seek it out…we could use ChatGPT to help teach design patterns where…You can use ChatGPT as a tool to implement all the little pieces. But we are going to try to break down this larger problem to figure out what pattern applies to it, and then use whatever you need to use to generate the individual pieces of code. But we really care about the overall structure and making sure that the students kind of have this broader understanding of the piece of software rather than a single algorithm or a single piece of code or a smaller scale program.”\n\n[4] Generative AI in Computing Education: Perspectives of Students and Instructors Generative models are now capable of producing natural language text that is, in some cases, comparable in quality to the text produced by people. In the computing education context, these models are being used to generate code, code explanations, and programming exercises. The rapid adoption of these models has prompted multiple position papers and workshops which discuss the implications of these models for computing education, both positive and negative. This paper presents results from a series of semi-structured interviews with 12 students and 6 instructors about their awareness, experiences, and preferences regarding the use of tools powered by generative AI in computing classrooms. The results suggest that Generative AI (GAI) tools will play an increasingly significant role in computing education. However, students and instructors also raised numerous concerns about how these models should be integrated to best support the needs and learning goals of students. We also identified interesting tensions and alignments that emerged between how instructors and students prefer to engage with these models. We discuss these results and provide recommendations related to curriculum development, assessment methods, and pedagogical practice. As GAI tools become increasingly prevalent, it's important to understand educational stakeholders' preferences and values to ensure that these tools can be used for good and that potential harms can be mitigated.\n\n[5] Generative AI in Computing Education: Perspectives of Students and Instructors GAI tools should be incorporated into classrooms. Unanimously, instructors (6/6) emphasized that they believe GAI tools should not be banned, and they should embrace their existence and include them in the classroom.Instructors believed it was imperative to understand how these tools work. Some of the ways they envisioned incorporating GAI tools into their courses included:•Let students use it but they should be able to explain why the output is correct or incorrect (I1, I3, I6)•Clearly articulate what students need to know how to do without using GAI tools (I4)Participant I4, detailed how, with any new tools which shift how courses are taught, clear expectations for students need to be established,“… before calculators were a thing we used to expect students to memorize multiplication and division tables. And you know, after the calculator came into being, of course, you know it’s reasonable not to expect people to memorize that when you know they’ve got a tool that they can use to figure out how to multiply things, you know. And I could see the same thing happening with us with programming now that tools like ChatGPT exist, you know, maybe there are certain things that we could expect students to use a tool for, and we don’t have to test them on it. But at the same time, you know, there has to be some core level of knowledge that we should expect from a student, you know, without the use of tools.”\n\n[6] Introducing Variational Autoencoders to High School Students Introducing Variational Autoencoders to High School Students Generative Artificial Intelligence (AI) models are a compelling way to introduce K-12 students to AI education using an artistic medium, and hence have drawn attention from K-12 AI educators. Previous Creative AI curricula mainly focus on Generative Adversarial Networks (GANs) while paying less attention to Autoregressive Models, Variational Autoencoders (VAEs), or other generative models, which have since become common in the field of generative AI. VAEs' latent-space structure and interpolation ability could effectively ground the interdisciplinary learning of AI, creative arts, and philosophy. Thus, we designed a lesson to teach high school students about VAEs. We developed a web-based game and used Plato's cave, a philosophical metaphor, to introduce how VAEs work. We used a Google Colab notebook for students to re-train VAEs with their hand-written digits to consolidate their understandings. Finally, we guided the exploration of creative VAE tools such as SketchRNN and MusicVAE to draw the connection between what they learned and real-world applications. This paper describes the lesson design and shares insights from the pilot studies with 22 students. We found that our approach was effective in teaching students about a novel AI concept. Introduction Generative Artificial Intelligence (AI) models have been integrated into the K-12 curricula, but the focus is mainly on Generative Adversarial Networks (GANs) (Goodfellow et al. 2014) with little attention on Autoregressive Models, Variational Autoencoders (VAEs) (Kingma and Welling 2013) or other generative models. The continuity of VAE's latent space enables interpolation, which provides a creative way to explore artistic ideas. Hence,\n\n[7] A Perspective on K-12 AI Education AI algorithms, mixing and matching strengths of different models to design an optimal model such as through ensemble learning. Finally, the students can expand beyond the AI itself and look towards integrating their model into a broader solution. Take a student who has developed an algorithm to detect the sound of chainsaws in a protected forest. What are the next steps? How and where would the AI be deployed? Who gets alerted? How can this solution synergize with existing systems? These are all open-ended questions that have many valid approaches. In general, the capacity for originality is proportional to the multiplicity of potential design decisions. AI projects present students many unique avenues for solving a problem and thus makes it easier to do something original. Regarding the second question, recall that versatility was established as one of the key characteristics of AI. Hence, one can readily find relevant and task appropriate AI applications in math, biology, chemistry, and physics. Thus, making time for AI projects in a STEM course is not an inappropriate excursion from class time, but an effective enrichment activity that reinforces class topics with real-world applications. In addition, as mentioned in the preceding section, software tailored to K-12 students can drastically lower the learning curve for building AI models by eliminating the need to spend weeks on teaching programming syntax up front. It is also possible for AI projects to be relevant in English and social studies classes. Students can \"plug and play\" with pre-built AI models, analyzing the outputs to explore the\n\n[8] AI + Ethics Curricula for Middle School Youth: Lessons Learned from Three Project-Based Curricula about AI do students demonstrate after engaging in our lessons, interactive activities, and tools? What student-driven projects did our curricula enable? We conclude with recommendations for future AI curricula that seek to broaden access to K-12 AI education. K-12 AI Literacy As applications of artificial intelligence become more common in the lives of children, it has become increasingly important to educate students about how AI technologies work and how they impact society. It is projected that by 2025, about half of all work tasks will be completed by automated systems (Leopold et al., 2018;U.S. Bureau of Labor Statistics, 2019). Students must be equipped with the skills to build and work with AI to meet the needs of a shifting workforce. This means not only teaching students about what AI is, but also enabling them to create with it, think critically about its impacts, and advocate for responsible use (Ali et al., 2019;Zimmerman, 2018). In recent years, researchers, educators, and technologists have come together to define what it means for K-12 students to be AI literate, drawing on inspiration from AI education at the collegiate level as well as K-12 computer science education (Kandlhofer et al., 2019;Lao, 2020;Long & Magerko, 2020;Touretzky et al., 2019b;Zimmerman, 2018;De La Higuera, 2019). The AI4K12 initiative, a collaborative effort between the Computer Science Teachers Association (CSTA) and Association for the Advancement of Artificial Intelligence (AAAI), was formalized in 2018 and defined the \"Five Big Ideas of AI\" that K-12 students must know. These are Perception, Representation and Reasoning, Learning, Natural Interaction, and\n\n[9] AI + Ethics Curricula for Middle School Youth: Lessons Learned from Three Project-Based Curricula as Machine Learning for Artists, 5 Computer Visions 6 ; Artists and Machine Intelligence, 7 and Creative Machine Learning for Design. 8 However, college courses are not accessible to the general public because they require a baseline knowledge of programming and costly computing resources. One of our curricula, Creative AI, tackles the topic of generative ML because of its relevance to students yet absence in other K-12 resources. (Fiesler et al., 2020). That is beginning to change as researchers such as those associated with MIT's Schwarzman College of Computing have created case studies for engineering students to explore ethics. At the K-12 level, ethics at the center of AI4K12's big ideas in AI. The centrality of ethics is critical as AI systems have historically been biased against marginalized groups such as women, people of African, Asian, and Latin descent, Indigenous peoples, and low-income individuals and communities (Buolamwini & Gebru, 2018;Eubanks, 2018;Noble, 2018;O'Neil, 2016). Skirpan et al. (2018) found that when students learn ethics throughout a computer science course, they think more holistically about the implications of the technology that they are building than if ethics is taught at the end of a course or in a different course all together. At the K-12 level, few papers discuss ethics with students and those that do focus on the legal and social implications of deployed or hypothetical systems (Lassnig, 2018;Opel et al., 2019). The Middle School AI + Ethics Curriculum is an exception to this rule; it adapts the approach of collegiate courses that embed ethics in technical\n\n[10] AI + Ethics Curricula for Middle School Youth: Lessons Learned from Three Project-Based Curricula AI. Students have a practical understanding of how AI algorithms (pertaining to each curriculum) function and humans' role in creating AI systems. • Ability to Think Critically about the Implications of AI: Students can think critically about the potential benefits or harms of AI systems and their impact on stakeholders. Students reflect on and discuss ethical issues (e.g., fairness, privacy, and responsible design) as they relate to AI. • Ability to Apply AI Knowledge: Students will be able to appropriately employ their AI knowledge and skills to topics they personally care about. Students can leverage skills like ethical thinking, creativity, empathy, and idea synthesis as they create AI artifacts. Creative AI Despite the widespread use and misuse of generative ML in the media, there are few existing efforts that educate school-age children on how AI can generate media and the implications of this technology. The goal of this curriculum is to teach middleschool children about creative ML techniques and how people can partner with AI to create digital art and expressive media. Students explored tools and techniques such as neural networks and generative adversarial networks (GANs) across various forms of media including text, images, and music. We developed web-based tools that allow students to input their own text and drawings to generate new media, for instance to create stories and illustrations using GANs. Throughout the workshop, we discussed important ethical issues surrounding generative AI art that allowed students to reflect on how machine-created art differs from human-created art. Finally, students learned about deepfakes, or fake media\n\n"}
